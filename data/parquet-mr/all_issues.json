{
    "PARQUET-1": {
        "Key": "PARQUET-1",
        "Summary": "need http://parquet.incubator.apache.org to be maintained",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris Aniszczyk",
        "Reporter": "bijaya",
        "Created": "14/Jun/14 03:54",
        "Updated": "22/Jul/14 18:38",
        "Resolved": "15/Jul/14 19:41",
        "Description": "http://parquet.incubator.apache.org is not available yet.",
        "Issue Links": []
    },
    "PARQUET-2": {
        "Key": "PARQUET-2",
        "Summary": "Adding Type Persuasion for Primitive Types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "19/Jun/14 21:04",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "24/Jun/14 17:20",
        "Description": "Pull request: https://github.com/apache/incubator-parquet-mr/pull/3\nOriginal from the old repo: Parquet/parquet-mr#410\nThese changes allow primitive types to be requested as different types than what is stored in the file format using a flag to turn off strict type checking (default is on). Types are cast to the requested type where possible and will suffer precision loss for casting where necessary (e.g. requesting a double as an int).\nNo performance penalty is imposed for using the type defined in the file type. A flag exists to\nA 6x6 test case is provided to test conversion between the primitive types.",
        "Issue Links": []
    },
    "PARQUET-3": {
        "Key": "PARQUET-3",
        "Summary": "tool to merge pull requests based on Spark",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "19/Jun/14 22:34",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "21/Jun/14 03:59",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/5\ngiven a pull request id on github.com/apache/incubator-parquet-mr this script will merge it\nrequires 2 remotes: apache-github and apache to point to the corresponding repos.\ntested here (pretending my fork is the apache remote):\nhttps://github.com/julienledem/incubator-parquet-mr/commit/485658a5b7654198ab1fcc77e2b850ee12999491\noriginal tool:\nhttps://github.com/apache/spark/blob/master/dev/merge_spark_pr.py",
        "Issue Links": []
    },
    "PARQUET-4": {
        "Key": "PARQUET-4",
        "Summary": "Use LRU caching for footers in ParquetInputFormat.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Matt Martin",
        "Reporter": "Matt Martin",
        "Created": "20/Jun/14 03:45",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "18/Jul/14 23:02",
        "Description": "The caching approach needs to change because of issues that occur when the same ParquetInputFormat instance is reused to generate splits for different input directories. For example, it causes problems in Hive's FetchOperator when the FetchOperator is attempting to operate over more than one partition (sidenote: as far as I could tell, Hive has been reusing inputformat instances in this way for quite some time). The details of how this issue manifests itself with respect to Hive are described in more detail here: https://groups.google.com/d/msg/parquet-dev/0aXql-3z7vE/Gn5m094V7PMJ\nThe proposed patch can be found here: https://github.com/apache/incubator-parquet-mr/pull/2",
        "Issue Links": [
            "/jira/browse/PARQUET-16"
        ]
    },
    "PARQUET-5": {
        "Key": "PARQUET-5",
        "Summary": "Add option to reuse (Avro) objects when reading",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sandy Ryza",
        "Created": "20/Jun/14 17:20",
        "Updated": "15/Jul/14 21:03",
        "Resolved": null,
        "Description": "Instead of creating an object for each record, Avro DatumReaders support passing in an object to be reused.  It would be nice to expose this in Parquet.\nThis could be turned on with a setting in the job configuration.",
        "Issue Links": []
    },
    "PARQUET-6": {
        "Key": "PARQUET-6",
        "Summary": "Document new jira/pr workflow for contributors",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Dmitriy V. Ryaboy",
        "Created": "21/Jun/14 22:21",
        "Updated": "11/Jul/14 19:44",
        "Resolved": "11/Jul/14 19:44",
        "Description": "We should create clear documentation for contributors on how to create new issues, PRs, etc. Spark has a pretty good page that we can use as a reference, since they use a similar workflow: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark\nWe should also update the README.md with a link to this more detailed document, and cut most of what we currently have in the contributing section.",
        "Issue Links": []
    },
    "PARQUET-7": {
        "Key": "PARQUET-7",
        "Summary": "[parquet-thrift] improve performance of thrift push-down code",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy V. Ryaboy",
        "Created": "21/Jun/14 22:24",
        "Updated": "24/Sep/18 12:13",
        "Resolved": null,
        "Description": "A user reported seeing slowness when projection push-down code is active, which seems to stem from ProtocolEventsAmender.\nDetails can be found in https://github.com/Parquet/parquet-mr/issues/406",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/7"
        ]
    },
    "PARQUET-8": {
        "Key": "PARQUET-8",
        "Summary": "[parquet-scrooge] mvn eclipse:eclipse fails on parquet-scrooge",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Dmitriy V. Ryaboy",
        "Reporter": "Dmitriy V. Ryaboy",
        "Created": "21/Jun/14 22:39",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "28/Aug/14 20:52",
        "Description": "`mvn eclipse:eclipse` on the parquet-mr project fails when it hits the scrooge sub-project. Scrooge being in scala, this is probably not very surprising but it means the target doesn't get to all the Hive stuff and Tools. We should at least skip it, if this isn't easy to fix.",
        "Issue Links": []
    },
    "PARQUET-9": {
        "Key": "PARQUET-9",
        "Summary": "InternalParquetRecordReader will not read multiple blocks when filtering",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas White",
        "Reporter": "Ryan Blue",
        "Created": "27/Jun/14 00:57",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "16/Jul/14 14:00",
        "Description": "The InternalParquetRecordReader keeps track of the count of records it has processed and uses that count to know when it is finished and when to load a new row group of data. But when it is wrapping a FilteredRecordReader, this count is not updated for records that are filtered, so when the reader exhausts the block it is reading, it will continue calling read() on the filtered reader and will pass null values to the caller.\nThe quick fix is to detect null values returned by the record reader and update the count to read the next row group. But the longer-term solution is to correctly account for the filtered records.\nThe pull request for the quick fix is #9.",
        "Issue Links": []
    },
    "PARQUET-10": {
        "Key": "PARQUET-10",
        "Summary": "Decommission google group",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Jason Altekruse",
        "Created": "30/Jun/14 15:31",
        "Updated": "30/Jul/14 00:46",
        "Resolved": "30/Jul/14 00:46",
        "Description": "Some members of the community are still sending mail to the old google groups list, but some of the discussions seem to have been neglected a bit, such as this one here\nhttps://groups.google.com/forum/#!topic/parquet-dev/eMoVUXxY044\nI think an auto-reply should be added to the old list to tell people to post to the apache list instead, and for members that post on the web forum interface, a message locked to the head of the list should tell people about the Apache JIRA.",
        "Issue Links": []
    },
    "PARQUET-11": {
        "Key": "PARQUET-11",
        "Summary": "Reduce memory pressure when reading footers",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy V. Ryaboy",
        "Created": "03/Jul/14 06:37",
        "Updated": "03/Jul/14 06:55",
        "Resolved": null,
        "Description": "I encountered OOMs reading metadata for a dataset with 500+ columns, many of them quite sparse.\nWe can reduce memory utilization significantly.",
        "Issue Links": []
    },
    "PARQUET-12": {
        "Key": "PARQUET-12",
        "Summary": "Add support for additional converted types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jacques Nadeau",
        "Created": "08/Jul/14 01:49",
        "Updated": "07/Aug/14 20:51",
        "Resolved": "07/Aug/14 20:51",
        "Description": "Add support for additional logical types.  Based on discussions here: https://docs.google.com/document/d/1y8UKDsdgT6d05xXXz1UjeDJhOYWAGZiZyDn4uHtXXXw/edit and https://github.com/Parquet/parquet-format/pull/94/files#r11984805",
        "Issue Links": [
            "/jira/browse/PARQUET-64"
        ]
    },
    "PARQUET-13": {
        "Key": "PARQUET-13",
        "Summary": "The `-d` option for `parquet-schema` shouldn't have optional argument",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "08/Jul/14 02:13",
        "Updated": "01/Aug/14 23:41",
        "Resolved": "01/Aug/14 23:41",
        "Description": "According to ShowSchemaCommand.java, the -d | --detailed option doesn't require an optional argument. The hasOptionalArg() call should be removed.",
        "Issue Links": []
    },
    "PARQUET-14": {
        "Key": "PARQUET-14",
        "Summary": "Pig and Hive cannot read repeated groups written with parquet-protobuf",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Nathan Howell",
        "Created": "08/Jul/14 23:36",
        "Updated": "20/Jul/20 10:43",
        "Resolved": null,
        "Description": "parquet-hive and parquet-pig make assumptions about list schemas that are not compatible with the more compact schemas generated by parquet-protobuf. This bug was discussed in more detail on https://github.com/Parquet/parquet-mr/issues/354",
        "Issue Links": []
    },
    "PARQUET-15": {
        "Key": "PARQUET-15",
        "Summary": "Add support for custom metadata",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Wesley Graham Peck",
        "Created": "09/Jul/14 18:15",
        "Updated": "09/Jul/14 18:15",
        "Resolved": null,
        "Description": "The parquet file format already supports extensible key/value storage in the column metadata and file metadata sections of the file. It would be nice if there was a facility in place for computing custom values to be placed in these key/value stores.\nSee: https://github.com/Parquet/parquet-mr/pull/185",
        "Issue Links": []
    },
    "PARQUET-16": {
        "Key": "PARQUET-16",
        "Summary": "Unnecessary getFileStatus() calls on all part-files in ParquetInputFormat.getSplits",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "10/Jul/14 20:11",
        "Updated": "17/Jul/14 08:15",
        "Resolved": null,
        "Description": "When testing Spark SQL Parquet support, we found that accessing large Parquet files located in S3 can be very slow. To be more specific, we have a S3 Parquet file with over 3,000 part-files, calling ParquetInputFormat.getSplits on it takes several minutes. (We were accessing this file from our office network rather than AWS.)\nAfter some investigation, we found that ParquetInputFormat.getSplits is trying to call getFileStatus() on all part-files one by one sequentially (here). And in the case of S3, each getFileStatus() call issues an HTTP request and wait for the reply in a blocking manner, which is considerably expensive.\nActually all these FileStatus objects have already been fetched when footers are retrieved (here). Caching these FileStatus objects can greatly improve our S3 case (reduced from over 5 minutes to about 1.4 minutes).\nWill submit a PR for this issue soon.",
        "Issue Links": [
            "/jira/browse/SPARK-2551",
            "/jira/browse/SPARK-2119",
            "/jira/browse/PARQUET-4"
        ]
    },
    "PARQUET-17": {
        "Key": "PARQUET-17",
        "Summary": "Parquet OutputFormat should allow controlling the file size",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nong Li",
        "Created": "10/Jul/14 22:50",
        "Updated": "10/Jul/14 22:50",
        "Resolved": null,
        "Description": "To generate the most efficient on disk file, the size of the file is important to control. It would be nice if we could configure the ouputformat to roll over new files when it reaches the right size and start a new file.\nThere's currently no easy way to tune this and requires indirect tuning (number of reduces, map input size).",
        "Issue Links": []
    },
    "PARQUET-18": {
        "Key": "PARQUET-18",
        "Summary": "Cannot read dictionary-encoded pages with all null values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/Jul/14 23:56",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "18/Jul/14 23:19",
        "Description": "This is issue #283. Parquet-mr will try to read the bit-width byte in DictionaryValuesReader#initPage even if the incoming offset is at the end of the byte array because there are no values.\nHere's the stack trace:\n\nCaused by: parquet.io.ParquetDecodingException: could not read page Page [id: 1, bytes.size=7, valueCount=100, uncompressedSize=7] in col [id] INT32\n\tat parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:532)\n\tat parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:493)\n\tat parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:546)\n\tat parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:339)\n\tat parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:63)\n\tat parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:58)\n\tat parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:265)\n\tat parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:60)\n\tat parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:74)\n\tat parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:112)\n\tat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:174)\n\t... 29 more\nCaused by: java.io.EOFException\n\tat parquet.bytes.BytesUtils.readIntLittleEndianOnOneByte(BytesUtils.java:76)\n\tat parquet.column.values.dictionary.DictionaryValuesReader.initFromPage(DictionaryValuesReader.java:55)\n\tat parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:530)\n\t... 39 more",
        "Issue Links": []
    },
    "PARQUET-19": {
        "Key": "PARQUET-19",
        "Summary": "NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Matt Martin",
        "Reporter": "Matt Martin",
        "Created": "12/Jul/14 00:33",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Jul/14 17:16",
        "Description": "Make sure the valueObj instance variable is always initialized.  This change is neeeded when running a Hive query that uses the CombineHiveInputFormat and the first file in the combined split is empty.  This can lead to a NullPointerException because the valueObj is null when the CombineHiveInputFormat calls the createValue method.",
        "Issue Links": [
            "/jira/browse/HIVE-7459"
        ]
    },
    "PARQUET-20": {
        "Key": "PARQUET-20",
        "Summary": "Better exception when files are unaccessible",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "14/Jul/14 20:01",
        "Updated": "01/Jun/18 13:47",
        "Resolved": null,
        "Description": "In some cases the Hadoop filesystem API will throw NullPointerException when trying to access files that have moved.\nWe'd want to catch those and give a better error message.\n\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1043)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:211)\n\tat parquet.hadoop.ParquetInputFormat.listStatus(ParquetInputFormat.java:395)\n\tat parquet.hadoop.ParquetInputFormat.getFooters(ParquetInputFormat.java:443)\n\tat parquet.hadoop.ParquetInputFormat.getGlobalMetaData(ParquetInputFormat.java:467)",
        "Issue Links": []
    },
    "PARQUET-21": {
        "Key": "PARQUET-21",
        "Summary": "Fix reference to 'github-apache' in dev docs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas White",
        "Reporter": "Thomas White",
        "Created": "16/Jul/14 14:18",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "18/Jul/14 23:04",
        "Description": "See https://issues.apache.org/jira/browse/PARQUET-13?focusedCommentId=14054493&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14054493",
        "Issue Links": []
    },
    "PARQUET-22": {
        "Key": "PARQUET-22",
        "Summary": "Parquet #13: Backport of HIVE-6938",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "19/Jul/14 00:11",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "19/Jul/14 01:36",
        "Description": "This patch was included in hive after the moving the Serde to hive (included in hive 0.14+). Backport is required for use with previous versions.\nhttps://github.com/apache/incubator-parquet-mr/pull/13",
        "Issue Links": []
    },
    "PARQUET-23": {
        "Key": "PARQUET-23",
        "Summary": "Change package names to org.apache.parquet.*",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "David Chen",
        "Created": "21/Jul/14 01:36",
        "Updated": "28/Apr/15 21:40",
        "Resolved": "28/Apr/15 21:40",
        "Description": "Parquet's package names are still parquet.*. Since Parquet is now in the Apache Incubator, the namespaces should be updated to be org.apache.parquet.*.",
        "Issue Links": []
    },
    "PARQUET-24": {
        "Key": "PARQUET-24",
        "Summary": "Enforce PARQUET jira prefix for PR names in merge script",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "21/Jul/14 23:17",
        "Updated": "11/Sep/14 18:10",
        "Resolved": "10/Sep/14 23:14",
        "Description": "merge_parquet_pr.py should:\n\nenforce that the pull request description starts with \"Parquet-X: \"\nautomatically close the corresponding JIRA (right now it does except it ask for the JIRA ID)\nask for JIRA creds (right now they have to be in env)\n\nhttps://github.com/apache/incubator-parquet-mr/blob/master/dev/merge_parquet_pr.py",
        "Issue Links": []
    },
    "PARQUET-25": {
        "Key": "PARQUET-25",
        "Summary": "Pushdown predicates only work with hardcoded arguments",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Sandy Ryza",
        "Reporter": "Sandy Ryza",
        "Created": "22/Jul/14 06:33",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "23/Jul/14 13:33",
        "Description": "As far as I can tell there is no way to pass a dynamic argument to use in filtering.\nUnboundRecordFilters should be initialized with a Configuration object that they can pull arguments form.",
        "Issue Links": []
    },
    "PARQUET-26": {
        "Key": "PARQUET-26",
        "Summary": "Parquet doesn't recognize the nested Array type in MAP as ArrayWritable.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Mala Chikka Kempanna",
        "Created": "23/Jul/14 16:58",
        "Updated": "10/Mar/15 22:26",
        "Resolved": "10/Mar/15 22:26",
        "Description": "When trying to insert hive data of type of MAP<string, array<int>> into Parquet, it throws the following error \nCaused by: parquet.io.ParquetEncodingException: This should be an ArrayWritable or MapWritable: org.apache.hadoop.hive.ql.io.parquet.writable.BinaryWritable@c644ef1c \nat org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(DataWritableWriter.java:86) \nProblem is reproducible with following steps:\nRelevant test data is attached.\n1. \nCREATE TABLE test_hive (\nnode string,\nstime string,\nstimeutc string,\nswver string,\nmoid MAP <string,string>,\npdfs MAP <string,array<int>>,\nutcdate string,\nmotype string)\nROW FORMAT DELIMITED\n    FIELDS TERMINATED BY '|'\n    COLLECTION ITEMS TERMINATED BY ','\n    MAP KEYS TERMINATED BY '=';\n2.\nLOAD DATA LOCAL INPATH '/root/38388/test.dat' INTO TABLE test_hive; \n3.\nCREATE TABLE test_parquet(\npdfs MAP <string,array<int>>\n)\nSTORED AS PARQUET ;\n4.\nINSERT INTO TABLE test_parquet SELECT pdfs FROM test_hive;",
        "Issue Links": [
            "/jira/browse/HIVE-8909",
            "/jira/browse/HIVE-8949"
        ]
    },
    "PARQUET-27": {
        "Key": "PARQUET-27",
        "Summary": "Parquet-hadoop's client side logs are buffered / do not print until the job completes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 05:45",
        "Updated": "24/Jul/14 05:45",
        "Resolved": null,
        "Description": "Log statements from the InputFormat or anywhere else in the hadoop client / submitter seem to get buffered until the MR job completes, instead of printing as the job progresses.",
        "Issue Links": []
    },
    "PARQUET-28": {
        "Key": "PARQUET-28",
        "Summary": "Parquet often OOMs when reading footers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 05:50",
        "Updated": "31/Mar/21 15:18",
        "Resolved": "31/Mar/21 15:18",
        "Description": "This is one PR to help with mem usage but may not be enough to solve the overall problem:\nhttps://github.com/apache/incubator-parquet-format/pull/2",
        "Issue Links": []
    },
    "PARQUET-29": {
        "Key": "PARQUET-29",
        "Summary": "Investigate automatic not null checks via annotations in place of checkNotNull calls",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 05:54",
        "Updated": "24/Jul/14 05:54",
        "Resolved": null,
        "Description": "We've discussed that it would be neat if we could replace a lot of the checkNotNull() calls in parquet-mr with an annotation like\n\n@NotNull\n\n\nor even make not null the default and annotate things that can be null with\n\n@Nullable\n\n\nand have this enforced by the compiler / annotation preprocessor.",
        "Issue Links": []
    },
    "PARQUET-30": {
        "Key": "PARQUET-30",
        "Summary": "There is overlap between ValidTypeMap and PrimitiveTypeName in parquet-mr",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 05:55",
        "Updated": "24/Jul/14 05:55",
        "Resolved": null,
        "Description": "It might be a good idea to unify these two classes",
        "Issue Links": []
    },
    "PARQUET-31": {
        "Key": "PARQUET-31",
        "Summary": "Choose a strategy for semantic versioning of the java artifacts",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 05:58",
        "Updated": "24/Jul/14 05:58",
        "Resolved": null,
        "Description": "We need to decide which java packages are \"internal\" and we are free to refactor without going up a major revision, and which are part of our public API and require strict semantic versioning. We need to then reflect this in the maven enforcer that detects these changes.",
        "Issue Links": []
    },
    "PARQUET-32": {
        "Key": "PARQUET-32",
        "Summary": "Refactor the Statistics classes to match the specialized pattern used throughout parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:04",
        "Updated": "24/Jul/14 06:04",
        "Resolved": null,
        "Description": "Because Parquet tries very hard to avoid autoboxing, most of the core classes are specialized for each primitive by having a method for each type, eg:\n\nvoid writeInt(int x);\nvoid writeLong(long x);\nvoid writeDouble(double x);\n\n\nand so on.\nHowever, the statistics classes take the other approach of having an InstStatistics class, a LongStatistics class, a DoubleStatistics class and so on. I think it's worth going for consistency and picking a pattern and sticking to it. Seems like the first pattern I mentioned is currently the more common one.\nWe may want to take this one step further and define an interface that these all conform to, eg:\n\npublic interface ParquetTypeVisitor {\n  void visitInt(int x);\n  void visitLong(long x);\n  void visitDouble(double x);\n}",
        "Issue Links": []
    },
    "PARQUET-33": {
        "Key": "PARQUET-33",
        "Summary": "Benchmark the assembly of thrift objects, and possibly create a more efficient ReplayingTProtocol",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:11",
        "Updated": "29/Aug/14 22:04",
        "Resolved": null,
        "Description": "The current implementation of parquet thrift creates an instance of TProtocol for each value of each record and builds a stack of these events, which are then replayed back to the TBase.\nI'd be curious to benchmark this, and if it's slow, try building a \"ReplayingTProtocol\" that instead of having a stack of TProtocol instances, contains a primitive array of each type. As events are fed into this replaying TProtocol, it would just add these primitives to its buffers, and then the TBase would drain them. This would effectively let us stream the values into the TBase without making an object allocation for each value.\nThe buffers could be set to a certain size, and if they fill up (which they sholdn't in most cases), the TBase could begin draining the protocol until it is empty again, at which point the TProtocol can block the TBase from draining further while the parque record assembly feeds it more events.\nThis is all moot if it turns out not to be bottleneck though",
        "Issue Links": []
    },
    "PARQUET-34": {
        "Key": "PARQUET-34",
        "Summary": "Add support for repeated columns in the filter2 API",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:13",
        "Updated": "18/Nov/16 17:17",
        "Resolved": null,
        "Description": "They currently are not supported. They would need their own set of operators, like contains() and size() etc.",
        "Issue Links": []
    },
    "PARQUET-35": {
        "Key": "PARQUET-35",
        "Summary": "Consider adding support for more column types in the filter2 API",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:15",
        "Updated": "24/Jul/14 06:15",
        "Resolved": null,
        "Description": "such as String columns as convenience over Binary columns, Maps, Sets, etc.\nMaybe we don't want to do this and just support the primitive column types though.",
        "Issue Links": []
    },
    "PARQUET-36": {
        "Key": "PARQUET-36",
        "Summary": "FilteringPrimitiveConverter should support dictionaries",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:17",
        "Updated": "31/May/15 13:28",
        "Resolved": null,
        "Description": "If the delegated PrimitiveConverter supports dictionaries, then FilteringPrimitiveConverter should too.",
        "Issue Links": []
    },
    "PARQUET-37": {
        "Key": "PARQUET-37",
        "Summary": "Record filtering in the filter2 API could possibly short circuit",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:19",
        "Updated": "24/Jul/14 06:19",
        "Resolved": null,
        "Description": "Record level filtering in the filter2 api still requires visiting every value of the record. We may be able to short circuit as soon as the filter predicate reaches a known state.\nAnother approach would be to figure out how to get essentially random access to the values referenced by the predicate and check them first. This could be tricky because it would require re-structuring the assembly algorithm.",
        "Issue Links": []
    },
    "PARQUET-38": {
        "Key": "PARQUET-38",
        "Summary": "Many classes in parquet-hadoop belong in parquet-column",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:21",
        "Updated": "24/Jul/14 06:21",
        "Resolved": null,
        "Description": "There are a handful of classes, like BlockMetaData and some of the other *MetaData classes that are currentlyin parquet hadoop but aren't hadoop specific. Which force some other classes (like the row group filter) to also live in parquet-hadoop when they really belong in parquet-column",
        "Issue Links": []
    },
    "PARQUET-39": {
        "Key": "PARQUET-39",
        "Summary": "Simplify ParquetReader's constructors",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:23",
        "Updated": "28/May/15 18:41",
        "Resolved": "28/May/15 18:41",
        "Description": "ParquetReader has a lot of constructors. Maybe we should use the Builder pattern instead.",
        "Issue Links": []
    },
    "PARQUET-40": {
        "Key": "PARQUET-40",
        "Summary": "Take advantage of dictionary pages when performing rowgroup filters in the filter2 API",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:26",
        "Updated": "24/Jul/14 06:32",
        "Resolved": null,
        "Description": "We currently only filter row groups via the min / max value in the row group.\nWe should additionally inspect the dictionary of unique values in a row group (if it has one) \u2013 this could dramatically increase our ability to drop entire rowgroups.",
        "Issue Links": []
    },
    "PARQUET-41": {
        "Key": "PARQUET-41",
        "Summary": "Add bloom filters to parquet statistics",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0,                                            1.12.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:28",
        "Updated": "18/Apr/21 04:11",
        "Resolved": "26/Feb/20 16:02",
        "Description": "For row groups with no dictionary, we could still produce a bloom filter. This could be very useful in filtering entire row groups.\nPull request:\nhttps://github.com/apache/parquet-mr/pull/215",
        "Issue Links": [
            "/jira/browse/PARQUET-1332",
            "/jira/browse/HIVE-9260",
            "/jira/browse/HIVE-24831",
            "/jira/browse/SPARK-34562",
            "/jira/browse/PARQUET-1979",
            "/jira/browse/DRILL-7895",
            "/jira/browse/PARQUET-1805",
            "https://docs.google.com/spreadsheets/d/1LQqGZ1EQSkPBXtdi9nyANiQOhwNFwqiiFe8Sazclf5Y/edit?usp=sharing",
            "https://github.com/apache/parquet-format/pull/62",
            "https://github.com/apache/parquet-format/pull/99",
            "https://github.com/apache/parquet-format/pull/112",
            "https://github.com/apache/parquet-format/pull/113",
            "https://github.com/apache/parquet-mr/pull/425",
            "https://github.com/apache/parquet-cpp/pull/431",
            "https://github.com/apache/parquet-cpp/pull/432",
            "https://github.com/apache/parquet-mr/pull/484",
            "https://github.com/apache/parquet-mr/pull/757"
        ]
    },
    "PARQUET-42": {
        "Key": "PARQUET-42",
        "Summary": "Add HyperLogLog / CountMinSketch to parquet statistics",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:30",
        "Updated": "25/Sep/20 17:45",
        "Resolved": null,
        "Description": "HLL and CMS for rowgroups could help with query planning (getting a sense of data skew) and with cheaply counting approximate distinct values. Both are commutative which means they can be combined across rowgroups (unlike an exact distinct count for example).",
        "Issue Links": []
    },
    "PARQUET-43": {
        "Key": "PARQUET-43",
        "Summary": "Support filtering on group-is-null in filter2 API",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:40",
        "Updated": "24/Jul/14 06:40",
        "Resolved": null,
        "Description": "Currently filters only apply to primitive (leaf node) columns. But given the column \"a.b.c.d.e.f\" it seems reasonable to want to filter on \"a.b.c\" != null or something like that.",
        "Issue Links": []
    },
    "PARQUET-44": {
        "Key": "PARQUET-44",
        "Summary": "Consider adding checks that validate eq(null) / notEq(null) is not used on a required column in the filter2 API",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Jul/14 06:44",
        "Updated": "24/Jul/14 06:44",
        "Resolved": null,
        "Description": "Is this too strict?",
        "Issue Links": []
    },
    "PARQUET-45": {
        "Key": "PARQUET-45",
        "Summary": "Increment a hadoop counter for bytes filtered / skipped",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "25/Jul/14 02:27",
        "Updated": "25/Jul/14 02:27",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-46": {
        "Key": "PARQUET-46",
        "Summary": "Consider making a specialized / non-generic UserDefinedPredicate",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "25/Jul/14 02:28",
        "Updated": "25/Jul/14 02:28",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-47": {
        "Key": "PARQUET-47",
        "Summary": "SERDE backed schema for parquet storage in Hive",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Abhishek Agarwal",
        "Created": "27/Jul/14 20:30",
        "Updated": "30/Apr/15 00:06",
        "Resolved": null,
        "Description": "As of now, for a hive table stored as parquet, the schema can only be specified in Hive MetaStore. For our use-case, it is desired that the schema be provided by Thrift SerDe rather than MetaStore. Using thrift IDL as a schema provider, allows us to maintain a consistent schema across executions engines other than Hive such as Pig and Native MR. \nAdditionally, for a large sparse schema, it is much easier to build thrift objects, and use parquet-thrift/elephant-bird to convert them into columns/tuples rather than constructing the whole big tuple itself.",
        "Issue Links": []
    },
    "PARQUET-48": {
        "Key": "PARQUET-48",
        "Summary": "Parquet scrooge for scala 2.10",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Ian O Connell",
        "Created": "29/Jul/14 01:06",
        "Updated": "08/Sep/14 21:14",
        "Resolved": "08/Sep/14 21:14",
        "Description": "Bit of a bummer its compiling with 2.9.2 rather than 2.10.4",
        "Issue Links": []
    },
    "PARQUET-49": {
        "Key": "PARQUET-49",
        "Summary": "Create a new filter API that supports filtering groups of records based on their statistics",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "29/Jul/14 21:42",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Jul/14 21:43",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/4",
        "Issue Links": []
    },
    "PARQUET-50": {
        "Key": "PARQUET-50",
        "Summary": "Remove items from semver blacklist",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "29/Jul/14 23:27",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "20/Aug/14 20:47",
        "Description": "parquet-hadoop currently has the semver checks disabled, and a few classes are blacklisted.\nWe need to 1) publish an artifact (maybe 1.6.0rc1) and set that as the \"previous version\" as far as the semver enforcer is concerned, and then re-enable the enforcer / clear its blacklist.",
        "Issue Links": []
    },
    "PARQUET-51": {
        "Key": "PARQUET-51",
        "Summary": "Add a check in the merge PR script to verify that the build on travis-ci passes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "30/Jul/14 00:29",
        "Updated": "30/Jul/14 00:29",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-52": {
        "Key": "PARQUET-52",
        "Summary": "Improve the encoding fall back mechanism for Parquet 2.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "30/Jul/14 00:30",
        "Updated": "03/Jan/23 14:24",
        "Resolved": "25/Nov/14 18:49",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/74\n-> moved to https://github.com/apache/parquet-mr/pull/74",
        "Issue Links": []
    },
    "PARQUET-53": {
        "Key": "PARQUET-53",
        "Summary": "Error in reading a list type data in hive which was generated through pig",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Abhishek Agarwal",
        "Created": "30/Jul/14 10:07",
        "Updated": "22/Nov/14 01:31",
        "Resolved": null,
        "Description": "ArrayWritableGroupConverter in Parquet-Hive binding expects that any array field contains either 1 or 2 fields. However, the schema which PigSchemaConverter generates, doesn't honour this condition. It can be reproduced through following code \n\nMessageType messageType = new PigSchemaConverter().convert(ThriftToPig.toSchema(Requestlist.class));\nDataWritableGroupConverter converter = new DataWritableGroupConverter(messageType, messageType);\n\n\nRequestlist is a thrift class \n\nstruct Request {\n    1: required string id,\n    2: i64 time,\n    3: required string source\n}\n\nstruct Requestlist {\n    1: optional list<Request> requests\n}",
        "Issue Links": []
    },
    "PARQUET-54": {
        "Key": "PARQUET-54",
        "Summary": "Parquet Hive should resolve column names in case insensitive manner",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Brock Noland",
        "Created": "30/Jul/14 15:34",
        "Updated": "30/Jul/14 15:34",
        "Resolved": null,
        "Description": "Backport HIVE-7554",
        "Issue Links": [
            "/jira/browse/HIVE-7554"
        ]
    },
    "PARQUET-55": {
        "Key": "PARQUET-55",
        "Summary": "Remove author tags from source",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jonathan Coveney",
        "Created": "30/Jul/14 16:21",
        "Updated": "30/Jul/14 16:21",
        "Resolved": null,
        "Description": "I'm pretty sure most apache projects eschew the use of author tags. We have git blame, and I feel they are useless clutter.\nAnyone else agree? If so I can get my sed on...",
        "Issue Links": []
    },
    "PARQUET-56": {
        "Key": "PARQUET-56",
        "Summary": "Added an accessor for the Long column type in example Group",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "James Scott",
        "Reporter": "James Scott",
        "Created": "30/Jul/14 19:43",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Jul/14 21:27",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/25\nI noticed there was a missing accessor for the Long column type in the example Group.",
        "Issue Links": []
    },
    "PARQUET-57": {
        "Key": "PARQUET-57",
        "Summary": "Make dev commit script easier to use",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Brock Noland",
        "Reporter": "Brock Noland",
        "Created": "30/Jul/14 21:30",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "31/Jul/14 21:59",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-58": {
        "Key": "PARQUET-58",
        "Summary": "Add PR merge tool to incubator-parquet-format",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Jul/14 22:29",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "05/Aug/14 18:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-59": {
        "Key": "PARQUET-59",
        "Summary": "Scrooge tests use hadoop-1 API, fail on hadoop-2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "01/Aug/14 23:27",
        "Updated": "05/Aug/14 02:05",
        "Resolved": "05/Aug/14 02:05",
        "Description": "The ParquetScroogeSchemeTest fails on hadoop-2 because it directly instantiates JobContext rather than using ContextUtil.",
        "Issue Links": []
    },
    "PARQUET-60": {
        "Key": "PARQUET-60",
        "Summary": "Add findbugs support, esp for @Nullable / @NonNull annotation",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jonathan Coveney",
        "Created": "01/Aug/14 23:36",
        "Updated": "01/Aug/14 23:36",
        "Resolved": null,
        "Description": "It'd be really great to have our CI run findbugs. This should document how to fail the build if there are any warnings or errors, but not turn it on yet (though it should still run with CI so we can reference it).\nIt would be great to have the build enforce using Preconditions and null checks, among other benefits of static analysis",
        "Issue Links": []
    },
    "PARQUET-61": {
        "Key": "PARQUET-61",
        "Summary": "Avoid fixing protocol events when there is not required field missing",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "02/Aug/14 00:11",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "03/Sep/14 22:39",
        "Description": "Currently, due to the projection pushdown, in the ThriftRecordConverter, it will check if there is any protocol events missing for the required fields and fix them if there is. This happens for all the records.\nThe fixing of missing protocol can be done in a more precise condition to improve performance:\nOnly when the requested schema missing some required fields that are present in the full schema\nSo even if there a projection, as long as the projection is not getting rid of the required field, the protocol events amender will not be called.\nhttps://github.com/apache/incubator-parquet-mr/pull/28",
        "Issue Links": []
    },
    "PARQUET-62": {
        "Key": "PARQUET-62",
        "Summary": "DictionaryValuesWriter dictionaries are corrupted by user changes.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "02/Aug/14 03:20",
        "Updated": "07/Jul/15 15:44",
        "Resolved": "20/Aug/14 21:02",
        "Description": "DictionaryValuesWriter passes incoming Binary objects directly to Object2IntMap to accumulate dictionary values. If the arrays backing the Binary objects passed in are reused by the caller, then the values are corrupted but still written without an error.\nBecause Hadoop reuses objects passed to mappers and reducers, this can happen easily. For example, Avro reuses the byte arrays backing Utf8 objects, which parquet-avro passes wrapped in a Binary object to writeBytes.\nThe fix is to make defensive copies of the values passed to the Dictionary writer code. I think this only affects the Binary dictionary classes because Strings, floats, longs, etc. are immutable.",
        "Issue Links": [
            "/jira/browse/PARQUET-326"
        ]
    },
    "PARQUET-63": {
        "Key": "PARQUET-63",
        "Summary": "Fixed-length columns cannot be dictionary encoded.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "02/Aug/14 04:01",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "04/Sep/14 18:29",
        "Description": "This was an oversight and the patch was reviewed on the Parquet github account, #407. It was just not committed.",
        "Issue Links": []
    },
    "PARQUET-64": {
        "Key": "PARQUET-64",
        "Summary": "Add new logical types to parquet-column",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "02/Aug/14 19:35",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "01/Oct/14 20:45",
        "Description": "Now that PR #3 / PARQUET-12 is committed, we need to add the new ConvertedTypes to parquet-mr as OriginalTypes. This will require a parquet-format release, 2.2.0.",
        "Issue Links": [
            "/jira/browse/PARQUET-12"
        ]
    },
    "PARQUET-65": {
        "Key": "PARQUET-65",
        "Summary": "Create a jackson integration module for pojo support",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "06/Aug/14 01:37",
        "Updated": "12/Apr/19 07:46",
        "Resolved": null,
        "Description": "There's currently a PR for pojo support:\nhttps://github.com/apache/incubator-parquet-mr/pull/21\nAnd it occurred to me that one way we could do this without re-inventing the wheel is to use jackson. Jackson can essentially take a parse tree, either the result of parsing XML, or json, or anything (for example there's a yaml plugin), and then, there are 3 things jackson lets you do with that tree. You can either visit the nodes in the tree (they call this streaming), you can map the tree onto the datastructures built into java (essentially get a Map<Object, Object>, or, you can map the tree onto a user defined class. The latter lets you work with a well typed class, and also lets you use jackson's annotations for controlling how the tree -> pojo mapping works (renaming fields and so on).\nWe could leverage all of that by creating something that goes from parquet data to the jackson parse tree, and then leave the rest of the work to jackson.",
        "Issue Links": []
    },
    "PARQUET-66": {
        "Key": "PARQUET-66",
        "Summary": "InternalParquetRecordWriter int overflow causes unnecessary memory check warning",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Eric Snyder",
        "Created": "07/Aug/14 13:13",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "20/Aug/14 21:10",
        "Description": "flushStore() contains a check on the amount of memory allocated and if it is > 3 * blockSize it logs a warning. If blockSize is large enough, e.g. 1GiB then 3 * blockSize will overflow and be negative causing the warning to always be logged.\n\nif (store.allocatedSize() > 3 * blockSize) {\n    LOG.warn(\"Too much memory used: \" + store.memUsageString());\n}",
        "Issue Links": []
    },
    "PARQUET-67": {
        "Key": "PARQUET-67",
        "Summary": "mechanism to add extra metadata in the footer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "07/Aug/14 23:33",
        "Updated": "04/Jan/17 17:23",
        "Resolved": null,
        "Description": "this expands on the idea proposed by @wesleypeck in https://github.com/Parquet/parquet-mr/pull/185",
        "Issue Links": []
    },
    "PARQUET-68": {
        "Key": "PARQUET-68",
        "Summary": "Incompatible behavior for ColumnChunk.file_offset between Parquet-mr and Impala",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Eunsoo Roh",
        "Created": "08/Aug/14 01:11",
        "Updated": "08/Aug/14 01:11",
        "Resolved": null,
        "Description": "According to comments in parquet.thrift, this field is supposed to store offset of ColumnMetaData within the file column chunk is stored. My understanding is that this allows omitting ColumnMetaData within ColumnChunk (it is optional field, after all). Unfortunately, two major implementations, Parquet-mr and Impala, deviate from this definition when writing Parquet files. Impala implementation writes offset pointing to the ColumnChunk rather than ColumnMetaData, as can be found in hdfs-parquet-table-reader.cc. While this is still incorrect behavior according to the comments in parquet.thrift, this still allows access to the ColumnMetaData necessary for reading data.\nParquet-mr implementation can be found in ParquetMetadataConverter, which writes the offset to the first data page. Not only this is incompatible behavior, but also it makes no sense because you cannot read the data with just data page offset. There is even a comment on that line saying \"verify this is the right offset.\"",
        "Issue Links": []
    },
    "PARQUET-69": {
        "Key": "PARQUET-69",
        "Summary": "Add committer doc and REVIEWERS files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "08/Aug/14 20:26",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "29/Aug/14 17:10",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/23",
        "Issue Links": []
    },
    "PARQUET-70": {
        "Key": "PARQUET-70",
        "Summary": "PARQUET #36: Pig Schema Storage to UDFContext",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Daniel Weeks",
        "Created": "11/Aug/14 21:45",
        "Updated": "14/Jul/15 15:27",
        "Resolved": "20/Aug/14 20:53",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/36\nThe ParquetLoader was not storing the pig schema into the udfcontext for the full load case which causes a schema reload on the task side, erases the requested schema, and causes problems with column index access.\nThis fix stores the pig schema to both the udfcontext (for task side init) and jobcontext (for TupleReadSupport) along with other properties that should be set in the loader context (required field list and column index access toggle).",
        "Issue Links": []
    },
    "PARQUET-71": {
        "Key": "PARQUET-71",
        "Summary": "column chunk page write store log message displays incorrect information",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ian Barfield",
        "Created": "13/Aug/14 11:20",
        "Updated": "29/Aug/14 23:48",
        "Resolved": null,
        "Description": "It is printing the size of the dictionary (in terms of the number of keys) twice and calling the second time the 'compressed byte count'. An accurate account of that number would be very helpful for accounting for disk space usage. The actual 'compressed byte count' is indeed calculated at a point near there so I am guessing this is a simple mistake.\nsee:\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-hadoop/src/main/java/parquet/hadoop/ColumnChunkPageWriteStore.java#L152",
        "Issue Links": []
    },
    "PARQUET-72": {
        "Key": "PARQUET-72",
        "Summary": "Prepare parquet-format for Apache release",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "13/Aug/14 19:24",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "03/Sep/14 20:42",
        "Description": "Need to prepare the parquet source for release. We're planning on two releases:\n\n2.2.0 as com.twitter:parquet-format\n2.3.0 as org.apache.parquet:parquet-format\n\n2.3.0 will be identical to 2.2.0 other than changing the parquet packages to org.apache.parquet and updating the coordinate.\nFor both releases, we need to go through the Incubator checklist and follow steps for publishing maven artifacts.",
        "Issue Links": []
    },
    "PARQUET-73": {
        "Key": "PARQUET-73",
        "Summary": "Add support for FilterPredicates to cascading schemes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "13/Aug/14 23:29",
        "Updated": "16/Sep/14 20:42",
        "Resolved": "16/Sep/14 20:42",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-74": {
        "Key": "PARQUET-74",
        "Summary": "Use thread local decoder cache in Binary toStringUsingUTF8()",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zhenxiao Luo",
        "Reporter": "Zhenxiao Luo",
        "Created": "20/Aug/14 19:33",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "28/Aug/14 21:20",
        "Description": "In Binary toStringUsingUTF8(), new String(...) is slower because it instantiates a new Decoder, while Charset#decode uses a thread-local decoder cache, it is much faster.\nByteArraySliceBackedBinary is using Charset#decode, while, ByteArrayBackedBinary and ByteBufferBackedBinary are still using new String()",
        "Issue Links": [
            "/jira/browse/PARQUET-75"
        ]
    },
    "PARQUET-75": {
        "Key": "PARQUET-75",
        "Summary": "String decode using 'new String' is slow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "20/Aug/14 19:55",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "28/Aug/14 18:31",
        "Description": "There are three implementations of the Binary class and only one is using the faster 'UTF8.decode' in the 'toStringUsingUTF8' method.  This fixes them to all use the faster UTF8.decode.   \nAs noted in the comments, the 'new String' approach creates a new decoder each time, which is slower than the cached instance used by 'UTF8.decode'.\n#1. ByteArraySliceBackedBinary   <-- UTF8.decode\n#2. ByteArrayBackedBinary  <-- new String\n#3. ByteBufferBackedBinary <-- new String\nhttps://github.com/apache/incubator-parquet-mr/pull/40",
        "Issue Links": [
            "/jira/browse/PARQUET-74"
        ]
    },
    "PARQUET-76": {
        "Key": "PARQUET-76",
        "Summary": "Hive cannot determine the list of columns automatically based on Parquet serde",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ashish Singh",
        "Reporter": "Pratik Khadloya",
        "Created": "20/Aug/14 21:35",
        "Updated": "06/Jan/16 17:40",
        "Resolved": null,
        "Description": "Today we are not able to create a parquet based hive table without having to specify the column names and types. When we try to define it the following way, we get the error \n\"14/08/20 17:27:46 ERROR ql.Driver: FAILED: SemanticException [Error 10043]: Either list of columns or a custom serializer should be specified\"\n\nCREATE  TABLE parquet_test\nROW FORMAT SERDE\n  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\nOUTPUTFORMAT\n  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION\n  '/user/pratik/campaigns';\n\n\nWhereas if we create a hive table on top of AVRO based files, we do not need to specify the column names, hive automatically figures out the schema through the SerDe.\n\nCREATE EXTERNAL TABLE campaigns\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION '/user/pratik/campaigns'\nTBLPROPERTIES ('avro.schema.url'='hdfs:///user/pratik/campaigns.avsc');",
        "Issue Links": []
    },
    "PARQUET-77": {
        "Key": "PARQUET-77",
        "Summary": "Improvements in ByteBuffer read path",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Jason Altekruse",
        "Reporter": "Parth Chandra",
        "Created": "22/Aug/14 23:06",
        "Updated": "05/Jun/18 22:17",
        "Resolved": "04/Nov/15 17:58",
        "Description": "For Apache Drill, we are looking to pass in a buffer that we have already allocated (in this case from Direct memory), wrapped in a ByteBuffer. \nThe current effort to allow a ByteBuffer read path is great except that the interface allocates the memory and there is no way for an application to pass in memory that has been allocated or (even better) to provide an allocator.\nAdditionally, it would be great to be able to use the same approach while decompressing. \nAs a starting point here is a patch on top of the ByteBuffer read effort that adds a function in CompatibilityUtils and also adds a ByteBuffer path for the Snappy Decompressor. The latter requires Hadoop 2.3 though, so some discussion on this would be called for.\nPlease let me have any feedback and I can make changes/additions.",
        "Issue Links": [
            "/jira/browse/PARQUET-251",
            "/jira/browse/DRILL-1410",
            "/jira/browse/PARQUET-1006",
            "https://github.com/apache/parquet-mr/pull/49"
        ]
    },
    "PARQUET-78": {
        "Key": "PARQUET-78",
        "Summary": "Provide a ByteBuffer based write path",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Parth Chandra",
        "Created": "22/Aug/14 23:18",
        "Updated": "18/Sep/14 21:36",
        "Resolved": null,
        "Description": "Similar to the effort to use a ByteBuffer in the read path, we should provide a path to use a ByteBuffer in the write path.\nThe proposed idea is to provide an allocator in the writer that is passed down to CapacityByteArrayOutputStream and is used to allocate a ByteBuffer instead of the byte array that it currently uses. \nI've attached a patch based on the 1.5 release that does that. There are two implementations of the allocator - one for Heap memory and one for Direct memory. In addition, the allocator interface allows for a 'release' method so that reference counted memory allocators like Netty's ByteBuf can be used to provide an allocator.",
        "Issue Links": [
            "/jira/browse/DRILL-1410"
        ]
    },
    "PARQUET-79": {
        "Key": "PARQUET-79",
        "Summary": "Add thrift streaming API to read metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "25/Aug/14 17:39",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Aug/14 00:35",
        "Description": "patch available: https://github.com/apache/incubator-parquet-format/pull/8",
        "Issue Links": []
    },
    "PARQUET-80": {
        "Key": "PARQUET-80",
        "Summary": "upgrade semver plugin version to 0.9.27",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "28/Aug/14 16:35",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "28/Aug/14 17:50",
        "Description": "to include the fix in:\nhttps://github.com/jeluard/semantic-versioning/pull/39",
        "Issue Links": []
    },
    "PARQUET-81": {
        "Key": "PARQUET-81",
        "Summary": "fix typo in parquet-format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy V. Ryaboy",
        "Created": "28/Aug/14 23:58",
        "Updated": "28/Aug/14 23:58",
        "Resolved": "28/Aug/14 23:58",
        "Description": "PR from Bruno Konishita:\nhttps://github.com/apache/incubator-parquet-format/pull/9",
        "Issue Links": []
    },
    "PARQUET-82": {
        "Key": "PARQUET-82",
        "Summary": "ColumnChunkPageWriteStore assumes pages are smaller than Integer.MAX_VALUE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "29/Aug/14 23:30",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "23/Sep/14 19:14",
        "Description": "The ColumnChunkPageWriteStore casts both the compressed size and uncompressed size of a page from a long to an int. If the uncompressed size of a page exceeds Integer.MAX_VALUE, the write doesn't fail, although it creates bad metadata:\n\nchunk1: BINARY GZIP DO:0 FPO:4 SZ:267184096/-2143335445/-8.02 VC:41 ENC:BIT_PACKED,PLAIN\n\n\nAt read time, the BytesInput will try to allocate a byte array for the uncompressed data and fails:\n\nCaused by: parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://nameservice1/OUTPUT/part-m-00000.gz.parquet \nat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:177) \nat parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:130) \nat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:95) \nat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:66) \nat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:51) \nat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65) \n... 16 more \nCaused by: java.lang.NegativeArraySizeException \nat parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:183) \nat parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:521) \nat parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:493) \nat parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:544) \nat parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:339) \nat parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:63) \nat parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:58) \nat parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:265) \nat parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:59) \nat parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:73) \nat parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:110) \nat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:172) \n... 21 more",
        "Issue Links": []
    },
    "PARQUET-83": {
        "Key": "PARQUET-83",
        "Summary": "Hive Query failed if the data type is array<string> with parquet files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Sathish",
        "Created": "22/Aug/14 09:23",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "24/Nov/14 22:24",
        "Description": "Created a parquet file from the Avro file which have 1 array data type and rest are primitive types. Avro Schema of the array data type. Eg:\n\n{ \"name\" : \"action\", \"type\" : [ { \"type\" : \"array\", \"items\" : \"string\" }, \"null\" ] }\n\n\nCreated External Hive table with the Array type as below,\n\ncreate external table paraArray (action Array) partitioned by (partitionid int) row format serde 'parquet.hive.serde.ParquetHiveSerDe' stored as inputformat 'parquet.hive.MapredParquetInputFormat' outputformat 'parquet.hive.MapredParquetOutputFormat' location '/testPara'; \nalter table paraArray add partition(partitionid=1) location '/testPara';\n\n\nRun the following query(select action from paraArray limit 10) and the Map reduce jobs are failing with the following exception.\n\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ClassCastException: parquet.hive.writable.BinaryWritable$DicBinaryWritable cannot be cast to org.apache.hadoop.io.ArrayWritable\nat parquet.hive.serde.ParquetHiveArrayInspector.getList(ParquetHiveArrayInspector.java:125)\nat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:315)\nat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)\nat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)\nat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)\nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)\nat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:405)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:336)\nat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1126)\nat org.apache.hadoop.mapred.Child.main(Child.java:264)\n]\nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:671)\nat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n... 8 more\n\n\n\nThis issue has long back posted on Parquet issues list and Since this is related to Parquet Hive serde, I have created the Hive issue here, The details and history of this information are as shown in the link here https://github.com/Parquet/parquet-mr/issues/281.",
        "Issue Links": [
            "/jira/browse/HIVE-8909"
        ]
    },
    "PARQUET-84": {
        "Key": "PARQUET-84",
        "Summary": "Add an option to read the rowgroup metadata on the task side.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "03/Sep/14 00:28",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Oct/14 23:28",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/45\nparquet.task.side.metadata=true",
        "Issue Links": [
            "/jira/browse/PARQUET-101"
        ]
    },
    "PARQUET-85": {
        "Key": "PARQUET-85",
        "Summary": "fix license headers in parquet-format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "03/Sep/14 01:08",
        "Updated": "03/Sep/14 16:01",
        "Resolved": "03/Sep/14 16:01",
        "Description": "https://github.com/apache/incubator-parquet-format/pull/10",
        "Issue Links": []
    },
    "PARQUET-86": {
        "Key": "PARQUET-86",
        "Summary": "parquet-hive (and therefore Hive) depends on ParquetInputSplit constructor",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "04/Sep/14 21:19",
        "Updated": "05/Sep/14 00:12",
        "Resolved": null,
        "Description": "The issue is not really parquet-hive which we can modify in sync but rather Hive itself. As we want to be able to change the split implementation without breaking Hive. (Users might want to use the latest Parquet with their version of Hive)\nParquetRecordReaderWrapper in parquet-hive\nand in Hive",
        "Issue Links": []
    },
    "PARQUET-87": {
        "Key": "PARQUET-87",
        "Summary": "Better and unified API for projection pushdown on cascading scheme",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "10/Sep/14 05:05",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "10/Sep/14 17:38",
        "Description": "Make the projection pushdown API avaliable in the cascading scheme level, so user can specify it for each Tap.\nAlso use config object for initializing a schema for projection and predicate pushdown instead of creating multiple constructors\nhttps://github.com/apache/incubator-parquet-mr/pull/51",
        "Issue Links": []
    },
    "PARQUET-88": {
        "Key": "PARQUET-88",
        "Summary": "Fix pre-version enforcement.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "10/Sep/14 16:26",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "10/Sep/14 19:45",
        "Description": "So that we can publish a 1.6.0rc2\nThe bug was fixed in the semver plugin: https://github.com/jeluard/semantic-versioning/pull/40\nAnd will be integrated in pull request #53\nhttps://github.com/apache/incubator-parquet-mr/pull/53",
        "Issue Links": []
    },
    "PARQUET-89": {
        "Key": "PARQUET-89",
        "Summary": "All Parquet CI tests should be run against hadoop-2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/Sep/14 18:37",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "22/Sep/14 22:07",
        "Description": "As discussed in the hangout.",
        "Issue Links": []
    },
    "PARQUET-90": {
        "Key": "PARQUET-90",
        "Summary": "Keep field ids in Parquet Schema",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "15/Sep/14 18:27",
        "Updated": "19/Jan/16 20:49",
        "Resolved": null,
        "Description": "https://github.com/apache/parquet-mr/pull/56\nThis pull request adds the field ids from Thrift and Protos in the Parquet schema. The goal is to simplify implementation of backward compatibility features when using those.",
        "Issue Links": []
    },
    "PARQUET-91": {
        "Key": "PARQUET-91",
        "Summary": "stream through files when writing the _metadata file to reduce memory usage",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "15/Sep/14 18:49",
        "Updated": "15/Sep/14 18:49",
        "Resolved": null,
        "Description": "Currently we load all the footers of the part files in memory in the committer to write the _metadata file.\nWe could open the _metadata file first and then add the metadata for each part file one by one. That would reduce the required memory.",
        "Issue Links": []
    },
    "PARQUET-92": {
        "Key": "PARQUET-92",
        "Summary": "Parallel Footer Read Control",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "15/Sep/14 20:46",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "22/Sep/14 18:21",
        "Description": "Just adding a property to control the footer read parallelism.  With S3 we can read a lot of objects in parallel and it will help the startup time.\nPull request: https://github.com/apache/incubator-parquet-mr/pull/57",
        "Issue Links": []
    },
    "PARQUET-93": {
        "Key": "PARQUET-93",
        "Summary": "Add support for protobuf to parquet-cascading",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Matt Martin",
        "Reporter": "Matt Martin",
        "Created": "17/Sep/14 23:51",
        "Updated": "17/Sep/14 23:51",
        "Resolved": null,
        "Description": "The parquet-cascading module currently includes code that lets Cascading write and read Thrift-based data to Parquet files.  Protobufs is another popular alternative to Thrift which can be explicitly supported by leveraging the existing code in parquet-protobuf.",
        "Issue Links": []
    },
    "PARQUET-94": {
        "Key": "PARQUET-94",
        "Summary": "ParquetScroogeScheme constructor ignores klass argument",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "19/Sep/14 22:01",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "22/Sep/14 18:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-95": {
        "Key": "PARQUET-95",
        "Summary": "Parquet-mr project does not cross-publish scala 2.9 or 2.11 artifacts",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "19/Sep/14 22:04",
        "Updated": "19/Sep/14 22:04",
        "Resolved": null,
        "Description": "The parquet-scrooge artifact is only published for scala 2.10.\nIn order to use parquet-scrooge from scalding, we will need a 2.9 artifact, and in the near future, a 2.11 artifact as well",
        "Issue Links": []
    },
    "PARQUET-96": {
        "Key": "PARQUET-96",
        "Summary": "parquet.example.data.Group is missing some methods",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Colin Marc",
        "Reporter": "Colin Marc",
        "Created": "22/Sep/14 18:34",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "25/Sep/14 23:46",
        "Description": "Namely:\n\n`add(int, Group)`\n`getDouble`\n`getFloat`\n\nThere is a pull request to add them here: https://github.com/apache/incubator-parquet-mr/pull/59",
        "Issue Links": []
    },
    "PARQUET-97": {
        "Key": "PARQUET-97",
        "Summary": "ProtoParquetReader builder factory method not static",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Viktor Szathm\u00e1ry",
        "Reporter": "Viktor Szathm\u00e1ry",
        "Created": "22/Sep/14 22:11",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "10/Mar/15 13:06",
        "Description": "ProtoParquetReader#builder should be static.",
        "Issue Links": []
    },
    "PARQUET-98": {
        "Key": "PARQUET-98",
        "Summary": "filter2 API performance regression",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Viktor Szathm\u00e1ry",
        "Created": "22/Sep/14 22:24",
        "Updated": "22/Jul/15 02:14",
        "Resolved": null,
        "Description": "The new filter API seems to be much slower (or perhaps I'm using it wrong :)\nCode using an UnboundRecordFilter:\n\nColumnRecordFilter.column(column,\n    ColumnPredicates.applyFunctionToBinary(\n    input -> Binary.fromString(value).equals(input)));\n\n\nvs. code using FilterPredicate:\n\neq(binaryColumn(column), Binary.fromString(value));\n\n\nThe latter performs twice as slow on the same Parquet file (built using 1.6.0rc2).\nNote: the reader is constructed using\n\nParquetReader.builder(new ProtoReadSupport().withFilter(filter).build()\n\n\nThe new filter API based approach seems to create a whole lot more garbage (perhaps due to reconstructing all the rows?).",
        "Issue Links": [
            "/jira/browse/PARQUET-182"
        ]
    },
    "PARQUET-99": {
        "Key": "PARQUET-99",
        "Summary": "Large rows cause unnecessary OOM exceptions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Weeks",
        "Reporter": "Tongjie Chen",
        "Created": "23/Sep/14 00:09",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "08/Dec/15 22:43",
        "Description": "If columns contains lots of lengthy string value, it will run into OOM error during writing.\n2014-09-22 19:16:11,626 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2271)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:83)\n\tat org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:76)\n\tat parquet.bytes.CapacityByteArrayOutputStream.writeTo(CapacityByteArrayOutputStream.java:144)\n\tat parquet.bytes.BytesInput$CapacityBAOSBytesInput.writeAllTo(BytesInput.java:308)\n\tat parquet.bytes.BytesInput$SequenceBytesIn.writeAllTo(BytesInput.java:233)\n\tat parquet.hadoop.CodecFactory$BytesCompressor.compress(CodecFactory.java:108)\n\tat parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:110)\n\tat parquet.column.impl.ColumnWriterImpl.writePage(ColumnWriterImpl.java:147)\n\tat parquet.column.impl.ColumnWriterImpl.flush(ColumnWriterImpl.java:236)\n\tat parquet.column.impl.ColumnWriteStoreImpl.flush(ColumnWriteStoreImpl.java:113)\n\tat parquet.hadoop.InternalParquetRecordWriter.flushStore(InternalParquetRecordWriter.java:151)\n\tat parquet.hadoop.InternalParquetRecordWriter.checkBlockSizeReached(InternalParquetRecordWriter.java:130)\n\tat parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:122)\n\tat parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)\n\tat parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)\n\tat org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:77)\n\tat org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:90)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:688)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:132)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-409",
            "https://github.com/apache/parquet-mr/pull/250",
            "https://github.com/apache/parquet-mr/pull/297"
        ]
    },
    "PARQUET-100": {
        "Key": "PARQUET-100",
        "Summary": "provide an option in parquet-pig to avoid reading footers in client side",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tongjie Chen",
        "Created": "23/Sep/14 00:17",
        "Updated": "13/Jul/15 21:58",
        "Resolved": "13/Jul/15 21:57",
        "Description": "Parquet Pig reads footer in client side, to calculate splits and retrieve schema etc.\nIn HCatalog environment, if there are large number of files generated by Hive, Parquet-Pig will spend significant chunk of time processing those footers in client side (before job is submitted to cluster).",
        "Issue Links": []
    },
    "PARQUET-101": {
        "Key": "PARQUET-101",
        "Summary": "Exception when reading data with parquet.task.side.metadata=false",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "23/Sep/14 03:25",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "25/Sep/14 17:13",
        "Description": "Error: java.lang.IllegalStateException: All the offsets listed in the split should be found in the file. expected: [178996073, 202818477] found: REDACTED] out of: [4, 30470363, 51019598, 69907789, 106365281, 126337626, 145280217, 178996073] in range 0, 202818477 \nat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:175)",
        "Issue Links": [
            "/jira/browse/PARQUET-84"
        ]
    },
    "PARQUET-102": {
        "Key": "PARQUET-102",
        "Summary": "no projection in thrift union types.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "25/Sep/14 22:00",
        "Updated": "25/Sep/14 22:00",
        "Resolved": null,
        "Description": "We should not allow projecting a subset of the fields in a Thrift union as this will result in errors.\nIn a union there will be exactly one defined field.\nWhich means reading the data will fail if we project it out as Thrift expects exactly one (and not 0)",
        "Issue Links": []
    },
    "PARQUET-103": {
        "Key": "PARQUET-103",
        "Summary": "Support empty structs in Thrift",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "25/Sep/14 22:01",
        "Updated": "25/Sep/14 22:03",
        "Resolved": null,
        "Description": "It is tricky to support in parquet because struct fields are stored in the definition levels of the fields they contain.\nSo if there is no content, they can not be stored.\nHere is a proposed solution to support this in Parquet: https://github.com/apache/incubator-parquet-mr/pull/65\nBasically, I'm adding an artificial optional field in the struct that will be always null but will store whether the struct that contains it is defined or not.",
        "Issue Links": []
    },
    "PARQUET-104": {
        "Key": "PARQUET-104",
        "Summary": "Parquet writes empty Rowgroup at the end of the file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "25/Sep/14 23:44",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Sep/14 19:00",
        "Description": "At then end of a parquet file, it may writes an empty rowgroup. \nThis happens when: numberOfRecords mod sizeOfRowGroup = 0",
        "Issue Links": []
    },
    "PARQUET-105": {
        "Key": "PARQUET-105",
        "Summary": "Refactor and Document Parquet Tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "26/Sep/14 17:42",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "21/Oct/14 16:54",
        "Description": "make Parquet Tools runnable from a bundle, create documentation on the usage of it.\nSo the use can use parquet tools with:\nhadoop jar parquet-tools xxxx",
        "Issue Links": []
    },
    "PARQUET-106": {
        "Key": "PARQUET-106",
        "Summary": "Relax InputSplit Protections",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "26/Sep/14 19:18",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Oct/14 18:10",
        "Description": "The new implementation of InputSplit is more restrictive with package and override permissions.  We need to relax these so we can override the InputSplit to subclass and allow for more advanced control of the splits (e.g. include partition information along with the split).\nhttps://github.com/apache/incubator-parquet-mr/pull/67",
        "Issue Links": []
    },
    "PARQUET-107": {
        "Key": "PARQUET-107",
        "Summary": "Add option to disable summary metadata aggregation after MR jobs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "27/Sep/14 20:30",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "01/Oct/14 21:16",
        "Description": "The summary metadata flies produced at the end of MR jobs aren't required for the job to complete successfully or for reading the data, so this should be an optional step in the MR commit phase. I'd like to add a property parquet.enable.summary-metadata that turns this off if set to false. The default will be true / enabled to match the current behavior.",
        "Issue Links": []
    },
    "PARQUET-108": {
        "Key": "PARQUET-108",
        "Summary": "Parquet Memory Management in Java",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Dong Chen",
        "Reporter": "Brock Noland",
        "Created": "29/Sep/14 15:27",
        "Updated": "29/Sep/15 05:33",
        "Resolved": "29/Dec/14 15:19",
        "Description": "As discussed in HIVE-7685, Hive + Parquet often runs out memory when writing to many Hive partitions. This is quite problematic for our users.",
        "Issue Links": [
            "/jira/browse/HIVE-7685",
            "/jira/browse/PARQUET-164"
        ]
    },
    "PARQUET-109": {
        "Key": "PARQUET-109",
        "Summary": "Fix LICENSE and NOTICE files for parquet-format release",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "29/Sep/14 21:00",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "01/Oct/14 22:41",
        "Description": "The current NOTICE and LICENSE files need to implement the Apache policy before we can do another RC.",
        "Issue Links": []
    },
    "PARQUET-110": {
        "Key": "PARQUET-110",
        "Summary": "Some schemas without column projection cause Pig failures",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "30/Sep/14 21:25",
        "Updated": "14/Jan/15 17:14",
        "Resolved": "14/Jan/15 17:14",
        "Description": "Parquet stores and loads the Pig schema in the Configuration. Along the way, Pig changes that Schema:\n\n// This schema is converted from Parquet and written in Configuration\nString schemaStr = \"my_list: {array: (array_element: (num1: int,num2: int))}\";\n// Reparsed using org.apache.pig.impl.util.Utils\nSchema schema = Utils.getSchemaFromString(schemaStr);\n// But no longer matches the original structure\nschema.toString();\n// => {my_list: {array_element: (num1: int,num2: int)}}\n\n\nNote that the intermediate bag, named either \"bag\" or \"array\", is removed when Pig reparses the Schema. I can work around this to an extent in the Parquet code, but the Pig behavior gets more strange. If there are two of these, the second is preserved but renamed to \"bag_0\". Something funny is going on there.",
        "Issue Links": [
            "/jira/browse/PIG-4219"
        ]
    },
    "PARQUET-111": {
        "Key": "PARQUET-111",
        "Summary": "Tasks for parquet-mr 1.6.0 release",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Sep/14 22:55",
        "Updated": "31/Mar/15 17:29",
        "Resolved": "09/Mar/15 20:01",
        "Description": "In order to get release 1.6 of parquet-mr out, we have to do some updates.\nFor Apache policy:\n1. Update the header in all Parquet source files (see PARQUET-72 for examples)\n2. Add DISCLAIMER and KEYS files (copy from parquet-format)\n3. Add the Apache Rat maven plugin to check source file license headers and make sure it passes\n4. For the overall project, go through any external source that is bundled and update the root LICENSE and NOTICE files\n5. For each module that produces a binary artifact, go through the content pulled in by the maven-shade-plugin or the maven-assembly-plugin and update a LICENSE and NOTICE file for that content (see PARQUET-109 for an example).\nWe also need to update the build (see PR #11):\n1. Inherit from the ASF pom to pull in Apache release settings (will be done for org.apache groupId)\n2. Update metadata, like SCM, mailing lists, and JIRA links\n3. Go through modules and remove any LICENSE and NOTICE files that are included by shading or assembly (relevant content should be included in parquet-mr LICENSE and NOTICE)",
        "Issue Links": [
            "/jira/browse/PARQUET-227"
        ]
    },
    "PARQUET-112": {
        "Key": "PARQUET-112",
        "Summary": "RunLengthBitPackingHybridDecoder: Reading past RLE/BitPacking stream.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Kristoffer Sj\u00f6gren",
        "Created": "02/Oct/14 07:34",
        "Updated": "22/Sep/20 23:20",
        "Resolved": null,
        "Description": "I am using Avro and Crunch 0.11 to write data into Hadoop CDH 4.6 in parquet format. This works fine for a few gigabytes but blows up in the RunLengthBitPackingHybridDecoder when reading a few thousands gigabytes.\n\nparquet.io.ParquetDecodingException: Can not read value at 19453 in block 0 in file hdfs://nn-ix01.se-ix.delta.prod:8020/user/stoffe/parquet/dogfight/2014/09/29/part-m-00153.snappy.parquet\n\tat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:177)\n\tat parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:130)\n\tat org.apache.crunch.impl.mr.run.CrunchRecordReader.nextKeyValue(CrunchRecordReader.java:157)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:483)\n\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:76)\n\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:85)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:139)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:672)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: parquet.io.ParquetDecodingException: Can't read value in column [action] BINARY at value 697332 out of 872236, 96921 out of 96921 in currentPage. repetition level: 0, definition level: 1\n\tat parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:466)\n\tat parquet.column.impl.ColumnReaderImpl.getBinary(ColumnReaderImpl.java:414)\n\tat parquet.filter.ColumnPredicates$1.apply(ColumnPredicates.java:64)\n\tat parquet.filter.ColumnRecordFilter.isMatch(ColumnRecordFilter.java:69)\n\tat parquet.io.FilteredRecordReader.skipToMatch(FilteredRecordReader.java:71)\n\tat parquet.io.FilteredRecordReader.read(FilteredRecordReader.java:57)\n\tat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:173)\n\t... 13 more\nCaused by: java.lang.IllegalArgumentException: Reading past RLE/BitPacking stream.\n\tat parquet.Preconditions.checkArgument(Preconditions.java:47)\n\tat parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readNext(RunLengthBitPackingHybridDecoder.java:80)\n\tat parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readInt(RunLengthBitPackingHybridDecoder.java:62)\n\tat parquet.column.values.dictionary.DictionaryValuesReader.readBytes(DictionaryValuesReader.java:73)\n\tat parquet.column.impl.ColumnReaderImpl$2$7.read(ColumnReaderImpl.java:311)\n\tat parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:462)\n\t... 19 more",
        "Issue Links": []
    },
    "PARQUET-113": {
        "Key": "PARQUET-113",
        "Summary": "Clarify parquet-format specification for LIST and MAP structures.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "06/Oct/14 17:43",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "04/Mar/15 20:09",
        "Description": "There are incompatibilities in the way that some parquet object models translate nested structures annotated by LIST and MAP / MAP_KEY_VALUE. We need to define clearly what the structures should look like and how to interpret existing structures, including what must be supported to read current parquet-avro, parquet-thrift, etc. files.",
        "Issue Links": [
            "/jira/browse/PARQUET-212"
        ]
    },
    "PARQUET-114": {
        "Key": "PARQUET-114",
        "Summary": "Sample NanoTime class serializes and deserializes Timestamp incorrectly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Brock Noland",
        "Reporter": "Brock Noland",
        "Created": "07/Oct/14 16:30",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "20/Nov/14 17:21",
        "Description": "The class should write timestamp in little endian with time of day nanos first and julian date second.",
        "Issue Links": [
            "/jira/browse/HIVE-8380",
            "/jira/browse/PARQUET-137",
            "https://github.com/apache/incubator-parquet-mr/pull/71"
        ]
    },
    "PARQUET-115": {
        "Key": "PARQUET-115",
        "Summary": "Pass a filter object to user defined predicate in filter2 api",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yash Datta",
        "Created": "18/Oct/14 05:41",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "03/Nov/14 19:36",
        "Description": "Currently for creating a user defined predicate using the new filter api,  no value can be passed to create a dynamic filter at runtime. This reduces the usefulness of the user defined predicate, and  meaningful predicates cannot be created. We can add a generic Object value that is passed through the api, which can internally be used in the keep function of the user defined predicate for creating many different types of filters.\nFor example, in spark sql, we can pass in a list of filter values for a where IN clause query and filter the row values based on that list.",
        "Issue Links": [
            "/jira/browse/PARQUET-116",
            "/jira/browse/HIVE-8122"
        ]
    },
    "PARQUET-116": {
        "Key": "PARQUET-116",
        "Summary": "Pass a filter object to user defined predicate in filter2 api",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yash Datta",
        "Created": "18/Oct/14 05:41",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "10/Feb/15 01:52",
        "Description": "Currently for creating a user defined predicate using the new filter api,  no value can be passed to create a dynamic filter at runtime. This reduces the usefulness of the user defined predicate, and  meaningful predicates cannot be created. We can add a generic Object value that is passed through the api, which can internally be used in the keep function of the user defined predicate for creating many different types of filters.\nFor example, in spark sql, we can pass in a list of filter values for a where IN clause query and filter the row values based on that list.",
        "Issue Links": [
            "/jira/browse/HIVE-8122",
            "/jira/browse/PARQUET-115"
        ]
    },
    "PARQUET-117": {
        "Key": "PARQUET-117",
        "Summary": "implement the new page format for Parquet 2.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "20/Oct/14 20:24",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "04/Dec/14 21:16",
        "Description": "as defined in \nhttps://github.com/Parquet/parquet-format/pull/64\nhttps://github.com/Parquet/parquet-format/issues/44\nPR: https://github.com/apache/incubator-parquet-mr/pull/75",
        "Issue Links": []
    },
    "PARQUET-118": {
        "Key": "PARQUET-118",
        "Summary": "Provide option to use on-heap buffers for Snappy compression/decompression",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Patrick Wendell",
        "Created": "22/Oct/14 23:57",
        "Updated": "21/Sep/21 16:06",
        "Resolved": null,
        "Description": "The current code uses direct off-heap buffers for decompression. If many decompressors are instantiated across multiple threads, and/or the objects being decompressed are large, this can lead to a huge amount of off-heap allocation by the JVM. This can be exacerbated if overall, there is not heap contention, since no GC will be performed to reclaim the space used by these buffers.\nIt would be nice if there was a flag we cold use to simply allocate on-heap buffers here:\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-hadoop/src/main/java/parquet/hadoop/codec/SnappyDecompressor.java#L28\nWe ran into an issue today where these buffers totaled a very large amount of storage and caused our Java processes (running within containers) to be terminated by the kernel OOM-killer.",
        "Issue Links": [
            "/jira/browse/SPARK-4073"
        ]
    },
    "PARQUET-119": {
        "Key": "PARQUET-119",
        "Summary": "add data_encodings to ColumnMetaData to enable dictionary based predicate push down",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "24/Oct/14 08:47",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Oct/14 20:49",
        "Description": "To implement predicate push down based on dictionary we need to know if fall back happened.\nIf all data pages are dictionary encoded we can use the dictionary for predicate-push down.\nIf not we can not.",
        "Issue Links": []
    },
    "PARQUET-120": {
        "Key": "PARQUET-120",
        "Summary": "Add dev scripts to parquet-cpp",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Nong Li",
        "Reporter": "Nong Li",
        "Created": "28/Oct/14 19:17",
        "Updated": "28/Oct/14 20:07",
        "Resolved": "28/Oct/14 20:07",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-121": {
        "Key": "PARQUET-121",
        "Summary": "Allow Parquet to build with Java 8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas White",
        "Reporter": "Thomas White",
        "Created": "30/Oct/14 12:34",
        "Updated": "10/Dec/15 18:57",
        "Resolved": "03/Nov/14 14:13",
        "Description": "We should ensure that Parquet can be built and all tests pass when using Java 8. Note that this will not change the default target bytecode version (still at Java 6).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/77"
        ]
    },
    "PARQUET-122": {
        "Key": "PARQUET-122",
        "Summary": "make parquet.task.side.metadata=true by default",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "30/Oct/14 19:15",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "07/Nov/14 19:03",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/78",
        "Issue Links": []
    },
    "PARQUET-123": {
        "Key": "PARQUET-123",
        "Summary": "Add dictionary support to AvroIndexedRecordReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Matt Massie",
        "Reporter": "Matt Massie",
        "Created": "31/Oct/14 02:47",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "03/Nov/14 14:01",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-124": {
        "Key": "PARQUET-124",
        "Summary": "parquet.hadoop.ParquetOutputCommitter.commitJob() throws parquet.io.ParquetEncodingException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Chris Albright",
        "Created": "31/Oct/14 04:33",
        "Updated": "10/Nov/15 06:07",
        "Resolved": "30/Jan/15 01:29",
        "Description": "I'm running an example combining Avro, Spark and Parquet (https://github.com/massie/spark-parquet-example), and in the process of updating the library versions, am getting the warning below.\nThe version of Parquet-Hadoop in the original example is 1.0.0. I am using 1.6.0rc3\nThe ParquetFileWriter.mergeFooters(Path, List<Footer>) method is performing a check to ensure the footers are all for files in the output directory. The output directory is supplied by ParquetFileWriter.writeMetadataFile; in 1.0.0, the output path was converted to a fully qualified output path before the call to mergeFooters, but in 1.6.0rc[2,3] that conversion happens after the call to mergeFooters. Because of this, the check within merge footers is failing (the URI for the footers starts with file:, but not the URI for the root path does not)\nHere is the warning message and stacktrace.\nOct 30, 2014 9:11:31 PM WARNING: parquet.hadoop.ParquetOutputCommitter: could not write summary file for /tmp/1414728690018-0/output\nparquet.io.ParquetEncodingException: file:/tmp/1414728690018-0/output/part-r-00000.parquet invalid: all the files must be contained in the root /tmp/1414728690018-0/output\n\tat parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:422)\n\tat parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:398)\n\tat parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:50)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:936)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:832)\n\tat com.zenfractal.SparkParquetExample$.main(SparkParquetExample.scala:72)\n\tat com.zenfractal.SparkParquetExample.main(SparkParquetExample.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)",
        "Issue Links": []
    },
    "PARQUET-125": {
        "Key": "PARQUET-125",
        "Summary": "Enhance the way path checking is done in ParquetFileWriter.mergeFooters",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Chris Albright",
        "Created": "01/Nov/14 18:45",
        "Updated": "07/Apr/15 20:46",
        "Resolved": null,
        "Description": "To ensure a metadata summary is only written to the same path as the files from which the summary is created, there is a check against the the root path for each footer's file. Only the path is compared, which leaves a possibility that the same path on 2 different types of filesystems (hdfs vs. file for example) could cause some unexpected or erroneous output.\nThis outcome would be unlikely, but it is not explicitly prevented anywhere which exposes risk to future changes.",
        "Issue Links": []
    },
    "PARQUET-126": {
        "Key": "PARQUET-126",
        "Summary": "build fail",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "hy5446",
        "Created": "07/Nov/14 14:18",
        "Updated": "25/Feb/15 23:45",
        "Resolved": "25/Feb/15 23:45",
        "Description": "I can't build the tree as advertised.\n git clone https://git-wip-us.apache.org/repos/asf/incubator-parquet-mr.git\n[...]\ncd incubator-parquet-mr/\nmvn -DskipTests package\n[...]\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Parquet MR ......................................... SUCCESS [  0.329 s]\n[INFO] Parquet Common ..................................... SUCCESS [  1.177 s]\n[INFO] Parquet Generator .................................. SUCCESS [  0.474 s]\n[INFO] Parquet Encodings .................................. SUCCESS [  4.443 s]\n[INFO] Parquet Column ..................................... SUCCESS [  6.153 s]\n[INFO] Parquet Jackson .................................... SUCCESS [  0.714 s]\n[INFO] Parquet Hadoop ..................................... SUCCESS [  2.475 s]\n[INFO] Parquet Avro ....................................... SUCCESS [  2.074 s]\n[INFO] Parquet Pig ........................................ SUCCESS [  2.275 s]\n[INFO] Parquet Thrift ..................................... FAILURE [  2.464 s]\n[INFO] Parquet Cascading .................................. SKIPPED\n[INFO] Parquet Pig bundle ................................. SKIPPED\n[INFO] Parquet Protobuf ................................... SKIPPED\n[INFO] Parquet Scala ...................................... SKIPPED\n[INFO] Parquet Scrooge .................................... SKIPPED\n[INFO] Parquet Hadoop Bundle .............................. SKIPPED\n[INFO] Parquet Hive ....................................... SKIPPED\n[INFO] Parquet Hive Binding Parent ........................ SKIPPED\n[INFO] Parquet Hive Binding Interface ..................... SKIPPED\n[INFO] Parquet Hive 0.10 Binding .......................... SKIPPED\n[INFO] Parquet Hive 0.12 Binding .......................... SKIPPED\n[INFO] Parquet Hive Binding Factory ....................... SKIPPED\n[INFO] Parquet Hive Binding Bundle ........................ SKIPPED\n[INFO] Parquet Hive Storage Handler ....................... SKIPPED\n[INFO] Parquet Hive Bundle ................................ SKIPPED\n[INFO] Parquet Tools ...................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 22.819 s\n[INFO] Finished at: 2014-11-07T15:18:07+01:00\n[INFO] Final Memory: 115M/423M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project parquet-thrift: Compilation failure: Compilation failure:\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[9,39] error: package org.apache.commons.lang3.builder does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:[41,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:41: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:[42,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:42: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:339: error: cannot find symbol\n[ERROR] private static class MapValueStructV2StandardSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:345: error: cannot find symbol\n[ERROR] private static class MapValueStructV2StandardScheme extends StandardScheme<MapValueStructV2> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class StandardScheme\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:412: error: cannot find symbol\n[ERROR] private static class MapValueStructV2TupleSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/MapValueStructV2.java:418: error: cannot find symbol\n[ERROR] private static class MapValueStructV2TupleScheme extends TupleScheme<MapValueStructV2> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TupleScheme\n[ERROR] location: class MapValueStructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:[41,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class StructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV1.java:41: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class StructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:[43,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class StructV3\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV3.java:43: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class StructV3\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:[41,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:41: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:307: error: cannot find symbol\n[ERROR] private static class OptionalStructV1StandardSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:313: error: cannot find symbol\n[ERROR] private static class OptionalStructV1StandardScheme extends StandardScheme<OptionalStructV1> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class StandardScheme\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:361: error: cannot find symbol\n[ERROR] private static class OptionalStructV1TupleSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/OptionalStructV1.java:367: error: cannot find symbol\n[ERROR] private static class OptionalStructV1TupleScheme extends TupleScheme<OptionalStructV1> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TupleScheme\n[ERROR] location: class OptionalStructV1\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:397: error: cannot find symbol\n[ERROR] private static class StructV2StandardSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:403: error: cannot find symbol\n[ERROR] private static class StructV2StandardScheme extends StandardScheme<StructV2> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class StandardScheme\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:464: error: cannot find symbol\n[ERROR] private static class StructV2TupleSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructV2.java:470: error: cannot find symbol\n[ERROR] private static class StructV2TupleScheme extends TupleScheme<StructV2> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TupleScheme\n[ERROR] location: class StructV2\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:[42,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:42: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:420: error: cannot find symbol\n[ERROR] private static class TestListsInMapStandardSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:426: error: cannot find symbol\n[ERROR] private static class TestListsInMapStandardScheme extends StandardScheme<TestListsInMap> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class StandardScheme\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:539: error: cannot find symbol\n[ERROR] private static class TestListsInMapTupleSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/TestListsInMap.java:545: error: cannot find symbol\n[ERROR] private static class TestListsInMapTupleScheme extends TupleScheme<TestListsInMap> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TupleScheme\n[ERROR] location: class TestListsInMap\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:[42,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:42: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[42,43] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class IScheme\n[ERROR] location: class Name\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:42: error: cannot find symbol\n[ERROR] private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class Name\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:417: error: cannot find symbol\n[ERROR] private static class RequiredSetFixtureStandardSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:423: error: cannot find symbol\n[ERROR] private static class RequiredSetFixtureStandardScheme extends StandardScheme<RequiredSetFixture> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class StandardScheme\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:502: error: cannot find symbol\n[ERROR] private static class RequiredSetFixtureTupleSchemeFactory implements SchemeFactory {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class SchemeFactory\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/RequiredSetFixture.java:508: error: cannot find symbol\n[ERROR] private static class RequiredSetFixtureTupleScheme extends TupleScheme<RequiredSetFixture> {\n[ERROR] ^\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TupleScheme\n[ERROR] location: class RequiredSetFixture\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:9: error: package org.apache.commons.lang3.builder does not exist\n[ERROR] import org.apache.commons.lang3.builder.HashCodeBuilder;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[10,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[11,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[12,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[14,31] error: package org.apache.thrift.scheme does not exist\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[15,33] error: cannot find symbol\n[ERROR] \n[ERROR] could not parse error message:   symbol:   class TTupleProtocol\n[ERROR] location: package org.apache.thrift.protocol\n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:20: error: package org.apache.thrift.server.AbstractNonblockingServer does not exist\n[ERROR] import org.apache.thrift.server.AbstractNonblockingServer.*;\n[ERROR] ^\n[ERROR] \n[ERROR] /tmp/incubator-parquet-mr/parquet-thrift/target/generated-test-sources/thrift/parquet/thrift/test/compat/StructWithMoreEnum.java:[41,43] error: cannot find symbol\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :parquet-thrift\nmvn -version\nApache Maven 3.2.3 (NON-CANONICAL_2014-08-19T19:16:17_root; 2014-08-19T17:16:17+02:00)\nMaven home: /opt/maven\nJava version: 1.8.0_25, vendor: Oracle Corporation\nJava home: /usr/lib/jvm/java-8-jdk/jre\nDefault locale: en_US, platform encoding: UTF-8\nOS name: \"linux\", version: \"3.16.4-1-arch\", arch: \"amd64\", family: \"unix\"",
        "Issue Links": []
    },
    "PARQUET-127": {
        "Key": "PARQUET-127",
        "Summary": "Use published Thrift plugin/artifact",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Frank Austin Nothaft",
        "Created": "07/Nov/14 15:56",
        "Updated": "24/Jan/19 09:24",
        "Resolved": null,
        "Description": "Thrift is now publishing the Thrift tools to Maven Central. We should move Parquet to depend on the Maven Central artifact instead of the artifact published in the Twitter repo. Also, we should depend on the Maven libthrift artifact instead of requiring users to have libthrift installed.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/81"
        ]
    },
    "PARQUET-128": {
        "Key": "PARQUET-128",
        "Summary": "Optimize the parquet RecordReader implementation when:  A. filterpredicate is pushed down , B. filterpredicate is pushed down on a flat schema",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yash Datta",
        "Created": "08/Nov/14 14:47",
        "Updated": "12/Nov/15 04:33",
        "Resolved": null,
        "Description": "The RecordReader implementation currently will read all the columns before applying the filter predicate and deciding whether to keep the row or discard it.\nWe can have a RecordReader which will only assemble the columns on which filters are applied (which are usually a few), then apply the filter and decide whether to keep the row or not , and then goes on to assemble the remaining columns or skip the remaining columns accordingly.\nAlso for applications like spark sql , the schema usually applied is a flat one with no repeating or nested columns. In such cases, its better to have a light-weight, faster RecordReader.\nThe performance improvement by this change is seen to be significant , and is better in case smaller number of rows are returned by filtering (which is usually the case) and there are many number of columns",
        "Issue Links": []
    },
    "PARQUET-129": {
        "Key": "PARQUET-129",
        "Summary": "AvroParquetWriter can't save object who can have link to object of same type",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy",
        "Created": "10/Nov/14 11:27",
        "Updated": "01/Nov/21 14:50",
        "Resolved": null,
        "Description": "When i try to write instance of UserTestOne created from following schema \n{\"namespace\": \"com.example.avro\",\n \"type\": \"record\",\n \"name\": \"UserTestOne\",\n \"fields\": [\n{\"name\": \"name\", \"type\": \"string\"}\n,   \n{\"name\": \"friend\",  \"type\": [\"null\", \"UserTestOne\"], \"default\":null}\n ]\n}\nI get java.lang.StackOverflowError",
        "Issue Links": []
    },
    "PARQUET-130": {
        "Key": "PARQUET-130",
        "Summary": "Failed to read write avro data, when it has no fields",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy",
        "Created": "10/Nov/14 11:44",
        "Updated": "11/Nov/14 08:19",
        "Resolved": null,
        "Description": "Failed to write/read event when it created from following avro schema:\n{\"namespace\": \"com.example.avro\",\n \"type\": \"record\",\n \"name\": \"UserTestTwo\",\n \"fields\": []\n}",
        "Issue Links": []
    },
    "PARQUET-131": {
        "Key": "PARQUET-131",
        "Summary": "Vectorized Reader In Parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Zhenxiao Luo",
        "Reporter": "Zhenxiao Luo",
        "Created": "11/Nov/14 00:25",
        "Updated": "25/Oct/18 15:39",
        "Resolved": null,
        "Description": "Vectorized Query Execution could have big performance improvement for SQL engines like Hive, Drill, and Presto. Instead of processing one row at a time, Vectorized Query Execution could streamline operations by processing a batch of rows at a time. Within one batch, each column is represented as a vector of a primitive data type. SQL engines could apply predicates very efficiently on these vectors, avoiding a single row going through all the operators before the next row can be processed.\nAs an efficient columnar data representation, it would be nice if Parquet could support Vectorized APIs, so that all SQL engines could read vectors from Parquet files, and do vectorized execution for Parquet File Format.\nDetail proposal:\nhttps://gist.github.com/zhenxiao/2728ce4fe0a7be2d3b30",
        "Issue Links": [
            "/jira/browse/HIVE-8128"
        ]
    },
    "PARQUET-132": {
        "Key": "PARQUET-132",
        "Summary": "AvroParquetInputFormat should use a parameterized type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "13/Nov/14 17:57",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "19/Nov/14 04:20",
        "Description": "The AvroParquetInputFormat currently extends ParquetInputFormat<IndexedRecord>, which works for regular MR cases. But Spark's hadoopRDD and newAPIHadoopRDD methods (correctly) create a RDD with the types from the InputFormat. This means that the RDD always uses IndexedRecord rather than the correct type.\nThe AvroParquetInputFormat should be AvroParquetInputFormat<T extends IndexedRecord> extends ParquetInputFormat<T>",
        "Issue Links": []
    },
    "PARQUET-133": {
        "Key": "PARQUET-133",
        "Summary": "Upgrade snappy-java to 1.1.1.6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Taro L. Saito",
        "Reporter": "Taro L. Saito",
        "Created": "14/Nov/14 01:51",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Jan/15 01:18",
        "Description": "Upgrade snappy-java to 1.1.1.6 (the latest vesrion), since 1.0.5 is no longer maintained in https://github.com/xerial/snappy-java, and 1.1.1.6 supports  broader platforms including PowerPC, IBM-AIX 6.4, SunOS, etc. And also it has a better native coding loading mechanism (allowing to use snappy-java from multiple class loaders)\nThe compression format between 1.0.5 and 1.1.1.6 are compatible. 1.1.1.x version adds framing format support, but currently Parquet is not using this framing format, so I think this upgrade cause no data format incompatibility.",
        "Issue Links": []
    },
    "PARQUET-134": {
        "Key": "PARQUET-134",
        "Summary": "Enhance ParquetWriter with file creation flag",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Mariappan Asokan",
        "Reporter": "Mariappan Asokan",
        "Created": "15/Nov/14 00:35",
        "Updated": "09/Mar/15 17:44",
        "Resolved": "05/Mar/15 02:27",
        "Description": "Currently, Parquet files can be created only with \"create in exclusive mode\" (like O_CREAT|O_EXCL in UNIX open system call.)  This may be inconvenient in a situation where user knows that overwriting an existing file is okay.  This is especially true for Parquet files created on a local disk.  For example, a user might want to specify a named pipe (on UNIX) as the target, encrypt the bytes on the fly by reading from the named pipe in another process, and write to disk.  The named pipe file has to exist first.\nI am thinking that we can have OVERWRITE and CREATE modes for now.  If appending to Parquet files is supported in the future, we can add APPEND mode later.  These mode flags can be defined as constants in ParquetWriter.java.\nThere are 7 constructors in ParquetWriter class.  I am thinking of adding one more that takes the mode flag (and all the arguments of the constructor with the most arguments today.)  Also, a new constructor of ParquetFileWriter will take the mode flag as an argument and the following statement\n\nthis.out = fs.create(file, false);\n\n\nwill be modified so that for OVERWRITE mode, the second argument to create() will be set to true.\nI can submit a patch with the above changes and a test.  Committers, please give your feedback with suggestions.",
        "Issue Links": []
    },
    "PARQUET-135": {
        "Key": "PARQUET-135",
        "Summary": "Input location is not getting set for the getStatistics in ParquetLoader when using two different loaders  within a Pig script.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "elif dede",
        "Reporter": "elif dede",
        "Created": "17/Nov/14 04:26",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "18/Nov/14 00:53",
        "Description": "We have encountered this problem on pig jobs loading two different datasets using different loaders.\nThe problem is the input location was not getting set correctly in getStatistics in ParquetLoader which was causing the path for the other loader to be used. \nTo fix we need a setInput call in getStatistics in:https://github.com/Parquet/parquet-mr/blob/master/parquet-pig/src/main/java/parquet/pig/ParquetLoader.java#L287",
        "Issue Links": []
    },
    "PARQUET-136": {
        "Key": "PARQUET-136",
        "Summary": "NPE thrown in StatisticsFilter when all values in a string/binary column trunk are null",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "19/Nov/14 00:46",
        "Updated": "13/Aug/15 08:20",
        "Resolved": "27/Jan/15 02:25",
        "Description": "For a string or a binary column, if all values in a single column trunk are null, so do the min & max values in the column trunk statistics. However, while checking the statistics for column trunk pruning, a null check is missing, and causes NPE. Corresponding code can be found here.\nThis issue can be steadily reproduced with the following Spark shell snippet against Spark 1.2.0-SNAPSHOT (013089794d):\n\nimport org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nimport sqlContext._\n\ncase class StringCol(value: String)\n\nsc.parallelize(StringCol(null) :: Nil, 1).saveAsParquetFile(\"/tmp/empty.parquet\")\nparquetFile(\"/tmp/empty.parquet\").registerTempTable(\"null_table\")\n\nsql(\"SET spark.sql.parquet.filterPushdown=true\")\nsql(\"SELECT * FROM null_table WHERE value = 'foo'\").collect()\n\n\nException thrown:\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException\n        at parquet.io.api.Binary$ByteArrayBackedBinary.compareTo(Binary.java:206)\n        at parquet.io.api.Binary$ByteArrayBackedBinary.compareTo(Binary.java:162)\n        at parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:100)\n        at parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:47)\n        at parquet.filter2.predicate.Operators$Eq.accept(Operators.java:162)\n        at parquet.filter2.statisticslevel.StatisticsFilter.canDrop(StatisticsFilter.java:52)\n        at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:46)\n        at parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:22)\n        at parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:108)\n        at parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:28)\n        at parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:158)\n        at parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)\n        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:135)\n        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:107)\n        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n        at org.apache.spark.scheduler.Task.run(Task.scala:56)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/SPARK-4258",
            "/jira/browse/PARQUET-161"
        ]
    },
    "PARQUET-137": {
        "Key": "PARQUET-137",
        "Summary": "Add support for Pig datetimes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Christian Rolf",
        "Created": "20/Nov/14 10:58",
        "Updated": "05/Apr/17 20:20",
        "Resolved": null,
        "Description": "There's currenly no support for conversion to/from Pig datetimes",
        "Issue Links": [
            "/jira/browse/PARQUET-114"
        ]
    },
    "PARQUET-138": {
        "Key": "PARQUET-138",
        "Summary": "Parquet should allow a merge between required and optional schemas",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Nicolas Trinquier",
        "Reporter": "Robert Justice",
        "Created": "21/Nov/14 19:15",
        "Updated": "22/Feb/19 11:05",
        "Resolved": "01/Feb/19 18:52",
        "Description": "In discussion with Ryan, he felt we should be able to merge from required binary to optional binary and the resulting schema would be optional\nhttps://github.com/Parquet/parquet-mr/blob/master/parquet-column/src/test/java/parquet/schema/TestMessageType.java\n\ntry {\n      t3.union(t4);\n      fail(\"moving from optional to required\");\n    } catch (IncompatibleSchemaModificationException e) {\n      assertEquals(\"repetition constraint is more restrictive: can not merge type required binary a into optional binary a\", e.getMessage());\n    }",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/550",
            "https://github.com/apache/parquet-mr/pull/623"
        ]
    },
    "PARQUET-139": {
        "Key": "PARQUET-139",
        "Summary": "Avoid reading file footers in parquet-avro InputFormat",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/Nov/14 18:08",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "05/Feb/15 23:06",
        "Description": "The AvroParquetInputFormat currently relies on the ParquetInputFormat that reads the footers for all of the files that will be processed. This is for two reasons:\n1. To plan splits (if using client side splits)\n2. To get a merged schema for all of the files\nReading all of the footers is a bottle-neck when working with a large number of files and can significantly delay a job because only one machine is working. This should be done in parallel on the task side. PARQUET-84 added the ability to avoid reading footers on the client for split planning, so the difficult task is to avoid reading footers to merge the Parquet schema.\nTo avoid merging the Parquet schema, the AvroParquetInputFormat should either use whatever schema a file contains or should reconcile the projection schema with the file schema on the task side.",
        "Issue Links": []
    },
    "PARQUET-140": {
        "Key": "PARQUET-140",
        "Summary": "Allow clients to control the GenericData object that is used to read Avro records",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Josh Wills",
        "Reporter": "Josh Wills",
        "Created": "01/Dec/14 22:17",
        "Updated": "28/Oct/15 16:39",
        "Resolved": "02/Dec/14 16:22",
        "Description": "Right now, Parquet always uses the default SpecificData instance (retrieved by SpecificData.get()) to lookup the schemas for SpecificRecord subclasses. Unfortunately, if the definition of the SpecificRecord subclass is not available to the classloader used in SpecificData.get(), we will fail to find the definition of the SpecificRecord subclass and will fall back to returning a GenericRecord, which will cause a ClassCastException in any client code that is expecting an instance of the SpecificRecord subclass.\nWe can fix this limitation by allowing the client code to specify how to construct a custom instance of SpecificData (or any other subclass of GenericData) for Parquet to use, including instances of SpecificData that use alternative classloaders.",
        "Issue Links": []
    },
    "PARQUET-141": {
        "Key": "PARQUET-141",
        "Summary": "improve parquet scrooge integration",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "01/Dec/14 22:29",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "13/Jan/15 00:01",
        "Description": "upgrade to scrooge 3.17.0, remove reflection based field info inspection, support enum and requirement type correctly\nThis PR is essential for scrooge write support\nPR: https://github.com/apache/incubator-parquet-mr/pull/88",
        "Issue Links": []
    },
    "PARQUET-142": {
        "Key": "PARQUET-142",
        "Summary": "parquet-tools doesn't filter _SUCCESS file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Neville Li",
        "Created": "04/Dec/14 22:43",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Jan/15 01:31",
        "Description": "Currently parquet-tools command fails when input is a directory with _SUCCESS file from mapreduce. Filtering those out like ParquetFileReader does fixes the problem.\n\nparquet-cat /tmp/parquet_write_test\nCould not read footer: java.lang.RuntimeException: file:/tmp/parquet_write_test/_SUCCESS is not a Parquet file (too small)\n\n$ tree /tmp/parquet_write_test\n/tmp/parquet_write_test\n\u251c\u2500\u2500 part-m-00000.parquet\n\u2514\u2500\u2500 _SUCCESS",
        "Issue Links": []
    },
    "PARQUET-143": {
        "Key": "PARQUET-143",
        "Summary": "Clean up parquet-cascading dependencies",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "05/Dec/14 20:37",
        "Updated": "05/Dec/14 20:37",
        "Resolved": null,
        "Description": "parquet-cascading depends on elephantbird and pig because it depends on parquet-thrift. But it seems like parquet-cascading should only depend on cascading and hadoop.",
        "Issue Links": []
    },
    "PARQUET-144": {
        "Key": "PARQUET-144",
        "Summary": "read a single file outside of mapreduce framework",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "hy5446",
        "Created": "08/Dec/14 17:52",
        "Updated": "31/Jul/15 15:29",
        "Resolved": "31/Jul/15 15:29",
        "Description": "In my test I would like to read a file that has been written through Parquet + Scrooge. I would like to do it outside of map/reduce or hadoop. Something like this:\nval bytes = readFile(\"my file\")\nval objects = deserializeWithParquetScrooge[MyObjectClass](bytes)\nIs something like this possible? How?",
        "Issue Links": []
    },
    "PARQUET-145": {
        "Key": "PARQUET-145",
        "Summary": "InternalParquetRecordReader.close() should not throw an exception if initialization has failed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Wolfgang Hoschek",
        "Reporter": "Wolfgang Hoschek",
        "Created": "09/Dec/14 12:14",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "11/Dec/14 22:01",
        "Description": "InternalParquetRecordReader.close() currently throws a NullPointerException if initialize() didn't succeed and hence the \"reader\" remains null. \nThis NPE happens with client usage idioms like this:\n\nAvroParquetReader reader = ...\ntry {\n       // read some parquet data\n} finally {\n  if (reader != null) { reader.close(); }\n}\n\n\nI believe close() should read as follows:\n\npublic void close() throws IOException {\n    if (reader != null) {\n      reader.close();\n    }\n  }\n\n\nThoughts?",
        "Issue Links": []
    },
    "PARQUET-146": {
        "Key": "PARQUET-146",
        "Summary": "make Parquet compile with java 7 instead of java 6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "09/Dec/14 22:30",
        "Updated": "15/Aug/16 22:00",
        "Resolved": "15/Aug/16 22:00",
        "Description": "currently Parquet is compatible with java 6. we should remove this constraint.",
        "Issue Links": [
            "https://github.com/apache/incubator-parquet-mr/pull/101",
            "https://github.com/apache/parquet-mr/pull/231"
        ]
    },
    "PARQUET-147": {
        "Key": "PARQUET-147",
        "Summary": "Parquet to report number of rows affected in the row group when corrupted bytes occur",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tongjie Chen",
        "Created": "10/Dec/14 19:16",
        "Updated": "10/Dec/14 19:16",
        "Resolved": null,
        "Description": "In case of hardware failure (disk, memory, etc),  there might be corrupted bytes. That will result in ArrayIndexOutOfBoundException or/and data garbled.\nParquet should report number of rows being affected in case of such scenario.",
        "Issue Links": []
    },
    "PARQUET-148": {
        "Key": "PARQUET-148",
        "Summary": "provide an option to skip entire row group in case corrupted bytes occur",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tongjie Chen",
        "Created": "10/Dec/14 19:20",
        "Updated": "10/Dec/14 19:20",
        "Resolved": null,
        "Description": "In case of hardware failure (disk, memory, etc),  there might be corrupted bytes. That will result in ArrayIndexOutOfBoundException or/and data garbled.\nCurrently, jobs reading those Parquet files will fail unless the corrupted files are deleted/moved.\nIt would be better if Parquet provide an option to skip entire row group (and report how many rows being affected) in case of corrupted bytes.\nrelated issue: PARQUET-147",
        "Issue Links": []
    },
    "PARQUET-149": {
        "Key": "PARQUET-149",
        "Summary": "provide an option to skip a page in case corrupted bytes occur",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tongjie Chen",
        "Created": "10/Dec/14 19:24",
        "Updated": "10/Dec/14 19:24",
        "Resolved": null,
        "Description": "In case of hardware failure (disk, memory, etc), there might be corrupted bytes. That will result in ArrayIndexOutOfBoundException or/and data garbled.\nCurrently, jobs reading those Parquet files will fail unless the corrupted files are deleted/moved.\nCurrently page metadata has a CRC field (not used so far), which can be used to check integrity of the page. If page data is corrupted, skip the whole page.\nrelated issue: PARQUET-148",
        "Issue Links": []
    },
    "PARQUET-150": {
        "Key": "PARQUET-150",
        "Summary": "Merge script requires ':' in PR names",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/Dec/14 22:06",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "16/Dec/14 17:56",
        "Description": "The merge script currently matches an issue id followed by ':', but not everyone uses the ':' and then we end up with orphaned pull requests.",
        "Issue Links": []
    },
    "PARQUET-151": {
        "Key": "PARQUET-151",
        "Summary": "Null Pointer exception in parquet.hadoop.ParquetFileWriter.mergeFooters",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Yash Datta",
        "Reporter": "Vladislav Kuzemchik",
        "Created": "13/Dec/14 03:15",
        "Updated": "01/Jun/15 21:49",
        "Resolved": "01/Jun/15 21:49",
        "Description": "Hi!\nI'm getting null pointer exception when I'm trying to write parquet files with spark.\n\nDec 13, 2014 3:05:10 AM WARNING: parquet.hadoop.ParquetOutputCommitter: could not write summary file for hdfs://phoenix-011.nym1.placeiq.net:8020/user/vkuzemchik/parquet_data/1789\njava.lang.NullPointerException\n\tat parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:426)\n\tat parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:402)\n\tat parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:51)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:936)\n\tat com.placeiq.spark.KafkaReader$.writeParquetHadoop(KafkaReader.scala:143)\n\tat com.placeiq.spark.KafkaReader$$anonfun$3.apply(KafkaReader.scala:165)\n\tat com.placeiq.spark.KafkaReader$$anonfun$3.apply(KafkaReader.scala:164)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1.apply(DStream.scala:527)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1.apply(DStream.scala:527)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:41)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:172)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\nHere is function I'm using:\nSpark.scala\n  def writeParquetHadoop(rdd:RDD[(Void,LogMessage)]):Unit =  {\n      val jobConf = new JobConf(ssc.sparkContext.hadoopConfiguration)\n      val job = new Job(jobConf)\n      val outputDir = \"hdfs://phoenix-011.nym1.placeiq.net:8020/user/vkuzemchik/parquet_data/\"\n\n      ParquetOutputFormat.setWriteSupportClass(job, classOf[AvroWriteSupport])\n      ParquetInputFormat.setReadSupportClass(job, classOf[AvroReadSupport[LogMessage]])\n      AvroParquetInputFormat.setAvroReadSchema(job, LogMessage.SCHEMA$)\n      AvroParquetOutputFormat.setSchema(job, LogMessage.SCHEMA$)\n      ParquetOutputFormat.setCompression(job,CompressionCodecName.SNAPPY)\n      ParquetOutputFormat.setBlockSize(job, 536870912)\n      job.setOutputKeyClass(classOf[Void])\n      job.setOutputValueClass(classOf[LogMessage])\n      job.setOutputFormatClass(classOf[ParquetOutputFormat[LogMessage]])\n      job.getConfiguration.set(\"mapred.output.dir\", outputDir+rdd.id)\n\n      rdd.saveAsNewAPIHadoopDataset(job.getConfiguration)\n  }\n\n\nI have this issue on 1.5. Trying to re-produce on newer versions.",
        "Issue Links": []
    },
    "PARQUET-152": {
        "Key": "PARQUET-152",
        "Summary": "Encoding issue with fixed length byte arrays",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Sergio Pe\u00f1a",
        "Reporter": "Nezih Yigitbasi",
        "Created": "18/Dec/14 00:12",
        "Updated": "16/Jan/23 10:15",
        "Resolved": "03/Jul/15 17:52",
        "Description": "While running some tests against the master branch I hit an encoding issue that seemed like a bug to me.\nI noticed that when writing a fixed length byte array and the array's size is > dictionaryPageSize (in my test it was 512), the encoding falls back to DELTA_BYTE_ARRAY as seen below:\n\nDec 17, 2014 3:41:10 PM INFO: parquet.hadoop.ColumnChunkPageWriteStore: written 12,125B for [flba_field] FIXED_LEN_BYTE_ARRAY: 5,000 values, 1,710B raw, 1,710B comp, 5 pages, encodings: [DELTA_BYTE_ARRAY]\n\n\nBut then read fails with the following exception:\n\nCaused by: parquet.io.ParquetDecodingException: Encoding DELTA_BYTE_ARRAY is only supported for type BINARY\n\tat parquet.column.Encoding$7.getValuesReader(Encoding.java:193)\n\tat parquet.column.impl.ColumnReaderImpl.initDataReader(ColumnReaderImpl.java:534)\n\tat parquet.column.impl.ColumnReaderImpl.readPageV2(ColumnReaderImpl.java:574)\n\tat parquet.column.impl.ColumnReaderImpl.access$400(ColumnReaderImpl.java:54)\n\tat parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:518)\n\tat parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:510)\n\tat parquet.column.page.DataPageV2.accept(DataPageV2.java:123)\n\tat parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:510)\n\tat parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:502)\n\tat parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:604)\n\tat parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:348)\n\tat parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:63)\n\tat parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:58)\n\tat parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:267)\n\tat parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)\n\tat parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)\n\tat parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)\n\tat parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)\n\tat parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:129)\n\tat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:198)\n\t... 16 more\n\n\nWhen the array's size is < dictionaryPageSize, RLE_DICTIONARY encoding is used and read works fine:\n\nDec 17, 2014 3:39:50 PM INFO: parquet.hadoop.ColumnChunkPageWriteStore: written 50B for [flba_field] FIXED_LEN_BYTE_ARRAY: 5,000 values, 3B raw, 3B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 1 entries, 8B raw, 1B comp}",
        "Issue Links": [
            "/jira/browse/PARQUET-2231",
            "https://github.com/apache/parquet-mr/pull/225"
        ]
    },
    "PARQUET-153": {
        "Key": "PARQUET-153",
        "Summary": "Broken link on Parquet site",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "18/Dec/14 19:07",
        "Updated": "28/May/18 12:28",
        "Resolved": "28/May/18 12:28",
        "Description": "Broken link on:\nhttp://parquet.incubator.apache.org/community/\n\"Follow our contribution guidelines when submitting a patch.\"\npoints to non existing: http://parquet.incubator.apache.org/docs/howtocontribute/\nSee:\nhttps://twitter.com/wsmoak/status/534847641423990785",
        "Issue Links": [
            "/jira/browse/PARQUET-803"
        ]
    },
    "PARQUET-154": {
        "Key": "PARQUET-154",
        "Summary": "parquet Constructors not taking file System object",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Manish Agarwal",
        "Created": "22/Dec/14 07:10",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Dec/14 08:32",
        "Description": "I am trying to create a file in parquet file format and in RC file format  . \nNo Parquet constructor accepts fileSystem object as an argument. This means that i will have to append the uri from the file system in front of the filepath object everytime i need to create a new file . \nIn RC format the  file system object is allowed to be passed in the constructor . \nThe advantage of passing the file System object into the  constructor is that i can specify my yarn  instance  file system pointer to be used while creating the file and its quite  straight forward . For example RC file constructors uses the file System which we have passed . \nIn parquet i see everywhere file System object being derived out of the Parameters or created a fresh . \nIs there some reason we avoided using the fileSystem object in constructor ? \nIf we allow a file System object constructor as well , I would not have to worry about modifying my file name to contain the uri part .",
        "Issue Links": []
    },
    "PARQUET-155": {
        "Key": "PARQUET-155",
        "Summary": "Hive Avro to Parquet table conversion",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy",
        "Created": "24/Dec/14 08:56",
        "Updated": "17/Mar/15 05:45",
        "Resolved": null,
        "Description": "Hi.\nI have following avro schema \n\n{\n     \"namespace\" : \"com.example.test\",\n     \"type\" : \"record\",\n     \"name\" : \"TestRecord\",\n     \"fields\" : [{\"name\" : \"objectLink\", \"type\" : [\n                           {\"type\": \"record\", \"name\" : \"TestObj1\", \"fields\" : [{\"name\":\"obj1VisitorId\",\"type\":[\"null\",\"string\"]}] },\n                           {\"type\": \"record\", \"name\" : \"TestObj2\", \"fields\" : [{\"name\":\"obj2VisitorId\",\"type\":[\"null\",\"string\"]}]}\n                       ]\n                 }],\n     \"doc\" : \"event for test purposes\"\n}\n\n\nUsing this schema I can create avro objects, also I'm able to create table backed by avro in Hive. But then I want to create a table backed by parquet I'm doing \nCREATE TABLE parquet_table \nSTORED AS parquet\nAS SELECT * FROM avro_table\nand i get \nSemanticException java.lang.UnsupportedOperationException: Unknown field type: uniontype<struct<obj1visitorid:string>,struct<obj2visitorid:string>>\nIs there a way to convert such structures, to store them in hive backed as parquet? This is a simple example, but I have big data structure described in avro, so I can't convert it manually, and also I have data which already stored in avro and need to be loaded in table, backed by parquet. Is there any way to this?\nI'm using hive 0.13.",
        "Issue Links": []
    },
    "PARQUET-156": {
        "Key": "PARQUET-156",
        "Summary": "Document recommendations for block size and page size given an expected number of writers",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Manish Agarwal",
        "Created": "29/Dec/14 18:23",
        "Updated": "31/Dec/14 05:44",
        "Resolved": null,
        "Description": "I sent a mail on dev list but I seem to have a problem with email on dev list  so opening a bug here . \nI am on a multithreaded system where there are M threads , each thread creating   an   independent parquet writer  and writing on the hdfs  in its own independent files  . I have a  finite amount of RAM  say R  .  \nNow when I created  parquet writer using default block and page size i get heap error (no memory )  on my set up  . so I reduced my block size and page size to very low and  my system stopped giving me these out of memory errors and started writing the file correctly . I am able to read these files correctly as well  . \nI should not have to make the memory low and parquet should automatically make sure i do not get these errors . \nBut in case i have to keep track of the memory my question is as follows. \nNow keeping these  values very less is not a recommended practice as i would loose on performance . I am particularly concerned about  write performance .  What math formula  do you recommend that I  should use to find correct blockSize , pageSize to be passed to the parquet constructor to have   the right  WRITE  performance  . ie how can i decide what should be the right blockSize , pageSize  for a parquet writer given that i have M threads and total RAM memory available is R   . I don't understand dictionaryPageSize need and in case i    need to bother about that as well kindly let me know but i have kept enableDictionary flag as false . \nI am using the bellow constructor .\npublic More ...ParquetWriter(\n162      Path file,\n163      WriteSupport<T> writeSupport,\n164      CompressionCodecName compressionCodecName,\n165      int blockSize,\n166      int pageSize,\n167      int dictionaryPageSize,\n168      boolean enableDictionary,\n169      boolean validating,\n170      WriterVersion writerVersion,\n171      Configuration conf) throws IOException {",
        "Issue Links": []
    },
    "PARQUET-157": {
        "Key": "PARQUET-157",
        "Summary": "Divide by zero in logging code",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Jim Carroll",
        "Reporter": "Jim Carroll",
        "Created": "29/Dec/14 18:34",
        "Updated": "25/Aug/22 13:42",
        "Resolved": "30/Jan/15 01:33",
        "Description": "There's a divide by zero in the timing measurement code of InternalParquetRecordReader at line 109 here: https://github.com/apache/incubator-parquet-mr/blob/master/parquet-hadoop/src/main/java/parquet/hadoop/InternalParquetRecordReader.java#L109\ntotalTime is 0 and there's no check.",
        "Issue Links": [
            "/jira/browse/SPARK-9442"
        ]
    },
    "PARQUET-158": {
        "Key": "PARQUET-158",
        "Summary": "Thrift binary fields are not serialized correctly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Laurence Rouesnel",
        "Created": "29/Dec/14 23:47",
        "Updated": "29/Dec/14 23:47",
        "Resolved": null,
        "Description": "Thrift binary fields are not serialized correctly - I believe UTF-8 encoding is applied.\nA demonstration of this bug is shown in https://github.com/laurencer/parquet-mr-bug\nThis appears to be an issue in Parquet-MR where Thrift TType is used determine the type of fields. TType actually represents the on-disk/encoded field type tag - that does not distinguish between binary and string fields.\nString data is not actually represented on-disk as being different - instead it is up to the program to interpret the binary data as a UTF-8 encoded string. Parquet-MR instead assumes that every binary field is a UTF-8 encoded string.\nThis may have arisen because the binary field tag is actually TType.String (where it actually just represents a raw binary field).",
        "Issue Links": []
    },
    "PARQUET-159": {
        "Key": "PARQUET-159",
        "Summary": "paquet-hadoop tests fail to compile",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dale Richardson",
        "Created": "04/Jan/15 05:17",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "30/Jan/15 00:09",
        "Description": "[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project parquet-hadoop: Compilation failure\n[ERROR] ... /incubator-parquet-mr/parquet-hadoop/src/test/java/parquet/format/converter/TestParquetMetadataConverter.java:[247,9] cannot find symbol\n[ERROR] symbol  : constructor AssertionError(java.lang.String,java.lang.AssertionError)\n[ERROR] location: class java.lang.AssertionError",
        "Issue Links": [
            "/jira/browse/PARQUET-174"
        ]
    },
    "PARQUET-160": {
        "Key": "PARQUET-160",
        "Summary": "Simplify CapacityByteArrayOutputStream",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "06/Jan/15 22:37",
        "Updated": "26/May/17 15:26",
        "Resolved": "05/Mar/15 01:27",
        "Description": "There's a lot of complicated logic around trying to re-use byte buffers as well as allocating slabs before they are needed as well as shrinking / consolidating. \nAfter discussing with julienledem we should simplify and not bother reusing the byte buffers, and not allocate buffers until they are needed. The hope is this will reduce the memory overhead and that the number of byte buffers should be small so they should not cause trouble for GC.",
        "Issue Links": [
            "/jira/browse/PARQUET-1006"
        ]
    },
    "PARQUET-161": {
        "Key": "PARQUET-161",
        "Summary": "Parquet does not write statistics for column chunks that are all null",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "08/Jan/15 19:45",
        "Updated": "27/Jan/15 06:29",
        "Resolved": "27/Jan/15 06:28",
        "Description": "See discussion in:\nhttps://github.com/apache/incubator-parquet-mr/pull/99",
        "Issue Links": [
            "/jira/browse/PARQUET-136"
        ]
    },
    "PARQUET-162": {
        "Key": "PARQUET-162",
        "Summary": "ParquetThrift should throw when unrecognized columns are passed to the column projection API",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Tim",
        "Reporter": "Alex Levenson",
        "Created": "08/Jan/15 23:03",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "05/Mar/15 23:22",
        "Description": "It appears that giving wrong / unrecognized column names to the projection API doesn't fail fast or even report that the columns were unrecognized at all.\nWe should also log at start up what columns are being projected, this is useful info to have in the logs.",
        "Issue Links": []
    },
    "PARQUET-163": {
        "Key": "PARQUET-163",
        "Summary": "BytesWritable is not in  parquet 's writePrimitive method in DataWritableWriter class",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Manish Agarwal",
        "Created": "09/Jan/15 01:18",
        "Updated": "09/Jan/15 17:29",
        "Resolved": null,
        "Description": "I have 2 int fields and 1 string field . To insert it into parquet File I use 2 IntWritable and 1 BinaryWritable object .\nFor next record i can reuse my 2 IntWritable objects but i have to create a new  BinaryWritable object because  BinaryWritable do not have  set method . This is a bottleneck for me in performance as i am doing new again for every nextrecord . \nHow do you guys recommend i solve this since BytesWritable is not in  parquet 's writePrimitive in DataWritableWriter class . it has ByteWritable and we need BytesWritable",
        "Issue Links": []
    },
    "PARQUET-164": {
        "Key": "PARQUET-164",
        "Summary": "Warn when parquet memory manager kicks in",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Dong Chen",
        "Reporter": "Brock Noland",
        "Created": "09/Jan/15 02:51",
        "Updated": "19/May/15 18:27",
        "Resolved": "19/May/15 18:27",
        "Description": "In PARQUET-108 we implemented a memory manager for parquet. I think we should warn in the close() method if we had to adjust the memory down so that users know their memory is being adjusted down and block size will be less than expected.",
        "Issue Links": [
            "/jira/browse/HIVE-9332",
            "/jira/browse/HIVE-7685",
            "/jira/browse/PARQUET-108",
            "/jira/browse/PARQUET-199",
            "https://github.com/apache/parquet-mr/pull/120"
        ]
    },
    "PARQUET-165": {
        "Key": "PARQUET-165",
        "Summary": "A benchmark module for Parquet would be nice",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Daniel Weeks",
        "Reporter": "Nezih Yigitbasi",
        "Created": "09/Jan/15 18:31",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "31/Mar/15 18:58",
        "Description": "As we discussed in the Parquet sync up here is the parquet-bencmark module that I put together when I started implementing vectorization. This would primarily be useful for keeping an eye on the performance across releases.",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/104",
            "https://github.com/apache/incubator-parquet-mr/pull/155"
        ]
    },
    "PARQUET-166": {
        "Key": "PARQUET-166",
        "Summary": "Validate parquet row group size and HDFS block size",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "09/Jan/15 22:08",
        "Updated": "20/Jul/15 23:33",
        "Resolved": null,
        "Description": "The OutputFormat should verify that parquet.block.size < dfs.blocksize to avoid bad performance. In addition, we could check that (dfs.blocksize % parquet.block.size) < 1MB to ensure that some number of row groups is approximately the size of an HDFS block.",
        "Issue Links": [
            "/jira/browse/PARQUET-306"
        ]
    },
    "PARQUET-167": {
        "Key": "PARQUET-167",
        "Summary": "Snappy Compression: Optimize DirectBuffer handling/release",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dale Richardson",
        "Created": "12/Jan/15 10:20",
        "Updated": "07/Apr/15 20:46",
        "Resolved": null,
        "Description": "The snappy compressor keeps buffers around even when they are not actively being used.  When the buffer size is large this can lead to a lot of unneeded memory overhead.",
        "Issue Links": []
    },
    "PARQUET-168": {
        "Key": "PARQUET-168",
        "Summary": "Wrong command line option description in parquet-tools",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "21/Jan/15 20:20",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "24/Jan/15 00:20",
        "Description": "The description for \"debug\" option is apparently wrong:\n\n    Option ncol = OptionBuilder.withLongOpt(\"no-color\")\n                               .withDescription(\"Disable color output even if supported\")\n                               .create();\n\n    Option debg = OptionBuilder.withLongOpt(\"debug\")\n                               .withDescription(\"Disable color output even if supported\")\n                               .create();",
        "Issue Links": []
    },
    "PARQUET-169": {
        "Key": "PARQUET-169",
        "Summary": "Parquet-cpp: Implement support for bulk reading and writing repetition/definition levels.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Nong Li",
        "Created": "22/Jan/15 05:01",
        "Updated": "11/Feb/16 04:44",
        "Resolved": "11/Feb/16 04:44",
        "Description": "Currently , parquet-cpp only supports reading definition levels.\nExtend the code to read/write repetition/definition levels.",
        "Issue Links": []
    },
    "PARQUET-170": {
        "Key": "PARQUET-170",
        "Summary": "update parquet-thrift to use elephant-bird and elephant-bird-pig to 4.5",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Soren Macbeth",
        "Created": "24/Jan/15 18:09",
        "Updated": "04/Feb/19 21:12",
        "Resolved": null,
        "Description": "I would be great to get parquet-thrift to use elephant-bird 4.5. There is a fix for using \"bean-style\" accessor functions in your thrift classes which doesn't work with 4.4.",
        "Issue Links": []
    },
    "PARQUET-171": {
        "Key": "PARQUET-171",
        "Summary": "AvroReadSupport does not support Avro schema resolution",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jeffrey Olchovy",
        "Created": "25/Jan/15 21:37",
        "Updated": "12/May/16 14:14",
        "Resolved": null,
        "Description": "Given multiple \"different-yet-compatible\" Avro-backed Parquet files, a runtime exception will be encountered when trying to merge the metadata values across the files if they are used as input sources for a MapReduce job.\nA contrived example of this problem is provided, along with a derived version of AvroReadSupport that can correctly handle valid schema resolution/evolution scenarios.\nIllustration of Problem\nA simple Avro schema exists, which contains a single record type that consists of a required String member.\n\n{\"type\":\"record\",\"name\":\"Foo\",\"namespace\":\"com.tapad.avro\",\"fields\":[{\"name\":\"my_field\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}]}\n\n\nWhen stored as Parquet-Avro the resulting schema is:\n\nmessage com.tapad.avro.Foo {\n  required binary my_field (UTF8);\n}\n\n\nData is written to a Parquet-Avro file with the following contents:\n\nmy_field = aaa\nmy_field = bbb\n\n\nThe schema for the Foo record is then changed so that its String member is made optional, with a default value of null now provided for the String member.\n\n{\"type\":\"record\",\"name\":\"Foo\",\"namespace\":\"com.tapad.avro\",\"fields\":[{\"name\":\"my_field\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null}]}\n\nmessage com.tapad.avro.Foo {\n  optional binary my_field (UTF8);\n}\n\n\nThis change adheres to the Avro Schema Resoution rules as found in http://avro.apache.org/docs/current/spec.html#Schema+Resolution.\nData is then written to a new Parquet-Avro file.\n\nmy_field = ccc\n\n\nWhen both Parquet-Avro files are used as input to a MapReduce job, wherein the schemas in the data files are considered to be the \"writer\" schemas and the schema on our job's classpath \u2013 in this case, the updated schema \u2013 is used as the \"reader\" schema, the following RuntimeException is encountered:\n\nCaused by: java.lang.RuntimeException: could not merge metadata: key avro.schema has conflicting values: [{\"type\":\"record\",\"name\":\"Foo\",\"namespace\":\"com.tapad.avro\",\"fields\":[{\"name\":\"my_field\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}]}, {\"type\":\"record\",    \"name\":\"Foo\",\"namespace\":\"com.tapad.avro\",\"fields\":[{\"name\":\"my_field\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null}]}]\n 42       at parquet.hadoop.api.InitContext.getMergedKeyValueMetaData(InitContext.java:67)\n 43       at parquet.hadoop.api.ReadSupport.init(ReadSupport.java:84)\n 44       at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:263)\n...\n\n\nSolution\nEach schema in every data file (the \"writer\" schemas) should check for schema compatibility with the \"reader\" schema. If all \"writer\" schemas are compatible with the \"reader\" schema, all records in all data files can be migrated to the \"reader\" schema.\nThe Apache Avro library provides utilities for performing compatibility checks across schemas and provided is a derived version of AvroReadSupport which uses these utilities to successfully process the records in the aforementioned data files when they are both used as input to a MapReduce job.\nNOTE: Solution will be provided as a hyperlink to a Github Pull Request https://github.com/apache/incubator-parquet-mr/pull/107",
        "Issue Links": []
    },
    "PARQUET-172": {
        "Key": "PARQUET-172",
        "Summary": "Add support for non-String binary in parquet-thrift",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "28/Jan/15 00:36",
        "Updated": "10/Mar/15 23:16",
        "Resolved": "10/Mar/15 23:02",
        "Description": "Thrift considers binary a \"special\" type that isn't in the official spec but is \"to provide better interoperability with java\". The parquet-thrift side doesn't currently support binary because Thrift String fields are converted to UTF8-annotated binary. The result is that binary fields get mangled when stored in Parquet because Parquet assumes they are UTF8.\nI think some storage layer in Java Thrift must know about binary and pass the unencoded bytes, but that Parquet hasn't implemented a similar hack. (The type conversion code at least has no entry for binary.)",
        "Issue Links": []
    },
    "PARQUET-173": {
        "Key": "PARQUET-173",
        "Summary": "StatisticsFilter doesn't handle And properly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "28/Jan/15 09:09",
        "Updated": "13/Aug/15 08:23",
        "Resolved": "03/Feb/15 20:54",
        "Description": "I guess it's a pretty straightforward mistake \n\n  @Override\n  public Boolean visit(And and) {\n    return and.getLeft().accept(this) && and.getRight().accept(this);\n  }\n\n  @Override\n  public Boolean visit(Or or) {\n    // seems unintuitive to put an && not an || here\n    // but we can only drop a chunk of records if we know that\n    // both the left and right predicates agree that no matter what\n    // we don't need this chunk.\n    return or.getLeft().accept(this) && or.getRight().accept(this);\n  }\n\n\nThe consequence is that filter predicates like a > 10 && a < 20 can never drop any row groups.",
        "Issue Links": [
            "/jira/browse/SPARK-5451",
            "/jira/browse/PARQUET-250"
        ]
    },
    "PARQUET-174": {
        "Key": "PARQUET-174",
        "Summary": "Fix Java6 compatibility",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Laurent Goujon",
        "Reporter": "Laurent Goujon",
        "Created": "28/Jan/15 22:35",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "29/Jan/15 00:08",
        "Description": "Test parquet-hadoop/src/test/java/parquet/format/converter/TestParquetMetadataConverter.java throws an AssertionError using a constructor introduced in Java7.\nAs such this class cannot be compiled with Java6",
        "Issue Links": [
            "/jira/browse/PARQUET-159"
        ]
    },
    "PARQUET-175": {
        "Key": "PARQUET-175",
        "Summary": "Allow setting of a custom protobuf class when reading parquet file using parquet-protobuf.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Lukas Nalezenec",
        "Reporter": "Lukas Nalezenec",
        "Created": "28/Jan/15 23:31",
        "Updated": "30/Apr/15 10:40",
        "Resolved": "30/Apr/15 10:40",
        "Description": "ProtoReadSupport expects that parquet file header contains name of protobuffer class. It means that we can read parquet as protobuffer only if the file was written as protobuffer.\nParquet should allow to set custom protobufer class.",
        "Issue Links": []
    },
    "PARQUET-176": {
        "Key": "PARQUET-176",
        "Summary": "Parquet fails to parse schema contains '\\r'",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Rekha Joshi",
        "Created": "02/Feb/15 23:44",
        "Updated": "29/Apr/15 14:16",
        "Resolved": null,
        "Description": "Parquet fails to parse schema contains '\\r' for windows operating system.\n\tat parquet.schema.MessageTypeParser.asRepetition(MessageTypeParser.java:203)\nat parquet.schema.MessageTypeParser.addType(MessageTypeParser.java:101)\nat parquet.schema.MessageTypeParser.addGroupTypeFields(MessageTypeParser.java:96)\nat parquet.schema.MessageTypeParser.parse(MessageTypeParser.java:89)\nat parquet.schema.MessageTypeParser.parseMessageType(MessageTypeParser.java:79)\nat org.apache.spark.sql.parquet.ParquetTestData$.writeFile(ParquetTestData.scala:221)\nat org.apache.spark.sql.parquet.ParquetQuerySuite.beforeAll(ParquetQuerySuite.scala:92)",
        "Issue Links": [
            "/jira/browse/SPARK-5192"
        ]
    },
    "PARQUET-177": {
        "Key": "PARQUET-177",
        "Summary": "MemoryManager ensure minimum Column Chunk size",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "04/Feb/15 00:56",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "05/Feb/15 22:37",
        "Description": "The memory manager currently has no limit to how small it will make row groups.  This is problematic because jobs that have a large number of writers can result in tiny row groups that hurt performance.\nThe following patch will allow a configurable minimum size before killing the job.  Default is currently no limit.",
        "Issue Links": [
            "/jira/browse/HIVE-7685"
        ]
    },
    "PARQUET-178": {
        "Key": "PARQUET-178",
        "Summary": "META-INF for slf4j should not be in parquet-format jar",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "koert kuipers",
        "Created": "05/Feb/15 03:44",
        "Updated": "16/Jun/15 16:52",
        "Resolved": "16/Jun/15 16:52",
        "Description": "$ jar tf parquet-format-2.2.0-rc1.jar  | grep org\\\\.slf\nMETA-INF/maven/org.slf4j/\nMETA-INF/maven/org.slf4j/slf4j-api/\nMETA-INF/maven/org.slf4j/slf4j-api/pom.xml\nMETA-INF/maven/org.slf4j/slf4j-api/pom.properties\n\n\nIt is not clear to me why these are here. I suspect they should not be.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/24"
        ]
    },
    "PARQUET-179": {
        "Key": "PARQUET-179",
        "Summary": "Retire _metadata generation",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "elif dede",
        "Reporter": "elif dede",
        "Created": "05/Feb/15 05:53",
        "Updated": "03/Jul/18 14:46",
        "Resolved": null,
        "Description": "We can disable  _metadata file generation since it is not being used anymore to reduce the memory usage during commit phase.\nWe are keeping the _common_metadata since it is less memory expensive.",
        "Issue Links": []
    },
    "PARQUET-180": {
        "Key": "PARQUET-180",
        "Summary": "Parquet-thrift compile issue with 0.9.2.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "05/Feb/15 17:53",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "11/Mar/15 22:22",
        "Description": "Thrift 0.9.2 removed setReadLength. This causes parquet-thrift to fail because it is called for TBinaryProtocol. The reason we use it is defensive: a size is read from the data and then that many bytes are read, so using this method sets a maximum and causes an exception rather than a strange failure later on. The code also has a comment that says it is okay when it can't be used.\n\n    /* Reduce the chance of OOM when data is corrupted. When readBinary is called on TBinaryProtocol, it reads the length of the binary first,\n     so if the data is corrupted, it could read a big integer as the length of the binary and therefore causes OOM to happen.\n     Currently this fix only applies to TBinaryProtocol which has the setReadLength defined.\n      */\n    if (protocol instanceof TBinaryProtocol) {\n      ((TBinaryProtocol)protocol).setReadLength(record.getLength());\n    }\n\n\nI think the fix is to remove the section above.",
        "Issue Links": []
    },
    "PARQUET-181": {
        "Key": "PARQUET-181",
        "Summary": "Scrooge Write Support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Colin Marc",
        "Reporter": "Colin Marc",
        "Created": "05/Feb/15 19:27",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "05/Feb/15 19:37",
        "Description": "Parquet has a read API for Scrooge - it should have a write API, too.\nPR: https://github.com/apache/incubator-parquet-mr/pull/58",
        "Issue Links": []
    },
    "PARQUET-182": {
        "Key": "PARQUET-182",
        "Summary": "FilteredRecordReader skips rows it shouldn't for schema with optional columns",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Steven Mellinger",
        "Created": "06/Feb/15 03:50",
        "Updated": "18/Jan/19 15:06",
        "Resolved": null,
        "Description": "When using UnboundRecordFilter with nested AND/OR filters over OPTIONAL columns, there seems to be a case with a mismatch between the current record's column value and the value read during filtering.\nThe structure of my filter predicate that results in incorrect filtering is: (x && (y || z))\nWhen I step through it with a debugger I can see that the value being read from the ColumnReader inside my Predicate is different than the value for that row.\nLooking deeper there seems to be a buffer with dictionary keys in RunLenghBitPackingHybridDecoder (I am using RLE). There are only two different keys in this array, [0,1], whereas my optional column has three different values, [null,0,1]. If I had a column with values 5,10,10,null,10, and keys 0 -> 5 and 1 -> 10, the buffer would hold 0,1,1,1,0, and in the case that it reads the last row, would return 0 -> 5.\nSo it seems that nothing is keeping track of where nulls appear.\nHope someone can take a look, as it is a blocker for my project.",
        "Issue Links": [
            "/jira/browse/PARQUET-98"
        ]
    },
    "PARQUET-183": {
        "Key": "PARQUET-183",
        "Summary": "Special case empty columns to store 0 pages and no column chunks in the footer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "06/Feb/15 19:32",
        "Updated": "26/Aug/16 19:19",
        "Resolved": null,
        "Description": "Currently when a column is empty, each row group will contain one page that encodes repetition and definition levels for this row group. These will be as many 0s as there are rows in the row group (stored in the row group metadata). These values are encoded using RLE so it ends up being very small. \nHowever in cases where there are a lot of columns in a very sparse dataset we end up with a lot of empty column chunks (a column chunk is the data for a given column in a given row group). The metadata could become much smaller by omitting empty column chunks as the metadata of an empty column chunk can be derived from the row count in the corresponding row group.\nI propose the following:\nWhen a column chunk is empty, do not write any page to it.\nDo not add the column chunk metadata in the footer for such empty columns.\nA column chunk is empty if when writing the row group to disk, there is only one page and this page contains rl and dl that are only 0s. (completely empty column).\nWhen reading the dataset:\n\nthe column is present in the schema.\nif there's no column chunk in the footer for a given row group that means we can just replace rls and dls with infinite streams of 0s.\nany stats information can be replaced by #rows count of nulls in predicate push down.\n\nThis will help in cases where we have huge schemas where actually a small subset of columns are populated. The file data will now look like as if we had declared only the schema for columns that actually have data in them. Only the schema in the footer will mention those empty columns.",
        "Issue Links": []
    },
    "PARQUET-184": {
        "Key": "PARQUET-184",
        "Summary": "Add release scripts and documentation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "07/Feb/15 02:18",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "09/Feb/15 20:21",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-185": {
        "Key": "PARQUET-185",
        "Summary": "Update parquet-format release scripts and POM",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/Feb/15 21:15",
        "Updated": "12/Feb/15 00:02",
        "Resolved": "12/Feb/15 00:02",
        "Description": "Updates based on 2.3.0-rc1 feedback\n\nDisable source zip created by maven\nMinor script updates to add \"-incubating\" to the maven version.",
        "Issue Links": []
    },
    "PARQUET-186": {
        "Key": "PARQUET-186",
        "Summary": "Poor performance in SnappyCodec because of string concat in tight loop",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cristian Opris",
        "Created": "12/Feb/15 19:12",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "05/Mar/15 01:57",
        "Description": "String concatenation for SnappyUtil.validateBuffer which is called in a tight \nloop from both compress/decompress.\nThis shows up as taking 60% of time in the profiler \n\n  public static void validateBuffer(byte[] buffer, int off, int len) {\n    Preconditions.checkNotNull(buffer, \"buffer\");\n    Preconditions.checkArgument(off >= 0 && len >= 0 && off <= buffer.length - len,\n        \"Invalid offset or length. Out of buffer bounds. buffer.length=\" + buffer.length\n        + \" off=\" + off + \" len=\" + len);\n  }",
        "Issue Links": []
    },
    "PARQUET-187": {
        "Key": "PARQUET-187",
        "Summary": "parquet-scrooge doesn't compile under 2.11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Colin Marc",
        "Reporter": "Colin Marc",
        "Created": "13/Feb/15 15:09",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "04/Mar/15 20:50",
        "Description": "JavaConversions.asJavaList was removed.",
        "Issue Links": []
    },
    "PARQUET-188": {
        "Key": "PARQUET-188",
        "Summary": "Parquet writes columns out of order (compared to the schema)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Colin Marc",
        "Created": "23/Feb/15 13:23",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "09/Mar/15 20:15",
        "Description": "When building from master, parquet seems to write row groups with the columns in arbitrary orders, not in the same order as the schema. This appears to happen regardless of the OutputFormat or WriteSupport used.\nThis breaks implementations that assume the columns will be in a specific order, in particular impala.",
        "Issue Links": []
    },
    "PARQUET-189": {
        "Key": "PARQUET-189",
        "Summary": "Support building parquet with thrift 0.9.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "ferdinand xu",
        "Reporter": "Ferdinand Xu",
        "Created": "25/Feb/15 04:23",
        "Updated": "25/Mar/15 02:16",
        "Resolved": "24/Mar/15 23:28",
        "Description": "Add one thrift profile to enable building parquet with thrift 0.9.0",
        "Issue Links": [
            "https://github.com/apache/incubator-parquet-mr/pull/124"
        ]
    },
    "PARQUET-190": {
        "Key": "PARQUET-190",
        "Summary": "Fix an inconsistent Javadoc comment of ReadSupport.prepareForRead",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Akihiro Okuno",
        "Created": "25/Feb/15 15:12",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "26/Feb/15 21:40",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-191": {
        "Key": "PARQUET-191",
        "Summary": "Avro schema conversion incorrectly converts maps with nullable values.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/Feb/15 00:10",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "04/Mar/15 20:12",
        "Description": "The current conversion from a Parquet type to an Avro schema doesn't preserve optional map types. This doesn't affect deserialization, so nullable map values will have a non-null Avro schema but may have null values.",
        "Issue Links": []
    },
    "PARQUET-192": {
        "Key": "PARQUET-192",
        "Summary": "Avro maps drop null values",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/Feb/15 00:18",
        "Updated": "07/Apr/15 20:47",
        "Resolved": "04/Mar/15 20:27",
        "Description": "The Avro serialization code for maps writes all keys and then all values without calling start and end to detect and handle null values. This causes the code to drop null values.",
        "Issue Links": []
    },
    "PARQUET-193": {
        "Key": "PARQUET-193",
        "Summary": "Avro: Implement read compatibility rules for nested types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/Feb/15 03:55",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "07/Mar/15 01:07",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-212"
        ]
    },
    "PARQUET-194": {
        "Key": "PARQUET-194",
        "Summary": "Provide callback to allow user defined key-value metadata merging strategy",
        "Type": "Improvement",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "26/Feb/15 05:20",
        "Updated": "04/Apr/15 17:36",
        "Resolved": null,
        "Description": "When merging footers, Parquet doesn't know how to merge conflicting user defined key-value metadata entries, and simply throws. It would be better to provide callbacks to let users define metadata merging strategies.\nFor example, in Spark SQL, we store our own schema information in Parquet files as key-value metadata (similar to parquet-avro). While trying to add schema merging support for reading Parquet files with different but compatible schemas, InitContext.getMergedKeyValueMetaData throws because we have different Spark SQL schemas stored in different Parquet data files. Thus, we have to overwrite ParquetInputFormat and merge the schema within getSplits, which is kinda hacky and inconvenient.",
        "Issue Links": [
            "/jira/browse/PARQUET-211"
        ]
    },
    "PARQUET-195": {
        "Key": "PARQUET-195",
        "Summary": "parquet-pig should not throw an exception reading a timestamp from impala file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "gpolaert",
        "Created": "27/Feb/15 08:16",
        "Updated": "11/Jan/18 21:51",
        "Resolved": null,
        "Description": "Hi, I need to load impala table with timestamp columns with pig. I think loading timestamp cols is still a NYI feature. Waiting for #218 TBD, pig-parquet should not throw an exception but at least should bind Int96 into a bytearray (users can write their own UDF to convert it into pig datetime type). \nIs there anyone working on that?",
        "Issue Links": []
    },
    "PARQUET-196": {
        "Key": "PARQUET-196",
        "Summary": "parquet-tools command to get rowcount & size",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Swapnil",
        "Created": "03/Mar/15 05:09",
        "Updated": "31/May/23 15:53",
        "Resolved": "12/May/17 22:02",
        "Description": "Parquet files contain metadata about rowcount & file size. We should have new commands to get rows count & size.\nThese command can be added in parquet-tools:\n1. rowcount : This should add number of rows in all footers to give total rows in data. \n2. size : This should give compresses size in bytes and human readable format too.\nThese command helps us to avoid parsing job logs or loading data once again to find number of rows in data. This comes very handy in complex processes, stats generation, QA etc..",
        "Issue Links": []
    },
    "PARQUET-197": {
        "Key": "PARQUET-197",
        "Summary": "parquet-cascading and the mapred API does not create metadata file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "03/Mar/15 23:27",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "13/Mar/15 20:37",
        "Description": "Repro: run a scalding job that writes parquet files to a folder. no _metadata and _common_metadata file is created\nImpact: potential performance problem if parquet metadata is read from client side, which is the case for sparkSQL\ncasue: the metatdata writing logic is in the mapreduce API but not the mapred API of parquet.",
        "Issue Links": [
            "/jira/browse/PARQUET-206"
        ]
    },
    "PARQUET-198": {
        "Key": "PARQUET-198",
        "Summary": "parquet-cascading Add Parquet Avro Scheme",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Antonios Chalkiopoulos",
        "Reporter": "Antonios Chalkiopoulos",
        "Created": "04/Mar/15 20:20",
        "Updated": "23/Apr/15 08:28",
        "Resolved": null,
        "Description": "At the moment using parquet-cascading writing Parquet files when using Avro as the object format is not possible as a Schema is not yet provided.\nAs part of this ticket, a ParquetAvroScheme will be introduced, and also a local.ParquetAvroScheme , so that from Cascading / Scalding developers can read / write and test in both --hdfs and --local mode Parquet-Avro files.",
        "Issue Links": []
    },
    "PARQUET-199": {
        "Key": "PARQUET-199",
        "Summary": "Add a callback when the MemoryManager adjusts row group size",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dong Chen",
        "Reporter": "Ryan Blue",
        "Created": "04/Mar/15 23:50",
        "Updated": "27/May/15 23:18",
        "Resolved": "27/May/15 23:18",
        "Description": "Parquet Hive would like to increment a counter when the row group size is altered by the memory manager so that Hive can detect when there are memory problems and inform the user. I think the right way to do this is to provide a callback that will be triggered when the memory manager hits its limit.",
        "Issue Links": [
            "/jira/browse/PARQUET-164",
            "https://github.com/apache/parquet-mr/pull/120"
        ]
    },
    "PARQUET-200": {
        "Key": "PARQUET-200",
        "Summary": "Add microsecond time and timestamp annotations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "05/Mar/15 00:02",
        "Updated": "16/Jun/15 16:57",
        "Resolved": "16/Jun/15 16:57",
        "Description": "When the date/time type annotations were added, we decided not to add precisions smaller than milliseconds because there wasn't a clear requirement. I think that the requirement is for nanosecond precision. The SQL spec requires at least microsecond. Some databases support nanosecond, including SQL engines on Hadoop like Phoenix. Hive and Impala currently support nanosecond times using an int96, but intend to move to microsecond precision with this spec.\nI propose adding the following type annotations:\n\nTIME_MICROS: annotates an int64 (8 bytes), represents the number of microseconds from midnight.\nTIMESTAMP_MICROS: annotates an int64 (8 bytes), represents the number of microseconds from the unix epoch.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/23"
        ]
    },
    "PARQUET-201": {
        "Key": "PARQUET-201",
        "Summary": "Column with OriginalType INT_8 failed at filtering",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Low Chin Wei",
        "Created": "05/Mar/15 06:45",
        "Updated": "05/Sep/19 07:55",
        "Resolved": "24/Jun/15 20:58",
        "Description": "I hit the following error when apply a filter predicate on a column with PrimitiveType = INT32 & OriginalType equal to INT_8.\njava.lang.IllegalArgumentException: FilterPredicate column: c's declared type (java.lang.Integer) does not match the schema found in file metadata. Column c is of type: FullTypeDescriptor(PrimitiveType: INT32, OriginalType: INT_8)\nValid types for this column are: null\n\tat parquet.filter2.predicate.ValidTypeMap.assertTypeValid(ValidTypeMap.java:114)",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "/jira/browse/PARQUET-247",
            "/jira/browse/SPARK-9407",
            "https://github.com/apache/parquet-mr/pull/219"
        ]
    },
    "PARQUET-202": {
        "Key": "PARQUET-202",
        "Summary": "Typo in the connection info in the pom prevents publishing an RC",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "06/Mar/15 02:25",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "06/Mar/15 02:26",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-203": {
        "Key": "PARQUET-203",
        "Summary": "Consolidate PathFilter for hidden files",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Neville Li",
        "Reporter": "Neville Li",
        "Created": "06/Mar/15 12:22",
        "Updated": "24/Mar/15 23:09",
        "Resolved": "24/Mar/15 23:08",
        "Description": "PathFilter is used in many places as anonymous classes for filtering out hidden files. It'd be nice to consolidate them into a single class.",
        "Issue Links": [
            "/jira/browse/PARQUET-204"
        ]
    },
    "PARQUET-204": {
        "Key": "PARQUET-204",
        "Summary": "Directory support for parquet-schema",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Neville Li",
        "Reporter": "Neville Li",
        "Created": "06/Mar/15 12:26",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "24/Mar/15 23:06",
        "Description": "Right now parquet-schema command in parquet-tools requires that the input to be a .parquet file and not directory. This is not most convenient since outputs are directories of part files.",
        "Issue Links": [
            "/jira/browse/PARQUET-203"
        ]
    },
    "PARQUET-205": {
        "Key": "PARQUET-205",
        "Summary": "Ignore _SUCCESS files in RAT plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Neville Li",
        "Created": "06/Mar/15 12:29",
        "Updated": "06/Mar/15 18:55",
        "Resolved": "06/Mar/15 18:55",
        "Description": "Each Hadoop output directory has an empty _SUCCESS file. Skipping them in Maven RAT plugin makes local development and testing easier.",
        "Issue Links": []
    },
    "PARQUET-206": {
        "Key": "PARQUET-206",
        "Summary": "MapredParquetOutputCommitter does not work in hadoop2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "06/Mar/15 21:03",
        "Updated": "24/Mar/15 23:25",
        "Resolved": "24/Mar/15 23:25",
        "Description": "https://github.com/apache/incubator-parquet-mr/pull/138",
        "Issue Links": [
            "/jira/browse/PARQUET-197"
        ]
    },
    "PARQUET-207": {
        "Key": "PARQUET-207",
        "Summary": "ParquetInputSplit end calculation bug",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "06/Mar/15 23:22",
        "Updated": "06/Mar/15 23:22",
        "Resolved": null,
        "Description": "The calculation for end of a split using the file metadata is broken by PARQUET-108. The calculation was updated to use the requested schema so that the end of a block would be the end of the last projected column. But the end logic actually calculates the total number of bytes that are selected.\nThe end of a split is only used to select row groups when a block has no row group offsets, which doesn't happen when the constructor that uses the broken method is called. However, this should still be removed.\nAfter 1.6.0, I want to move Hive to pass FileSplits directly rather than wrapping them in ParquetInputSplit. The internal reader code can handle mapping row groups to splits because it needs to for PARQUET-84.",
        "Issue Links": []
    },
    "PARQUET-208": {
        "Key": "PARQUET-208",
        "Summary": "revert PARQUET-197",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "06/Mar/15 23:27",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "07/Mar/15 00:39",
        "Description": "It's failing the hadoop 2 tests",
        "Issue Links": []
    },
    "PARQUET-209": {
        "Key": "PARQUET-209",
        "Summary": "Enhance ParquetWriter with exposing in-memory size of writer object",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Reza Shiftehfar",
        "Created": "07/Mar/15 01:58",
        "Updated": "07/Mar/15 02:05",
        "Resolved": null,
        "Description": "While using ParquetWriter and before closing it to write the content out to the disk, there is no way to check/estimate the size of the output file. This is useful in case we want to close files and upload them based on a minimum size threshold. Since ParquetWriter keeps everything in memory and only writes it out to disk at the very end when writer is closed, it is not possible to have an estimate of the output file size before closing the writer.\nBased on Parquet documentation, the data is written into memory object in the final format, meaning that the size of the object in memory is the very close to the final size on disk. it would be great if you can expose the current size of the parquetWriter object in memory. It is true that such a size will be different than the final output size because of adding the schema and other metadata at the end of the file but it still gives a close estimation of the output file size that will be very useful when reading/writing streams.",
        "Issue Links": []
    },
    "PARQUET-210": {
        "Key": "PARQUET-210",
        "Summary": "JSON output for parquet-cat",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Neville Li",
        "Reporter": "Neville Li",
        "Created": "09/Mar/15 13:06",
        "Updated": "31/Mar/15 23:36",
        "Resolved": "31/Mar/15 23:36",
        "Description": "Some of our internal users reported that right parquet-cat in parquet-tools prints one line per column and is not grep friendly. It'd be nice to support JSON as an output format. I have a patch and will submit soon.",
        "Issue Links": []
    },
    "PARQUET-211": {
        "Key": "PARQUET-211",
        "Summary": "Release parquet-mr 1.6.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Mar/15 20:06",
        "Updated": "17/Apr/15 00:51",
        "Resolved": "15/Apr/15 23:41",
        "Description": "Need to determine a list of tasks that should be done before release. Please add issues as sub-tasks.",
        "Issue Links": [
            "/jira/browse/PARQUET-194",
            "/jira/browse/PARQUET-235",
            "/jira/browse/PARQUET-236",
            "/jira/browse/PARQUET-242",
            "/jira/browse/PARQUET-214",
            "/jira/browse/PARQUET-234",
            "/jira/browse/PARQUET-237",
            "/jira/browse/PARQUET-239",
            "/jira/browse/PARQUET-165",
            "/jira/browse/PARQUET-230",
            "/jira/browse/HIVE-10372"
        ]
    },
    "PARQUET-212": {
        "Key": "PARQUET-212",
        "Summary": "Implement nested type read rules in parquet-thrift",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Mar/15 20:25",
        "Updated": "12/Jan/16 22:20",
        "Resolved": "12/Jan/16 22:19",
        "Description": "Need to implement the rules from PARQUET-113 in parquet-thrift. Similar to PARQUET-193, which tracked the read-side updates to parquet-avro.",
        "Issue Links": [
            "/jira/browse/PARQUET-113",
            "/jira/browse/PARQUET-364",
            "/jira/browse/PARQUET-193",
            "https://github.com/apache/parquet-mr/pull/144",
            "https://github.com/apache/parquet-mr/pull/300"
        ]
    },
    "PARQUET-213": {
        "Key": "PARQUET-213",
        "Summary": "Log the projected schema",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "09/Mar/15 22:22",
        "Updated": "09/Mar/15 22:23",
        "Resolved": null,
        "Description": "We should log the schema and describe what columns are being kept and which are being projected out in the projection API.\nThis will be useful for users wondering why some of their columns are missing.",
        "Issue Links": []
    },
    "PARQUET-214": {
        "Key": "PARQUET-214",
        "Summary": "Avro: Regression caused by schema handling",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Mar/15 23:18",
        "Updated": "31/Mar/15 23:50",
        "Resolved": "31/Mar/15 23:50",
        "Description": "Older version of parquet-avro evidently didn't correctly use the UTF8 annotation and converted a string to binary and stored the Avro schema in avro.schema.",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/142"
        ]
    },
    "PARQUET-215": {
        "Key": "PARQUET-215",
        "Summary": "Parquet Thrift should discard records with unrecognized union members",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "11/Mar/15 07:28",
        "Updated": "07/Apr/15 20:46",
        "Resolved": "12/Mar/15 21:25",
        "Description": "When writing parquet-thrift files, when a thrift record with an unknown union member is encountered, it should be considered a bad record and discarded. Currently, because unions are treated as structs with one optional field per union member, parquet-thrift happily writes the empty struct, but then crashes in the read path when trying to read this record.\nWe should discard these records in the write path, just as we discard other unparseable  records.",
        "Issue Links": []
    },
    "PARQUET-216": {
        "Key": "PARQUET-216",
        "Summary": "Decrease the default page size to 64k",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "12/Mar/15 23:48",
        "Updated": "17/Mar/15 05:09",
        "Resolved": null,
        "Description": "Impala uses 64k for a page size and recommends smaller page sizes over the current 1MB default. We're considering an absolute minimum row group size of 1MB, so it makes sense to dial the page size down a bit.",
        "Issue Links": []
    },
    "PARQUET-217": {
        "Key": "PARQUET-217",
        "Summary": "Memory Manager's min allocation heuristic is not valid for schemas with many columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "13/Mar/15 19:50",
        "Updated": "07/Apr/15 20:45",
        "Resolved": "13/Mar/15 19:55",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-218": {
        "Key": "PARQUET-218",
        "Summary": "Macro based Scala APIs for Predicate and Projection push down",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "16/Mar/15 17:22",
        "Updated": "18/Mar/15 05:33",
        "Resolved": null,
        "Description": "There has been some experiments with Scala macros to make Predicate and Projection APIs in Scala.\nIn particular by neville:\nhttps://github.com/nevillelyh/parquet-avro-extra/blob/master/parquet-avro-examples/src/main/scala/me/lyh/parquet/avro/examples/ExampleApp.scala\nhttps://github.com/nevillelyh/parquet-avro-extra/commit/090f8d1d1d7d44489bb71e04465d4a12a46432c9#diff-89dca6adcdf105c6063661de43931c29R15\nThere have also been some discussion at Twitter.",
        "Issue Links": []
    },
    "PARQUET-219": {
        "Key": "PARQUET-219",
        "Summary": "Add measurement of compression/decompression time",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "16/Mar/15 17:53",
        "Updated": "16/Mar/15 17:53",
        "Resolved": null,
        "Description": "As we compress/decompress a full page at a time, it is easy to measure with negligible overhead.\nWe should measure the time spent in compressing/decompressing and put that in a hadoop counter.\nThis way we have visibility in time spend compressing/decompressing.",
        "Issue Links": []
    },
    "PARQUET-220": {
        "Key": "PARQUET-220",
        "Summary": "Unnecessary warning in ParquetRecordReader.initialize",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.10.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "19/Mar/15 02:08",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "06/Dec/16 01:01",
        "Description": "When reading a parquet file using spark 1.3.0 lots of warnings are printed in the log:\n\nWARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n\n\nI have checked the source of ParquetRecordReader and found that while it checks for context to be TaskInputOutputContext it seems to never actually rely on this fact.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/280"
        ]
    },
    "PARQUET-221": {
        "Key": "PARQUET-221",
        "Summary": "For array type, inconsistent names are used as the array element name.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yin Huai",
        "Created": "19/Mar/15 04:57",
        "Updated": "19/Mar/15 18:22",
        "Resolved": null,
        "Description": "When creating a convert for an array, Parquet Avro uses \"array\" as the field name name (see here) , but Parquet Hive SerDe uses \"array_element\" as the field name see here. In Spark SQL, our native Parquet support is following Parquet Avro's convention, for data generated by Parquet Hive SerDe, the array value cannot be correctly read and null will be returned.",
        "Issue Links": [
            "/jira/browse/SPARK-5508"
        ]
    },
    "PARQUET-222": {
        "Key": "PARQUET-222",
        "Summary": "parquet writer runs into OOM during writing when calling DataFrame.saveAsParquetFile in Spark SQL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Chaozhong Yang",
        "Created": "19/Mar/15 07:16",
        "Updated": "12/Jun/15 17:41",
        "Resolved": "12/Jun/15 16:58",
        "Description": "In Spark SQL, there is a function saveAsParquetFile in DataFrame or SchemaRDD. That function calls method in parquet-mr, and sometimes it will fail due to the OOM error thrown by parquet-mr. We can see the exception stack trace  as follows:\n\nWARN] [task-result-getter-3] 03-19 11:17:58,274 [TaskSetManager] - Lost task 0.2 in stage 137.0 (TID 309, hb1.avoscloud.com): java.lang.OutOfMemoryError: Java heap space\n        at parquet.column.values.dictionary.IntList.initSlab(IntList.java:87)\n        at parquet.column.values.dictionary.IntList.<init>(IntList.java:83)\n        at parquet.column.values.dictionary.DictionaryValuesWriter.<init>(DictionaryValuesWriter.java:85)\n        at parquet.column.values.dictionary.DictionaryValuesWriter$PlainIntegerDictionaryValuesWriter.<init>(DictionaryValuesWriter.java:549)\n        at parquet.column.ParquetProperties.getValuesWriter(ParquetProperties.java:88)\n        at parquet.column.impl.ColumnWriterImpl.<init>(ColumnWriterImpl.java:74)\n        at parquet.column.impl.ColumnWriteStoreImpl.newMemColumn(ColumnWriteStoreImpl.java:68)\n        at parquet.column.impl.ColumnWriteStoreImpl.getColumnWriter(ColumnWriteStoreImpl.java:56)\n        at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.<init>(MessageColumnIO.java:178)\n        at parquet.io.MessageColumnIO.getRecordWriter(MessageColumnIO.java:369)\n        at parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:108)\n        at parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:94)\n        at parquet.hadoop.ParquetRecordWriter.<init>(ParquetRecordWriter.java:64)\n        at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:282)\n        at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)\n        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:304)\n        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)\n        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n        at org.apache.spark.scheduler.Task.run(Task.scala:56)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n\n\nBy the way, there is another similar issue https://issues.apache.org/jira/browse/PARQUET-99. But the reporter has closed it and mark it as resolved.",
        "Issue Links": []
    },
    "PARQUET-223": {
        "Key": "PARQUET-223",
        "Summary": "Add Map and List builiders",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Ashish Singh",
        "Created": "19/Mar/15 21:09",
        "Updated": "26/May/15 23:09",
        "Resolved": "26/May/15 23:09",
        "Description": "As of now, Parquet does not provide builders for Maps and Lists. This leaves margin for user errors. Having Map and List builders will make it easier for users to build these types.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/148"
        ]
    },
    "PARQUET-224": {
        "Key": "PARQUET-224",
        "Summary": "Implement writing Parquet files into Cassandra natively",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Issac Buenrostro",
        "Created": "23/Mar/15 18:39",
        "Updated": "03/Apr/15 00:07",
        "Resolved": null,
        "Description": "Writing Parquet files into Cassandra could allow parallel writes of multiple pages into different cells, and low latency reads with a persistent connection to C*.\nEach page could be written to separate C* cells, with metadata written into a separate column family.\nA possible way of implementing is:\n\nabstract ParquetFileWriter -> ParquetDataWriter. writeDictionaryPage, writeDataPage are abstract methods.\nParquetFileWriter implements ParquetDataWriter, writing the data to Hadoop compatible files.\nParquetCassandraWriter implements ParquetDataWriter, writing data to Cassandra\n\t\nfor each page, metadata is written to Metadata CF, with key <parquet-file-name>:<row-chunk>:<column>:<page>\nfor each page, data is written to Data CF, with key <parquet-file-name>:<row-chunk>:<column>:<page>\nfooter is written to Metadata CF, with key <parquet-file-name>\n\n\n\n\nabstract ParquetFileReader -> ParquetDataReader. readNextRowGroup, readFooter are abstract methods. Chunk will also need to be abstract.\nParquetFileReader implements ParquetDataReader, reading from Hadoop compatible files.\nParquetCassandraReader implements ParquetDataReader, reading from Cassandra\n\n\nParquetDataWriter and ParquetDataReader are instantiated through reflection.",
        "Issue Links": []
    },
    "PARQUET-225": {
        "Key": "PARQUET-225",
        "Summary": "INT64 support for Delta Encoding",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Vassil Lunchev",
        "Reporter": "Vassil Lunchev",
        "Created": "24/Mar/15 23:57",
        "Updated": "12/Dec/16 17:35",
        "Resolved": "21/Apr/16 18:44",
        "Description": "As of now, parquet doesn't support delta encoding for INT64. However it is planned in the format:\nhttps://github.com/Parquet/parquet-format/blob/master/Encodings.md\nThe benefits of this feature are huge. For timestamps it achieves twice better compression than SNAPPY on plain encoding, and the reading is faster. This feature is actually advertised on the home page of Parquet, even though it is not yet implemented:\nhttp://parquet.incubator.apache.org/\nhttp://image.slidesharecdn.com/hadoopsummit-140630160016-phpapp01/95/efficient-data-storage-for-analytics-with-apache-parquet-20-30-638.jpg?cb=1404162126",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/issues/154"
        ]
    },
    "PARQUET-226": {
        "Key": "PARQUET-226",
        "Summary": "Introduce an interface for controlling the encoding per column",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Boris Peltekov",
        "Created": "25/Mar/15 04:10",
        "Updated": "03/Apr/15 17:56",
        "Resolved": null,
        "Description": "There are use-cases in which it is advantageous to have control over the type of encoding used for a given column. To achieve this, the ParquetProperties class must be refactored to facilitate its subclassing. The constructors of ParquetWriter and (probably) InternalParquetRecordWriter have to be modified as well. All the internal parquet classes have to refrain from directly instantiating ParquetProperties and should accept the instance from outside. \nCurrently me and my team are using a good amount of reflection and unenforceable assumptions in order to gain control of this behavior. I'd like to implement the interface change. Are you interested in this feature?",
        "Issue Links": []
    },
    "PARQUET-227": {
        "Key": "PARQUET-227",
        "Summary": "Parquet thrift can write unions that have 0 or more than 1 set value",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "25/Mar/15 07:35",
        "Updated": "30/Apr/15 06:19",
        "Resolved": "30/Apr/15 06:19",
        "Description": "Having 0 or more than 1 set values means the data will be unreadable in the read path, this should not be allowed in the write path.",
        "Issue Links": [
            "/jira/browse/PARQUET-111"
        ]
    },
    "PARQUET-228": {
        "Key": "PARQUET-228",
        "Summary": "parquet Binary Writable result is not proper",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Manish Agarwal",
        "Created": "25/Mar/15 17:35",
        "Updated": "26/Mar/15 10:48",
        "Resolved": "26/Mar/15 10:48",
        "Description": "My table is having a binary column . I pushed 2 bytes it which were FF FF .\nvalues[i]= new BinaryWritable(Binary.fromByteArray(bytes,offset,length));\nWhen i do a print of Binary.fromByteArray(bytes,offset,length)  i get \nbinary is [Binary\n{2 bytes, [-1, -1]}\n]\nNow when I read the same from hive using \nhive -e \"select hex(binarycolumn) from mytable;\" > /data/myresultFile\nI get EFBFBDEFBFBD instead of FFFF . \nWhat am i doing wrong ?",
        "Issue Links": []
    },
    "PARQUET-229": {
        "Key": "PARQUET-229",
        "Summary": "Make an alternate, stricter thrift column projection API",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "25/Mar/15 21:21",
        "Updated": "01/May/15 00:45",
        "Resolved": "01/May/15 00:45",
        "Description": "See description here: https://github.com/apache/incubator-parquet-mr/pull/150",
        "Issue Links": []
    },
    "PARQUET-230": {
        "Key": "PARQUET-230",
        "Summary": "Add build instructions to the README",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "31/Mar/15 18:24",
        "Updated": "03/Apr/15 22:42",
        "Resolved": "03/Apr/15 22:42",
        "Description": "Need to add instructions to the README for building parquet-thrift and parquet-protobuf. Basically copy what we have Travis CI do in .travis.yml.",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/156"
        ]
    },
    "PARQUET-231": {
        "Key": "PARQUET-231",
        "Summary": "Parquet-hive build fails on debian and OSX yosemite",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Boris Peltekov",
        "Created": "01/Apr/15 22:24",
        "Updated": "02/Apr/15 01:21",
        "Resolved": null,
        "Description": "I am trying to compile the latest (1.6.0rc8 / git master > PARQUET-214: Fix Avro string regression.) version of parquet-mr. The required versions of protobuf (2.5.0) and thrift (0.7) are installed and working properly (parquet\n{protobuf,thrift}\n compile successfully). If I run mvn test, I get:\n[ERROR] /Users/peltekov/Documents/incubator-parquet-mr/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java:[46,20] cannot find symbol\n[ERROR] symbol: class HiveBinding\n[ERROR] location: package parquet.hive\n[ERROR] /Users/peltekov/Documents/incubator-parquet-mr/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java:[47,20] cannot find symbol\n[ERROR] symbol: class HiveBindingFactory\n[ERROR] location: package parquet.hive\n[ERROR] /Users/peltekov/Documents/incubator-parquet-mr/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java:[63,17] cannot find symbol\n[ERROR] symbol: class HiveBinding\n[ERROR] location: class org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper\n[ERROR] /Users/peltekov/Documents/incubator-parquet-mr/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java:[80,13] cannot find symbol\n[ERROR] symbol: class HiveBinding\n[ERROR] location: class org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper\nSince I have no experience with hive, I am running out of ideas quite quickly. Obviously the generated files in parquet.hive are not included in the classpath when the tests are runned.\nAlso if I run\nmvn clean install ; mvn test\nthen the tests are passed successfully.\nAnd btw mvn compile fails in the same way as mvn test.\nIs it a bug or I do something wrong?",
        "Issue Links": []
    },
    "PARQUET-232": {
        "Key": "PARQUET-232",
        "Summary": "minor compilation issue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Fabrizio Fabbri",
        "Reporter": "Fabrizio Fabbri",
        "Created": "02/Apr/15 16:47",
        "Updated": "02/Apr/15 22:59",
        "Resolved": "02/Apr/15 22:57",
        "Description": "I find out some very minor issue when I tried to compile the reader on my environment due to some namespace clashing. \nAs example shared_ptr and unordered_map are also in C++11 std namespace. Some compile don't like it.\nAlso I find that with my test files that I'm reading there was a dereference to a null pointer, if the field is required definition_level_decoder_ is null.\nI've make the fork and the change https://github.com/ffabbri4/incubator-parquet-cpp/tree/candidate",
        "Issue Links": []
    },
    "PARQUET-233": {
        "Key": "PARQUET-233",
        "Summary": "Update README with how to contribute",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Nong Li",
        "Reporter": "Nong Li",
        "Created": "02/Apr/15 23:05",
        "Updated": "02/Apr/15 23:05",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-234": {
        "Key": "PARQUET-234",
        "Summary": "Restore ParquetInputSplit methods from 1.5.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "03/Apr/15 04:44",
        "Updated": "07/Apr/15 20:16",
        "Resolved": "07/Apr/15 20:16",
        "Description": "Need to add back API methods on ParquetInputSplit that were present in 1.5.0",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/159"
        ]
    },
    "PARQUET-235": {
        "Key": "PARQUET-235",
        "Summary": "Fix compatibility of parquet.metadata with 1.5.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "03/Apr/15 04:47",
        "Updated": "07/Apr/15 20:15",
        "Resolved": "07/Apr/15 20:15",
        "Description": "There are quite a few incompatible changes to parquet.metadata (see below). It seems like it is part of the API because the metadata is exposed. Fixing it will be annoying because ColumnPath was removed entirely.\nIf this is public, then we need to fix compatibility.\nparquet.metadata changes\nClass parquet.hadoop.metadata.Canonicalizer\n Removed Class , access public super synchronized\nClass parquet.hadoop.metadata.ColumnChunkMetaData\n Added Method getPath, desc ()Lparquet/common/schema/ColumnPath;, access public\n Removed Method get, sig (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;Lparquet/column/statistics/Statistics;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, desc (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;Lparquet/column/statistics/Statistics;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, access public static\n Added Method get, sig (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, desc (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, access public static\n Added Method get, sig (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;Lparquet/column/statistics/Statistics;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, desc (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;Lparquet/column/statistics/Statistics;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, access public static\n Removed Method getPath, desc ()Lparquet/hadoop/metadata/ColumnPath;, access public\n Removed Method get, sig (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, desc (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;JJJJJ)Lparquet/hadoop/metadata/ColumnChunkMetaData;, access public static\nClass parquet.hadoop.metadata.ColumnChunkProperties\n Added Method getPath, desc ()Lparquet/common/schema/ColumnPath;, access public\n Removed Method getPath, desc ()Lparquet/hadoop/metadata/ColumnPath;, access public\n Removed Method get, sig (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;)Lparquet/hadoop/metadata/ColumnChunkProperties;, desc (Lparquet/hadoop/metadata/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;)Lparquet/hadoop/metadata/ColumnChunkProperties;, access public static\n Added Method get, sig (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set<Lparquet/column/Encoding;>;)Lparquet/hadoop/metadata/ColumnChunkProperties;, desc (Lparquet/common/schema/ColumnPath;Lparquet/schema/PrimitiveType$PrimitiveTypeName;Lparquet/hadoop/metadata/CompressionCodecName;Ljava/util/Set;)Lparquet/hadoop/metadata/ColumnChunkProperties;, access public static\nClass parquet.hadoop.metadata.ColumnPath\n Removed Class , access final public super synchronized",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/166"
        ]
    },
    "PARQUET-236": {
        "Key": "PARQUET-236",
        "Summary": "Check parquet-scrooge compatibility",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "03/Apr/15 04:48",
        "Updated": "07/Apr/15 20:17",
        "Resolved": "07/Apr/15 19:44",
        "Description": "ParquetScroogeScheme#sink and #isSink were removed. (ScroogeStructConverter had removals, but I consider it internal)",
        "Issue Links": [
            "/jira/browse/PARQUET-211"
        ]
    },
    "PARQUET-237": {
        "Key": "PARQUET-237",
        "Summary": "Check ParquetWriter constructor compatibility with 1.5.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "03/Apr/15 04:49",
        "Updated": "07/Apr/15 17:03",
        "Resolved": "07/Apr/15 17:03",
        "Description": "ParquetWriter constructors may have an incompatible change",
        "Issue Links": [
            "/jira/browse/PARQUET-211"
        ]
    },
    "PARQUET-238": {
        "Key": "PARQUET-238",
        "Summary": "Unable to Install C++ Driver - reference to 'share_ptr' is ambiguous",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Aaron Benz",
        "Created": "03/Apr/15 22:07",
        "Updated": "26/Jan/16 01:20",
        "Resolved": "26/Jan/16 01:20",
        "Description": "Install commands worked up until the make cmd\nAarons-MBP:parquet-cpp Aaron$ make\nScanning dependencies of target ThriftParquet\n[ 12%] Building CXX object generated/gen-cpp/CMakeFiles/ThriftParquet.dir/parquet_constants.cpp.o\n[ 25%] Building CXX object generated/gen-cpp/CMakeFiles/ThriftParquet.dir/parquet_types.cpp.o\nLinking CXX static library ../../build/libThriftParquet.a\n[ 25%] Built target ThriftParquet\nScanning dependencies of target Parquet\n[ 37%] Building CXX object src/CMakeFiles/Parquet.dir/parquet.cc.o\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:79:5: warning: variable 'value_byte_size'\n      is used uninitialized whenever switch default is taken [-Wsometimes-uninitialized]\n    default:\n    ^~~~~~~\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:94:46: note: uninitialized use occurs here\n  values_buffer_.resize(config_.batch_size * value_byte_size);\n                                             ^~~~~~~~~~~~~~~\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:59:22: note: initialize the variable\n      'value_byte_size' to silence this warning\n  int value_byte_size;\n                     ^\n                      = 0\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:167:37: error: reference to 'shared_ptr' is\n      ambiguous\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                    ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name\n      lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: \n      candidate found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:167:48: error: 'Decoder' does not refer to\n      a value\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                               ^\n/Users/Aaron/myProgs/parquet-cpp/src/encodings/encodings.h:27:7: note: declared here\nclass Decoder {\n      ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:169:11: error: use of undeclared identifier\n      'it'\n      if (it != decoders_.end()) {\n          ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:176:7: error: reference to 'shared_ptr' is\n      ambiguous\n      shared_ptr<Decoder> decoder(new DictionaryDecoder(schema_->type, &dictionary));\n      ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name\n      lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: \n      candidate found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:177:45: error: use of undeclared identifier\n      'decoder'; did you mean 'decoders_'?\n      decoders_[Encoding::RLE_DICTIONARY] = decoder;\n                                            ^~~~~~~\n                                            decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared\n      here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:177:43: error: no viable overloaded '='\n      decoders_[Encoding::RLE_DICTIONARY] = decoder;\n      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ ~~~~~~~\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:500:18: note: candidate function not\n      viable: no known conversion from 'boost::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<Decoder> >' to 'const boost::shared_ptr<parquet_cpp::Decoder>' for\n      1st argument\n    shared_ptr & operator=( shared_ptr const & r ) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:509:18: note: candidate template ignored:\n      could not match 'shared_ptr' against 'unordered_map'\n    shared_ptr & operator=(shared_ptr<Y> const & r) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:520:18: note: candidate template ignored:\n      could not match 'auto_ptr' against 'unordered_map'\n    shared_ptr & operator=( std::auto_ptr<Y> & r )\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:538:77: note: candidate template ignored:\n      substitution failure [with Ap =\n      boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >]: no type\n      named 'type' in\n      'boost::detail::sp_enable_if_auto_ptr<boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >,\n      boost::shared_ptr<parquet_cpp::Decoder> &>'\n    typename boost::detail::sp_enable_if_auto_ptr< Ap, shared_ptr & >::type operato...\n                                                                       ~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:202:37: error: reference to 'shared_ptr' is\n      ambiguous\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                    ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name\n      lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: \n      candidate found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:202:48: error: 'Decoder' does not refer to\n      a value\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                               ^\n/Users/Aaron/myProgs/parquet-cpp/src/encodings/encodings.h:27:7: note: declared here\nclass Decoder {\n      ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:204:11: error: use of undeclared identifier\n      'it'\n      if (it != decoders_.end()) {\n          ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:205:28: error: use of undeclared identifier\n      'it'\n        current_decoder_ = it->second.get();\n                           ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:209:13: error: reference to 'shared_ptr' is\n      ambiguous\n            shared_ptr<Decoder> decoder;\n            ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name\n      lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: \n      candidate found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:211:15: error: use of undeclared identifier\n      'decoder'; did you mean 'decoders_'?\n              decoder.reset(new BoolDecoder());\n              ^~~~~~~\n              decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared\n      here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:211:23: error: no member named 'reset' in\n      'boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n              decoder.reset(new BoolDecoder());\n              ~~~~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:213:15: error: use of undeclared identifier\n      'decoder'; did you mean 'decoders_'?\n              decoder.reset(new PlainDecoder(schema_->type));\n              ^~~~~~~\n              decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared\n      here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:213:23: error: no member named 'reset' in\n      'boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n              decoder.reset(new PlainDecoder(schema_->type));\n              ~~~~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:215:35: error: use of undeclared identifier\n      'decoder'; did you mean 'decoders_'?\n            decoders_[encoding] = decoder;\n                                  ^~~~~~~\n                                  decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared\n      here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:215:33: error: no viable overloaded '='\n            decoders_[encoding] = decoder;\n            ~~~~~~~~~~~~~~~~~~~ ^ ~~~~~~~\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:500:18: note: candidate function not\n      viable: no known conversion from 'boost::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<Decoder> >' to 'const boost::shared_ptr<parquet_cpp::Decoder>' for\n      1st argument\n    shared_ptr & operator=( shared_ptr const & r ) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:509:18: note: candidate template ignored:\n      could not match 'shared_ptr' against 'unordered_map'\n    shared_ptr & operator=(shared_ptr<Y> const & r) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:520:18: note: candidate template ignored:\n      could not match 'auto_ptr' against 'unordered_map'\n    shared_ptr & operator=( std::auto_ptr<Y> & r )\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:538:77: note: candidate template ignored:\n      substitution failure [with Ap =\n      boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >]: no type\n      named 'type' in\n      'boost::detail::sp_enable_if_auto_ptr<boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >,\n      boost::shared_ptr<parquet_cpp::Decoder> &>'\n    typename boost::detail::sp_enable_if_auto_ptr< Ap, shared_ptr & >::type operato...\n                                                                       ~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:216:32: error: use of undeclared identifier\n      'decoder'; did you mean 'decoders_'?\n            current_decoder_ = decoder.get();\n                               ^~~~~~~\n                               decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared\n      here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:216:40: error: no member named 'get' in\n      'boost::unordered::unordered_map<parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n            current_decoder_ = decoder.get();\n                               ~~~~~~~ ^\n1 warning and 19 errors generated.\nmake[2]: *** [src/CMakeFiles/Parquet.dir/parquet.cc.o] Error 1\nmake[1]: *** [src/CMakeFiles/Parquet.dir/all] Error 2\nmake: *** [all] Error 2\nAarons-MBP:parquet-cpp Aaron$ \nAarons-MBP:parquet-cpp Aaron$ git pull\nAlready up-to-date.\nAarons-MBP:parquet-cpp Aaron$ make\n[ 25%] Built target ThriftParquet\n[ 37%] Building CXX object src/CMakeFiles/Parquet.dir/parquet.cc.o\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:79:5: warning: variable 'value_byte_size' is used uninitialized whenever switch default is\n      taken [-Wsometimes-uninitialized]\n    default:\n    ^~~~~~~\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:94:46: note: uninitialized use occurs here\n  values_buffer_.resize(config_.batch_size * value_byte_size);\n                                             ^~~~~~~~~~~~~~~\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:59:22: note: initialize the variable 'value_byte_size' to silence this warning\n  int value_byte_size;\n                     ^\n                      = 0\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:167:37: error: reference to 'shared_ptr' is ambiguous\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                    ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: candidate\n      found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:167:48: error: 'Decoder' does not refer to a value\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                               ^\n/Users/Aaron/myProgs/parquet-cpp/src/encodings/encodings.h:27:7: note: declared here\nclass Decoder {\n      ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:169:11: error: use of undeclared identifier 'it'\n      if (it != decoders_.end()) {\n          ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:176:7: error: reference to 'shared_ptr' is ambiguous\n      shared_ptr<Decoder> decoder(new DictionaryDecoder(schema_->type, &dictionary));\n      ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: candidate\n      found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:177:45: error: use of undeclared identifier 'decoder'; did you mean 'decoders_'?\n      decoders_[Encoding::RLE_DICTIONARY] = decoder;\n                                            ^~~~~~~\n                                            decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:177:43: error: no viable overloaded '='\n      decoders_[Encoding::RLE_DICTIONARY] = decoder;\n      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^ ~~~~~~~\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:500:18: note: candidate function not viable: no known conversion from\n      'boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> >' to 'const boost::shared_ptr<parquet_cpp::Decoder>' for 1st\n      argument\n    shared_ptr & operator=( shared_ptr const & r ) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:509:18: note: candidate template ignored: could not match 'shared_ptr' against\n      'unordered_map'\n    shared_ptr & operator=(shared_ptr<Y> const & r) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:520:18: note: candidate template ignored: could not match 'auto_ptr' against\n      'unordered_map'\n    shared_ptr & operator=( std::auto_ptr<Y> & r )\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:538:77: note: candidate template ignored: substitution failure [with Ap =\n      boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder> > > >]: no type named 'type' in\n      'boost::detail::sp_enable_if_auto_ptr<boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>,\n      boost::hash<parquet::Encoding::type>, std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >, boost::shared_ptr<parquet_cpp::Decoder> &>'\n    typename boost::detail::sp_enable_if_auto_ptr< Ap, shared_ptr & >::type operator=( Ap r )\n                                                                       ~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:202:37: error: reference to 'shared_ptr' is ambiguous\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                    ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: candidate\n      found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:202:48: error: 'Decoder' does not refer to a value\n      unordered_map<Encoding::type, shared_ptr<Decoder> >::iterator it =\n                                               ^\n/Users/Aaron/myProgs/parquet-cpp/src/encodings/encodings.h:27:7: note: declared here\nclass Decoder {\n      ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:204:11: error: use of undeclared identifier 'it'\n      if (it != decoders_.end()) {\n          ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:205:28: error: use of undeclared identifier 'it'\n        current_decoder_ = it->second.get();\n                           ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:209:13: error: reference to 'shared_ptr' is ambiguous\n            shared_ptr<Decoder> decoder;\n            ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:717:36: note: candidate found by name lookup is 'boost::shared_ptr'\n    template<class Y> friend class shared_ptr;\n                                   ^\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/memory:3750:29: note: candidate\n      found by name lookup is 'std::__1::shared_ptr'\nclass _LIBCPP_TYPE_VIS_ONLY shared_ptr\n                            ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:211:15: error: use of undeclared identifier 'decoder'; did you mean 'decoders_'?\n              decoder.reset(new BoolDecoder());\n              ^~~~~~~\n              decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:211:23: error: no member named 'reset' in\n      'boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>,\n      boost::hash<parquet::Encoding::type>, std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n              decoder.reset(new BoolDecoder());\n              ~~~~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:213:15: error: use of undeclared identifier 'decoder'; did you mean 'decoders_'?\n              decoder.reset(new PlainDecoder(schema_->type));\n              ^~~~~~~\n              decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:213:23: error: no member named 'reset' in\n      'boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>,\n      boost::hash<parquet::Encoding::type>, std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n              decoder.reset(new PlainDecoder(schema_->type));\n              ~~~~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:215:35: error: use of undeclared identifier 'decoder'; did you mean 'decoders_'?\n            decoders_[encoding] = decoder;\n                                  ^~~~~~~\n                                  decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:215:33: error: no viable overloaded '='\n            decoders_[encoding] = decoder;\n            ~~~~~~~~~~~~~~~~~~~ ^ ~~~~~~~\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:500:18: note: candidate function not viable: no known conversion from\n      'boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> >' to 'const boost::shared_ptr<parquet_cpp::Decoder>' for 1st\n      argument\n    shared_ptr & operator=( shared_ptr const & r ) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:509:18: note: candidate template ignored: could not match 'shared_ptr' against\n      'unordered_map'\n    shared_ptr & operator=(shared_ptr<Y> const & r) BOOST_NOEXCEPT\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:520:18: note: candidate template ignored: could not match 'auto_ptr' against\n      'unordered_map'\n    shared_ptr & operator=( std::auto_ptr<Y> & r )\n                 ^\n/usr/local/include/boost/smart_ptr/shared_ptr.hpp:538:77: note: candidate template ignored: substitution failure [with Ap =\n      boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>, boost::hash<parquet::Encoding::type>,\n      std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const parquet::Encoding::type,\n      boost::shared_ptr<parquet_cpp::Decoder> > > >]: no type named 'type' in\n      'boost::detail::sp_enable_if_auto_ptr<boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>,\n      boost::hash<parquet::Encoding::type>, std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >, boost::shared_ptr<parquet_cpp::Decoder> &>'\n    typename boost::detail::sp_enable_if_auto_ptr< Ap, shared_ptr & >::type operator=( Ap r )\n                                                                       ~~~~ ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:216:32: error: use of undeclared identifier 'decoder'; did you mean 'decoders_'?\n            current_decoder_ = decoder.get();\n                               ^~~~~~~\n                               decoders_\n/Users/Aaron/myProgs/parquet-cpp/src/parquet/parquet.h:152:78: note: 'decoders_' declared here\n  boost::unordered_map<parquet::Encoding::type, boost::shared_ptr<Decoder> > decoders_;\n                                                                             ^\n/Users/Aaron/myProgs/parquet-cpp/src/parquet.cc:216:40: error: no member named 'get' in\n      'boost::unordered::unordered_map<parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder>,\n      boost::hash<parquet::Encoding::type>, std::_1::equal_to<parquet::Encoding::type>, std::1::allocator<std::_1::pair<const\n      parquet::Encoding::type, boost::shared_ptr<parquet_cpp::Decoder> > > >'\n            current_decoder_ = decoder.get();",
        "Issue Links": []
    },
    "PARQUET-239": {
        "Key": "PARQUET-239",
        "Summary": "Make AvroParquetReader#builder() static",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "03/Apr/15 22:44",
        "Updated": "07/Apr/15 20:43",
        "Resolved": "07/Apr/15 20:43",
        "Description": "The new way to get an `AvroParquetReader` instance is to use the builder, but the `builder` factory method was accidentally added as an instance method rather than a static method.",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/158"
        ]
    },
    "PARQUET-240": {
        "Key": "PARQUET-240",
        "Summary": "Typo in LIST logical type description",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Colin Marc",
        "Reporter": "Colin Marc",
        "Created": "04/Apr/15 12:12",
        "Updated": "04/Apr/15 17:41",
        "Resolved": "04/Apr/15 17:41",
        "Description": "The description mistakenly refers to the repeated element being named \"array\" instead of \"list\" in the spec, in a single place.\nPR at https://github.com/apache/incubator-parquet-format/pull/25.",
        "Issue Links": []
    },
    "PARQUET-241": {
        "Key": "PARQUET-241",
        "Summary": "ParquetInputFormat.getFooters() should return in the same order as what listStatus() returns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Mingyu Kim",
        "Reporter": "Mingyu Kim",
        "Created": "04/Apr/15 23:27",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "29/Oct/15 22:45",
        "Description": "Because of how the footer cache is implemented, getFooters() returns the footers in a different order than what listStatus() returns.\nWhen I provided url \"hdfs://.../part-00001.parquet,hdfs://.../part-00002.parquet,hdfs://.../part-00003.parquet\", ParquetInputFormat.getSplits(), which internally calls getFooters(), returned the splits in a wrong order.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/164"
        ]
    },
    "PARQUET-242": {
        "Key": "PARQUET-242",
        "Summary": "AvroReadSupport.setAvroDataSupplier is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "06/Apr/15 17:47",
        "Updated": "07/Apr/15 16:44",
        "Resolved": "07/Apr/15 16:44",
        "Description": "The convenience method to set the data supplier uses Class#toString instead of Class#getName, which sets a String that can't be loaded by Configuration#getClass.",
        "Issue Links": [
            "/jira/browse/PARQUET-211",
            "https://github.com/apache/incubator-parquet-mr/pull/161"
        ]
    },
    "PARQUET-243": {
        "Key": "PARQUET-243",
        "Summary": "Add avro-reflect support",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "06/Apr/15 21:49",
        "Updated": "18/May/15 17:10",
        "Resolved": "18/May/15 17:10",
        "Description": "Parquet's Avro object model currently supports only specific (generated from Avro schemas) and generic objects, but Avro has support to work with any object through reflection.",
        "Issue Links": [
            "/jira/browse/PARQUET-286",
            "https://github.com/apache/parquet-mr/pull/165"
        ]
    },
    "PARQUET-244": {
        "Key": "PARQUET-244",
        "Summary": "DeltaByteArrayReader fails with ArrayIndexOutOfBoundsException when moving across pages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alosh Bennett",
        "Created": "07/Apr/15 12:22",
        "Updated": "17/Jun/15 16:18",
        "Resolved": "17/Jun/15 16:18",
        "Description": "DeltaByteArrayReader.readBytes() fails with ArrayIndexOutOfBoundsException soon after it has processed a new page via initFromPage(). This issue can be reproduced by trying to read a Binary column that is encoded using delta byte array and spans multiple pages.\nThis is happening because ColumnReaderImpl.initDataReader() creates a new ValueReader every time a new page is processed (see this.dataColumn = dataEncoding.getValuesReader(path, VALUES)). The DeltaByteArrayReader is stateful and needs to remember the previous Binary value that was read across pages. When a new DeltaByteArrayReader is created, this information is lost.",
        "Issue Links": [
            "/jira/browse/PARQUET-246"
        ]
    },
    "PARQUET-245": {
        "Key": "PARQUET-245",
        "Summary": "Travis CI runs tests even if build fails",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "07/Apr/15 20:23",
        "Updated": "28/Apr/15 21:40",
        "Resolved": "28/Apr/15 21:40",
        "Description": "The Travis CI config contains:\n.travis.yml\ninstall: mvn install ... > mvn_install.log || cat mvn_install.log\n\n\nThis always returns true because cat succeeds, so the tests aren't aborted by the maven return value. This should be: mvn install || ( cat log && false )",
        "Issue Links": [
            "https://github.com/apache/incubator-parquet-mr/pull/167"
        ]
    },
    "PARQUET-246": {
        "Key": "PARQUET-246",
        "Summary": "ArrayIndexOutOfBoundsException with Parquet write version v2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Konstantin Shaposhnikov",
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "08/Apr/15 07:49",
        "Updated": "09/Jul/15 17:52",
        "Resolved": "09/Jul/15 17:36",
        "Description": "I am getting the following exception when reading a parquet file that was created using Avro WriteSupport and Parquet write version v2.0:\n\nCaused by: parquet.io.ParquetDecodingException: Can't read value in column [colName, rows, array, name] BINARY at value 313601 out of 428260, 1 out of 39200 in currentPage. repetition level: 0, definition level: 2\n\tat parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:462)\n\tat parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:364)\n\tat parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:405)\n\tat parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:209)\n\t... 27 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat parquet.column.values.deltastrings.DeltaByteArrayReader.readBytes(DeltaByteArrayReader.java:70)\n\tat parquet.column.impl.ColumnReaderImpl$2$6.read(ColumnReaderImpl.java:307)\n\tat parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:458)\n\t... 30 more\n\n\n\nThe file is quite big (500Mb) so I cannot upload it here, but possibly there is enough information in the exception message to understand the cause of error.",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "/jira/browse/PARQUET-244",
            "https://github.com/apache/parquet-mr/issues/171",
            "https://github.com/apache/parquet-mr/pull/235"
        ]
    },
    "PARQUET-247": {
        "Key": "PARQUET-247",
        "Summary": "Add DATE mapping in ValidTypeMap of filter2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Dong Chen",
        "Reporter": "Dong Chen",
        "Created": "08/Apr/15 08:19",
        "Updated": "05/Sep/19 07:55",
        "Resolved": "05/Sep/19 07:55",
        "Description": "When Hive use Parquet filter predicate, the Date type is converted to Integer. In ValidTypeMap, it map the class and Parquet type. It throw exception when checking the data type Date.\nWe should add the map to support Date.",
        "Issue Links": [
            "/jira/browse/HIVE-10253",
            "/jira/browse/HIVE-10255",
            "/jira/browse/PARQUET-201",
            "https://github.com/apache/parquet-mr/pull/169"
        ]
    },
    "PARQUET-248": {
        "Key": "PARQUET-248",
        "Summary": "Simplify ParquetWriters's constructors",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "09/Apr/15 01:49",
        "Updated": "25/Jun/15 16:41",
        "Resolved": "25/Jun/15 16:41",
        "Description": "ParquetWriter has a lot of constructors. A builder pattern can be used to simplify construction of ParquetWriter objects (similar to ParquetReader, see PARQUET-39).\nParquetWriter subclasses (like AvroParquetWriter) should be updated to provide reasonable builder() static factory method.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/199"
        ]
    },
    "PARQUET-249": {
        "Key": "PARQUET-249",
        "Summary": "Parquet does not compile with -Phadoop-2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tony Reix",
        "Created": "10/Apr/15 15:15",
        "Updated": "23/Apr/15 15:02",
        "Resolved": "23/Apr/15 15:02",
        "Description": "When running:\nmvn clean install -l ../mvn.CleanInstallTest.res -fn ; mvn test -l ../mvn.Test.res6 -fn -X\nI get 510 tests run and 0 error and 0 failure.\nIf now I do (adding -Phadoop-2) :\nmvn clean install -l ../mvn.CleanInstallTest.res -fn -Phadoop-2\nParquet no more compiles. With errors:\n[INFO] \u2014 maven-compiler-plugin:3.1:testCompile (default-testCompile) @ parquet-common \u2014\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 3 source files to /home/reixt/PARQUET/APACHE/incubator-parquet-mr/parquet-common/target/test-classes\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] /home/reixt/PARQUET/APACHE/incubator-parquet-mr/parquet-common/src/test/java/parquet/bytes/TestBytesUtil.java:[22,28] cannot find symbol\n  symbol:   class BytesUtils\n  location: package parquet.bytes\n[ERROR] /home/reixt/PARQUET/APACHE/incubator-parquet-mr/parquet-common/src/test/java/parquet/bytes/TestBytesUtil.java:[22,1] static import only from classes and interfaces\n[ERROR] /home/reixt/PARQUET/APACHE/incubator-parquet-mr/parquet-common/src/test/java/parquet/bytes/TestBytesUtil.java:[30,21] cannot find symbol\n  symbol:   method getWidthFromMaxInt(int)\nAm I wrong ? or is there an issue with Parquet 1.6.0 ?\nI did:\n$ git clone https://github.com/apache/incubator-parquet-mr.git\n$ git status\n\nOn branch master\n$ mvn clean install -l ../mvn.CleanInstallTest.res-Phadoop-2 -fn -Phadoop-2\nNo changes in source code.\n\nI've recreated my environment on x86_64 to check and to be sure. The same.\nDoes it mean that Parquet is not compatible with Hadoop 2 ?\nOr an issue in pom.xml code ?",
        "Issue Links": []
    },
    "PARQUET-250": {
        "Key": "PARQUET-250",
        "Summary": "StatisticsFilter \"And\"'s semantic error",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yijie Shen",
        "Created": "11/Apr/15 11:51",
        "Updated": "12/Apr/15 08:30",
        "Resolved": "12/Apr/15 08:30",
        "Description": "public Boolean visit(And and) {\n    return and.getLeft().accept(this) && and.getRight().accept(this);\n  }\n\n\nvisit(And and) is used to determine whether a row group \"canDrop\", therefore, in \"And\" situation, either left \"canDrop\" or right \"canDrop\" would lead the row group \"canDrop\", so we need a \"||\" here to make the choice.\nTherefore, it should be:\n\npublic Boolean visit(And and) {\n    return and.getLeft().accept(this) || and.getRight().accept(this);\n  }",
        "Issue Links": [
            "/jira/browse/PARQUET-173"
        ]
    },
    "PARQUET-251": {
        "Key": "PARQUET-251",
        "Summary": "Binary column statistics error when reuse byte[] among rows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Yijie Shen",
        "Created": "13/Apr/15 01:11",
        "Updated": "07/Jul/15 16:02",
        "Resolved": "01/Jul/15 01:34",
        "Description": "I think it is a common practice when inserting table data as parquet file, one would always reuse the same object among rows, and if a column is byte[] of fixed length, the byte[] would also be reused. \nIf I use ByteArrayBackedBinary for my byte[], the bug occurs: All of the row groups created by a single task would have the same max & min binary value, just as the last row's binary content.\nThe reason is BinaryStatistic just keep max & min as parquet.io.api.Binary references, since I use ByteArrayBackedBinary for byte[], the real content of max & min would always point to the reused byte[], therefore the latest row's content.\nDoes parquet declare somewhere that the user shouldn't reuse byte[] for Binary type?  If it doesn't, I think it's a bug and can be reproduced by Spark SQL's RowWriteSupport \nThe related Spark JIRA ticket: SPARK-6859",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "/jira/browse/PARQUET-77",
            "/jira/browse/PARQUET-326",
            "/jira/browse/PARQUET-258",
            "https://github.com/apache/parquet-mr/pull/197"
        ]
    },
    "PARQUET-252": {
        "Key": "PARQUET-252",
        "Summary": "parquet scrooge support should support nested container type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "14/Apr/15 22:59",
        "Updated": "04/May/15 19:09",
        "Resolved": "04/May/15 19:09",
        "Description": "parquet should support nested container type for scrooge, like list<list> or list<map>",
        "Issue Links": []
    },
    "PARQUET-253": {
        "Key": "PARQUET-253",
        "Summary": "AvroSchemaConverter has confusing Javadoc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "15/Apr/15 00:42",
        "Updated": "15/May/15 19:44",
        "Resolved": "15/May/15 19:42",
        "Description": "Got confused by the original Javadoc at first and didn't realize AvroSchemaConverter is also capable to convert a Parquet schema to an Avro schema.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/173"
        ]
    },
    "PARQUET-254": {
        "Key": "PARQUET-254",
        "Summary": "Wrong exception message for unsupported INT96 type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "15/Apr/15 00:46",
        "Updated": "15/May/15 19:43",
        "Resolved": "15/May/15 19:43",
        "Description": "In AvroSchemaConverter, the message says \"INT64 not yet implemented\".",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/174"
        ]
    },
    "PARQUET-255": {
        "Key": "PARQUET-255",
        "Summary": "Typo in decimal type specification",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "15/Apr/15 00:58",
        "Updated": "16/Oct/17 23:56",
        "Resolved": "24/Feb/16 09:41",
        "Description": "The original document says:\n\nint32: for 1 <= precision <= 9\nint64: for 1 <= precision <= 18; precision <= 10 will produce a warning\n...\n\nFor int64, the warning should be produced when precision < 10 (rather than <= 10).",
        "Issue Links": []
    },
    "PARQUET-256": {
        "Key": "PARQUET-256",
        "Summary": "Deprecate ConversionPatterns",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "15/Apr/15 16:31",
        "Updated": "13/Jan/16 18:43",
        "Resolved": null,
        "Description": "Methods in ConversionPatterns doesn't conform to standard LIST and MAP  schema, and should be deprecated. We can either suggest users to use Types builder methods or create new wrapper methods for LIST and MAP.",
        "Issue Links": []
    },
    "PARQUET-257": {
        "Key": "PARQUET-257",
        "Summary": "Download documentation references non-existent \"2.1.0\" version of com.twitter:parquet",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Joshua Hyde",
        "Created": "20/Apr/15 14:44",
        "Updated": "04/Feb/19 20:47",
        "Resolved": "04/Feb/19 20:47",
        "Description": "The documentation for including Parquet as a Maven-managed dependency references a non-existent \"2.1.0\" version - perhaps the \"1.2.0\" version got accidentally transposed?\nhttps://parquet.incubator.apache.org/downloads/",
        "Issue Links": []
    },
    "PARQUET-258": {
        "Key": "PARQUET-258",
        "Summary": "Binary statistics is not updated correctly if an underlying Binary array is modified in place",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "22/Apr/15 15:14",
        "Updated": "28/Apr/15 00:50",
        "Resolved": "28/Apr/15 00:50",
        "Description": "The following test case shows the problem:\n\n    byte[] bytes = new byte[] { 49 };\n    BinaryStatistics reusableStats =  new BinaryStatistics();\n    reusableStats.updateStats(Binary.fromByteArray(bytes));\n    bytes[0] = 50;\n    reusableStats.updateStats(Binary.fromByteArray(bytes, 0, 1));\n \n    assertArrayEquals(new byte[] { 49 }, reusableStats.getMinBytes());\n    assertArrayEquals(new byte[] { 50 }, reusableStats.getMaxBytes());\n\n\nI discovered the bug when converting an AVRO file to a Parquet file by reading GenericRecords from a file using DataFileStream.next(D reuse) method. The problem is that underlying byte array of avro Utf8 object is passed to parquet that saves it as part of BinaryStatistics and then the same array is modified in place on the next read.\nI am not sure what is the right way to fix the problem (in BinaryStatistics or AvroWriteSupport).\nIf BinaryStatistics implementation is correct (for performance reasons) then this behavior should be documented and AvroWriteSupport.fromAvroString should be fixed to duplicate underlying Utf8 array.\nI am happy to create a pull request once the desired way to fix the issue is discussed.",
        "Issue Links": [
            "/jira/browse/PARQUET-251"
        ]
    },
    "PARQUET-259": {
        "Key": "PARQUET-259",
        "Summary": "Support Travis CI in parquet-cpp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kalon Mills",
        "Reporter": "Kalon Mills",
        "Created": "22/Apr/15 17:29",
        "Updated": "07/May/15 11:50",
        "Resolved": "07/May/15 11:49",
        "Description": "Having a continuous build env helps ensure that pull requests compile and pass tests.  It provides valuable feedback for ensuring various environments support desired changes.\nPull request that gets Travis CI - GitHub integration up and running for parquet-cpp:\nhttps://github.com/apache/parquet-cpp/pull/9",
        "Issue Links": []
    },
    "PARQUET-260": {
        "Key": "PARQUET-260",
        "Summary": "Graduation Checklist",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jake Farrell",
        "Reporter": "Jake Farrell",
        "Created": "22/Apr/15 20:38",
        "Updated": "23/Apr/15 13:02",
        "Resolved": "23/Apr/15 13:02",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-261": {
        "Key": "PARQUET-261",
        "Summary": "Classes fail to load when not packaged with Parquet.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alex Robbins",
        "Created": "27/Apr/15 18:41",
        "Updated": "28/Apr/15 19:19",
        "Resolved": null,
        "Description": "Parquet cannot dynamically load classes from another package. This has already been noticed and addressed here: https://github.com/Parquet/parquet-mr/pull/289\nHowever, the fix didn't address all occurrences of Class.forName. parquet-cascading jobs fail trying to load cascading.scheme.Scheme, even though it is present on the classpath.\nThis bug shows up in cloudera distributions of parquet since they provide parquet jars on the hadoop classpath.",
        "Issue Links": []
    },
    "PARQUET-262": {
        "Key": "PARQUET-262",
        "Summary": "When 1.7.0 is released, restore semver plugin config",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "28/Apr/15 21:47",
        "Updated": "21/May/15 23:19",
        "Resolved": "21/May/15 23:19",
        "Description": "In order to rename everything for the 1.7.0 release, we had to turn off semver checking because it is an incompatible change. This is a reminder to turn it back on once 1.7.0 is released to validate changes going into 1.8.0.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/200"
        ]
    },
    "PARQUET-263": {
        "Key": "PARQUET-263",
        "Summary": "Release apache-parquet-1.7.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.7.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "28/Apr/15 21:50",
        "Updated": "27/May/15 23:00",
        "Resolved": "19/May/15 21:36",
        "Description": "Need to release 1.7.0. The rename is done, we should just need to switch over to Apache release infrastructure. See:\n\nhttps://github.com/apache/parquet-format/commit/c3afbd22\nhttps://github.com/apache/parquet-format/commit/26b2c68f",
        "Issue Links": []
    },
    "PARQUET-264": {
        "Key": "PARQUET-264",
        "Summary": "Update README docs for graduation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Ryan Blue",
        "Created": "28/Apr/15 21:52",
        "Updated": "11/Jul/15 23:33",
        "Resolved": "18/Jun/15 18:35",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "/jira/browse/PARQUET-274",
            "https://github.com/apache/parquet-mr/pull/216"
        ]
    },
    "PARQUET-265": {
        "Key": "PARQUET-265",
        "Summary": "Update build for graduation from incubator",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "29/Apr/15 00:03",
        "Updated": "17/Jun/15 23:13",
        "Resolved": "17/Jun/15 23:13",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "https://github.com/apache/parquet-mr/pull/196"
        ]
    },
    "PARQUET-266": {
        "Key": "PARQUET-266",
        "Summary": "Add support for lists of primitives to Pig schema converter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Christian Rolf",
        "Created": "29/Apr/15 08:45",
        "Updated": "05/Jun/15 17:35",
        "Resolved": "05/Jun/15 17:33",
        "Description": "Right now lists of primitives are not supported in Pig (exception thrown from the PigSchemaConverter.java, line 292 in Parquet 1.6). \nPatch converts Parquet-arrays of primitives into Pig-bags, the closest representation of an array in Pig.",
        "Issue Links": []
    },
    "PARQUET-267": {
        "Key": "PARQUET-267",
        "Summary": "Detach thirdparty code from build configuration.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kalon Mills",
        "Reporter": "Kalon Mills",
        "Created": "29/Apr/15 16:18",
        "Updated": "19/Jan/16 22:06",
        "Resolved": "19/Jan/16 22:04",
        "Description": "The existing repo has source code for third party dependencies checked into the repo.  The build system expects those dependencies in a certain place.  This enforces that the built library conform to those exact dependencies without customization.\nManaging third party dependencies is better handled through a build environment.  It allows the library builder more flexibility over dependency versions and locations.  It also cleans up the repo from this third party code.",
        "Issue Links": [
            "/jira/browse/PARQUET-416"
        ]
    },
    "PARQUET-268": {
        "Key": "PARQUET-268",
        "Summary": "Build is failing with parquet-scrooge errors.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.6.1",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "29/Apr/15 19:46",
        "Updated": "30/Apr/15 00:47",
        "Resolved": "30/Apr/15 00:47",
        "Description": "The build is currently failing for all PRs in Travis CI. According to Alex:\n. . . one of the scrooge dependencies transitively pulled in a snapshot that has since been purged. Seems like that dependency was improperly published. Upgrading the scrooge plugin should fix this.",
        "Issue Links": [
            "/jira/browse/PARQUET-269",
            "https://github.com/apache/parquet-mr/pull/187"
        ]
    },
    "PARQUET-269": {
        "Key": "PARQUET-269",
        "Summary": "Restore scrooge-maven-plugin to 3.17.0 or greater",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "29/Apr/15 21:06",
        "Updated": "30/Apr/15 23:59",
        "Resolved": "30/Apr/15 23:59",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-268"
        ]
    },
    "PARQUET-270": {
        "Key": "PARQUET-270",
        "Summary": "Add legend to parquet-tools readme.md",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7.0",
        "Component/s": "parquet-mr",
        "Assignee": "Brett Stime",
        "Reporter": "Brett Stime",
        "Created": "29/Apr/15 21:46",
        "Updated": "30/Apr/15 00:55",
        "Resolved": "30/Apr/15 00:55",
        "Description": "Improve the documentation for parquet-tools by describing the output in more detail.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/178"
        ]
    },
    "PARQUET-271": {
        "Key": "PARQUET-271",
        "Summary": "Fix -jar switch in parquet-tools examples",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7.0",
        "Component/s": "parquet-mr",
        "Assignee": "Brett Stime",
        "Reporter": "Brett Stime",
        "Created": "29/Apr/15 21:51",
        "Updated": "30/Apr/15 00:57",
        "Resolved": "30/Apr/15 00:57",
        "Description": "At https://github.com/apache/parquet-mr/tree/master/parquet-tools, it says you can run the tools via:\n    java jar\n...but it should say:\n    java -jar",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/177"
        ]
    },
    "PARQUET-272": {
        "Key": "PARQUET-272",
        "Summary": "Updates docs decscription to match data model",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ben Pence",
        "Reporter": "Ben Pence",
        "Created": "06/May/15 17:25",
        "Updated": "16/May/15 20:17",
        "Resolved": "06/May/15 23:34",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-273": {
        "Key": "PARQUET-273",
        "Summary": "should remove usage of ReflectiveOperationException to support java6",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tim",
        "Created": "06/May/15 18:48",
        "Updated": "04/Feb/19 20:55",
        "Resolved": "04/Feb/19 20:55",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-274": {
        "Key": "PARQUET-274",
        "Summary": "Updates URLs to link against the apache user instead of Parquet on github",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Ben Pence",
        "Reporter": "Ben Pence",
        "Created": "06/May/15 19:07",
        "Updated": "15/May/15 19:50",
        "Resolved": "15/May/15 19:50",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-264",
            "https://github.com/apache/parquet-mr/pull/192"
        ]
    },
    "PARQUET-275": {
        "Key": "PARQUET-275",
        "Summary": "It should not be possible to project away columns of a key in a map in parquet-thrift",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "06/May/15 21:07",
        "Updated": "15/May/15 07:51",
        "Resolved": null,
        "Description": "That wouldn't make much sense when re-assembling the map, there could be unexpected key collisions",
        "Issue Links": []
    },
    "PARQUET-276": {
        "Key": "PARQUET-276",
        "Summary": "Updates CONTRIBUTING file with new repo info",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Ben Pence",
        "Reporter": "Ben Pence",
        "Created": "07/May/15 16:55",
        "Updated": "16/May/15 20:16",
        "Resolved": "07/May/15 21:17",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-277": {
        "Key": "PARQUET-277",
        "Summary": "Remove boost dependency",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Hyunsik Choi",
        "Created": "08/May/15 07:26",
        "Updated": "16/Feb/16 00:20",
        "Resolved": "16/Feb/16 00:20",
        "Description": "At a glance, parquet-cpp slightly uses boost dependency. It seems to be possible to remove boost dependency if we use C++11 feature.\nIf we remove boost dependency, parquet-cpp can be more portable and lightweight. Also, C++11 would allow us to modernize C++ codes.",
        "Issue Links": []
    },
    "PARQUET-278": {
        "Key": "PARQUET-278",
        "Summary": "enforce non empty group on MessageType level",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "11/May/15 18:04",
        "Updated": "05/Feb/18 19:41",
        "Resolved": "15/May/15 20:50",
        "Description": "As columnar format, parquet currently does not support empty struct/group without leaves. We should throw when constructing an empty GroupType to give a clear message.",
        "Issue Links": [
            "/jira/browse/PARQUET-363",
            "/jira/browse/HIVE-11611",
            "https://github.com/apache/drill/pull/1111",
            "https://github.com/apache/parquet-mr/pull/195"
        ]
    },
    "PARQUET-279": {
        "Key": "PARQUET-279",
        "Summary": "Check empty struct in the CompatibilityChecker util",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tim",
        "Created": "11/May/15 18:07",
        "Updated": "17/Jul/15 19:54",
        "Resolved": "17/Jul/15 19:54",
        "Description": "Add the empty struct check in the CompatibilityChecker util.\nParquet currently does not support empty struct/group without leaves.\nSame goal as in https://issues.apache.org/jira/browse/PARQUET-278",
        "Issue Links": []
    },
    "PARQUET-280": {
        "Key": "PARQUET-280",
        "Summary": "Please create a DOAP file for your TLP",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Sebb",
        "Created": "12/May/15 01:03",
        "Updated": "14/May/15 23:22",
        "Resolved": "14/May/15 22:57",
        "Description": "Please can you set up a DOAP for your project and get it added to files.xml?\nSee http://projects.apache.org/create.html\nOnce you have created the DOAP, please submit it for inclusion in the Apache projects listing as per:\nhttp://projects.apache.org/create.html#submit\nRemember, if you ever move or rename the doap file in future, please\nensure that files.xml is updated to point to the new location.\nIt is recommended that the DOAP is published with the website, e.g. at\nhttp://parquet.apache.org/doap_Parquet.rdf\nas this URL is unlikely to change.\nThanks!",
        "Issue Links": []
    },
    "PARQUET-281": {
        "Key": "PARQUET-281",
        "Summary": "Statistic and Filter need a mechanism to get customized comparator from high layer user",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dong Chen",
        "Reporter": "Dong Chen",
        "Created": "13/May/15 07:36",
        "Updated": "07/May/19 20:01",
        "Resolved": null,
        "Description": "As discussed in HIVE-10254, we might need a customized comparator from high layer user for generating statistic when writing and applying filter when reading. \nThe problem is that (use Decimal type in Hive as an example):\nDecimal in Hive is mapped to Binary in Parquet. When using predicate and statistic to filter values, comparing Binary values in Parquet cannot reflect the correct relationship of Decimal values in Hive. This type mapping causes 2 problems:\n1. When writing Decimal column, Binary.compareTo() is used to judge and set the column statistic (min, max). The generated statistic value is not correct from a Decimal perspective.\n2. When reading with Predicate (also Filter), in which the expected Decimal value is converted to Binary type, Binary.compareTo() is used to compare the expected value and column statistic value. They are Binary perspective, and also the result is not right.\nWe could add an interface for customized comparator, and high level user like Hive provides the comparator to Parquet, since Hive knows how to decode the binary to Decimal and compare. Then Parquet could switch between customized and original comparison method.",
        "Issue Links": [
            "/jira/browse/HIVE-10254"
        ]
    },
    "PARQUET-282": {
        "Key": "PARQUET-282",
        "Summary": "OutOfMemoryError in job commit / ParquetMetadataConverter",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "hy5446",
        "Created": "14/May/15 08:06",
        "Updated": "08/Mar/19 21:02",
        "Resolved": null,
        "Description": "We're trying to write some 14B rows (about 3.6 TB in parquets) to parquet files. When our ETL job finishes, it throws this exception, and the status is \"died in job commit\".\n2015-05-14 09:24:28,158 FATAL CommitterEvent Processor #4 org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread ThreadCommitterEvent Processor #4,5,main threw an Error.  Shutting down now...\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:373)\n\tat java.nio.ByteBuffer.wrap(ByteBuffer.java:396)\n\tat parquet.format.Statistics.setMin(Statistics.java:237)\n\tat parquet.format.converter.ParquetMetadataConverter.toParquetStatistics(ParquetMetadataConverter.java:243)\n\tat parquet.format.converter.ParquetMetadataConverter.addRowGroup(ParquetMetadataConverter.java:167)\n\tat parquet.format.converter.ParquetMetadataConverter.toParquetMetadata(ParquetMetadataConverter.java:79)\n\tat parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:405)\n\tat parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:433)\n\tat parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:423)\n\tat parquet.hadoop.ParquetOutputCommitter.writeMetaDataFile(ParquetOutputCommitter.java:58)\n\tat parquet.hadoop.mapred.MapredParquetOutputCommitter.commitJob(MapredParquetOutputCommitter.java:43)\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:259)\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:253)\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:216)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nThis seems to have something to do with the _metadata file creation, as the parquet files are perfectly fine and usable. Also I'm not sure how to alleviate this (i.e. add more heap space) since the crash is outside the Map/Reduce tasks themselves but seems in the job/application controller itself.",
        "Issue Links": []
    },
    "PARQUET-283": {
        "Key": "PARQUET-283",
        "Summary": "Parquet-Thrift does not handle dropping values in a map",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "15/May/15 07:53",
        "Updated": "20/May/15 02:36",
        "Resolved": "20/May/15 02:36",
        "Description": "Thrift does not have a notion of null, and it does not treat map values as optional. We need to apply the same trick as we do for unions to map values \u2013 keep at least 1 primitive column even when the values of the map are not requested.",
        "Issue Links": []
    },
    "PARQUET-284": {
        "Key": "PARQUET-284",
        "Summary": "Should use ConcurrentHashMap instead of HashMap in ParquetMetadataConverter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.1",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Tony Yan",
        "Created": "15/May/15 10:52",
        "Updated": "11/Jul/15 23:32",
        "Resolved": "24/Jun/15 23:02",
        "Description": "When using parquet in Spark Environment, sometimes got hang  with following thread dump:\n\"Executor task launch worker-0\" daemon prio=10 tid=0x000000004073d000 nid=0xd6c5 runnable [0x00007ff3fda40000]\njava.lang.Thread.State: RUNNABLE\nat java.util.HashMap.get(HashMap.java:303)\nat parquet.format.converter.ParquetMetadataConverter.fromFormatEncodings(ParquetMetadataConverter.java:218)\nat parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:543)\nat parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:520)\nat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:426)\nat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:381)\nat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:161)\nat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)\nat org.apache.spark.rdd.NewHadoopRDD$$anon$1.(NewHadoopRDD.scala:135)\nat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:107)\nat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\nat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\nat org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\nat org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:230)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\nat org.apache.spark.scheduler.Task.run(Task.scala:56)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)\nat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\nat java.lang.Thread.run(Thread.java:662)\nFrom the source code of ParquetMetadataConverter:\nprivate Map> encodingLists = new HashMap>();\nIt use HashMap instead of ConcurrentHashMap. Because HashMap is not thread safe and can cause hang when run in multithread environment. So it need to change to ConcurrentHashMap",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "https://github.com/apache/parquet-mr/pull/220"
        ]
    },
    "PARQUET-285": {
        "Key": "PARQUET-285",
        "Summary": "Implement nested types write rules in parquet-avro",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "16/May/15 01:00",
        "Updated": "02/Jun/15 00:47",
        "Resolved": "02/Jun/15 00:47",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/198"
        ]
    },
    "PARQUET-286": {
        "Key": "PARQUET-286",
        "Summary": "Avro object model should use Utf8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "16/May/15 20:41",
        "Updated": "23/Nov/15 17:11",
        "Resolved": "04/Jun/15 23:28",
        "Description": "The parquet-avro object model currently only uses String for binary/utf8 data. It should use Utf8, which doesn't copy data and require 2x the space whenever Avro would. That is by default in generic, unless avro.java.string is set to \"string\". Avro reflect always uses String, and both reflect and specific will respect the java-class property for stringable types.",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "/jira/browse/PARQUET-243",
            "https://github.com/apache/parquet-mr/pull/201"
        ]
    },
    "PARQUET-287": {
        "Key": "PARQUET-287",
        "Summary": "Projecting unions in thrift causes TExceptions in deserializatoin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "20/May/15 02:35",
        "Updated": "11/Jul/15 23:33",
        "Resolved": "20/May/15 02:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-288": {
        "Key": "PARQUET-288",
        "Summary": "Add dictionary support to Avro converters",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "26/May/15 18:03",
        "Updated": "15/Jul/15 21:32",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-289": {
        "Key": "PARQUET-289",
        "Summary": "Allow object models to extend the ParquetReader builders",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/May/15 18:06",
        "Updated": "02/Jul/15 00:20",
        "Resolved": "02/Jul/15 00:20",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-290",
            "https://github.com/apache/parquet-mr/pull/203"
        ]
    },
    "PARQUET-290": {
        "Key": "PARQUET-290",
        "Summary": "Add Avro data model to the reader builder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/May/15 18:13",
        "Updated": "02/Jun/20 18:08",
        "Resolved": "02/Jul/15 00:31",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-289",
            "https://github.com/apache/parquet-mr/pull/204"
        ]
    },
    "PARQUET-291": {
        "Key": "PARQUET-291",
        "Summary": "Difference between parquet-mr implementation and parquet-format documentation",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "27/May/15 14:34",
        "Updated": "18/Aug/17 15:27",
        "Resolved": null,
        "Description": "Documentation at https://github.com/apache/parquet-format/blob/master/src/thrift/parquet.thrift\n\nstruct ColumnChunk {\n  /** File where column data is stored.  If not set, assumed to be same file as\n    * metadata.  This path is relative to the current file.\n    **/\n  1: optional string file_path\n\n  /** Byte offset in file_path to the ColumnMetaData **/\n  2: required i64 file_offset\n\n...\n\n\nand https://github.com/apache/parquet-format\n\n4-byte magic number \"PAR1\"\n<Column 1 Chunk 1 + Column Metadata>\n<Column 2 Chunk 1 + Column Metadata>\n...\n\n\nsuggests that ColumnChunk data should be followed by ColumnChunkMetaData.\nHowever it looks like parquet-mr doesn't write ColumnMetaData after Columns at all and populates ColumnChunk.file_offset with an offset of the first data page:\nfrom ParquetMetadataConverter.java:153:\n\n    for (ColumnChunkMetaData columnMetaData : columns) {\n      ColumnChunk columnChunk = new ColumnChunk(columnMetaData.getFirstDataPageOffset()); // verify this is the right offset\n      columnChunk.file_path = block.getPath(); // they are in the same file for now\n\n\n\n Is it a bug in parquet-mr or in the documentation?",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/56"
        ]
    },
    "PARQUET-292": {
        "Key": "PARQUET-292",
        "Summary": "Release Parquet 1.8.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "27/May/15 22:41",
        "Updated": "15/Jul/15 21:34",
        "Resolved": "15/Jul/15 21:34",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-251",
            "/jira/browse/PARQUET-201",
            "/jira/browse/PARQUET-246",
            "/jira/browse/PARQUET-284",
            "/jira/browse/PARQUET-297",
            "/jira/browse/PARQUET-320",
            "/jira/browse/PARQUET-329",
            "/jira/browse/PARQUET-332",
            "/jira/browse/PARQUET-264",
            "/jira/browse/PARQUET-265",
            "/jira/browse/PARQUET-286"
        ]
    },
    "PARQUET-293": {
        "Key": "PARQUET-293",
        "Summary": "ScalaReflectionException when trying to convert an RDD of Scrooge to a DataFrame",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Tim Chan",
        "Created": "28/May/15 19:02",
        "Updated": "10/Jun/15 17:45",
        "Resolved": "10/Jun/15 17:45",
        "Description": "I get \"scala.ScalaReflectionException: <none> is not a term\" when I try to convert an RDD of Scrooge to a DataFrame, e.g. myScroogeRDD.toDF\nHas anyone else encountered this problem? \nI'm using Spark 1.3.1, Scala 2.10.4 and scrooge-sbt-plugin 3.16.3\nHere is my thrift IDL:\n\nnamespace scala com.junk\nnamespace java com.junk\n\nstruct Junk {\n    10: i64 junkID,\n    20: string junkString\n}\n\n\nfrom a spark-shell: \n\nval junks = List( Junk(123L, \"junk1\"), Junk(567L, \"junk2\"), Junk(789L, \"junk3\") )\nval junksRDD = sc.parallelize(junks)\njunksRDD.toDF\n\n\nException thrown:\n\nscala.ScalaReflectionException: <none> is not a term\n\tat scala.reflect.api.Symbols$SymbolApi$class.asTerm(Symbols.scala:259)\n\tat scala.reflect.internal.Symbols$SymbolContextApiImpl.asTerm(Symbols.scala:73)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:148)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:107)\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:30)\n\tat org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:316)\n\tat org.apache.spark.sql.SQLContext$implicits$.rddToDataFrameHolder(SQLContext.scala:254)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:38)\n\tat $iwC$$iwC$$iwC.<init>(<console>:40)\n\tat $iwC$$iwC.<init>(<console>:42)\n\tat $iwC.<init>(<console>:44)\n\tat <init>(<console>:46)\n\tat .<init>(<console>:50)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)\n\tat org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)\n\tat org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)\n\tat org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:656)\n\tat org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:664)\n\tat org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:669)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:996)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)\n\tat org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)\n\tat scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n\tat org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)\n\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)\n\tat org.apache.spark.repl.Main$.main(Main.scala:31)\n\tat org.apache.spark.repl.Main.main(Main.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",
        "Issue Links": [
            "/jira/browse/SPARK-8288"
        ]
    },
    "PARQUET-294": {
        "Key": "PARQUET-294",
        "Summary": "NPE in ParquetInputFormat.getSplits when no .parquet files exist",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Paul Nepywoda",
        "Created": "29/May/15 21:34",
        "Updated": "07/Jun/15 14:31",
        "Resolved": null,
        "Description": "JavaSparkContext context = ...\n        JavaRDD<Row> rdd1 = context.parallelize(ImmutableList.<Row> of());\n        SQLContext sqlContext = new SQLContext(context);\n        StructType schema = DataTypes.createStructType(ImmutableList.of(DataTypes.createStructField(\"col1\", DataTypes.StringType, true)));\n        DataFrame df = sqlContext.createDataFrame(rdd1, schema);\n        String url = \"file:///tmp/emptyRDD\";\n        df.saveAsParquetFile(url);\n\n        Configuration configuration = SparkHadoopUtil.get().newConfiguration(context.getConf());\n        JobConf jobConf = new JobConf(configuration);\n        ParquetInputFormat.setReadSupportClass(jobConf, RowReadSupport.class);\n        FileInputFormat.setInputPaths(jobConf, url);\n        JavaRDD<Row> rdd2 = context.newAPIHadoopRDD(\n            jobConf, ParquetInputFormat.class, Void.class, Row.class).values();\n        rdd2.count();\n\n        df = sqlContext.createDataFrame(rdd2, schema);\n        url = \"file:///tmp/emptyRDD2\";\n        df.saveAsParquetFile(url);\n\n        FileInputFormat.setInputPaths(jobConf, url);\n        JavaRDD<Row> rdd3 = context.newAPIHadoopRDD(\n            jobConf, ParquetInputFormat.class, Void.class, Row.class).values();\n        rdd3.count();\n\n\nThe NPE happens here:\n\njava.lang.NullPointerException\n\tat parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:263)\n\tat parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:245)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95)\n\n\nThis stems from ParquetFileWriter.getGlobalMetaData returning null when there are no footers to read.",
        "Issue Links": []
    },
    "PARQUET-295": {
        "Key": "PARQUET-295",
        "Summary": "Filter predicate should be able to filter on columns not specified in projection pushdown",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tim Chan",
        "Created": "30/May/15 03:38",
        "Updated": "03/Jun/15 01:18",
        "Resolved": null,
        "Description": "When using both filter predicate and projection pushdown of columns, the column specified in the filter predicate, should not have to be specified in the projection pushdown. \nFor example, if I want the first names of students that are over the age of 16, then I should be able to specify ParquetInputFormat.setFilterPredicate(jobConf, IntColumn(\"age\") > 16) and only have to specify ThriftReadSupport.setProjectionPushdown(jobConf, \"fname\")",
        "Issue Links": []
    },
    "PARQUET-296": {
        "Key": "PARQUET-296",
        "Summary": "Set master branch version back to 1.8.0-SNAPSHOT",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "01/Jun/15 22:50",
        "Updated": "11/Jul/15 23:43",
        "Resolved": "11/Jul/15 23:43",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/206"
        ]
    },
    "PARQUET-297": {
        "Key": "PARQUET-297",
        "Summary": "created_by in file meta data doesn't contain parquet library version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "02/Jun/15 12:22",
        "Updated": "11/Jul/15 23:32",
        "Resolved": "18/Jun/15 23:58",
        "Description": "E.g. in a file produced with parquet 1.6.0 created_by = \"parquet-mr (build 6aa21f8776625b5fa6b18059cfebe7549f2e00cb)\", while it should be \"parquet-mr version 1.6.0 (build 6aa21f8776625b5fa6b18059cfebe7549f2e00cb)\"\nTwo problems populating Version.VERSION_NUMBER\n\ngetJarPath() is broken\nreadVersionNumber() reads wrong file from the wrong jar file",
        "Issue Links": [
            "/jira/browse/PARQUET-292"
        ]
    },
    "PARQUET-298": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Support Complex Types (Map, Array, Struct) in Parquet Vectorized Reader",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:02",
        "Updated": "13/Aug/15 18:31",
        "Resolved": null,
        "Description": "https://github.com/zhenxiao/incubator-parquet-mr/tree/vector\nHas support for primitive types. Needs to support complex types as well",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/257"
        ]
    },
    "PARQUET-299": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] ColumnVector length should be in terms of rows, not DataPages",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:03",
        "Updated": "13/Aug/15 18:30",
        "Resolved": null,
        "Description": "In https://github.com/zhenxiao/incubator-parquet-mr/tree/vector\nColumnVector length is in terms of DataPages, need to be in terms of rows",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/257"
        ]
    },
    "PARQUET-300": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Make sure all encodings work in Parquet Vectorized Reader",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:04",
        "Updated": "13/Aug/15 18:31",
        "Resolved": null,
        "Description": "Make sure all encodings working in Vectorized reader:\nhttps://github.com/zhenxiao/incubator-parquet-mr/tree/vector",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/257"
        ]
    },
    "PARQUET-301": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Lazy Load in Vectorized Reader",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:06",
        "Updated": "02/Jun/15 19:06",
        "Resolved": null,
        "Description": "don\u2019t read column data until required",
        "Issue Links": []
    },
    "PARQUET-302": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Lazy Decoding in Vectorized Reader",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:07",
        "Updated": "02/Jun/15 19:07",
        "Resolved": null,
        "Description": "Do not decode data until required",
        "Issue Links": []
    },
    "PARQUET-303": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Add Testcases/Benchmarks for ParquetVectorizedReader",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Zhenxiao Luo",
        "Created": "02/Jun/15 19:07",
        "Updated": "13/Aug/15 18:31",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/257"
        ]
    },
    "PARQUET-304": {
        "Key": "PARQUET-304",
        "Summary": "Add an option for making requested schema case insensitive while reading",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yash Datta",
        "Created": "09/Jun/15 05:21",
        "Updated": "09/Jun/15 05:46",
        "Resolved": null,
        "Description": "For projects such as hive and spark-sql which use parquet, the schema of the stored tables is always lowercase (because of limitations of hive metastore). It would be great if we can have a configurable option to read data from parquet irrespective of the case of the requested schema , supplied via ReadContext object.",
        "Issue Links": []
    },
    "PARQUET-305": {
        "Key": "PARQUET-305",
        "Summary": "Logger instantiated for package org.apache.parquet may be GC-ed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "09/Jun/15 06:53",
        "Updated": "12/May/22 04:52",
        "Resolved": "08/Dec/15 18:40",
        "Description": "This ticket is derived from SPARK-8122.\nAccording to Javadoc of java.util.Logger:\n\nIt is important to note that the Logger returned by one of the getLogger factory methods may be garbage collected at any time if a strong reference to the Logger is not kept.\nHowever, the only reference to the Logger created for package org.apache.parquet goes out of scope outside the static initialization block, and thus is possible to be garbage collected.\nMore details can be found in this comment.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-395",
            "/jira/browse/PARQUET-401",
            "/jira/browse/PARQUET-412",
            "/jira/browse/SPARK-39156",
            "https://github.com/apache/parquet-mr/pull/290"
        ]
    },
    "PARQUET-306": {
        "Key": "PARQUET-306",
        "Summary": "Improve alignment between row groups and HDFS blocks",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Jun/15 18:21",
        "Updated": "20/Jul/15 23:33",
        "Resolved": "23/Jun/15 00:12",
        "Description": "Row groups should not span HDFS blocks to avoid remote reads. There are 3 things we can use to avoid this:\n1. Set the next row group's size to the remaining bytes in the current HDFS block\n2. Use HDFS-3689, variable-length HDFS blocks, when available\n3. Pad after row groups close to the block boundary to start the next row group at the start of the next block",
        "Issue Links": [
            "/jira/browse/PARQUET-166",
            "https://github.com/apache/parquet-mr/pull/211"
        ]
    },
    "PARQUET-307": {
        "Key": "PARQUET-307",
        "Summary": "Parquet Thrift/Scrooge uses file schema as requested schema when projection is not specified",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "11/Jun/15 22:13",
        "Updated": "11/Jun/15 22:49",
        "Resolved": null,
        "Description": "Parquet is using file schema as requested schema when projection is not specified.\nInstead it should use the schema from the thrift class the user provided.",
        "Issue Links": []
    },
    "PARQUET-308": {
        "Key": "PARQUET-308",
        "Summary": "Add accessor to ParquetWriter to get current data size",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "16/Jun/15 22:07",
        "Updated": "03/Feb/17 17:41",
        "Resolved": "01/Jul/15 23:54",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-860",
            "https://github.com/apache/parquet-mr/pull/212"
        ]
    },
    "PARQUET-309": {
        "Key": "PARQUET-309",
        "Summary": "Remove unnecessary compile dependency on parquet-generator",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Konstantin Shaposhnikov",
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "17/Jun/15 02:33",
        "Updated": "23/Jun/15 10:43",
        "Resolved": "17/Jun/15 23:26",
        "Description": "parquet-generator is used during build time only. Other parquet-jars (e.g. parquet-encoding) should not depend on it.\nhttps://github.com/apache/parquet-mr/pull/214",
        "Issue Links": []
    },
    "PARQUET-310": {
        "Key": "PARQUET-310",
        "Summary": "ParquetMetadataConverter.java is too long",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Alex Levenson",
        "Created": "19/Jun/15 02:06",
        "Updated": "22/Jun/15 18:31",
        "Resolved": null,
        "Description": "ParquetMetadataConverter is huge and could benefit from splitting into a few smaller classes with well defined responsibilities.",
        "Issue Links": []
    },
    "PARQUET-311": {
        "Key": "PARQUET-311",
        "Summary": "NPE when debug logging file metadata",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "19/Jun/15 19:03",
        "Updated": "22/Jun/15 21:29",
        "Resolved": "22/Jun/15 21:29",
        "Description": "When debug logging is enabled and when all values are null in a block, Parquet throws NPE when pretty printing the metadata as the metadata doesn't have min/max defined.\n\njava.io.IOException: Could not read footer: java.lang.RuntimeException: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[\"blocks\"]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[\"columns\"]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[\"statistics\"]->org.apache.parquet.column.statistics.BinaryStatistics[\"maxBytes\"])\n\tat org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:247)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(ParquetFileReader.java:188)\n\tat org.apache.parquet.hadoop.ParquetReader.<init>(ParquetReader.java:124)\n\tat org.apache.parquet.hadoop.ParquetReader.<init>(ParquetReader.java:55)\n\tat org.apache.parquet.hadoop.ParquetReader$Builder.build(ParquetReader.java:264)\n\tat org.apache.parquet.hadoop.TestParquetVectorReader.testNullReads(TestParquetVectorReader.java:355)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:157)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\nCaused by: java.lang.RuntimeException: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[\"blocks\"]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[\"columns\"]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[\"statistics\"]->org.apache.parquet.column.statistics.BinaryStatistics[\"maxBytes\"])\n\tat org.apache.parquet.hadoop.metadata.ParquetMetadata.toJSON(ParquetMetadata.java:72)\n\tat org.apache.parquet.hadoop.metadata.ParquetMetadata.toPrettyJSON(ParquetMetadata.java:62)\n\tat org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:528)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:430)\n\tat org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:237)\n\tat org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:233)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.codehaus.jackson.map.JsonMappingException: (was java.lang.NullPointerException) (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[\"blocks\"]->java.util.ArrayList[0]->org.apache.parquet.hadoop.metadata.BlockMetaData[\"columns\"]->java.util.UnmodifiableRandomAccessList[8]->org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[\"statistics\"]->org.apache.parquet.column.statistics.BinaryStatistics[\"maxBytes\"])\n\tat org.codehaus.jackson.map.JsonMappingException.wrapWithPath(JsonMappingException.java:218)\n\tat org.codehaus.jackson.map.JsonMappingException.wrapWithPath(JsonMappingException.java:183)\n\tat org.codehaus.jackson.map.ser.std.SerializerBase.wrapAndThrow(SerializerBase.java:140)\n\tat org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:158)\n\tat org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)\n\tat org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)\n\tat org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)\n\tat org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)\n\tat org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:122)\n\tat org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:71)\n\tat org.codehaus.jackson.map.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:86)\n\tat org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)\n\tat org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)\n\tat org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)\n\tat org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:122)\n\tat org.codehaus.jackson.map.ser.std.StdContainerSerializers$IndexedListSerializer.serializeContents(StdContainerSerializers.java:71)\n\tat org.codehaus.jackson.map.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:86)\n\tat org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:446)\n\tat org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)\n\tat org.codehaus.jackson.map.ser.BeanSerializer.serialize(BeanSerializer.java:112)\n\tat org.codehaus.jackson.map.ser.StdSerializerProvider._serializeValue(StdSerializerProvider.java:610)\n\tat org.codehaus.jackson.map.ser.StdSerializerProvider.serializeValue(StdSerializerProvider.java:256)\n\tat org.codehaus.jackson.map.ObjectMapper._configAndWriteValue(ObjectMapper.java:2575)\n\tat org.codehaus.jackson.map.ObjectMapper.writeValue(ObjectMapper.java:2081)\n\tat org.apache.parquet.hadoop.metadata.ParquetMetadata.toJSON(ParquetMetadata.java:68)\n\t... 9 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.parquet.column.statistics.BinaryStatistics.getMaxBytes(BinaryStatistics.java:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat org.codehaus.jackson.map.ser.BeanPropertyWriter.get(BeanPropertyWriter.java:483)\n\tat org.codehaus.jackson.map.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:418)\n\tat org.codehaus.jackson.map.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:150)\n\t... 30 more",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/221"
        ]
    },
    "PARQUET-312": {
        "Key": "PARQUET-312",
        "Summary": "Version class is not generated",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Nezih Yigitbasi",
        "Created": "19/Jun/15 19:23",
        "Updated": "19/Jun/15 23:45",
        "Resolved": "19/Jun/15 23:45",
        "Description": "When I try to build a particular module of parquet, the Version class is not generated. I am using:\n\nOS X 10.9.5\nJava 1.6.0_65\nApache Maven 3.2.3 (33f8c3e1027c3ddde99d3cdebad2656a31e8fdf4; 2014-08-11T13:58:10-07:00)\n\n\nmvn clean package -pl parquet-hadoop-bundle -am -DskipTests",
        "Issue Links": []
    },
    "PARQUET-313": {
        "Key": "PARQUET-313",
        "Summary": "Implement 3 level list writing rule for Parquet-Thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Ashish Singh",
        "Created": "19/Jun/15 21:45",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "05/Oct/20 15:02",
        "Description": "Parquet-thrift does not write three level lists, unlike other object models in the project. This JIRA should add 3 level list writing rule's implementation for parquet-thrift.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/222"
        ]
    },
    "PARQUET-314": {
        "Key": "PARQUET-314",
        "Summary": "Fix broken equals implementation(s)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "19/Jun/15 22:16",
        "Updated": "22/Jun/15 19:38",
        "Resolved": "22/Jun/15 19:38",
        "Description": "The equals implementation in ColumnDescriptor and Statistics classes are broken.",
        "Issue Links": []
    },
    "PARQUET-315": {
        "Key": "PARQUET-315",
        "Summary": "Add PARQUET_1_0 and non-repeated data performance tests to parquet-benchmarks",
        "Type": "Test",
        "Status": "In Progress",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Sergio Pe\u00f1a",
        "Reporter": "Sergio Pe\u00f1a",
        "Created": "22/Jun/15 16:43",
        "Updated": "22/Jun/15 18:27",
        "Resolved": null,
        "Description": "The current parquet-benchmarks module run some performance tests between different block & page sizes for PARQUET_2_0 version only. We should run some tests with PARQUET_1_0 version as well in order to get a view about new parquet version enhancements, and be able to catch possible overheads early by comparing with the old file format.\nAlso, this module uses repeated data to benchmark the settings. We should also use random data to get different results about how current and new encodings work with real world data.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/224"
        ]
    },
    "PARQUET-316": {
        "Key": "PARQUET-316",
        "Summary": "Run.sh is broken in parquet-benchmarks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "23/Jun/15 18:35",
        "Updated": "30/Jun/15 18:01",
        "Resolved": "30/Jun/15 18:01",
        "Description": "With the package renaming (to org.apache.parquet) the run.sh script is now broken.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/226"
        ]
    },
    "PARQUET-317": {
        "Key": "PARQUET-317",
        "Summary": "writeMetaDataFile crashes when a relative root Path is used",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Steven She",
        "Reporter": "Steven She",
        "Created": "24/Jun/15 18:21",
        "Updated": "26/Jun/15 04:50",
        "Resolved": "26/Jun/15 04:50",
        "Description": "In Spark, I can save an RDD to the local file system using a relative path, e.g.:\n\nrdd.saveAsNewAPIHadoopFile(\n        \"relativeRoot\",\n        classOf[Void],\n        tag.runtimeClass.asInstanceOf[Class[T]],\n        classOf[ParquetOutputFormat[T]],\n        job.getConfiguration)\n\n\nThis leads to a crash in the ParquetFileWriter.mergeFooters(..) method since the footer paths are read as fully qualified paths, but the root path is provided as a relative path:\n\norg.apache.parquet.io.ParquetEncodingException: /Users/stevenshe/schema/relativeRoot/part-r-00000.snappy.parquet invalid: all the files must be contained in the root relativeRoot",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/228"
        ]
    },
    "PARQUET-318": {
        "Key": "PARQUET-318",
        "Summary": "Remove unnecessary objectmapper from ParquetMetadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "24/Jun/15 18:21",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "16/Dec/15 19:42",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/227"
        ]
    },
    "PARQUET-319": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Define the parquet bloom filter statistics in parquet format",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Junjie Chen",
        "Reporter": "Ferdinand Xu",
        "Created": "25/Jun/15 02:38",
        "Updated": "10/Oct/19 07:53",
        "Resolved": "19/Sep/19 03:13",
        "Description": "As discussed in Parquet-41, we should define the bloom filter in binary level.",
        "Issue Links": [
            "/jira/browse/PARQUET-1608",
            "https://docs.google.com/document/d/1mIZ0W24Cr79QHJWN1sQ3dIUc4lAK5AVqozwSwtpFhW8/edit#heading=h.hmt1hrab3fpc",
            "https://github.com/apache/parquet-format/pull/28",
            "https://github.com/apache/parquet-format/pull/112"
        ]
    },
    "PARQUET-320": {
        "Key": "PARQUET-320",
        "Summary": "Restore semver checks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Jun/15 00:54",
        "Updated": "01/Jul/15 23:34",
        "Resolved": "01/Jul/15 23:34",
        "Description": "The exclusion for parquet-format classes was parquet/**, which evidently matches everything. Even classes in org.apache.parquet. We need remove that check and fix any problems that have cropped up since it was added.",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "https://github.com/apache/parquet-mr/pull/230"
        ]
    },
    "PARQUET-321": {
        "Key": "PARQUET-321",
        "Summary": "Set the HDFS padding default to 8MB",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Jun/15 19:27",
        "Updated": "30/Mar/18 21:37",
        "Resolved": "08/Dec/16 15:00",
        "Description": "PARQUET-306 added the ability to pad row groups so that they align with HDFS blocks to avoid remote reads. The ParquetFileWriter will now either pad the remaining space in the block or target a row group for the remaining size.\nThe padding maximum controls the threshold of the amount of padding that will be used. If the space left is under this threshold, it is padded. If it is greater than this threshold, then the next row group is fit into the remaining space. The current padding maximum is 0.\nI think we should change the padding maximum to 8MB. My reasoning is this: we want this number to be small enough that it won't prevent the library from writing reasonable row groups, but larger than the minimum size row group we would want to write. 8MB is 1/16th of the row group default, so I think it is reasonable: we don't want a row group to be smaller than 8 MB.\nWe also want this to be large enough that a few row groups in a  block don't cause a tiny row group to be written in the excess space. 8MB accounts for 4 row groups that are 2MB under-size. In addition, it is reasonable to not allow row groups under 8MB.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/232",
            "https://github.com/apache/parquet-mr/pull/391"
        ]
    },
    "PARQUET-322": {
        "Key": "PARQUET-322",
        "Summary": "Document ENUM as a logical type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Jakub Kukul",
        "Reporter": "Cheng Lian",
        "Created": "01/Jul/15 07:37",
        "Updated": "16/Oct/17 23:56",
        "Resolved": "06/Oct/17 23:45",
        "Description": "ENUM is used to annotate enum type in Thrift, Avro, and ProtoBuf, but it's not documented anywhere in parquet-format.\nAccording to current (1.8-SNAPSHOT) code base, ENUM is only used to annotate BINARY. For data models which lack a native enum type, BINARY (ENUM) should be interpreted as a UTF-8 string.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/54"
        ]
    },
    "PARQUET-323": {
        "Key": "PARQUET-323",
        "Summary": "INT96 should be marked as deprecated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Cheng Lian",
        "Created": "01/Jul/15 07:43",
        "Updated": "28/Dec/20 13:06",
        "Resolved": "22/Mar/18 22:27",
        "Description": "As discussed in the mailing list, INT96 is only used to represent nanosec timestamp in Impala for some historical reasons, and should be deprecated. Since nanosec precision is rarely a real requirement, one possible and simple solution would be replacing INT96 with INT64 (TIMESTAMP_MILLIS) or INT64 (TIMESTAMP_MICROS).\nSeveral projects (Impala, Hive, Spark, ...) support INT96.\nWe need a clear spec of the replacement and the path to deprecation.",
        "Issue Links": [
            "/jira/browse/PARQUET-1928",
            "/jira/browse/PARQUET-904",
            "/jira/browse/SPARK-8824",
            "https://github.com/apache/parquet-format/pull/86",
            "https://issues.cloudera.org/browse/IMPALA-5049"
        ]
    },
    "PARQUET-324": {
        "Key": "PARQUET-324",
        "Summary": "row count incorrect if data file has more than 2^31 rows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0,                                            1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas Friedrich",
        "Reporter": "Thomas Friedrich",
        "Created": "01/Jul/15 20:25",
        "Updated": "03/Jul/15 17:54",
        "Resolved": "03/Jul/15 17:54",
        "Description": "If a parquet file has more than 2^31 rows, the row count written into the file metadata is incorrect. \nThe cause of the problem is the use of an int instead of long data type for numRows in ParquetMetadataConverter, toParquetMetadata:\n    int numRows = 0;\n    for (BlockMetaData block : blocks) \n{\n      numRows += block.getRowCount();\n      addRowGroup(parquetMetadata, rowGroups, block);\n    }",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/233"
        ]
    },
    "PARQUET-325": {
        "Key": "PARQUET-325",
        "Summary": "Do not target row group sizes if padding is set to 0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "01/Jul/15 23:38",
        "Updated": "01/Jul/15 23:47",
        "Resolved": "01/Jul/15 23:47",
        "Description": "The new padding commit will target row group sizes to the remaining space in a block if the space remaining is less than the padding size. This is the intended behavior if padding is set, but if padding is 0, then it will target a row group to be in any remaining bytes, even if that makes no sense.\nThe behavior when padding is 0 should be to always target row groups to the default row group size. This will match the existing behavior up to 1.8.0.\n(In the error case, row groups will still span the block boundary, but will most likely be created with the minimum number of rows before checking whether or not it should be flushed.)",
        "Issue Links": []
    },
    "PARQUET-326": {
        "Key": "PARQUET-326",
        "Summary": "Binary statistics are invalid if buffers are reused",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Thomas White",
        "Created": "07/Jul/15 15:24",
        "Updated": "07/Jul/15 16:02",
        "Resolved": "07/Jul/15 16:02",
        "Description": "BinaryStatistics does not copy the min or max binary objects, so if the buffers are shared (as they usually are in MapReduce), then the statistics are incorrect.",
        "Issue Links": [
            "/jira/browse/PARQUET-251",
            "/jira/browse/PARQUET-62"
        ]
    },
    "PARQUET-327": {
        "Key": "PARQUET-327",
        "Summary": "Show statistics in the dump output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas White",
        "Reporter": "Thomas White",
        "Created": "07/Jul/15 15:46",
        "Updated": "24/May/16 00:28",
        "Resolved": "21/Apr/16 04:01",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-386",
            "https://github.com/apache/parquet-mr/pull/237"
        ]
    },
    "PARQUET-328": {
        "Key": "PARQUET-328",
        "Summary": "ParquetReader not using FileSystem cache effectively?",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tim",
        "Created": "07/Jul/15 18:15",
        "Updated": "21/Nov/15 00:30",
        "Resolved": null,
        "Description": "We've seen spark job stucked with following trace:\njava.util.HashMap.put(HashMap.java:494)\norg.apache.hadoop.conf.Configuration.set(Configuration.java:1065)\norg.apache.hadoop.conf.Configuration.set(Configuration.java:1035)\norg.apache.hadoop.fs.viewfs.HDFSCompatibleViewFileSystem.mergeViewFsHdfsMountPoints(HDFSCompatibleViewFileSystem.java:491)\norg.apache.hadoop.fs.viewfs.HDFSCompatibleViewFileSystem.mergeConfFromDirectory(HDFSCompatibleViewFileSystem.java:413)\norg.apache.hadoop.fs.viewfs.HDFSCompatibleViewFileSystem.mergeViewFsAndHdfs(HDFSCompatibleViewFileSystem.java:273)\norg.apache.hadoop.fs.viewfs.HDFSCompatibleViewFileSystem.initialize(HDFSCompatibleViewFileSystem.java:190)\norg.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2438)\norg.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:90)\norg.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2472)\norg.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2454)\norg.apache.hadoop.fs.FileSystem.get(FileSystem.java:384)\norg.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\nparquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:384)\nparquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:157)\nparquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)\norg.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133)\norg.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)\norg.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD.compute(NewHadoopRDD.scala:244)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\norg.apache.spark.rdd.RDD.iterator(RDD.scala:244)\norg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\norg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\norg.apache.spark.scheduler.Task.run(Task.scala:64)\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\njava.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "PARQUET-329": {
        "Key": "PARQUET-329",
        "Summary": "ThriftReadSupport#THRIFT_COLUMN_FILTER_KEY was removed (incompatible change)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Ashish Singh",
        "Reporter": "Ashish Singh",
        "Created": "09/Jul/15 20:28",
        "Updated": "11/Jul/15 23:29",
        "Resolved": "11/Jul/15 23:29",
        "Description": "This renames, and so removes, a public field. It could possibly break apps.",
        "Issue Links": [
            "/jira/browse/PARQUET-292",
            "https://github.com/apache/parquet-mr/pull/239"
        ]
    },
    "PARQUET-330": {
        "Key": "PARQUET-330",
        "Summary": "Semver checks is excluded for o.a.p.thrift.projection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "None",
        "Assignee": "Ashish Singh",
        "Reporter": "Ashish Singh",
        "Created": "12/Jul/15 00:49",
        "Updated": "21/Nov/15 00:29",
        "Resolved": "21/Nov/15 00:29",
        "Description": "Semver checks is excluded for o.a.p.thrift.projection. Without the check backwards incompatible changes like, this are getting skipped. Is there a reason for excluding o.a.p.thrift.projection in semver checks?",
        "Issue Links": []
    },
    "PARQUET-331": {
        "Key": "PARQUET-331",
        "Summary": "Merge script doesn't surface stderr from failed sub processes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "13/Jul/15 23:26",
        "Updated": "17/Jul/15 18:44",
        "Resolved": "14/Jul/15 16:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-332": {
        "Key": "PARQUET-332",
        "Summary": "Incompatible changes in o.a.p.thrift.projection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "14/Jul/15 01:47",
        "Updated": "15/Jul/15 21:25",
        "Resolved": "15/Jul/15 21:25",
        "Description": "There are incompatible changes in o.a.p.thrift.projection that weren't caught because of PARQUET-330:\n\nThe return type of FieldsPath#push(ThriftField) changed (return type compatibility ref)\nFieldProjectionFilter changed to an interface\n\nBoth of these are incompatibilities if FieldProjectionFilter is part of the public API, which it appears to be because it is used by the ScroogeReadSupport and the ThriftSchemaConverter (public constructor).",
        "Issue Links": [
            "/jira/browse/PARQUET-292"
        ]
    },
    "PARQUET-333": {
        "Key": "PARQUET-131 Vectorized Reader In Parquet",
        "Summary": "[Vectorized Reader] Add attributes in ColumnVector and RowBatch",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Dong Chen",
        "Created": "14/Jul/15 07:07",
        "Updated": "18/Aug/15 05:32",
        "Resolved": null,
        "Description": "As discussed in HIVE-8128, we want to add some attributes in vector.\n\nIn ColumnVector, add two attributes: one is boolean noNulls, which indicates whether the whole column vector has no null value. The other is boolean isRepeating, which indicates whether the same value repeats for whole column vector. They could be calculated at the same time when we read a vector. SQL engines (like Hive) can check these attribute to skip some values.\nIn RowBatch, add one attribute int size, which indicates the number of rows in this batch. This is just for easy usage. Its value should be the same as RowBatch.columns[0].numValues.",
        "Issue Links": []
    },
    "PARQUET-334": {
        "Key": "PARQUET-334",
        "Summary": "UT TestSummary failed with \"java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null\" when Pig >=0.15",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "format-2.3.1",
        "Component/s": "parquet-mr",
        "Assignee": "Thomas Friedrich",
        "Reporter": "Xiang Li",
        "Created": "15/Jul/15 13:30",
        "Updated": "14/Dec/15 23:30",
        "Resolved": "12/Dec/15 23:35",
        "Description": "org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias B\nat org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1694)\nat org.apache.pig.PigServer.registerQuery(PigServer.java:623)\nat org.apache.pig.PigServer.registerQuery(PigServer.java:636)\nat parquet.pig.summary.TestSummary.testMaxIsZero(TestSummary.java:154)\n...\nCaused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null\nat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:307)\nat org.apache.pig.PigServer.launchPlan(PigServer.java:1390)\nat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375)\nat org.apache.pig.PigServer.execute(PigServer.java:1364)\nat org.apache.pig.PigServer.access$500(PigServer.java:113)\nat org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1689)\n... 32 more\nCaused by: java.lang.RuntimeException: Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from null\nat parquet.pig.summary.Summary.setInputSchema(Summary.java:266)\nat org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor.visit(ExpToPhyTranslationVisitor.java:530)\nat org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:132)\nat org.apache.pig.newplan.ReverseDependencyOrderWalkerWOSeenChk.walk(ReverseDependencyOrderWalkerWOSeenChk.java:69)\nat org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor.visit(LogToPhyTranslationVisitor.java:808)\nat org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:87)\nat org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)\nat org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)\nat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:258)\nat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:293)\n... 37 more\nCaused by: java.lang.NullPointerException\nat parquet.pig.summary.Summary.setInputSchema(Summary.java:261)\n... 46 more\nIt relates to a change on pig side: pig/src/org/apache/pig/newplan/logical/expression/ExpToPhyTranslationVisitor.java introduced by PIG-3294",
        "Issue Links": [
            "/jira/browse/PARQUET-365",
            "/jira/browse/PIG-3294",
            "https://github.com/apache/parquet-mr/pull/292"
        ]
    },
    "PARQUET-335": {
        "Key": "PARQUET-335",
        "Summary": "Avro object model should not require MAP_KEY_VALUE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "15/Jul/15 22:58",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "11/Sep/15 22:16",
        "Description": "The Avro object model currently includes a check that requires maps to use MAP_KEY_VALUE to annotate the repeated key_value group. This is not required by the map type spec and should be removed.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/241"
        ]
    },
    "PARQUET-336": {
        "Key": "PARQUET-336",
        "Summary": "ArrayIndexOutOfBounds in checkDeltaByteArrayProblem",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "16/Jul/15 03:32",
        "Updated": "17/Jul/15 19:50",
        "Resolved": "16/Jul/15 23:42",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-337": {
        "Key": "PARQUET-337",
        "Summary": "binary fields inside map/set/list are not handled in parquet-scrooge",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "None",
        "Assignee": "Jake Donham",
        "Reporter": "Jake Donham",
        "Created": "16/Jul/15 22:40",
        "Updated": "17/Jul/15 19:50",
        "Resolved": "16/Jul/15 23:41",
        "Description": "Binary fields inside map/set/list are not handled; using them produces a ScroogeSchemaConversionException.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/243"
        ]
    },
    "PARQUET-338": {
        "Key": "PARQUET-338",
        "Summary": "Readme references wrong format of pull request title",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "16/Jul/15 23:38",
        "Updated": "17/Jul/15 19:50",
        "Resolved": "16/Jul/15 23:39",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-339": {
        "Key": "PARQUET-339",
        "Summary": "Add Alex Levenson to KEYS file",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.1",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "17/Jul/15 00:07",
        "Updated": "17/Jul/15 19:53",
        "Resolved": "17/Jul/15 00:18",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-340": {
        "Key": "PARQUET-340",
        "Summary": "totalMemoryPool is truncated to 32 bits",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Chris Bannister",
        "Reporter": "Chris Bannister",
        "Created": "17/Jul/15 16:14",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "20/Jul/15 17:00",
        "Description": "with heap set to 50gb seeings lots of errors like this\nJul 17, 2015 3:18:14 PM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (2,147,483,647 bytes) of heap memory\nthis is because the ratio passed into calculate the total memory is a float which when used is causing the long value to be truncate to max int32.\nFix this by making it a double instead",
        "Issue Links": []
    },
    "PARQUET-341": {
        "Key": "PARQUET-341",
        "Summary": "Improve write performance with wide schema sparse data",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "21/Jul/15 22:34",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "21/Nov/15 00:26",
        "Description": "In write path, when there are tons of sparse data, most of time is spent on writing nulls.\nCurrently writing nulls has the same code path as writing values, which is reclusive traverse all the leaves when a group is null.\nDue to the fact that when a group is null all the leaves beneath it should be written with null value with the same repetition level and definition level, we can eliminate the recursion call to get the leaves",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/247"
        ]
    },
    "PARQUET-342": {
        "Key": "PARQUET-342",
        "Summary": "Can't build Parquet on Java 6",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.0.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "22/Jul/15 17:45",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "28/Jul/15 21:55",
        "Description": "I am having problems building the latest master with Java 6. I tried building the whole project \n\nmvn clean install -DskipTests\n\n and I also tried building individual modules \n\nmvn --projects parquet-common -am -amd -U clean install -DskipTests\n\nBoth fail with the same error shown below. SemanticVersion class uses Integer.compare(int,int) which was introduced in Java 1.7, but, AFAIK and according to the pom file (and to PARQUET-146) Parquet should still be built using Java 6. Similarly TestInputOutputFormatWithPadding and TestInputOutputFormat classes depend on java.nio.file package that was introduced in Java 1.7\nWhen I switch over to Java 7 the build succeeds.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/248"
        ]
    },
    "PARQUET-343": {
        "Key": "PARQUET-343",
        "Summary": "Caching nulls on group node to improve write performance on wide schema sparse data",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Tim",
        "Reporter": "Tim",
        "Created": "24/Jul/15 23:13",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "21/Nov/15 00:25",
        "Description": "or really wide schema with sparse data, If a group node is empty, it could have a huge number of leaves underneath it. Calling writeMull for each leaf every time when it's ancestor group node is null is in-effcient and is bad for data locality in the memory especially when the number of leaves is huge.\nInstead, null can be cached on the group node. Flushing is only triggered when a group node becomes non-null from null. This way, all the cached null values will be flushed to the leaf nodes in a tight loop and improves write performance.\nWe tested this approach combined with PARQUET-341 on a really large schema and gave us ~2X improvement on write performance",
        "Issue Links": [
            "/jira/browse/PARQUET-1119",
            "https://github.com/apache/parquet-mr/pull/249"
        ]
    },
    "PARQUET-344": {
        "Key": "PARQUET-344",
        "Summary": "Limit the number of rows per block and per split",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Quentin Francois",
        "Created": "28/Jul/15 13:31",
        "Updated": "25/Aug/15 22:39",
        "Resolved": null,
        "Description": "We use Parquet to store raw metrics data and then query this data with Hadoop-Pig. \nThe issue is that sometimes we end up with small Parquet files (~80mo) that contain more than 300 000 000 rows, usually because of a constant metric which results in a very good compression. Too good. As a result we have a very few number of maps that process up to 10x more rows than the other maps and we lose the benefits of the parallelization. \nThe fix for that has two components I believe:\n1. Be able to limit the number of rows per Parquet block (in addition to the size limit).\n2. Be able to limit the number of rows per split.",
        "Issue Links": []
    },
    "PARQUET-345": {
        "Key": "PARQUET-345",
        "Summary": "ThriftMetaData toString() should not try to load class reflectively",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "28/Jul/15 19:51",
        "Updated": "31/Jul/15 23:59",
        "Resolved": "31/Jul/15 23:59",
        "Description": "Currently:\n\n  @Override\n  public String toString() {\n    return \"ThriftMetaData\" + toExtraMetaData();\n  }\n\n/**\n   * generates a map of key values to store in the footer\n   * @return the key values\n   */\n  public Map<String, String> toExtraMetaData() {\n    final Map<String, String> map = new HashMap<String, String>();\n    map.put(THRIFT_CLASS, getThriftClass().getName());\n    map.put(THRIFT_DESCRIPTOR, descriptor.toJSON());\n    return map;\n  }\n\n\nThere's no guarantee that getThriftClass() won't throw (a class in the file footer doesn't actually need to be on the classpath if its never used) \u2013 this masks real error when the error message is being generated.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/252"
        ]
    },
    "PARQUET-346": {
        "Key": "PARQUET-346",
        "Summary": "ThriftSchemaConverter throws for unknown struct or union type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "28/Jul/15 19:54",
        "Updated": "15/Dec/15 00:03",
        "Resolved": "31/Jul/15 23:57",
        "Description": "ThriftSchemaConverter should either only be called on ThriftStruct's that have populated structOrUnionType metadata, or should support a mode where this data is unknown w/o throwing an exception.\nCurrently it is called using the file's metadata here:\nhttps://github.com/apache/parquet-mr/blob/d6f082b9be5d507ff60c6bc83a179cc44015ab97/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftRecordConverter.java#L797\nOne workaround is not not use the file matadata here but rather the schema from the thrift class. The other is to support unknown struct or union types",
        "Issue Links": [
            "/jira/browse/PARQUET-405",
            "https://github.com/apache/parquet-mr/pull/252"
        ]
    },
    "PARQUET-347": {
        "Key": "PARQUET-347",
        "Summary": "Thrift projection does not handle new (optional) fields in requestedSchema",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "28/Jul/15 19:56",
        "Updated": "28/Jul/15 19:59",
        "Resolved": null,
        "Description": "It should be valid to request an optional field that is not present in a file (it should be assumed to be null) but instead this throws eagerly in:\nhttps://github.com/apache/parquet-mr/blob/d6f082b9be5d507ff60c6bc83a179cc44015ab97/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/api/ReadSupport.java#L58",
        "Issue Links": []
    },
    "PARQUET-348": {
        "Key": "PARQUET-348",
        "Summary": "shouldIgnoreStatistics too noisy",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "28/Jul/15 20:56",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "31/Jul/15 23:59",
        "Description": "This method is called many times (probably once per row group per file?) and on a large schema (thousands of columns) it prints a lot of logs. Maybe this should log only once, and we certainly don't need to log the full stack trace every time.\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L263",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/252"
        ]
    },
    "PARQUET-349": {
        "Key": "PARQUET-349",
        "Summary": "VersionParser does not handle versions like \"parquet-mr 1.6.0rc4\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Alex Levenson",
        "Created": "28/Jul/15 20:58",
        "Updated": "21/Oct/21 19:25",
        "Resolved": "08/Dec/15 18:08",
        "Description": "I'm not sure how we missed this one (I thought I added a test for this but apparently not ) but the tests don't cover it, and the parser can't parse it even though the intention was to support a missing (build abcd) section in the version string.",
        "Issue Links": [
            "/jira/browse/ARROW-14422",
            "https://github.com/apache/parquet-mr/pull/283"
        ]
    },
    "PARQUET-350": {
        "Key": "PARQUET-350",
        "Summary": "ThriftRecordConverter throws NPE for unrecognized enum values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "29/Jul/15 22:45",
        "Updated": "12/Aug/15 16:50",
        "Resolved": "31/Jul/15 23:58",
        "Description": "currently:\n\n    @Override\n    public void addBinary(final Binary value) {\n      final int id = enumLookup.get(value);\n      events.add(new ParquetProtocol(\"readI32() enum\") {\n        @Override\n        public int readI32() throws TException {\n          return id;\n        }\n      });\n    }\n\n\nthe auto-unboxing from Integer to into throws a NPE when enumLookup.get(value) == null \u2013 we should throw a better exception here that includes the value in question.\nThis was actually triggered by someone renaming an enum, and because parquet stores enums by name instead of ID it is not compatible. I'm not sure why we store enums as strings, but we might want to reconsider that.",
        "Issue Links": [
            "/jira/browse/PARQUET-351",
            "https://github.com/apache/parquet-mr/pull/252"
        ]
    },
    "PARQUET-351": {
        "Key": "PARQUET-351",
        "Summary": "Backwards compat check should enforce enums are not renamed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "29/Jul/15 22:45",
        "Updated": "21/Nov/15 00:23",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-350"
        ]
    },
    "PARQUET-352": {
        "Key": "PARQUET-352",
        "Summary": "Add tags to \"created by\" metadata in the file footer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Jul/15 16:55",
        "Updated": "13/Aug/18 12:38",
        "Resolved": "08/Dec/15 18:16",
        "Description": "Hive has been writing timestamp (without time zone) data incorrectly by converting a timestamp with no time zone to UTC. There is no way to know that Hive definitely wrote the files, so it is difficult to add a recovery mechanism in the object model. Adding a tag for the object model to the \"created by\" metadata will allow us to detect situations like this in the future.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-899",
            "https://github.com/apache/parquet-mr/pull/289"
        ]
    },
    "PARQUET-353": {
        "Key": "PARQUET-353",
        "Summary": "Compressors not getting recycled while writing parquet files, causing memory leak",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Nitin Goyal",
        "Reporter": "Nitin Goyal",
        "Created": "01/Aug/15 17:37",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "11/Dec/15 21:19",
        "Description": "Compressors are not getting recycled while writing parquet files. This is causing native/physical memory leak in my spark app which is parquet write intensive since its creating new compressors everytime i write parquet files.\nThe actual code issue is that we are creating 'codecFactory' in 'getRecordWriter' method of ParquetOutputFormat.java but not calling codecFactory.release() which is responsible for recycling compressors.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/282",
            "https://github.com/apache/parquet-mr/pull/295"
        ]
    },
    "PARQUET-354": {
        "Key": "PARQUET-354",
        "Summary": "Question on parquet-protobuf and parquet-pig",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Christian Nguyen Van Than",
        "Created": "03/Aug/15 16:05",
        "Updated": "03/Aug/15 16:05",
        "Resolved": null,
        "Description": "Hi,\nI have a question about protobuf to parquet conversion.\nI have a message like this (simplified) :\n\n \nmessage MyMessage {\n  repeated string language = 1;\n}\n\n\nparquet-protobuf convert it to the following schema :\n\nmessage MyMessage {\n  repeated binary language (UTF8);\n}\n\n\nBut, according to TestPigSchameConverter.java, the correct schema should be :\n\nmessage MyMessage {\n  optional group language (LIST) {\n    repeated binary value (UTF8);\n  }\n}\n\n\nLanguage is an optional list of language, i want to store zero or more language.\nWho have the correct schema for my case ? parquet-protobuf or parquet-pig ?",
        "Issue Links": []
    },
    "PARQUET-355": {
        "Key": "PARQUET-355",
        "Summary": "Create Integration tests to validate statistics",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Reuben Kuhnert",
        "Created": "07/Aug/15 13:29",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "22/Sep/15 16:04",
        "Description": "In response to PARQUET-251 create unit tests that validate the statistics fields for each column type.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/255"
        ]
    },
    "PARQUET-356": {
        "Key": "PARQUET-356",
        "Summary": "Add ElephantBird section to LICENSE file",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "2.0.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "12/Aug/15 19:00",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "20/Aug/15 21:27",
        "Description": "Commit 9993450 brought in a section of LzoRecordReader.java from ElephantBird. The license for ElephantBird is ASL 2.0 so the inclusion is fine. We just need to add it to the root LICENSE file because it is included in the source distribution and in the parquet-hadoop binary LICENSE file because it is in that binary package.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/256"
        ]
    },
    "PARQUET-357": {
        "Key": "PARQUET-357",
        "Summary": "Parquet-thrift generates wrong schema for Thrift binary fields",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Cheng Lian",
        "Created": "13/Aug/15 11:03",
        "Updated": "10/Jan/18 14:01",
        "Resolved": "04/Jan/18 16:52",
        "Description": "Thrift doesn't have true BINARY type. The BINARY type is actually just an unencoded STRING. Quoted from Thrift Types section of official Thrift documentation:\n\nbinary: a sequence of unencoded bytes\nN.B.: This is currently a specialized form of the string type above, added to provide better interoperability with Java. The current plan-of-record is to elevate this to a base type at some point.\nThe consequence is that, Thrift BINARY and STRING are both passed to parquet-thrift as STRING, and are always encoded as BINARY (UTF8).\nThis is really a problem on Thrift side. One possible workaround is to inspect binary fields in the actual generated Java classes to see whether the type is ByteBuffer.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/439"
        ]
    },
    "PARQUET-358": {
        "Key": "PARQUET-358",
        "Summary": "Add support for temporal logical types to AVRO/Parquet conversion",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Konstantin Shaposhnikov",
        "Created": "14/Aug/15 03:38",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "20/Apr/16 15:43",
        "Description": "Both AVRO and Parquet support logical types for dates, times and timestamps however this information is not transfered from AVRO schema to Parquet schema during conversion.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/318"
        ]
    },
    "PARQUET-359": {
        "Key": "PARQUET-359",
        "Summary": "Existing _common_metadata should be deleted when ParquetOutputCommitter fails to write summary files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "16/Aug/15 08:24",
        "Updated": "21/Nov/15 01:05",
        "Resolved": "21/Nov/15 01:05",
        "Description": "ParquetOutputCommitter only deletes _metadata when fails to write summary files. This may leave inconsistent existing _common_metadata out there.\nThis issue can be reproduced via the following Spark shell snippet:\n\nimport sqlContext.implicits._\n\nval path = \"file:///tmp/foo\"\n(0 until 3).map(i => Tuple1((s\"a_$i\", s\"b_$i\"))).toDF().coalesce(1).write.mode(\"overwrite\").parquet(path)\n(0 until 3).map(i => Tuple1((s\"a_$i\", s\"b_$i\", s\"c_$i\"))).toDF().coalesce(1).write.mode(\"append\").parquet(path)\n\n\nThe 2nd write job fails to write the summary file because two written Parquet files contain different user-defined metadata (Spark SQL schema). We can find out that there is an _common_metadata left there:\n\n$ tree /tmp/foo\n/tmp/foo\n\u251c\u2500\u2500 _SUCCESS\n\u251c\u2500\u2500 _common_metadata\n\u251c\u2500\u2500 part-r-00000-1c8bcb7f-84cf-43e3-9cd6-04d371322d95.gz.parquet\n\u2514\u2500\u2500 part-r-00000-d759c53f-d12f-4555-9b27-8b03a8343b17.gz.parquet\n\n\nCheck its schema, the nested group contains only 2 fields, which is wrong:\n\n$ parquet-schema /tmp/foo/_common_metadata\nmessage root {\n  optional group _1 {\n    optional binary _1 (UTF8);\n    optional binary _2 (UTF8);\n  }\n}",
        "Issue Links": [
            "/jira/browse/PARQUET-381",
            "https://github.com/apache/parquet-mr/pull/258"
        ]
    },
    "PARQUET-360": {
        "Key": "PARQUET-360",
        "Summary": "parquet-cat json dump is broken for maps",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "18/Aug/15 07:42",
        "Updated": "17/Sep/15 18:47",
        "Resolved": "17/Sep/15 18:47",
        "Description": "When dumping parquet maps as json with the parquet-cat tool it throws class cast exception.\n\nparquet-cat --debug --json file:///tmp/test.parquet\n\njava.lang.ClassCastException: [B cannot be cast to java.lang.String\n\tat org.apache.parquet.tools.read.SimpleMapRecord.toJsonObject(SimpleMapRecord.java:34)\n\tat org.apache.parquet.tools.read.SimpleRecord.toJsonValue(SimpleRecord.java:119)\n\tat org.apache.parquet.tools.read.SimpleRecord.toJsonObject(SimpleRecord.java:112)\n\tat org.apache.parquet.tools.read.SimpleRecord.prettyPrintJson(SimpleRecord.java:106)\n\tat org.apache.parquet.tools.command.CatCommand.execute(CatCommand.java:76)\n\tat org.apache.parquet.tools.Main.main(Main.java:222)\n[B cannot be cast to java.lang.String",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/259"
        ]
    },
    "PARQUET-361": {
        "Key": "PARQUET-361",
        "Summary": "Add prerelease logic to semantic versions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "19/Aug/15 21:15",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "20/Aug/15 22:50",
        "Description": "CDH is including fixes for PARQUET-251. That means that we need to add the fixed versions to the logic that tests whether the fix is present and that requires the appropriate semver logic for prerelease versions because CDH versions are formatted like this: 1.5.0-cdh5.5.0 / <upstream-base>-cdh<cdh-release>.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/261"
        ]
    },
    "PARQUET-362": {
        "Key": "PARQUET-362",
        "Summary": "Parquet buffered writer is too sensitive regarding unions and unknown fields",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0,                                            1.8.0,                                            1.8.1",
        "Fix Version/s": "2.0.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Laurent Goujon",
        "Created": "20/Aug/15 20:43",
        "Updated": "20/Aug/15 20:53",
        "Resolved": "20/Aug/15 20:53",
        "Description": "Parquet does prevent records with unknown union fields to be written as it would create a TProtocol violation. But it also prevents records with unions having one their field itself having an unknown field (which is acceptable if it is a struct).\nThe recursive check should be removed, and only when a union field is unknown should the record be deemed invalid.",
        "Issue Links": []
    },
    "PARQUET-363": {
        "Key": "PARQUET-363",
        "Summary": "Cannot construct empty MessageType for ReadContext.requestedSchema",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "21/Aug/15 06:46",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "11/Sep/15 22:15",
        "Description": "In parquet-mr 1.8.1, constructing empty GroupType (and thus MessageType) is not allowed anymore (see PARQUET-278). This change makes sense in most cases since Parquet doesn't support empty groups. However, there is one use case where an empty MessageType is valid, namely passing an empty MessageType as the requestedSchema constructor argument of ReadContext when counting rows in a Parquet file. The reason why it works is that, Parquet can retrieve row count from block metadata without materializing any columns. Take the following PySpark shell snippet (1.5-SNAPSHOT, which uses parquet-mr 1.7.0) as an example:\n\n>>> path = 'file:///tmp/foo'\n>>> # Writes 10 integers into a Parquet file\n>>> sqlContext.range(10).coalesce(1).write.mode('overwrite').parquet(path)\n>>> sqlContext.read.parquet(path).count()\n\n10\n\n\nParquet related log lines:\n\n15/08/21 12:32:04 INFO CatalystReadSupport: Going to read the following fields from the Parquet file:\n\nParquet form:\nmessage root {\n}\n\n\nCatalyst form:\nStructType()\n\n15/08/21 12:32:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10 records.\n15/08/21 12:32:04 INFO InternalParquetRecordReader: at row 0. reading next block\n15/08/21 12:32:04 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 10\n\n\nWe can see that Spark SQL passes no requested columns to the underlying Parquet reader. What happens here is that:\n\nSpark SQL creates a CatalystRowConverter with zero converters (and thus only generates empty rows).\nInternalParquetRecordReader first obtain the row count from block metadata (here).\nMessageColumnIO returns an EmptyRecordRecorder for reading the Parquet file (here).\nInternalParquetRecordReader.nextKeyValue() is invoked n times, where n equals to the row count. Each time, it invokes the converter created by Spark SQL and produces an empty Spark SQL row object.\n\nThis issue is also the cause of HIVE-11611.  Because when upgrading to Parquet 1.8.1, Hive worked around this issue by using tableSchema as requestedSchema when no columns are requested (here). IMO this introduces a performance regression in cases like counting, because now we need to materialize all columns just for counting.",
        "Issue Links": [
            "/jira/browse/HIVE-11611",
            "/jira/browse/PARQUET-278",
            "https://github.com/apache/parquet-mr/pull/263"
        ]
    },
    "PARQUET-364": {
        "Key": "PARQUET-364",
        "Summary": "Parquet-avro cannot decode Avro/Thrift array of primitive array (e.g. array<array<int>>)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "21/Aug/15 13:49",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "20/Nov/15 23:01",
        "Description": "The problematic Avro and Thrift schemas are:\n\nrecord AvroArrayOfArray {\n  array<array<int>> int_arrays_column;\n}\n\n\nand\n\nstruct ThriftListOfList {\n  1: list<list<i32>> intArraysColumn;\n}\n\n\nThey are converted to the following structurally equivalent Parquet schemas by parquet-avro 1.7.0 and parquet-thrift 1.7.0 respectively:\n\nmessage AvroArrayOfArray {\n  required group int_arrays_column (LIST) {\n    repeated group array (LIST) {\n      repeated int32 array;\n    }\n  }\n}\n\n\nand\n\nmessage ParquetSchema {\n  required group intListsColumn (LIST) {\n    repeated group intListsColumn_tuple (LIST) {\n      repeated int32 intListsColumn_tuple_tuple;\n    }\n  }\n}\n\n\nAvroIndexedRecordConverter cannot decode such records correctly. The reason is that the 2nd level repeated group array doesn't pass AvroIndexedRecordConverter.isElementType() check. We should check for field name \"array\" and field name suffix \"_thrift\" in isElementType() to fix this issue.",
        "Issue Links": [
            "/jira/browse/PARQUET-212",
            "/jira/browse/SPARK-10136",
            "https://github.com/apache/parquet-mr/pull/272"
        ]
    },
    "PARQUET-365": {
        "Key": "PARQUET-365",
        "Summary": "Class Summary does not provide a getter to return inputSchema",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.8.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Xiang Li",
        "Created": "26/Aug/15 02:37",
        "Updated": "30/Nov/15 21:51",
        "Resolved": null,
        "Description": "In Pig code, https://github.com/apache/pig/blob/trunk/src/org/apache/pig/EvalFunc.java. A private number \"inputSchemaInternal\" represents the schema. Setter and Getter are also provided\n\n316     private Schema inputSchemaInternal=null;\n\n328     /**\n329      * This method is for internal use. It is called by Pig core in both front-end\n330      * and back-end to setup the right input schema for EvalFunc\n331      */\n332     public void setInputSchema(Schema input){\n333         this.inputSchemaInternal=input;\n334     }\n335 \n336     /**\n337      * This method is intended to be called by the user in {@link EvalFunc} to get the input\n338      * schema of the EvalFunc\n339      */\n340     public Schema getInputSchema(){\n341         return this.inputSchemaInternal;\n342     }\n\n\nIn parquet-mr/parquet-pig/src/main/java/parquet/pig/summary/Summary.java, class Summary extends EvalFunc. It uses a new number called inputSchema(vs. inputSchemaInternal used in class EvalFunc in Pig) to represent schema and override setInputSchema(), but the class does not override getInputSchema() to return inputSchema.\n\n51  public class Summary extends EvalFunc<String> implements Algebraic {\n\n54     private Schema inputSchema;\n\n257   @Override\n258   public void setInputSchema(Schema input) {\n259     try {\n260       // relation.bag.tuple\n261       this.inputSchema=input.getField(0).schema.getField(0).schema;\n262       saveSchemaToUDFContext();\n263     } catch (FrontendException e) {\n264       throw new RuntimeException(\"Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from \" + input, e);\n265     } catch (RuntimeException e) {\n266       throw new RuntimeException(\"Usage: B = FOREACH (GROUP A ALL) GENERATE Summary(A); Can not get schema from \"+input, e);\n267     }\n268   }\n\n\nIf setInputSchema() of class Summary is called, inputSchema is set. But if we call getInputSchema() afterwards, it will return the value of inputSchemaInternal, which can be still null.",
        "Issue Links": [
            "/jira/browse/PARQUET-334"
        ]
    },
    "PARQUET-366": {
        "Key": "PARQUET-366",
        "Summary": "Add a flag to allow empty structs in the thrift schema compat checker",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "26/Aug/15 23:28",
        "Updated": "26/Aug/15 23:31",
        "Resolved": null,
        "Description": "This is useful as we migrate schemas, we want to be able to turn this check off.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/266/"
        ]
    },
    "PARQUET-367": {
        "Key": "PARQUET-367",
        "Summary": "\"parquet-cat -j\" doesn't show all records",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1,                                            1.9.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Cheng Lian",
        "Created": "27/Aug/15 13:43",
        "Updated": "05/May/16 20:57",
        "Resolved": "05/May/16 20:57",
        "Description": "Inspect the attached Parquet file generated by parquet-proto to reproduce this issue:\n\n$ parquet-cat old-repeated-int.parquet\nrepeatedInt = 1\nrepeatedInt = 2\nrepeatedInt = 3\n\n$ parquet-cat -j old-repeated-int.parquet\n{\"repeatedInt\":3}\n\n\nExpected output should be something like\n\n{\"repeatedInt\":[1,2,3]}\n\n\nSchema of the attached testing Parquet file is:\n\nmessage TestProtobuf.RepeatedIntMessage {\n  repeated int32 repeatedInt;\n}",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/281"
        ]
    },
    "PARQUET-368": {
        "Key": "PARQUET-368",
        "Summary": "Support dictionaries in parquet-thrift",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "03/Sep/15 09:53",
        "Updated": "03/Sep/15 09:53",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-369": {
        "Key": "PARQUET-369",
        "Summary": "Shading SLF4J prevents SLF4J locating org.slf4j.impl.StaticLoggerBinder",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.3.1",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "05/Sep/15 12:36",
        "Updated": "16/Jun/17 19:48",
        "Resolved": "27/Oct/15 22:14",
        "Description": "Parquet-format shades SLF4J to parquet.org.slf4j (see here). This also accidentally shades this line\n\nprivate static String STATIC_LOGGER_BINDER_PATH = \"org/slf4j/impl/StaticLoggerBinder.class\";\n\n\nto\n\nprivate static String STATIC_LOGGER_BINDER_PATH = \"parquet/org/slf4j/impl/StaticLoggerBinder.class\";\n\n\nand thus LoggerFactory can never find the correct StaticLoggerBinder implementation even if we provide dependencies like slf4j-log4j12 on the classpath.\nThis happens in Spark. Whenever we write a Parquet file, we see the following famous message and can never get rid of it:\n\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.",
        "Issue Links": [
            "/jira/browse/PARQUET-401",
            "/jira/browse/PARQUET-408",
            "/jira/browse/PARQUET-412",
            "https://github.com/apache/parquet-format/pull/32"
        ]
    },
    "PARQUET-370": {
        "Key": "PARQUET-370",
        "Summary": "Nested records are not properly read if none of their fields are requested",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "08/Sep/15 10:01",
        "Updated": "20/Nov/15 22:56",
        "Resolved": null,
        "Description": "Say we have a Parquet file F with the following schema S1:\n\nmessage root {\n  required group n {\n    optional int32 a;\n    optional int32 b;\n  }\n}\n\n\nLater on, as the schema evolves, fields a and b are removed, while c and d are added. Now we have schema S2:\n\nmessage root {\n  required group n {\n    optional int32 c;\n    optional int32 d;\n  }\n}\n\n\nS1 and S2 are compatible, so it should be OK to read F with S2 as requested schema.\nSay F contains a single record:\n\n{\"n\": {\"a\": 1, \"b\": 2}}\n\n\nWhen reading F with S2, expected output should be:\n\n{\"n\": {\"c\": null, \"d\": null}}\n\n\nBut currently parquet-mr gives\n\n{\"n\": null}\n\n\nThis is because MessageColumnIO finds that the physical Parquet file contains no leaf columns defined in the requested schema, and shortcuts record reading with an EmptyRecordReader for column n. See here.",
        "Issue Links": []
    },
    "PARQUET-371": {
        "Key": "PARQUET-371",
        "Summary": "Bumps Thrift version to 0.9.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Cheng Lian",
        "Created": "09/Sep/15 16:02",
        "Updated": "10/Oct/17 20:48",
        "Resolved": "10/Oct/17 20:48",
        "Description": "Thrift 0.7.0 is too old a version, and it doesn't compile on Mac. Would be nice to bump Thrift version.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-412",
            "https://github.com/apache/parquet-format/pull/31"
        ]
    },
    "PARQUET-372": {
        "Key": "PARQUET-372",
        "Summary": "Parquet stats can have awkwardly large values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/Sep/15 18:41",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "05/May/16 20:55",
        "Description": "If a column is storing very large values, say 2-4 MB, then the page header's min and max values can also be this large.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-411",
            "https://github.com/apache/parquet-mr/pull/275"
        ]
    },
    "PARQUET-373": {
        "Key": "PARQUET-373",
        "Summary": "MemoryManager tests are flaky",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/Sep/15 21:57",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "19/Oct/15 22:52",
        "Description": "The memory manager tests are flaky, depending on the heap allocation for the JVM they run in. This is caused by over-specific tests that assert the memory allocation down to the byte and the fact that some assertions implicitly cast long values to doubles to use the \"within\" form of assertEquals.\nThe tests should not validate a specific allocation strategy, but should instead assert that:\n1. The allocation for a file is the row group size until room runs out\n2. When scaling row groups, the total allocation does not exceed the pool size",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/269"
        ]
    },
    "PARQUET-374": {
        "Key": "PARQUET-374",
        "Summary": "Add api to read dictionary from each column chunk for predicate pushdown",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Zhenxiao Luo",
        "Reporter": "Zhenxiao Luo",
        "Created": "12/Sep/15 01:33",
        "Updated": "09/Mar/16 18:27",
        "Resolved": "09/Mar/16 18:27",
        "Description": "Parquet files's dictionary could be used for predicate pushdown\neg.\nSQL query:\nselect * from table where column = 10;\ncould skip reading the whole row group if the dictionary for column has values [5, 11, 17, 20]\nThis could save IO and improve performance.\nWe implemented predicate pushdown using dictionary in Presto for parquet files, and benchmark shows up to 40X speedup for selective queries.\nNeed to add an api to ParquetFileReader, so that it returns dictionaries for requested columns.\nIf the column is not dictionary encoded in this row group, return null.\nIf the not all column pages are dictionary encoded in this row group, return null.",
        "Issue Links": [
            "/jira/browse/PARQUET-384"
        ]
    },
    "PARQUET-375": {
        "Key": "PARQUET-375",
        "Summary": "Parquet Readme.md still points to release 1.7.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "2.0.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Luciano Resende",
        "Created": "14/Sep/15 17:30",
        "Updated": "22/Sep/15 02:17",
        "Resolved": "14/Sep/15 23:57",
        "Description": "The section \"Add Parquet as a dependency in Maven\" in the Readme.md at https://github.com/apache/parquet-mr still points to version 1.7.0",
        "Issue Links": []
    },
    "PARQUET-376": {
        "Key": "PARQUET-376",
        "Summary": "Tolerate square brackets in PR titles",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.0.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "14/Sep/15 19:37",
        "Updated": "14/Sep/15 23:39",
        "Resolved": "14/Sep/15 23:39",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-377": {
        "Key": "PARQUET-377",
        "Summary": "The file written in version 2 can't be read back",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "1.8.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Davies Liu",
        "Created": "18/Sep/15 19:03",
        "Updated": "18/Sep/15 19:17",
        "Resolved": "18/Sep/15 19:17",
        "Description": "I tried to save a TPC-DS table store_sales as version 2, but it can't been read back:\n\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 852499 in block 0 in file file:/opt/store_sales/part-r-00002-f0497de9-0bf7-4cb2-98d0-b1dcf5a71ca8.gz.parquet\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:201)\n\tat org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:168)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1551)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1121)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1843)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1843)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.parquet.io.ParquetDecodingException: Encoding DELTA_BYTE_ARRAY is only supported for type BINARY\n\tat org.apache.parquet.column.Encoding$7.getValuesReader(Encoding.java:196)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.initDataReader(ColumnReaderImpl.java:537)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.readPageV2(ColumnReaderImpl.java:577)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.access$400(ColumnReaderImpl.java:57)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:521)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:513)\n\tat org.apache.parquet.column.page.DataPageV2.accept(DataPageV2.java:141)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:513)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:505)\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:607)\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:407)\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:209)\n\t... 13 more",
        "Issue Links": []
    },
    "PARQUET-378": {
        "Key": "PARQUET-378",
        "Summary": "Add thoroughly parquet test encodings",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Sergio Pe\u00f1a",
        "Reporter": "Sergio Pe\u00f1a",
        "Created": "23/Sep/15 15:53",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "19/Nov/15 18:47",
        "Description": "PARQUET-246 highlighted the need to have more thorough validation of the Parquet encodings. With the 2.0 encodings as high priority, we need to validate each encoding with its spec (and ensure there is a spec). \nWe also need to add a suite of validations that are used for all encodings that test:\n\nWriting multiple pages (v1 and v2 pages) and multiple row groups\nValidating that each page can be read individually or with last-to-first page order and has the correct set of values (would have caught PARQUET-246).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/274"
        ]
    },
    "PARQUET-379": {
        "Key": "PARQUET-379",
        "Summary": "PrimitiveType.union erases original type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "23/Sep/15 20:44",
        "Updated": "18/Oct/16 00:21",
        "Resolved": "06/Feb/16 19:58",
        "Description": "The following ScalaTest test case\n\n  test(\"merge primitive types\") {\n    val expected =\n      Types.buildMessage()\n        .addField(\n          Types\n            .required(INT32)\n            .as(DECIMAL)\n            .precision(7)\n            .scale(2)\n            .named(\"f\"))\n        .named(\"root\")\n\n    assert(expected.union(expected) === expected)\n  }\n\n\nproduces the following assertion error\n\nmessage root {\n  required int32 f;\n}\n did not equal message root {\n  required int32 f (DECIMAL(9,0));\n}\n\n\nThis is because PrimitiveType.union doesn't handle original type properly. An open question is that, can two primitive types with the same primitive type name but different original types be unioned?",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-385",
            "/jira/browse/PARQUET-753",
            "https://github.com/apache/parquet-mr/pull/315"
        ]
    },
    "PARQUET-380": {
        "Key": "PARQUET-380",
        "Summary": "Cascading and scrooge builds fail when using thrift 0.9.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "23/Sep/15 20:58",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "17/Nov/15 23:56",
        "Description": "This is caused by a transitive dependency on libthrift 0.7.0 from elephantbird. The solution is to add thrift as an explicit (but provided) dependency to those projects.",
        "Issue Links": [
            "/jira/browse/PARQUET-391",
            "https://github.com/apache/parquet-mr/pull/276"
        ]
    },
    "PARQUET-381": {
        "Key": "PARQUET-381",
        "Summary": "It should be possible to merge summary files, and control which files are generated",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "24/Sep/15 02:02",
        "Updated": "12/Dec/22 18:10",
        "Resolved": "13/Oct/15 22:54",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-359",
            "https://github.com/apache/parquet-mr/pull/277"
        ]
    },
    "PARQUET-382": {
        "Key": "PARQUET-382",
        "Summary": "Add a way to append encoded blocks in ParquetFileWriter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "24/Sep/15 17:03",
        "Updated": "29/Nov/18 09:26",
        "Resolved": "08/Dec/15 22:46",
        "Description": "Concatenating two files together currently requires reading the source files and rewriting the content from scratch. This ends up taking a lot of memory, even if the data is already encoded correctly and blocks just need to be appended and have their metadata updated. Merging two files should be fast and not take much memory.",
        "Issue Links": [
            "/jira/browse/PARQUET-1465",
            "/jira/browse/HIVE-9490",
            "https://github.com/apache/parquet-mr/pull/278"
        ]
    },
    "PARQUET-383": {
        "Key": "PARQUET-383",
        "Summary": "ParquetOutputCommitter should propagate errors when writing metadata files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alex Levenson",
        "Created": "24/Sep/15 21:55",
        "Updated": "03/Jul/18 15:17",
        "Resolved": null,
        "Description": "There's a lot of different ways the output committer can fail, or fail to rollback after failing to write metadata files. We should decide whether metadata files are required, and fatal (I think that's reasonable if the user asked for them), and propagate without squashing exceptions.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/502"
        ]
    },
    "PARQUET-384": {
        "Key": "PARQUET-384",
        "Summary": "Add Dictionary Based Filtering to Filter2 API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "24/Sep/15 22:48",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "09/Mar/16 21:22",
        "Description": "Dictionary based predicate evaluation show very significant gains for certain cases like the following:\n\nSparse values in dictionary encoded columns\nData staged sorted by fields used in predicate evaluation\n\nThis would be good to introduce into the filter2 API.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-548",
            "/jira/browse/PARQUET-374",
            "https://github.com/apache/parquet-mr/pull/330"
        ]
    },
    "PARQUET-385": {
        "Key": "PARQUET-385",
        "Summary": "PrimitiveType.union accepts fixed_len_byte_array fields with different lengths when strict mode is on",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "28/Sep/15 18:41",
        "Updated": "06/Feb/16 19:57",
        "Resolved": "06/Feb/16 19:57",
        "Description": "The following two schemas probably shouldn't be allowed to be union-ed when strict schema-merging mode is on:\n\nmessage t1 {\n  required fixed_len_byte_array(10) f;\n}\n\nmessage t2 {\n  required fixed_len_byte_array(5) f;\n}\n\n\nBut currently t1.union(t2, true) yields t1.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-379",
            "https://github.com/apache/parquet-mr/pull/315"
        ]
    },
    "PARQUET-386": {
        "Key": "PARQUET-386",
        "Summary": "Printing out the statistics of metadata in parquet-tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Onur Soyer",
        "Created": "02/Oct/15 00:33",
        "Updated": "07/Jan/19 11:56",
        "Resolved": "10/Jan/18 14:00",
        "Description": "While playing with \"parquet-tools\", I found that the statistics data of columns is not being printed out when the following is executed;\n$ java -jar parquet-tools-1.6.0rc3-SNAPSHOT.jar schema --detailed perf.1000.parquet\nAnd the output for a row group like this;\n=====================================================================================================================\nrow group 1: RC:747388 TS:134218473 OFFSET:4\n--------------------------------------------------------------------------------\ncust_key:  INT64 UNCOMPRESSED DO:0 FPO:4 SZ:5979444/5979444/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\nname:  BINARY UNCOMPRESSED DO:0 FPO:5979448 SZ:16443766/16443766/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\naddress:  BINARY UNCOMPRESSED DO:0 FPO:22423214 SZ:21716568/21716568/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\nnation_key:  INT32 UNCOMPRESSED DO:0 FPO:44139782 SZ:2989697/2989697/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\nphone:  BINARY UNCOMPRESSED DO:0 FPO:47129479 SZ:14201364/14201364/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\nacctbal:  DOUBLE UNCOMPRESSED DO:0 FPO:61330843 SZ:5979444/5979444/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\nmktsegment:  BINARY UNCOMPRESSED DO:0 FPO:67310287 SZ:9714675/9714675/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\ncomment_col:  BINARY UNCOMPRESSED DO:0 FPO:77024962 SZ:57193515/57193515/1.00 VC:747388 ENC:PLAIN,RLE,BIT_PACKED\n=====================================================================================================================\nHowever, it would be great to print out the data of statistics of metadata.",
        "Issue Links": [
            "/jira/browse/PARQUET-327",
            "https://github.com/apache/parquet-mr/pull/279",
            "https://github.com/apache/parquet-mr/pull/442"
        ]
    },
    "PARQUET-387": {
        "Key": "PARQUET-387",
        "Summary": "TwoLevelListWriter does not handle null values in array",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Taras Bobrovytsky",
        "Created": "08/Oct/15 23:26",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "04/Dec/15 19:49",
        "Description": "parquet-mr is unable to handle the following avro schema:\n\n{\"type\": \"record\",\n \"namespace\": \"com.cloudera.impala\",\n \"name\": \"table_3\",\n \"fields\": [\n   {\"name\": \"field_6\", \"type\":\n     {\"type\": \"array\", \"items\": [\"null\",\n       {\"type\": \"map\", \"values\": [\"null\", \"string\"]}]}}]}\n\n\nIf map is null, the following exception happens:\n\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat parquet.avro.AvroWriteSupport.writeMap(AvroWriteSupport.java:185)\n\tat parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:277)\n\tat parquet.avro.AvroWriteSupport.access$400(AvroWriteSupport.java:48)\n\tat parquet.avro.AvroWriteSupport$TwoLevelListWriter.writeCollection(AvroWriteSupport.java:473)\n\tat parquet.avro.AvroWriteSupport$ListWriter.writeList(AvroWriteSupport.java:322)\n\tat parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:275)\n\tat parquet.avro.AvroWriteSupport.writeRecordFields(AvroWriteSupport.java:169)\n\tat parquet.avro.AvroWriteSupport.write(AvroWriteSupport.java:144)\n\tat parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:116)\n\tat parquet.hadoop.ParquetWriter.write(ParquetWriter.java:324)\n\tat com.cloudera.impala.datagenerator.RandomNestedDataGenerator.writeFile(RandomNestedDataGenerator.java:69)\n\tat com.cloudera.impala.datagenerator.RandomNestedDataGenerator.main(RandomNestedDataGenerator.java:284)\n\n\nThe cause is probably because if there is a null value in the array, the TwoLevelListWriter does not check if an element is null: https://github.com/apache/parquet-mr/blob/master/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java#L456",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/291"
        ]
    },
    "PARQUET-388": {
        "Key": "PARQUET-388",
        "Summary": "ProtoRecordConverter might wrongly cast a Message.Builder to Message",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Wu Xiang",
        "Created": "16/Oct/15 05:41",
        "Updated": "20/May/16 03:27",
        "Resolved": null,
        "Description": "ProtoRecordConverter returns current record as follows:\n\n  public T getCurrentRecord() {\n    if (buildBefore) {\n      return (T) this.reusedBuilder.build();\n    } else {\n      return (T) this.reusedBuilder;\n    }\n  }\n\n\nHowever this might fail if T is subclass of Message and buildBefore == false, since it's actually casting a Message.Builder instance to Message type.",
        "Issue Links": []
    },
    "PARQUET-389": {
        "Key": "PARQUET-389",
        "Summary": "Filter predicates should work with missing columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6.0,                                            1.7.0,                                            1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "28/Oct/15 08:07",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "15/Jul/16 16:54",
        "Description": "This issue originates from SPARK-11103, which contains detailed information about how to reproduce it.\nThe major problem here is that, filter predicates pushed down assert that columns they touch must exist in the target physical files. But this isn't true in case of schema merging.\nActually this assertion is unnecessary, because if a column is missing in the filter schema, the column is considered to be filled by nulls, and all the filters should be able to act accordingly. For example, if we push down a = 1 but a is missing in the underlying physical file, all records in this file should be dropped since a is always null. On the other hand, if we push down a IS NULL, all records should be preserved.",
        "Issue Links": [
            "/jira/browse/SPARK-18539",
            "/jira/browse/SPARK-11103",
            "/jira/browse/SPARK-20364",
            "https://github.com/apache/parquet-mr/pull/354"
        ]
    },
    "PARQUET-390": {
        "Key": "PARQUET-390",
        "Summary": "GroupType.union(Type toMerge, boolean strict) does not honor strict parameter",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Michael MacFadden",
        "Created": "04/Nov/15 14:30",
        "Updated": "20/Nov/15 22:41",
        "Resolved": null,
        "Description": "This is the code as it currently stands in master:\n\n@Override\nprotected Type union(Type toMerge, boolean strict) {\n  if (toMerge.isPrimitive()) {\n    throw new IncompatibleSchemaModificationException(\"can not merge primitive type \" + toMerge + \" into group type \" + this);\n  }\n  return new GroupType(toMerge.getRepetition(), getName(), mergeFields(toMerge.asGroupType()));\n}\n\n\nNote the call to mergeFields omits the strict parameter. I believe the code should be:\n\n@Override\nprotected Type union(Type toMerge, boolean strict) {\n  if (toMerge.isPrimitive()) {\n    throw new IncompatibleSchemaModificationException(\"can not merge primitive type \" + toMerge + \" into group type \" + this);\n  }\n  return new GroupType(toMerge.getRepetition(), getName(), mergeFields(toMerge.asGroupType(), strict));\n}\n\n\nNote the call to mergeFields includes the strict parameter.\nI would work on this myself, but I'm having considerable trouble working with the codebase (see e.g. http://stackoverflow.com/questions/31229445/build-failure-apache-parquet-mr-source-mvn-install-failure). Given the (assumed) simplicity of the fix, can a seasoned Parquet contributor take this up? Cheers.",
        "Issue Links": []
    },
    "PARQUET-391": {
        "Key": "PARQUET-391",
        "Summary": "Parquet build fails with thrift9 profile",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yash Datta",
        "Created": "10/Nov/15 18:52",
        "Updated": "20/Nov/15 22:40",
        "Resolved": "20/Nov/15 22:40",
        "Description": "compile parquet build using:\nmvn clean install -Pthrift9 -DskipTests\nbuild fails in parquet-cascading project :\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[10,32] package org.apache.thrift.scheme does not exist\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[11,32] package org.apache.thrift.scheme does not exist\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[12,32] package org.apache.thrift.scheme does not exist\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[14,32] package org.apache.thrift.scheme does not exist\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[15,34] cannot find symbol\n  symbol:   class TTupleProtocol\n  location: package org.apache.thrift.protocol\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[40,44] cannot find symbol\n  symbol:   class IScheme\n  location: class parquet.thrift.test.Name\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[40,54] cannot find symbol\n  symbol:   class SchemeFactory\n  location: class parquet.thrift.test.Name\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[395,61] cannot find symbol\n  symbol:   class SchemeFactory\n  location: class parquet.thrift.test.Name\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[401,51] cannot find symbol\n  symbol:   class StandardScheme\n  location: class parquet.thrift.test.Name\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[462,58] cannot find symbol\n  symbol:   class SchemeFactory\n  location: class parquet.thrift.test.Name\n[ERROR] /mnt/devel/yash/parquet-mr-1/parquet-cascading/target/generated-test-sources/thrift/parquet/thrift/test/Name.java:[468,48] cannot find symbol",
        "Issue Links": [
            "/jira/browse/PARQUET-380"
        ]
    },
    "PARQUET-392": {
        "Key": "PARQUET-392",
        "Summary": "Release Parquet-mr 1.9.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Delivered",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "17/Nov/15 23:34",
        "Updated": "07/Jun/17 22:42",
        "Resolved": "07/Jun/17 22:41",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-415",
            "/jira/browse/PARQUET-352",
            "/jira/browse/PARQUET-353",
            "/jira/browse/PARQUET-367",
            "/jira/browse/PARQUET-372",
            "/jira/browse/PARQUET-379",
            "/jira/browse/PARQUET-385",
            "/jira/browse/PARQUET-387",
            "/jira/browse/PARQUET-400",
            "/jira/browse/PARQUET-569",
            "/jira/browse/PARQUET-623",
            "/jira/browse/PARQUET-686",
            "/jira/browse/PARQUET-751",
            "/jira/browse/PARQUET-305",
            "/jira/browse/PARQUET-225",
            "/jira/browse/PARQUET-99",
            "/jira/browse/PARQUET-327",
            "/jira/browse/PARQUET-384",
            "/jira/browse/PARQUET-397",
            "/jira/browse/PARQUET-318",
            "/jira/browse/PARQUET-393"
        ]
    },
    "PARQUET-393": {
        "Key": "PARQUET-393",
        "Summary": "release parquet-format 2.3.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.3.1,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Julien Le Dem",
        "Created": "18/Nov/15 05:59",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "18/Dec/15 16:45",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-403"
        ]
    },
    "PARQUET-394": {
        "Key": "PARQUET-394",
        "Summary": "OOM when writing a table with 1700 columns in hive",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yuren Wu",
        "Created": "19/Nov/15 15:18",
        "Updated": "19/Nov/15 23:13",
        "Resolved": "19/Nov/15 21:06",
        "Description": "When running insert into <tablename> select * from <source> following exception were thrown. There are 1700 columns (all string), and total length of a single row is around 7000 bytes. \nYarn container size has been increased from 1GB to 4GB. Not quit helping. \nFATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Java heap space\n\tat parquet.column.values.dictionary.IntList.initSlab(IntList.java:90)\n\tat parquet.column.values.dictionary.IntList.<init>(IntList.java:86)\n\tat parquet.column.values.dictionary.DictionaryValuesWriter.<init>(DictionaryValuesWriter.java:93)\n\tat parquet.column.values.dictionary.DictionaryValuesWriter$PlainBinaryDictionaryValuesWriter.<init>(DictionaryValuesWriter.java:229)\n\tat parquet.column.ParquetProperties.dictionaryWriter(ParquetProperties.java:131)\n\tat parquet.column.ParquetProperties.dictWriterWithFallBack(ParquetProperties.java:178)\n\tat parquet.column.ParquetProperties.getValuesWriter(ParquetProperties.java:203)\n\tat parquet.column.impl.ColumnWriterV1.<init>(ColumnWriterV1.java:84)\n\tat parquet.column.impl.ColumnWriteStoreV1.newMemColumn(ColumnWriteStoreV1.java:68)\n\tat parquet.column.impl.ColumnWriteStoreV1.getColumnWriter(ColumnWriteStoreV1.java:56)\n\tat parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.<init>(MessageColumnIO.java:183)\n\tat parquet.io.MessageColumnIO.getRecordWriter(MessageColumnIO.java:375)\n\tat parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:107)\n\tat parquet.hadoop.InternalParquetRecordWriter.checkBlockSizeReached(InternalParquetRecordWriter.java:127)\n\tat parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:118)\n\tat parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)\n\tat parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)\n\tat org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:111)\n\tat org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:124)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:695)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)",
        "Issue Links": []
    },
    "PARQUET-395": {
        "Key": "PARQUET-395",
        "Summary": "System.out is used as logger in org.apache.parquet.Log",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Henrik Baastrup",
        "Created": "25/Nov/15 17:22",
        "Updated": "09/Dec/15 01:00",
        "Resolved": "09/Dec/15 01:00",
        "Description": "The use of System.out in the StreamHandler at line 62 in the org.apache.parquet.Log provoke that the java.util.logging.LogManager will close System.out in case the reset method is called on the LogManager.\nThis is special problematic when Shutdown Hooks are used in a project, as the LogManager set-up one, there call the reset method, and for this reason a race condition exist on System.out, if used in the project Shutdown Hook.\nOther scenarios might also exist where a program call the LogManager reset method.\nAn eventual solution is to NOT use java.util.logging.Logger at all in the Parquet environment but use either log4j or slf4j as used in almost every other part in the Hadoop environment, this would also allow the user to control the logging much better than today.",
        "Issue Links": [
            "/jira/browse/PARQUET-305"
        ]
    },
    "PARQUET-396": {
        "Key": "PARQUET-396",
        "Summary": "The builder for AvroParquetReader loses the record type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-avro",
        "Assignee": "Chris Bannister",
        "Reporter": "Chris Bannister",
        "Created": "27/Nov/15 14:20",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "01/Dec/15 00:29",
        "Description": "The builder for AvroParquetReader loses the record type, this is because AvroParquetReader builder does not extend ParquetReader.builder<T>",
        "Issue Links": []
    },
    "PARQUET-397": {
        "Key": "PARQUET-397",
        "Summary": "Pig Predicate Pushdown using Filter2 API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-pig",
        "Assignee": "Daniel Weeks",
        "Reporter": "Daniel Weeks",
        "Created": "01/Dec/15 19:41",
        "Updated": "26/Feb/16 18:29",
        "Resolved": "26/Feb/16 18:29",
        "Description": "Use the pushdown API from Pig to build filter predicates for parquet.  See PIG-3760.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "/jira/browse/PIG-4092",
            "https://github.com/apache/parquet-mr/pull/331"
        ]
    },
    "PARQUET-398": {
        "Key": "PARQUET-398",
        "Summary": "Testing JIRA ticket for testing committership",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "03/Dec/15 06:57",
        "Updated": "03/Dec/15 06:57",
        "Resolved": null,
        "Description": "This ticket is only used for testing committership. Please keep it open.\nNew committers can submit a PR to add their names to dev/COMMITTERS.md, and attach ID of this JIRA ticket to the PR title (this convention is required by the dev/merge_parquet_pr.py script).",
        "Issue Links": []
    },
    "PARQUET-399": {
        "Key": "PARQUET-399",
        "Summary": "website should list mailing list adresses",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andr\u00e9 Kelpe",
        "Created": "07/Dec/15 10:39",
        "Updated": "13/Dec/15 00:01",
        "Resolved": null,
        "Description": "Other apache projects are listing their mailing lists on their web-site. The information is not easy to find for parquet.",
        "Issue Links": []
    },
    "PARQUET-400": {
        "Key": "PARQUET-400",
        "Summary": "Error reading some files after PARQUET-77 bytebuffer read path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Jason Altekruse",
        "Reporter": "Jason Altekruse",
        "Created": "08/Dec/15 15:28",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "16/Aug/16 17:48",
        "Description": "This issue is based on a discussion on the list started by dweeks\nFull discussion:\nhttps://mail-archives.apache.org/mod_mbox/parquet-dev/201512.mbox/%3CCAMpYv7C_szTheua9N95bXvbd2ROmV63BFiJTK-K-aDNK6ZNBKA%40mail.gmail.com%3E\nFrom the thread (he later provided a small repro file that is attached here):\nJust wanted to see if you or anyone else has run into problems reading\nfiles after the ByteBuffer patch.  I've been running into issues and have\nnarrowed it down to the ByteBuffer commit using a small repro file (written\nwith 1.6.0, unfortunately can't share the data).\nIt doesn't happen for every file, but those that fail give this error:\ncan not read class org.apache.parquet.format.PageHeader: Required field\n'uncompressed_page_size' was not found in serialized data! Struct:\nPageHeader(type:null, uncompressed_page_size:0, compressed_page_size:0)",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/306",
            "https://github.com/apache/parquet-mr/pull/349"
        ]
    },
    "PARQUET-401": {
        "Key": "PARQUET-401",
        "Summary": "Deprecate Log and move to SLF4J Logger",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "08/Dec/15 18:44",
        "Updated": "08/Aug/17 16:38",
        "Resolved": null,
        "Description": "The current Log class is intended to allow swapping out logger back-ends, but SLF4J already does this. It also doesn't expose as nice of an API as SLF4J, which can handle formatting to avoid the cost of building log messages that won't be used. I think we should deprecate the org.apache.parquet.Log class and move to using SLF4J directly, instead of wrapping SLF4J (PARQUET-305).\nThis will require deprecating the current Log class and replacing the current uses of it with SLF4J.",
        "Issue Links": [
            "/jira/browse/PARQUET-528",
            "/jira/browse/PARQUET-529",
            "/jira/browse/PARQUET-369",
            "/jira/browse/PARQUET-305",
            "/jira/browse/PARQUET-408"
        ]
    },
    "PARQUET-402": {
        "Key": "PARQUET-402",
        "Summary": "Apache Pig cannot store Map data type into Parquet format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-pig",
        "Assignee": null,
        "Reporter": "Jerry Ylilammi",
        "Created": "10/Dec/15 10:53",
        "Updated": "12/Dec/15 23:57",
        "Resolved": "10/Dec/15 14:22",
        "Description": "Trying to store simple map with two entries gives me following exception:\n\ntable_with_map_data: {my_map: map[]}\n2015-12-10 11:58:54,478 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n2015-12-10 11:58:54,498 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. Invalid map Schema, schema should contain exactly one field: my_map: map\n\nFor example taking any input and doing this gives me the exception:\n\ntable_with_map_data = FOREACH random_data GENERATE TOMAP('123', 'hello', '456', 'world') as (my_map);\nDESCRIBE table_with_map_data;\nSTORE table_with_map_data INTO '...' USING ParquetStorer();\n\nI'm using latest version of Pig: Apache Pig version 0.15.0 (r1682971) compiled Jun 01 2015, 11:44:35\nand Parquet: parquet-pig-bundle-1.6.0.jar\nEDIT: I noticed Parquet 1.8.1 is out. I switched to it and were forced to update the pig script to use full path with ParquetStorer. However this gives me same error as 1.6.0.\n\nSTORE table_with_map_data INTO '/Users/jerry/tmp/parquet/output/parquet' USING org.apache.parquet.pig.ParquetStorer();",
        "Issue Links": []
    },
    "PARQUET-403": {
        "Key": "PARQUET-403",
        "Summary": "Remove incubating from parquet-format release process",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.3.1",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/Dec/15 22:06",
        "Updated": "14/Dec/15 18:00",
        "Resolved": "14/Dec/15 18:00",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-393",
            "https://github.com/apache/parquet-format/pull/34"
        ]
    },
    "PARQUET-404": {
        "Key": "PARQUET-404",
        "Summary": "Replace git@github.com.apache for HTTPS URL on dev/README.md to avoid permission issues",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Sergio Pe\u00f1a",
        "Reporter": "Sergio Pe\u00f1a",
        "Created": "14/Dec/15 16:30",
        "Updated": "17/Dec/15 23:43",
        "Resolved": "17/Dec/15 23:43",
        "Description": "When trying to merge a PR following the notes from dev/README.md and using dev/merge_parquet_pr.py and without having a public key attached to the committer github account, then a permission error is displayed when attempting to fetch from git@github.com:apache/parquet-mr.git.\nWe should add a note to dev/README.md that mentions that a SSH public key on the committer github account is needed in order to fetch code from apache-github.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/301"
        ]
    },
    "PARQUET-405": {
        "Key": "PARQUET-405",
        "Summary": "Backwards-incompatible change to thrift metadata",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ben Kirwin",
        "Created": "14/Dec/15 17:21",
        "Updated": "15/Dec/15 00:05",
        "Resolved": "14/Dec/15 23:54",
        "Description": "Sometime in the last few versions, a isStructOrUnion field has been added to the `thrift.descriptor` written to the parquet header:\n\n{\n    \"children\": [ ... ],\n    \"id\": \"STRUCT\", \n    \"structOrUnionType\": \"STRUCT\"\n}\n\n\nThe current release now throws an exception when that field is missing  / UNKNOWN). This makes it impossible to read back thrift data written using a previous release.",
        "Issue Links": [
            "/jira/browse/PARQUET-346"
        ]
    },
    "PARQUET-406": {
        "Key": "PARQUET-406",
        "Summary": "Counter Initialization causes NPE",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Reuben Kuhnert",
        "Reporter": "Reuben Kuhnert",
        "Created": "14/Dec/15 21:07",
        "Updated": "15/Dec/15 16:05",
        "Resolved": null,
        "Description": "CREATE EXTERNAL TABLE api_hit_parquet_test ROW FORMAT SERDE 'com.foursquare.hadoop.hive.serde.RecordV2SerDe' WITH SERDEPROPERTIES ('serialization.class' = 'com.foursquare.logs.gen.ApiHit') STORED AS INPUTFORMAT 'com.foursquare.hadoop.hive.io.HiveThriftParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '/user/bly/api_hit_parquet' TBLPROPERTIES ('thrift.parquetfile.input.format.thrift.class' = 'com.foursquare.logs.gen.ApiHit\u2019)\n\n\nThe table is successfully created, and I can verify the schema is correct by running DESCRIBE FORMATTED on it. However, when I try to do a simple SELECT * on the table, I get the following stack trace:\n\njava.io.IOException: java.lang.RuntimeException: Could not read first record (and it was not an EOF)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1657)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:756)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)\nCaused by: java.lang.RuntimeException: Could not read first record (and it was not an EOF)\n        at com.twitter.elephantbird.mapred.input.DeprecatedInputFormatWrapper$RecordReaderWrapper.initKeyValueObjects(DeprecatedInputFormatWrapper.java:280)\n        at com.twitter.elephantbird.mapred.input.DeprecatedInputFormatWrapper$RecordReaderWrapper.createValue(DeprecatedInputFormatWrapper.java:297)\n        at com.foursquare.hadoop.hive.io.HiveThriftParquetInputFormat$$anon$1.<init>(HiveThriftParquetInputFormat.scala:47)\n        at com.foursquare.hadoop.hive.io.HiveThriftParquetInputFormat.getRecordReader(HiveThriftParquetInputFormat.scala:46)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:667)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:323)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:445)\n        ... 9 more\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://hadoop-alidoro-nn-vip/user/bly/api_hit_parquet/part-m-00000.parquet\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:243)\n        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)\n        at com.twitter.elephantbird.mapred.input.DeprecatedInputFormatWrapper$RecordReaderWrapper.initKeyValueObjects(DeprecatedInputFormatWrapper.java:271)\n        ... 15 more\nCaused by: java.lang.NullPointerException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.parquet.hadoop.util.ContextUtil.invoke(ContextUtil.java:264)\n        at org.apache.parquet.hadoop.util.ContextUtil.incrementCounter(ContextUtil.java:273)\n        at org.apache.parquet.hadoop.util.counters.mapreduce.MapReduceCounterAdapter.increment(MapReduceCounterAdapter.java:38)\n        at org.apache.parquet.hadoop.util.counters.BenchmarkCounter.incrementTotalBytes(BenchmarkCounter.java:78)\n        at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:497)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:130)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:214)\n        ... 17 more\n\n\nI have spent some time following this stack trace, and it appears that the error lies in the Counter code, which is odd because I don\u2019t do anything with that. Is there some way I need to initialize counters?\nTo be specific, I have found that MapReduceCounterAdapter is being created with a null parameter. Here is the constructor:\n\npublic MapReduceCounterAdapter(Counter adaptee) {\n    this.adaptee = adaptee;\n  }\n\n\nSo adaptee is being passed as null, and then getting called later on, causing my NullPointerException.\nThe adaptee parameter is created by this method:\n\npublic static Counter getCounter(TaskInputOutputContext context,\n                                   String groupName, String counterName) {\n    return (Counter) invoke(GET_COUNTER_METHOD, context, groupName, counterName);\n  }",
        "Issue Links": []
    },
    "PARQUET-407": {
        "Key": "PARQUET-407",
        "Summary": "Incorrect delta-encoding example",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "choi woo cheol",
        "Created": "15/Dec/15 01:52",
        "Updated": "07/Jan/16 00:10",
        "Resolved": "06/Jan/16 23:51",
        "Description": "The minimum and the number of bits are incorrect at delta encoding Example 2 In Encodings.md.\nIn the example, \n\nExample 2\n\n7, 5, 3, 1, 2, 3, 4, 5, the deltas would be\n\n-2, -2, -2, 1, 1, 1, 1\nThe minimum is -2, so the relative deltas are:\n\n0, 0, 0, 3, 3, 3, 3\n\nThe encoded data is\n\nheader: 8 (block size), 1 (miniblock count), 8 (value count), 7 (first value)\n\nblock 0 (minimum delta), 2 (bitwidth), 000000111111b (0,0,0,3,3,3 packed on 2 bits)\n\n\nThe minimum is -2 and the relative deltas are 0, 0, 0, 3, 3, 3, 3. So, this should be corrected as below:\n\nblock -2 (minimum delta), 2 (bitwidth), 00000011111111b (0,0,0,3,3,3,3 packed on 2 bits)",
        "Issue Links": []
    },
    "PARQUET-408": {
        "Key": "PARQUET-408",
        "Summary": "Shutdown hook in parquet-avro library corrupts data and disables logging",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Michal Turek",
        "Reporter": "Michal Turek",
        "Created": "16/Dec/15 13:34",
        "Updated": "16/Feb/16 12:57",
        "Resolved": "16/Feb/16 12:57",
        "Description": "Parquet-avro and probably also other Parquet libraries are not well behaved. It registers a shutdown hook that bypasses application shutdown sequence, corrupts data written to currently opened Parquet file(s) and disables or reconfigures slf4j/logback logger so no further log message is visible.\nScope\nOur application is a microservice that handles stop request in form of signal SIGTERM, resp. JVM shutdown hook. If it arrives the application will close all opened files (writers), release all other resources and gracefully shutdown. We are swiching from sequence files to Parquet at the moment and using Maven dependency org.apache.parquet:parquet-avro:1.8.1 which is current latest version. We are using Runtime.getRuntime().addShutdownHook() to handle SIGTERM.\nExample code\nSee archive in attachment.\n\nOptionally update version of hadoop-client in pom.xml to match your Hadoop.\nUse mvn package to compile.\nCopy Hadoop configuration XMLs to config directory.\nUpdate configuration at the top of ParquetBrokenShutdown class.\nExecute ParquetBrokenShutdown class.\nSend SIGTERM to shutdown the application (kill PID).\n\nInitial analysis\nParquet library tries to care about application shutdown but this introduces more issues than solves. If application is writing to a file and the library asynchronously decides to close underlying writer, data loss will occur. The handle is just closed and all remaining records can't be written.\n\nWriting to HDFS/Parquet failed\njava.io.IOException: can not write PageHeader(type:DICTIONARY_PAGE, uncompressed_page_size:14, compressed_page_size:34, dictionary_page_header:DictionaryPageHeader(num_values:1, encoding:PLAIN))\n\tat org.apache.parquet.format.Util.write(Util.java:224)\n\tat org.apache.parquet.format.Util.writePageHeader(Util.java:61)\n\tat org.apache.parquet.format.converter.ParquetMetadataConverter.writeDictionaryPageHeader(ParquetMetadataConverter.java:760)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:307)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:179)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:238)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:165)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)\n\tat org.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:297)\n\tat com.avast.bugreport.parquet.ParquetBrokenShutdown.writeParquetFile(ParquetBrokenShutdown.java:86)\n\tat com.avast.bugreport.parquet.ParquetBrokenShutdown.run(ParquetBrokenShutdown.java:53)\n\tat com.avast.bugreport.parquet.ParquetBrokenShutdown.main(ParquetBrokenShutdown.java:153)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)\nCaused by: parquet.org.apache.thrift.transport.TTransportException: java.nio.channels.ClosedChannelException\n\tat parquet.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)\n\tat parquet.org.apache.thrift.transport.TTransport.write(TTransport.java:105)\n\tat parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:424)\n\tat parquet.org.apache.thrift.protocol.TCompactProtocol.writeByteDirect(TCompactProtocol.java:431)\n\tat parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBeginInternal(TCompactProtocol.java:194)\n\tat parquet.org.apache.thrift.protocol.TCompactProtocol.writeFieldBegin(TCompactProtocol.java:176)\n\tat org.apache.parquet.format.InterningProtocol.writeFieldBegin(InterningProtocol.java:74)\n\tat org.apache.parquet.format.PageHeader.write(PageHeader.java:918)\n\tat org.apache.parquet.format.Util.write(Util.java:222)\n\t... 16 more\nCaused by: java.nio.channels.ClosedChannelException\n\tat org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1635)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:104)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat parquet.org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:145)\n\t... 24 more\n\n\nStored file is also currupted in such case. It contains only four bytes \"PAR1\" header so actually all data are lost.\nTogether with that whole application logging using slf4j/logback is reconfigured or completely disabled so no more application's log messages appear in the output. This may be related to PARQUET-369 and will be hopefully fixed in PARQUET-401. No such issue was observed when sequence files were used in the past.\nKnown workaround\nJava executes threads of shutdown hooks independently and in parallel, but it is possible for the application to force its own shutdown to be executed before the Parquet's one. Parquet library uses Hadoop's ShutdownHookManager to register the hook which has a possibility to define a priority.\n\nif (!USE_WORKAROUND) {\n\tRuntime.getRuntime().addShutdownHook(shutdownHook);\n} else {\n\tint maxPriority = Integer.MAX_VALUE; // Execute application hook before hook in Hadoop library\n\tShutdownHookManager.get().addShutdownHook(shutdownHook, maxPriority);\n}\n\n\nThis workaround would be impossible if there are two or more such nasty libraries.\nNo workaround has been found for the second issue with disabled logging until now.\nSuggestions for fix\n\nNever touch logging configuration in libraries.\n\t\nUse slf4j-api interfaces and just output log messages.\nApplication is responsible for configuration of logging.\n\n\nDo not register any shutdown hook in libraries that releases resources that are still possibly in use.\n\t\nInstead provide an API to the application to close/free the allocated resources (already present).\nThe application is responsible to call close on all Closeables during shutdown in correct order and correct time.\nNo library has enough information for that.\n\n\nBe well behaved library, do not try to be smarter than the application, it isn't possible.",
        "Issue Links": [
            "/jira/browse/PARQUET-369",
            "/jira/browse/PARQUET-401",
            "/jira/browse/PARQUET-412"
        ]
    },
    "PARQUET-409": {
        "Key": "PARQUET-409",
        "Summary": "InternalParquetRecordWriter doesn't use min/max row counts",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "16/Dec/15 18:50",
        "Updated": "18/Jan/19 15:06",
        "Resolved": null,
        "Description": "PARQUET-99 added settings to control the min and max number of rows between size checks when flushing pages, and a setting to control whether to always use a static size (the min). The InternalParquetRecordWriter has similar checks that don't use those settings. We should determine if it should update it to use those settings or similar.",
        "Issue Links": [
            "/jira/browse/PARQUET-869",
            "/jira/browse/PARQUET-99",
            "https://github.com/apache/parquet-mr/pull/495"
        ]
    },
    "PARQUET-410": {
        "Key": "PARQUET-410",
        "Summary": "Fix subprocess hang in merge_parquet_pr.py",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "18/Dec/15 17:11",
        "Updated": "03/Feb/16 19:52",
        "Resolved": "03/Feb/16 19:52",
        "Description": "When I merge pull requests with python 2.7.9, the git push command hangs after succeeding. This appears to be caused by something similar to the subprocess PIPE problem although I'm not sure exactly what's happening. Everything works fine if I remove the stderr=subprocess.STDOUT option.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/302"
        ]
    },
    "PARQUET-411": {
        "Key": "PARQUET-411",
        "Summary": "Format: Add a flag when min/max are truncated",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "format-2.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "18/Dec/15 17:27",
        "Updated": "18/Dec/15 17:28",
        "Resolved": null,
        "Description": "PARQUET-372 drops page and column chunk stats when values are larger than 4k to avoid storing very large values in page headers and the file footer. An alternative approach is to truncate the values, which would still allow filtering on page stats. The problem with truncating values is that the value in stats may not be the true min or max so engines that use these values as the result of aggregations like min(col) would return incorrect data. We should consider adding metadata to allow truncating values for filtering that captures the fact that the values have been modified.",
        "Issue Links": [
            "/jira/browse/PARQUET-372"
        ]
    },
    "PARQUET-412": {
        "Key": "PARQUET-412",
        "Summary": "Format: Do not shade slf4j-api",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Ryan Blue",
        "Created": "18/Dec/15 17:36",
        "Updated": "10/Oct/17 20:51",
        "Resolved": "10/Oct/17 20:51",
        "Description": "PARQUET-369 fixed warnings from shading slf4j-api, but a consequence of shading is that the log messages from thrift for parquet-format classes are dropped. This was an accepted trade-off until PARQUET-305 changed logging in the rest of the library to SLF4J. Now that the slf4j-api is a dependency for all of Parquet except parquet-format, it no longer makes sense to suppress the format thrift logs to avoid exposing it.\nThis also requires PARQUET-371 because thrift 0.7.0 relies on a very old version of slf4j-api.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-369",
            "/jira/browse/PARQUET-305",
            "/jira/browse/PARQUET-408",
            "/jira/browse/PARQUET-371",
            "https://github.com/apache/parquet-format/pull/50"
        ]
    },
    "PARQUET-413": {
        "Key": "PARQUET-413",
        "Summary": "Test failures for Java 8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "18/Dec/15 23:02",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "07/Jan/16 17:34",
        "Description": "There are a few test failures in Java 8 that should be fixed. Mostly due to relying on the order of set and map iteration. This can be fixed by using LinkedHashSet and LinkedHashMap.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/304"
        ]
    },
    "PARQUET-414": {
        "Key": "PARQUET-414",
        "Summary": "Binary tests check conditions not part of the API contract",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "22/Dec/15 21:36",
        "Updated": "22/Dec/15 21:36",
        "Resolved": null,
        "Description": "TestBinary#testConstantCopy validates that when the backing buffer for a Binary is changed, the changes can be seen when calling either Binary#getBytes or Binary#getBytesUnsafe. This is not a part of the API contract for constant binary. The API guarantees the opposite: that if a reused buffer changes then those changes won't affect the value already returned by getBytes.\nByteBufferBackedBinary caches the value returned by getBytes, which is allowed in the API. This causes tests to fail when it is expected that a change to the underlying buffer changes the value of the binary, when the contract for constant buffers is that the underlying buffer will not be changed.",
        "Issue Links": []
    },
    "PARQUET-415": {
        "Key": "PARQUET-415",
        "Summary": "ByteBufferBackedBinary serialization is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "22/Dec/15 23:02",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "03/Feb/16 20:46",
        "Description": "While working on logical types for parquet-avro, I updated the FromStringBinary to work with a CharSequence instead of a String, which broke TestFilterApiMethods#testSerializable. The underlying problem is that when the buffer-backed binary is deserialized, length and offset are not initialized so the buffer is correct but the apparent length of the binary is 0.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/305",
            "https://github.com/apache/parquet-mr/pull/305/files"
        ]
    },
    "PARQUET-416": {
        "Key": "PARQUET-416",
        "Summary": "C++11, cpplint cleanup, package target and header installation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "24/Dec/15 21:29",
        "Updated": "08/Jan/16 23:55",
        "Resolved": "08/Jan/16 23:52",
        "Description": "I'm planning to work on building out parquet-cpp with columnar data structures (see Arrow proposal) for materialized in-memory data and feature complete reader/writers so that native-code consumers like Python can finally read and write Parquet files at native speeds. It would be great to have all this officially a part of Apache Parquet. \nThis adds minimal support to be able to install the resulting libparquet.so and its various header files to support minimally viable development on downstream C++ and Python projects that will need to depend on this. It also builds in C++11 mode and passes Google's cpplint.",
        "Issue Links": [
            "/jira/browse/PARQUET-267"
        ]
    },
    "PARQUET-417": {
        "Key": "PARQUET-417",
        "Summary": "Questionable encoding decisions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Benjamin Anderson",
        "Created": "05/Jan/16 02:19",
        "Updated": "06/Jan/16 18:41",
        "Resolved": "06/Jan/16 17:33",
        "Description": "(Opening a ticket here because my mail to dev@ disappeared\nand there doesn't seem to be any other way to contact Parquet\ndevs - feel free to redirect me somewhere else)\nI'm working on a small Parquet project and encountering\nsome surprising results with regard to encoding decisions.\nMy dataset consists of ~1.5MM log lines parsed to an Avro schema and\nwritten to a Parquet file via AvroParquetWriter. According to its log\noutput, Parquet is writing all int/long columns out with either\n[BIT_PACKED, PLAIN] or [BIT_PACKED, PLAIN_DICTIONARY]. This surprised\nme - at least one of those columns is a monotonic epoch value that should be\nquite amenable to the DELTA_BINARY_PACKED. What's the best way to\nunderstand Parquet's encoding choices?\nSecondary question: Is  DELTA_BINARY_PACKED supported for INT64\ncolumns? The documentation[1] says it is, but the code[2] suggests\notherwise.\nCheers.\n[1]: https://github.com/apache/parquet-format/blob/master/Encodings.md#delta-encoding-delta_binary_packed--5\n[2]: https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/Encoding.java#L166-L168",
        "Issue Links": []
    },
    "PARQUET-418": {
        "Key": "PARQUET-418",
        "Summary": "Add a utility to print contents of a Parquet file to stdout",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "05/Jan/16 21:16",
        "Updated": "27/Jan/16 00:53",
        "Resolved": "27/Jan/16 00:53",
        "Description": "To improve the usability/testability of parquet-cpp, the library needs a utility to print the contents of a Parquet file. incubator-parquet-cpp used to have a parquet_reader utility, but a) it was not ported to the Apache, and b) it had memory leaks and mismanaged file handles, and required a lot of improvement.\nUsing parquet_reader as a starting point, I will build a utility for printing a Parquet file contents.",
        "Issue Links": []
    },
    "PARQUET-419": {
        "Key": "PARQUET-419",
        "Summary": "Update dev script in parquet-cpp to remove incubator.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nong Li",
        "Created": "07/Jan/16 04:01",
        "Updated": "13/Jan/16 19:36",
        "Resolved": "13/Jan/16 19:36",
        "Description": "Current script has some wrong paths. It still references incubator.",
        "Issue Links": []
    },
    "PARQUET-420": {
        "Key": "PARQUET-420",
        "Summary": "Provide example to write/save your own object (without thrift, avro, protoc)",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nicolas Romanetti",
        "Created": "08/Jan/16 10:04",
        "Updated": "12/Jan/16 22:34",
        "Resolved": null,
        "Description": "I am studying parquet and found that it is not so easy to grasp the basic mechanism for writing/reading your own objects to a parquet file when you are not using protocol buffer, avro or thrift.\nSo my wish is to have a module that cover this topic.\nI think for educational purposes it may have some value as it would help people get into the code.\nI did the exercice, the code is here:\nhttps://github.com/nromanetti/parquet-mr/tree/master/parquet-manual",
        "Issue Links": []
    },
    "PARQUET-421": {
        "Key": "PARQUET-421",
        "Summary": "Fix mismatch of javadoc names and method parameters in module encoding, column, and hadoop",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "12/Jan/16 01:35",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "12/Jan/16 22:25",
        "Description": "Codes change now and then, but some corresponding doc comments are left out.\nThis issue fixes only the doc comments that should have been changed. It should be OK, since none codes are touched.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/307"
        ]
    },
    "PARQUET-422": {
        "Key": "PARQUET-422",
        "Summary": "Fix a potential bug in MessageTypeParser where we ignore and overwrite the initial value of a method parameter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "12/Jan/16 02:19",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "12/Jan/16 22:46",
        "Description": "In org.apache.parquet.schema.MessageTypeParser, for addGroupType() and addPrimitiveType(), the initial value of this parameter t is ignored, and t is overwritten here.\nThis often indicates a mistaken belief that the write to the parameter will be conveyed back to the caller.\nThis is a bug found by FindBugs\u2122.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/308"
        ]
    },
    "PARQUET-423": {
        "Key": "PARQUET-423",
        "Summary": "Make writing Avro to Parquet less noisy",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.10.0,                                            1.8.2",
        "Component/s": "parquet-avro",
        "Assignee": "Niels Basjes",
        "Reporter": "Niels Basjes",
        "Created": "12/Jan/16 15:34",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "26/Oct/16 16:13",
        "Description": "When writing Avro files to disk using the AvroParquetWriter for each column in the file some statistics are written to the Logging system.\nWhen writing files based on a large Avro schema often the output of this logging is no longer useful and becomes a hassle.\nBecause the logging level is hardcoded (why?) into the parquet library I would like to introduce a switch that allows to enable/disable this type of logging.\n\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 90B for [IPAddress] BINARY: 60 values, 26B raw, 47B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 7 entries, 77B raw, 7B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 102B for [country] BINARY: 60 values, 26B raw, 47B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 7 entries, 119B raw, 7B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 152B for [windowid] BINARY: 60 values, 33B raw, 51B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 12 entries, 480B raw, 12B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 77B for [customerId] BINARY: 58 values, 22B raw, 42B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 7 entries, 49B raw, 7B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 86B for [sessionId] BINARY: 58 values, 28B raw, 43B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 10 entries, 110B raw, 10B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 93B for [sessionEventNr] INT64: 58 values, 34B raw, 48B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 14 entries, 112B raw, 14B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 114B for [visitId] BINARY: 58 values, 28B raw, 43B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 10 entries, 250B raw, 10B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 90B for [visitEventNr] INT64: 58 values, 34B raw, 45B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 11 entries, 88B raw, 11B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 112B for [timestamp] INT64: 58 values, 50B raw, 66B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 46 entries, 368B raw, 46B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 85B for [IPAddress] BINARY: 58 values, 22B raw, 42B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 7 entries, 77B raw, 7B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [country] BINARY: 58 values, 22B raw, 42B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 7 entries, 119B raw, 7B comp}\nJan 12, 2016 1:43:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 144B for [windowid] BINARY: 58 values, 28B raw, 43B comp, 1 pages, encodings: [RLE_DICTIONARY, PLAIN], dic { 10 entries, 400B raw, 10B comp}",
        "Issue Links": []
    },
    "PARQUET-424": {
        "Key": "PARQUET-424",
        "Summary": "Vectorized Read",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "13/Jan/16 00:25",
        "Updated": "13/Jan/16 00:25",
        "Resolved": null,
        "Description": "The goal is to implement more CPU efficient vectorized readers for columns.\nVectorized execution engines can use them and skip assembly altogether.",
        "Issue Links": []
    },
    "PARQUET-425": {
        "Key": "PARQUET-425",
        "Summary": "Fix the bug when predicate contains columns not specified in prejection, to prevent filtering out data improperly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "13/Jan/16 06:59",
        "Updated": "02/Jan/18 10:22",
        "Resolved": null,
        "Description": "As is reported in Parquet-427, data will be filtered out improperly under the case where:\n1. predicates - requested schema \u2260 \u2205\n2. predicates - file schema \uff1d \u2205\nTo give an example:\ndata:\n\n\n\na\nb\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n\nfile schema: a,b, requested schema: a, and predicate: b = 1\nwe should get:\n\n\n\na\n\n\n1\n\n\n\nbut we'll end up get nothing, which is wrong.\nThis issue proposes to fix this.",
        "Issue Links": []
    },
    "PARQUET-426": {
        "Key": "PARQUET-426",
        "Summary": "Throw Exception when predicate contains columns not specified in prejection, to prevent filtering out data improperly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "13/Jan/16 07:09",
        "Updated": "06/Jan/17 17:47",
        "Resolved": null,
        "Description": "As is reported by Parquet-425, data will be filtered out improperly under certain cases.\nBefore Parquet-425 is fixed, let's throw an Exception to warn the upper application that work-arounds should be done.",
        "Issue Links": []
    },
    "PARQUET-427": {
        "Key": "PARQUET-427",
        "Summary": "Push predicates into the whole read path",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "13/Jan/16 07:23",
        "Updated": "06/Jan/17 17:41",
        "Resolved": null,
        "Description": "Regarding primitive types, there are 3 import primitive types set:\n\nthe file schema primitive type set\nthe requested schema primitive type set\nthe predicates primitive type set\n\nCurrently the file schema primitive type set and the requested schema primitive type set has been pushed into the whole read path, but not the the predicates primitive type set.\nThis brings some problems like:\n\nPARQUET-389, SPARK-11103, SPARK-11434\nPARQUET-425, PARQUET-426\nPARQUET-295\n\nThis issue propose to push the predicates primitive type set into the whole read path as well. When this is resolved, hopefully the issues listed above would also be resolved as well.\nAttached is a change proposal.",
        "Issue Links": []
    },
    "PARQUET-428": {
        "Key": "PARQUET-428",
        "Summary": "Support INT96 and FIXED_LEN_BYTE_ARRAY types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "13/Jan/16 21:16",
        "Updated": "29/Jan/16 01:39",
        "Resolved": "29/Jan/16 01:39",
        "Description": "I would like to add support for INT96 and FIXED_LEN_BYTE_ARRAY parquet types.\nHive data types DATE and TIMESTAMP get mapped to INT96 parquet type.\nHive DECIMAL gets mapped to parquet FIXED_LEN_BYTE_ARRAY type.",
        "Issue Links": []
    },
    "PARQUET-429": {
        "Key": "PARQUET-429",
        "Summary": "Enables predicates collecting their referred columns",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "14/Jan/16 03:47",
        "Updated": "06/Jan/17 17:37",
        "Resolved": null,
        "Description": "Sometimes we need to collect the columns referred by the predicates to, say, do validation.\nThis issue propose to enable all 3 FilterCompats to collect the referred columns.",
        "Issue Links": []
    },
    "PARQUET-430": {
        "Key": "PARQUET-430",
        "Summary": "Change to use Locale parameterized version of String.toUpperCase()/toLowerCase",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "17/Jan/16 09:03",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "16/Feb/16 00:36",
        "Description": "A String is being converted to upper or lowercase, using the platform's default encoding. This may result in improper conversions when used with international characters.\nFor instance, \"TITLE\".toLowerCase() in a Turkish locale returns \"t\u0131tle\", where '\u0131' \u2013 without a dot \u2013 is the LATIN SMALL LETTER DOTLESS I character. To obtain correct results for locale insensitive strings, we'd better use toLowerCase(Locale.ENGLISH).\nFor more information on this, please see:\n\nhttp://stackoverflow.com/questions/11063102/using-locales-with-javas-tolowercase-and-touppercase\nhttp://lotusnotus.com/lotusnotus_en.nsf/dx/dotless-i-tolowercase-and-touppercase-functions-use-responsibly.htm\nhttp://java.sys-con.com/node/46241\n\nThis ticket proposes to change our use of String.toUpperCase()/toLowerCase() to String.toUpperCase(Locale.ENGLISH)/toLowerCase(Locale.ENGLISH)",
        "Issue Links": []
    },
    "PARQUET-431": {
        "Key": "PARQUET-431",
        "Summary": "Make ParquetOutputFormat.memoryManager volatile",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "17/Jan/16 09:20",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "16/Feb/16 00:37",
        "Description": "Currently ParquetOutputFormat.getRecordWriter() contains an unsynchronized lazy initialization of the non-volatile static field memoryManager.\nBecause the compiler or processor may reorder instructions, threads are not guaranteed to see a completely initialized object, when ParquetOutputFormat.getRecordWriter() is called by multiple threads.\nThis ticket proposes to make memoryManager volatile to correct the problem.",
        "Issue Links": []
    },
    "PARQUET-432": {
        "Key": "PARQUET-432",
        "Summary": "Complete a todo for method ColumnDescriptor.compareTo()",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "19/Jan/16 08:59",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "29/Jan/16 19:39",
        "Description": "The ticket proposes to consider the case path.length < o.path.length in, for method ColumnDescriptor.compareTo().",
        "Issue Links": []
    },
    "PARQUET-433": {
        "Key": "PARQUET-433",
        "Summary": "Specialize ColumnReaders based on the column type",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "19/Jan/16 21:01",
        "Updated": "01/Mar/16 21:33",
        "Resolved": "28/Jan/16 05:37",
        "Description": "ColumnReader class is used to read columns of all types. This leads to a lot of type checking and 'switch' statements. ColumnReaders should be specialized to different types, while sharing the same interface.",
        "Issue Links": []
    },
    "PARQUET-434": {
        "Key": "PARQUET-434",
        "Summary": "Add a ParquetFileReader class to encapsulate some low-level details of interacting with Parquet files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:10",
        "Updated": "26/Jan/16 23:55",
        "Resolved": "26/Jan/16 23:55",
        "Description": "This is also related to PARQUET-418. I'm beginning work on an adapter between Parquet and in-memory C++ data structures, and it would be helpful for the moment to encapsulate various details like metadata deserialization. \nThis class can be expanded to include other features (such as yielding column readers) in future patches. \nI've inspected the patch in https://github.com/apache/parquet-cpp/pull/18 and expect there to be little overlap. nongli if you can have a look at that and let us know how to proceed, that would be great.",
        "Issue Links": []
    },
    "PARQUET-435": {
        "Key": "PARQUET-435",
        "Summary": "Provide vectorized ColumnReader interface",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:13",
        "Updated": "30/Jan/16 21:16",
        "Resolved": "30/Jan/16 21:16",
        "Description": "Related to PARQUET-433. The library user should be able to retrieve a batch of column values, repetition levels, or definition levels with a particular size into a preallocated C array.",
        "Issue Links": []
    },
    "PARQUET-436": {
        "Key": "PARQUET-436",
        "Summary": "Implement ParquetFileWriter class entry point for generating new Parquet files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:14",
        "Updated": "26/Apr/16 17:43",
        "Resolved": "26/Apr/16 17:43",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-437": {
        "Key": "PARQUET-437",
        "Summary": "Incorporate googletest thirdparty dependency and add cmake tools (ADD_PARQUET_TEST) to simplify adding new unit tests",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:15",
        "Updated": "26/Jan/16 18:05",
        "Resolved": "26/Jan/16 18:05",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-438": {
        "Key": "PARQUET-438",
        "Summary": "Update RLE encoder/decoder modules from Impala upstream changes and adapt unit tests",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:17",
        "Updated": "02/Feb/16 00:24",
        "Resolved": "02/Feb/16 00:24",
        "Description": "Depends on PARQUET-437",
        "Issue Links": []
    },
    "PARQUET-439": {
        "Key": "PARQUET-439",
        "Summary": "Conform all copyright headers to ASF requirements",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 21:58",
        "Updated": "28/Jan/16 18:56",
        "Resolved": "28/Jan/16 18:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-440": {
        "Key": "PARQUET-440",
        "Summary": "Error handling: C++ exceptions or Status",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/16 23:08",
        "Updated": "20/Jan/16 23:41",
        "Resolved": "20/Jan/16 23:41",
        "Description": "This library currently throws C++ exceptions. I would very much prefer to use Google's convention of using Status objects to communicate errors and force explicit action to be taken on the part of the developer if an error occurs in a particular function call. It will also make it much easier to incorporate libparquet into other libraries that do not use C++ exceptions, and also to provide an ANSI C API wrapper.",
        "Issue Links": []
    },
    "PARQUET-441": {
        "Key": "PARQUET-441",
        "Summary": "Schema resolution: one, two, and three-level array encoding",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "20/Jan/16 16:57",
        "Updated": "02/Jun/20 22:42",
        "Resolved": null,
        "Description": "While the Parquet spec recommends the \"three-level\" array encoding, two other styles are possible in the wild, see for example:\nhttps://github.com/cloudera/Impala/blob/cdh5-trunk/be/src/exec/hdfs-parquet-scanner.cc#L1986",
        "Issue Links": []
    },
    "PARQUET-442": {
        "Key": "PARQUET-442",
        "Summary": "Convert flat SchemaElement vector to implied nested schema data structure",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Jan/16 17:00",
        "Updated": "06/Feb/16 02:02",
        "Resolved": "06/Feb/16 02:02",
        "Description": "To assist with conversion to in-memory nested data structures. Related: PARQUET-441",
        "Issue Links": [
            "/jira/browse/PARQUET-508"
        ]
    },
    "PARQUET-443": {
        "Key": "PARQUET-443",
        "Summary": "Schema resolution: map encoding",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "20/Jan/16 17:01",
        "Updated": "02/Jun/20 22:42",
        "Resolved": null,
        "Description": "Related: PARQUET-441 and PARQUET-442",
        "Issue Links": []
    },
    "PARQUET-444": {
        "Key": "PARQUET-444",
        "Summary": "Metadata generation: Nested physical schema builder",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Jan/16 17:05",
        "Updated": "03/Jul/19 02:27",
        "Resolved": "03/Jul/19 02:26",
        "Description": "The idea here is to define a simple API for creating logical schemas, which will be then automatically flattened in DFS order to a vector of SchemaElement structs. This will spare users from having to necessarily implement their own flattening / unflattening code",
        "Issue Links": []
    },
    "PARQUET-445": {
        "Key": "PARQUET-445",
        "Summary": "Batch/vectorized decoding of array sizes within each repetition level",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Jan/16 17:08",
        "Updated": "12/Nov/18 19:07",
        "Resolved": "12/Nov/18 19:07",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-446": {
        "Key": "PARQUET-446",
        "Summary": "Hide thrift dependency in parquet-cpp",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Nong Li",
        "Created": "20/Jan/16 18:50",
        "Updated": "01/Mar/16 21:33",
        "Resolved": "15/Feb/16 23:55",
        "Description": "Pulling in thrift compiled headers tend to pull in a lot of things. It would be nice to not expose them in the parquet library (the application should be able to use a different version of thrift, etc). \nWe can also see if it is practical to not depend on thrift at all and replicate the logic we need. Thrift is fairly stable at this point so this might be feasible. This would allow us to do things like not rely on boost.",
        "Issue Links": []
    },
    "PARQUET-447": {
        "Key": "PARQUET-447",
        "Summary": "Add Debug and Release build types and associated compiler flags",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "21/Jan/16 05:02",
        "Updated": "11/Feb/16 06:22",
        "Resolved": "11/Feb/16 06:22",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-448": {
        "Key": "PARQUET-448",
        "Summary": "Add cmake option to skip building the unit tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "21/Jan/16 05:03",
        "Updated": "06/Feb/16 20:31",
        "Resolved": "06/Feb/16 20:31",
        "Description": "This will speed up build-and-install in downstream applications",
        "Issue Links": []
    },
    "PARQUET-449": {
        "Key": "PARQUET-449",
        "Summary": "Update to latest parquet.thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "21/Jan/16 06:17",
        "Updated": "26/Jan/16 00:17",
        "Resolved": "25/Jan/16 22:24",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-450": {
        "Key": "PARQUET-450",
        "Summary": "Small typos/issues in parquet-format documentation",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Laurent Goujon",
        "Reporter": "Laurent Goujon",
        "Created": "21/Jan/16 18:27",
        "Updated": "29/Jan/16 18:30",
        "Resolved": "29/Jan/16 18:30",
        "Description": "I noticed several typos/omissions in parquet format documentation:\n\nHDFS should be all uppercase (acronym)\nenncoding instead of encoding\nmarkdown issues\nno link to the thrift definition file\nthe integer format (LE vs BE) is not specified for the file metadata\nthe order of informations in a data page",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/36"
        ]
    },
    "PARQUET-451": {
        "Key": "PARQUET-451",
        "Summary": "Add a RowGroup reader interface class",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 19:43",
        "Updated": "01/Mar/16 21:33",
        "Resolved": "28/Jan/16 05:13",
        "Description": "Currently the logic for interacting with row group metadata and constructing column decoders is embedded in the parquet_reader.cc executable here:\nhttps://github.com/apache/parquet-cpp/blob/master/example/parquet_reader.cc\nWith PARQUET-434, we have a file reader container, which can then provide a row group reader container, something like \n\nRowGroupReader* group_reader = file_reader->row_group(i);",
        "Issue Links": []
    },
    "PARQUET-452": {
        "Key": "PARQUET-452",
        "Summary": "Add a RowGroup writer interface class",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 19:45",
        "Updated": "27/Apr/16 16:55",
        "Resolved": "27/Apr/16 16:55",
        "Description": "Per PARQUET-436; as soon as we are able to begin constructing new Parquet files, we can provide an interface class for writing data to a new row group, which will automatically set the appropriate Thrift metadata",
        "Issue Links": []
    },
    "PARQUET-453": {
        "Key": "PARQUET-453",
        "Summary": "Refactor parquet_reader.cc into a ParquetFileReader::DebugPrint method",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 19:46",
        "Updated": "01/Mar/16 21:33",
        "Resolved": "28/Jan/16 05:37",
        "Description": "This is follow up work per discussion in PARQUET-418 and https://github.com/apache/parquet-cpp/pull/18",
        "Issue Links": []
    },
    "PARQUET-454": {
        "Key": "PARQUET-454",
        "Summary": "Address inconsistencies in boolean decoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 19:56",
        "Updated": "02/Feb/16 23:01",
        "Resolved": "02/Feb/16 23:01",
        "Description": "See patch https://github.com/apache/parquet-cpp/pull/12\nI suggest adding unit tests to verify the fix proposed in this patch.",
        "Issue Links": []
    },
    "PARQUET-455": {
        "Key": "PARQUET-455",
        "Summary": "Fix compiler warnings on OS X / Clang",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 19:58",
        "Updated": "06/Feb/16 20:30",
        "Resolved": "06/Feb/16 20:30",
        "Description": "There have been two patches addressing OS X-related issues:\nhttps://github.com/apache/parquet-cpp/pull/15\nhttps://github.com/apache/parquet-cpp/pull/6",
        "Issue Links": []
    },
    "PARQUET-456": {
        "Key": "PARQUET-456",
        "Summary": "Add zlib codec support",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 20:02",
        "Updated": "12/Feb/16 23:39",
        "Resolved": "12/Feb/16 23:39",
        "Description": "See https://github.com/apache/parquet-cpp/pull/11",
        "Issue Links": []
    },
    "PARQUET-457": {
        "Key": "PARQUET-457",
        "Summary": "Add compressed data page unit tests",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 20:05",
        "Updated": "21/Feb/16 00:12",
        "Resolved": "21/Feb/16 00:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-458": {
        "Key": "PARQUET-458",
        "Summary": "[C++] Implement support for DataPageV2",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/16 20:19",
        "Updated": "25/Mar/20 20:56",
        "Resolved": "25/Mar/20 20:56",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1560",
            "/jira/browse/ARROW-6057",
            "https://github.com/apache/arrow/pull/6481"
        ]
    },
    "PARQUET-459": {
        "Key": "PARQUET-459",
        "Summary": "Improve handling of null values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "22/Jan/16 22:44",
        "Updated": "31/Jan/16 20:34",
        "Resolved": "31/Jan/16 20:32",
        "Description": "Currently, the default value of the type is returned for NULL values and is incorrect.\nThis JIRA will correctly identify a NULL value with the help of an additional variable that will be set for NULL values. \nThis feature depends on reading the repetition level (PARQUET-169).",
        "Issue Links": []
    },
    "PARQUET-460": {
        "Key": "PARQUET-460",
        "Summary": "Parquet files concat tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0,                                            1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "flykobe cheng",
        "Reporter": "flykobe cheng",
        "Created": "23/Jan/16 10:04",
        "Updated": "15/Dec/21 03:04",
        "Resolved": "16/Aug/16 17:41",
        "Description": "Currently the parquet file generation is time consuming, most of time used for serialize and compress. It cost about 10mins to generate a 100MB~ parquet file in our scenario. We want to improve write performance without generate too many small files, which will impact read performance.\nWe propose to:\n1. generate several small parquet files concurrently\n2. merge small files to one file: concat the parquet blocks in binary (without SerDe), merge footers and modify the path and offset metadata.\nWe create ParquetFilesConcat class to finish step 2. It can be invoked by parquet.tools.command.ConcatCommand. If this function approved by parquet community, we will integrate it in spark.\nIt will impact compression and introduced more dictionary pages, but it can be improved by adjusting the concurrency of step 1.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/327"
        ]
    },
    "PARQUET-461": {
        "Key": "PARQUET-461",
        "Summary": "Improve ColumnReader API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "25/Jan/16 15:59",
        "Updated": "12/Feb/16 16:27",
        "Resolved": "12/Feb/16 16:27",
        "Description": "I would like to add some more extensions to the ColumnReader API. These extensions will query certain fields from the corresponding schema element and the column metadata.",
        "Issue Links": []
    },
    "PARQUET-462": {
        "Key": "PARQUET-462",
        "Summary": "Implement a LevelDecoder class (like Impala) which dispatches to RLE or BIT_PACKED decoding as appropriate",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "25/Jan/16 18:43",
        "Updated": "11/Feb/16 16:19",
        "Resolved": "11/Feb/16 16:19",
        "Description": "This class extends the RleDecoder class.",
        "Issue Links": []
    },
    "PARQUET-463": {
        "Key": "PARQUET-463",
        "Summary": "Add DCHECK* macros for assertions in debug builds",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jan/16 20:47",
        "Updated": "01/Mar/16 21:32",
        "Resolved": "01/Mar/16 00:52",
        "Description": "Some of these macros are already defined in parquet/util/logging.h, but they are no-ops. This will assist in \"can't fail\" assertions. See https://www.chromium.org/developers/coding-style#TOC-CHECK-DCHECK-and-NOTREACHED-",
        "Issue Links": []
    },
    "PARQUET-464": {
        "Key": "PARQUET-464",
        "Summary": "Add cmake option and #defines to enable/disable struct packing",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 00:39",
        "Updated": "02/Apr/17 16:13",
        "Resolved": "02/Apr/17 16:13",
        "Description": "Follow-up to conversation on PARQUET-428. This will make it easier to run performance experiments without changing any code.",
        "Issue Links": []
    },
    "PARQUET-465": {
        "Key": "PARQUET-465",
        "Summary": "Parquet-Avro does not support field removal",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Thomas Omans",
        "Created": "26/Jan/16 00:49",
        "Updated": "26/Jan/16 21:29",
        "Resolved": null,
        "Description": "Parquet avro does not support removal of fields, when used with the new compatibility layer:\nGiven a parquet file written with parquet avro at v1 and the following schema:\n\nrecord FooBar {\n  long foo;\n  string bar;\n}\n\n\nAnd the following configuration settings:\n\njob.getConfiguration.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false)\nAvroParquetInputFormat.setAvroReadSchema(job, avroReaderSchema)\n\n\nA job fails when trying to read it using schema version v2:\n\nrecord FooBar {\n  string bar;\n}\n\n\nWith the error:\n\norg.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'foo' not found\n\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:159)\n\n\nIt looks like because it sees the field in the original version it assumes the new version must expect it, but this case just means that the field was removed. Avro schema resolution dictates that you just ignore this field, since it is not relevant in the new version.",
        "Issue Links": []
    },
    "PARQUET-466": {
        "Key": "PARQUET-466",
        "Summary": "Make parquet-format a git submodule and add tool for updating generated Thrift code",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 01:52",
        "Updated": "20/Feb/16 03:21",
        "Resolved": "20/Feb/16 03:21",
        "Description": "As a follow up to PARQUET-449",
        "Issue Links": []
    },
    "PARQUET-467": {
        "Key": "PARQUET-467",
        "Summary": "Implement and test BIT_PACKED level encoding / decoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 05:27",
        "Updated": "11/Feb/16 16:20",
        "Resolved": "11/Feb/16 16:20",
        "Description": "While RLE is the preferred encoding format (and BIT_PACKED is deprecated in Parquet 2.0), we will need to support this encoding format for legacy Parquet files that use it. As part of this JIRA we will verify round-tripping levels to this encoding format.\nSee also PARQUET-462",
        "Issue Links": []
    },
    "PARQUET-468": {
        "Key": "PARQUET-468",
        "Summary": "Add a cmake option to generate the Parquet thrift headers with the thriftc in the environment",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 18:36",
        "Updated": "18/Feb/16 23:54",
        "Resolved": "18/Feb/16 23:54",
        "Description": "Follow-up to PARQUET-449. This will help toolchains which are unable to upgrade to the latest version of Thrift.",
        "Issue Links": []
    },
    "PARQUET-469": {
        "Key": "PARQUET-469",
        "Summary": "Roll back Thrift bindings to 0.9.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 20:02",
        "Updated": "01/Mar/16 21:32",
        "Resolved": "26/Jan/16 22:08",
        "Description": "Thrift 0.9.3 conflicts with googletest in ugly ways on gcc 4.9. This is a stopgap until PARQUET-468",
        "Issue Links": []
    },
    "PARQUET-470": {
        "Key": "PARQUET-470",
        "Summary": "Thrift 0.9.3 cannot be used in conjunction with googletest and C++11 on Linux",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 20:16",
        "Updated": "19/Feb/16 02:32",
        "Resolved": "19/Feb/16 02:32",
        "Description": "Thrift 0.9.3 introduces a #include <thrift/cxxfunctional.h> include which causes tr1/functional to be included, causing a compiler conflict with googletest, which has its own portability macros surrounding its use of std::tr1::tuple. I spent a bunch of time twiddling compiler flags to try to resolve this conflict, but wasn't able to figure it out. \nIf this is a Thrift bug, we should report it to Thrift. If it's fixable by compiler flags, then we should figure that out and track the issue here, otherwise users with the latest version of Thrift will be unable to compile the parquet-cpp test suite.",
        "Issue Links": []
    },
    "PARQUET-471": {
        "Key": "PARQUET-471",
        "Summary": "Use the same environment setup script for Travis CI as local sandbox development",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/16 23:48",
        "Updated": "18/Feb/16 23:29",
        "Resolved": "18/Feb/16 23:29",
        "Description": "Currently the environment setups are slightly different, and so a passing Travis CI build might have a problem with the sandbox build and vice versa.",
        "Issue Links": []
    },
    "PARQUET-472": {
        "Key": "PARQUET-472",
        "Summary": "Clean up InputStream ownership semantics in ColumnReader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Wes McKinney",
        "Created": "27/Jan/16 01:00",
        "Updated": "29/Jan/16 17:05",
        "Resolved": "29/Jan/16 17:02",
        "Description": "Follow-up to PARQUET-418, PARQUET-433. The ColumnReader destructor uses delete on an InputStream*. The lifetime of this object should be managed by a std::unique_ptr.",
        "Issue Links": []
    },
    "PARQUET-473": {
        "Key": "PARQUET-473",
        "Summary": "[C++] Develop external predicate pushdown API for column readers",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "27/Jan/16 16:24",
        "Updated": "03/Jul/19 02:27",
        "Resolved": null,
        "Description": "This will happen significantly downstream of where we are at right now, but we should be planning ahead to facilitate scanning Parquet files with externally-defined predicates as a primary use case. \nI suggest that the most general (and high performance) predicate will be batch-oriented; i.e. the predicate will be passed a batch of materialized values from one or more columns, and it returns an array of booleans indicating whether or not the predicate is true. We can also develop a row-by-row \"scalar\" predicate API if users need that.",
        "Issue Links": []
    },
    "PARQUET-474": {
        "Key": "PARQUET-474",
        "Summary": "[C++] InputStream and RandomAccessSource classes are not threadsafe",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "27/Jan/16 16:49",
        "Updated": "02/May/19 19:38",
        "Resolved": "02/May/19 19:38",
        "Description": "We need to ensure that files can be processed in multithreaded applications",
        "Issue Links": []
    },
    "PARQUET-475": {
        "Key": "PARQUET-475",
        "Summary": "Run DebugPrint on all data files in the data/ directory",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Wes McKinney",
        "Created": "27/Jan/16 21:18",
        "Updated": "09/Feb/16 01:28",
        "Resolved": "09/Feb/16 01:25",
        "Description": "As a smoke test. Follow-up to PARQUET-453",
        "Issue Links": []
    },
    "PARQUET-476": {
        "Key": "PARQUET-476",
        "Summary": "Add a utility function to print the raw repetition / definition levels to an std::ostream",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "28/Jan/16 00:20",
        "Updated": "02/May/19 19:35",
        "Resolved": "02/May/19 19:35",
        "Description": "This will facilitate development of nested data features and debugging",
        "Issue Links": []
    },
    "PARQUET-477": {
        "Key": "PARQUET-477",
        "Summary": "Enable clang-format check during the Travis CI build",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "28/Jan/16 20:02",
        "Updated": "01/May/16 17:25",
        "Resolved": "01/May/16 15:57",
        "Description": "Add clang-format and check the code style during CI builds using clang-tidy.",
        "Issue Links": []
    },
    "PARQUET-478": {
        "Key": "PARQUET-478",
        "Summary": "Reassembly algorithms for Arrow in-memory columnar memory layout",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "28/Jan/16 23:07",
        "Updated": "12/May/16 16:43",
        "Resolved": "12/May/16 16:43",
        "Description": "I plan to use parquet-cpp primarily in conjunction with columnar data structures (http://arrow.apache.org). \nSpecifically, this requires in the interpretation of repetition / definition levels:\n\nComputing null bits / bytes for each logical level of nested tree (group, array, primitive leaf)\nComputing implied array sizes for each repeated group (according to 1, 2, or 3-level array encoding)\n\nThe results of this reconstruction will be simply C arrays accompanied by the parquet-cpp logical schema; this way we can make it easy to adapt to different in-memory columnar memory schemes. \nAs far as implementation, it would make sense to proceed first with functional unit tests of the reassembly algorithms using repetition / definition levels declared in the test suite as C++ vectors \u2013 otherwise it's going to be too tedious trying to produce valid Parquet test data files which explore all of the different edge cases.\nSeveral other teams (Spark, Drill, Parquet-Java) are currently working on related efforts along these lines, so we can engage when appropriate to collaborate on algorithms and nuances of this approach to avoid unnecessary code churn / bugs.",
        "Issue Links": []
    },
    "PARQUET-479": {
        "Key": "PARQUET-479",
        "Summary": "Improve/expand functional unit tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "29/Jan/16 14:39",
        "Updated": "01/Feb/16 16:02",
        "Resolved": "01/Feb/16 16:02",
        "Description": "We need to add a testing framework for unit tests, and run it as a part of each Travis CI build.",
        "Issue Links": []
    },
    "PARQUET-480": {
        "Key": "PARQUET-480",
        "Summary": "Update for Cascading 3.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Cyrille Ch\u00e9p\u00e9lov",
        "Reporter": "Cyrille Ch\u00e9p\u00e9lov",
        "Created": "29/Jan/16 16:34",
        "Updated": "01/Feb/16 03:23",
        "Resolved": "01/Feb/16 03:22",
        "Description": "The code in parquet-cascading is adapted to the API as of Cascading 2.5.3\nSome incompatible changes were introduced in Cascading 3.0. This patch forks the parquet-cascading module to also provide a parquet-cascading3 module, which is about identical save for overloads which changed from requiring a Foo to requiring a Foo<? extends JobConf>",
        "Issue Links": []
    },
    "PARQUET-481": {
        "Key": "PARQUET-481",
        "Summary": "[C++] Refactor and expand reader-test",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "29/Jan/16 17:02",
        "Updated": "16/Aug/19 14:05",
        "Resolved": "16/Aug/19 14:05",
        "Description": "reader-test currently tests with a parquet file and only verifies that we can read it, not the correctness of the output.\nProposed changes:\n\nRemove the use of parquet files and use mock objects instead.\nMove tests for Scanner to scanner-test.cc\nGet rid of DebugPrint() tests, move to ParquetFilePrinter as a part of PARQUET-508.",
        "Issue Links": [
            "/jira/browse/PARQUET-526"
        ]
    },
    "PARQUET-482": {
        "Key": "PARQUET-482",
        "Summary": "Organize src code file structure to have a very clear folder with public headers.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Nong Li",
        "Created": "29/Jan/16 18:16",
        "Updated": "02/Mar/16 01:36",
        "Resolved": "02/Mar/16 01:36",
        "Description": "We should organize the source code structure to have a folder where all the public headers are and nothing else. This makes it easy to understand what is the public API and which APIs needed to be looked at wrt to compatibility.",
        "Issue Links": []
    },
    "PARQUET-483": {
        "Key": "PARQUET-483",
        "Summary": "Write tests investigating failure modes with malformed encoded levels in data pages",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "29/Jan/16 22:20",
        "Updated": "03/Jul/19 02:27",
        "Resolved": "03/Jul/19 02:27",
        "Description": "Follow-up to PARQUET-435. If we are not able to decode as many levels as we expect, this should be caught and raised with a helpful error message. There are some other hypothetical cases we should check for and verify in tests.",
        "Issue Links": []
    },
    "PARQUET-484": {
        "Key": "PARQUET-484",
        "Summary": "Warn when Decimal is stored as INT64 while could be stored as INT32",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "30/Jan/16 13:57",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "19/Apr/16 16:17",
        "Description": "Below is documented in https://github.com/Parquet/parquet-format/blob/master/LogicalTypes.md#decimal:\n int32: for 1 <= precision <= 9\nint64: for 1 <= precision <= 18; precision < 10 will produce a warning \nThis JIRA proposes to implement the `precision < 10 will produce a warning` part.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/316"
        ]
    },
    "PARQUET-485": {
        "Key": "PARQUET-485",
        "Summary": "Decouple data page delimiting from column reader / scanner classes, create test fixtures",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Jan/16 15:49",
        "Updated": "02/Feb/16 18:50",
        "Resolved": "02/Feb/16 18:50",
        "Description": "It is difficult to test the column reader classes with mock data because the data page resolution is tightly coupled to the actual file format layout in ColumnReader::ReadNewPage.\nI plan to separate these concerns, so that the column readers can be tested with a sequence of data pages encoded in memory, but never actually assembled into a file stream layout with thrift-serialized page headers. Patch forthcoming.",
        "Issue Links": []
    },
    "PARQUET-486": {
        "Key": "PARQUET-486",
        "Summary": "Add gcov to Travis CI build and upload results to coveralls",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "30/Jan/16 18:28",
        "Updated": "20/Feb/16 05:32",
        "Resolved": "20/Feb/16 05:32",
        "Description": "Here is a tool we can use to track code coverage in an automated fashion:\nhttps://github.com/eddyxu/cpp-coveralls",
        "Issue Links": []
    },
    "PARQUET-487": {
        "Key": "PARQUET-487",
        "Summary": "Add cmake options for various dependency-linking scenarios",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Jan/16 21:03",
        "Updated": "30/Mar/17 22:49",
        "Resolved": "30/Mar/17 22:49",
        "Description": "Unless libparquet is being built as part of a statically-linked application toolchain, it may be preferable in a lot of cases to use dynamic linking with its dependencies, rather than statically linking everything as it does now.\nI envision several distinct use cases we should support\n\nlibparquet.so, with no dependencies statically linked that have shared libraries available\nlibparquet.so, with all dependencies statically linked (this is what we are doing now)\nlibparquet.a, with all dependencies statically linked\nlibparquet.a, without any unnecessary static linking\n\nHaving these options accessible through cmake flags will help users of parquet-cpp avoid dependency hell in some cases, and incorporate libparquet in the desired way into a 3rd-party build toolchain",
        "Issue Links": []
    },
    "PARQUET-488": {
        "Key": "PARQUET-488",
        "Summary": "Add SSE-related cmake options to manage compiler flags",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Jan/16 21:14",
        "Updated": "14/Mar/16 20:50",
        "Resolved": "14/Mar/16 20:50",
        "Description": "Currently, compiling with SSE3 is the default option, but we may wish to utilize optimizations that require SSE 4.x. Also, it should be possible to disable SSE / SIMD entirely (not sure if anyone would want to compile all this to run on a Raspberry Pi but you never know).",
        "Issue Links": []
    },
    "PARQUET-489": {
        "Key": "PARQUET-489",
        "Summary": "Add visibility macros to be used for public and internal APIs of libparquet",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Jan/16 21:34",
        "Updated": "06/Jul/16 01:08",
        "Resolved": "01/Jul/16 21:45",
        "Description": "e.g. PARQUET_EXPORT and PARQUET_NO_EXPORT",
        "Issue Links": [
            "/jira/browse/PARQUET-653"
        ]
    },
    "PARQUET-490": {
        "Key": "PARQUET-490",
        "Summary": "[C++] Incorporate DELTA_BINARY_PACKED value encoder into library and add unit tests",
        "Type": "New Feature",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "31/Jan/16 21:23",
        "Updated": "15/Sep/22 00:56",
        "Resolved": null,
        "Description": "There is some code for this currently found in examples/decode_benchmark.cc",
        "Issue Links": [
            "/jira/browse/ARROW-17619",
            "/jira/browse/ARROW-13677",
            "/jira/browse/ARROW-13206",
            "https://github.com/apache/parquet-testing/pull/19",
            "https://github.com/apache/arrow/pull/10627"
        ]
    },
    "PARQUET-491": {
        "Key": "PARQUET-491",
        "Summary": "[C++] Incorporate DELTA_LENGTH_BYTE_ARRAY value encoder into library and add unit tests",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "31/Jan/16 21:24",
        "Updated": "15/Sep/22 00:56",
        "Resolved": "28/Jun/22 20:34",
        "Description": "See examples/decode_benchmark.cc",
        "Issue Links": [
            "/jira/browse/ARROW-17619",
            "/jira/browse/ARROW-13388"
        ]
    },
    "PARQUET-492": {
        "Key": "PARQUET-492",
        "Summary": "[C++] Incorporate DELTA_BYTE_ARRAY value encoder into library and add unit tests",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-7.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Shan Huang",
        "Reporter": "Wes McKinney",
        "Created": "31/Jan/16 21:25",
        "Updated": "08/Dec/21 21:02",
        "Resolved": "08/Nov/21 18:19",
        "Description": "See examples/decode_benchmark.cc",
        "Issue Links": [
            "/jira/browse/ARROW-14089",
            "https://github.com/apache/parquet-testing/pull/20",
            "https://github.com/apache/arrow/pull/10978"
        ]
    },
    "PARQUET-493": {
        "Key": "PARQUET-493",
        "Summary": "Adapt DictEncoder from Impala (or implement a new one) and unit test",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "31/Jan/16 21:27",
        "Updated": "26/Feb/16 19:13",
        "Resolved": "26/Feb/16 19:13",
        "Description": "Only decoding is available in the library at the moment. Without this, we cannot generate dictionary-encoded data pages for unit testing purposes.",
        "Issue Links": []
    },
    "PARQUET-494": {
        "Key": "PARQUET-494",
        "Summary": "Implement PLAIN_DICTIONARY encoding and decoding",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "31/Jan/16 21:34",
        "Updated": "26/Feb/16 17:53",
        "Resolved": "26/Feb/16 17:53",
        "Description": "parquet-cpp currently only supports Encoding::RLE_DICTIONARY. Some implementations of Parquet still use Encoding::PLAIN_DICTIONARY (the dictionary indices are not RLE-encoded).",
        "Issue Links": []
    },
    "PARQUET-495": {
        "Key": "PARQUET-495",
        "Summary": "Fix mismatches in Types class comments",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "01/Feb/16 03:55",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "01/Feb/16 18:09",
        "Description": "To produce:\nrequired group User {\n    required int64 id;\noptional binary email (UTF8);\n}\nwe should do:\nTypes.requiredGroup()\n      .required(INT64).named(\"id\")\n      .required (BINARY).as(UTF8).named(\"email\")\n      .optional (BINARY).as(UTF8).named(\"email\")\n      .named(\"User\")",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/317"
        ]
    },
    "PARQUET-496": {
        "Key": "PARQUET-496",
        "Summary": "Fix cpplint configuration to be more restrictive",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/16 04:33",
        "Updated": "01/Feb/16 20:04",
        "Resolved": "01/Feb/16 20:04",
        "Description": "Indentation errors and other issues are passing through the Travis CI checks (e.g. https://github.com/apache/parquet-cpp/pull/30), let's figure out why this is and fix it.",
        "Issue Links": []
    },
    "PARQUET-497": {
        "Key": "PARQUET-497",
        "Summary": "Decouple Parquet physical file structure from FileReader class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/16 19:04",
        "Updated": "11/Feb/16 08:21",
        "Resolved": "11/Feb/16 08:21",
        "Description": "It should be possible to unit test this class without creating an actual Parquet file. We can do this while also keeping the file-based initialization code path (see parquet_reader.cc) about as simple as it is now.",
        "Issue Links": []
    },
    "PARQUET-498": {
        "Key": "PARQUET-498",
        "Summary": "Add a ColumnChunk builder abstraction as part of creating new row groups",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/16 19:10",
        "Updated": "01/May/16 17:26",
        "Resolved": "01/May/16 17:26",
        "Description": "Necessary for PARQUET-452, but we should treat as an independent task.\nThis class will be responsible for encapsulating creating a serialized sequence of data pages. This way, users on the write path need only specify the desired data page size, then write arrays of values, repetition, and definiton levels",
        "Issue Links": []
    },
    "PARQUET-499": {
        "Key": "PARQUET-499",
        "Summary": "Complete PlainEncoder implementation for all primitive types and test end to end",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/16 19:12",
        "Updated": "18/Feb/16 23:25",
        "Resolved": "18/Feb/16 23:25",
        "Description": "As part of PARQUET-485, I added a partial Encoding::PLAIN encoder implementation. This needs to be finished, with a test suite that validates data round-trips across all primitive types.",
        "Issue Links": []
    },
    "PARQUET-500": {
        "Key": "PARQUET-500",
        "Summary": "Enable coveralls.io for apache/parquet-cpp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/16 21:40",
        "Updated": "09/Feb/16 01:38",
        "Resolved": "09/Feb/16 01:38",
        "Description": "This will enable me to upload code coverage re: PARQUET-486. This can be handled by anyone with admin on parquet-cpp. Please let me know the API token details by some means when you do that.",
        "Issue Links": []
    },
    "PARQUET-501": {
        "Key": "PARQUET-501",
        "Summary": "Add an OutputStream abstraction (capable of memory allocation) for Encoder public API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "02/Feb/16 13:21",
        "Updated": "11/Feb/16 06:31",
        "Resolved": "11/Feb/16 06:31",
        "Description": "As a follow up to PARQUET-485",
        "Issue Links": []
    },
    "PARQUET-502": {
        "Key": "PARQUET-502",
        "Summary": "Scanner segfaults when its batch size is smaller than the number of rows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "02/Feb/16 16:15",
        "Updated": "24/Feb/16 20:57",
        "Resolved": "24/Feb/16 20:57",
        "Description": "TypedScanner<>::HasNext() does not work properly when it reaches the end of the column. HasNext() still returns true and a subsequent attempt to get the value (e.g. in PrintNext()) causes a segfault.\nTo reproduce, use the attached file and run\nparquet-reader all_types.parquet",
        "Issue Links": [
            "/jira/browse/PARQUET-526"
        ]
    },
    "PARQUET-503": {
        "Key": "PARQUET-503",
        "Summary": "Re-enable parquet 2.0 encodings",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Nong Li",
        "Reporter": "Nong Li",
        "Created": "02/Feb/16 19:44",
        "Updated": "02/Feb/16 22:52",
        "Resolved": "02/Feb/16 22:50",
        "Description": "This was commented out after some code rebasing. We should add this back.",
        "Issue Links": []
    },
    "PARQUET-504": {
        "Key": "PARQUET-504",
        "Summary": "Integrate asan into pre commit hook",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nong Li",
        "Created": "02/Feb/16 20:17",
        "Updated": "02/Feb/16 20:17",
        "Resolved": null,
        "Description": "Asan is super helpful finding bugs particular for the low level byte read/write logic we have. We should integrate this to some of the pre commit tests.",
        "Issue Links": []
    },
    "PARQUET-505": {
        "Key": "PARQUET-505",
        "Summary": "Column reader: automatically handle large data pages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "02/Feb/16 20:25",
        "Updated": "11/Feb/16 17:25",
        "Resolved": "11/Feb/16 17:25",
        "Description": "Currently, we are only supporting data pages whose headers are 64K or less (see parquet/column/serialized-page.cc. Since page headers can essentially be arbitrarily large (in pathological cases) because of the page statistics, if deserializing the page header fails, we should attempt to read a progressively larger amount of file data in effort to find the end of the page header. \nAs part of this (and to make testing easier!), the maximum data page header size should be configurable. We can write test cases by defining appropriate Statistics structs to yield serialized page headers of whatever desired size.\nOn malformed files, we may run past the end of the file, in such cases we should raise a reasonable exception.",
        "Issue Links": []
    },
    "PARQUET-506": {
        "Key": "PARQUET-506",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0,                                            1.8.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Sebb",
        "Created": "02/Feb/16 22:45",
        "Updated": "24/Oct/16 19:06",
        "Resolved": "24/Oct/16 19:06",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\ni.e. the ones listed as affected.\nThanks!\nAlso, if you have a release guide, perhaps you could add a cleanup stage to it?\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "PARQUET-507": {
        "Key": "PARQUET-507",
        "Summary": "Improve runtime of rle-test.cc",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "02/Feb/16 23:50",
        "Updated": "06/Feb/16 20:09",
        "Resolved": "06/Feb/16 20:08",
        "Description": "This test module takes about 3 seconds to run on my laptop which seems too long. I took a look and found some ways to make it run much faster (< 1 second).",
        "Issue Links": []
    },
    "PARQUET-508": {
        "Key": "PARQUET-508",
        "Summary": "Add ParquetFilePrinter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "03/Feb/16 17:42",
        "Updated": "21/Apr/17 14:22",
        "Resolved": "21/Apr/17 14:22",
        "Description": "We should move DebugPrint from ParquetFileReader to a separate class parquet_cpp::test::ParquetFilePrinter.",
        "Issue Links": [
            "/jira/browse/PARQUET-442"
        ]
    },
    "PARQUET-509": {
        "Key": "PARQUET-509",
        "Summary": "Incorrect number of args passed to string.format calls",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "04/Feb/16 19:58",
        "Updated": "06/Feb/16 19:42",
        "Resolved": "06/Feb/16 19:42",
        "Description": "In DirectCodecFactory there are calls to String.format (called in DEBUG mode) with incorrect number of args for the placeholders, which will fail during runtime when log level is DEBUG.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/320"
        ]
    },
    "PARQUET-510": {
        "Key": "PARQUET-510",
        "Summary": "Add ability to parse nested schemas from text specification like parquet-mr",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "05/Feb/16 00:18",
        "Updated": "05/Feb/16 00:18",
        "Resolved": null,
        "Description": "related to PARQUET-442",
        "Issue Links": []
    },
    "PARQUET-511": {
        "Key": "PARQUET-511",
        "Summary": "Integer overflow on counting values in column",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Michal Gorecki",
        "Reporter": "Michal Gorecki",
        "Created": "05/Feb/16 20:47",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "01/Aug/16 21:38",
        "Description": "Parquet will ignore a column if the combined amount of elements in the column is larger than the size of an int.\nThe issue is that as the column reader is initialized and the rep and def levels are initialized per column, the size of the integer will overflow, causing these values to not be set properly. Then, during read, the level will not match the current level of the reader, and a null value will be provided. Since there is no overflow checking, no exception is thrown, and it appears that the data is corrupted.\nThis happened to us with a fairly complex schema, with an array of maps, which contained arrays as well. There were over 4 billion values in all column pages in one row group, which is what triggered the overflow.\nRelevant stack trace\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 172310 in block 0 in file <redacted>\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:245)\n        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)\n   ...\n        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:143)\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n        at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1626)\n        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099)\n        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1099)\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n        at org.apache.spark.scheduler.Task.run(Task.scala:70)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by:      <redacted> INT64 at value 95584934 out of 95530352, 130598 out of 130598 in currentPage. repetition level: 0, definition level: 2\n        at org.apache.parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:484)\n        at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:370)\n        at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:405)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:218)\n        ... 18 more\nCaused by: java.lang.IllegalArgumentException: Reading past RLE/BitPacking stream.\n        at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)\n        at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readNext(RunLengthBitPackingHybridDecoder.java:82)\n        at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readInt(RunLengthBitPackingHybridDecoder.java:64)\n        at org.apache.parquet.column.values.dictionary.DictionaryValuesReader.readLong(DictionaryValuesReader.java:121)\n        at org.apache.parquet.column.impl.ColumnReaderImpl$2$4.read(ColumnReaderImpl.java:263)\n        at org.apache.parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:464)\n        ... 21 more",
        "Issue Links": []
    },
    "PARQUET-512": {
        "Key": "PARQUET-512",
        "Summary": "Add optional google/benchmark 3rd-party dependency for performance testing",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "06/Feb/16 18:23",
        "Updated": "02/May/16 00:53",
        "Resolved": "02/May/16 00:53",
        "Description": "As part of optimizing parquet-cpp's performance, we will want to start a benchmarking suite so that we can monitor performance for regressions in critical code paths as patches are applied. \nI would, at some point, like to explore web-based perf tracking using a tool like ASV:\nhttp://asv.readthedocs.org/en/latest/",
        "Issue Links": []
    },
    "PARQUET-513": {
        "Key": "PARQUET-513",
        "Summary": "Valgrind errors are not failing the Travis CI build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Feb/16 23:17",
        "Updated": "09/Feb/16 01:37",
        "Resolved": "09/Feb/16 01:37",
        "Description": "While testing some things I found that schema-converter-test.cc core dumps when run manually, even though ctest seems to succeed without a hitch. Valgrind reveals a bad memory access, but this isn't causing the build to fail because of a misconfiguration.",
        "Issue Links": []
    },
    "PARQUET-514": {
        "Key": "PARQUET-514",
        "Summary": "Automate coveralls.io updates in Travis CI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "07/Feb/16 00:38",
        "Updated": "01/Mar/16 21:31",
        "Resolved": "19/Feb/16 22:16",
        "Description": "The repo has been enabled in INFRA-11273, so all that's left is to work on the Travis CI build matrix and add coveralls to one of the builds (rather than running it for all of them)",
        "Issue Links": []
    },
    "PARQUET-515": {
        "Key": "PARQUET-515",
        "Summary": "Add \"Reset\" to LevelEncoder and LevelDecoder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "09/Feb/16 15:22",
        "Updated": "16/Feb/16 23:34",
        "Resolved": "16/Feb/16 23:12",
        "Description": "The rle-encoder and rle-decoder classes have a \"Reset\" method as a quick way to initialize the objects. This method resets the encoder an decoder state to work on a new buffer without the need to create a new object at every DATA PAGE granularity.",
        "Issue Links": []
    },
    "PARQUET-516": {
        "Key": "PARQUET-516",
        "Summary": "Add better error handling for reading local files",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/16 15:58",
        "Updated": "01/Mar/16 01:02",
        "Resolved": "01/Mar/16 01:02",
        "Description": "The LocalFile reader class does not handle the various failure modes for the cstdio system calls.",
        "Issue Links": []
    },
    "PARQUET-517": {
        "Key": "PARQUET-517",
        "Summary": "[C++] Use arrow::MemoryPool for all heap allocations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Zherui Cao",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/16 16:02",
        "Updated": "08/Nov/19 17:09",
        "Resolved": "19/Nov/18 19:25",
        "Description": "We are using std::vector in many places for memory allocation; if we want to use SSE on this memory we may run into some problems.\nCouple things we should do\n\nAdd an STL allocator for std::vector that ensure 16-byte aligned memory\nCheck user-provided memory for alignment before utilizing an SSE-accelerated routine (e.g. SSE hash functions for dictionary encoding) and decide whether to copy and use SSE or no-copy and use no-SSE code.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5496"
        ]
    },
    "PARQUET-518": {
        "Key": "PARQUET-518",
        "Summary": "Review usages of size_t and unsigned integers generally per Google style guide",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/16 21:49",
        "Updated": "01/Mar/16 00:14",
        "Resolved": "01/Mar/16 00:14",
        "Description": "The Google style guide recommends generally avoiding unsigned integers for the bugs they can silently introduce. \nhttps://google.github.io/styleguide/cppguide.html#Integer_Types",
        "Issue Links": []
    },
    "PARQUET-519": {
        "Key": "PARQUET-519",
        "Summary": "Disable compiler warning supressions and fix all DEBUG build warnings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Feb/16 01:01",
        "Updated": "02/Mar/16 01:35",
        "Resolved": "02/Mar/16 01:35",
        "Description": "Related to PARQUET-447",
        "Issue Links": []
    },
    "PARQUET-520": {
        "Key": "PARQUET-520",
        "Summary": "Add version of LocalFileSource that uses memory-mapping for zero-copy reads",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Feb/16 23:06",
        "Updated": "01/Mar/16 00:17",
        "Resolved": "01/Mar/16 00:17",
        "Description": "Repurposed this JIRA after PARQUET-533. Memory-mapping will save us memory allocations and performance in some applications. We could have it as an optional API.",
        "Issue Links": []
    },
    "PARQUET-521": {
        "Key": "PARQUET-521",
        "Summary": "Add Brotli compression to Parquet",
        "Type": "Wish",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Martin W. Kirst",
        "Created": "10/Feb/16 23:32",
        "Updated": "11/May/16 18:00",
        "Resolved": null,
        "Description": "Hi,\nGoogle release a new compression algorithm called Brotli.\nA new compression algorithm for the internet, as they claim.\nCurrently Firefox and Chrome (both developer editions) already\nsupport Brotli, as such as e.g. ngingx.\nhttp://google-opensource.blogspot.de/2015/09/introducing-brotli-new-compression.html\nI'm the author of jbrotli - the Java bindings for brotli.\nI'm curios and doing a survey, if jBrotli is an option for parquet?\nFeedback is welcome.\nThanks in advance\nMartin",
        "Issue Links": [
            "/jira/browse/PARQUET-609",
            "/jira/browse/HADOOP-13126",
            "https://github.com/apache/parquet-mr/pull/344"
        ]
    },
    "PARQUET-522": {
        "Key": "PARQUET-522",
        "Summary": "#include cleanup with include-what-you-use",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Feb/16 17:11",
        "Updated": "16/Feb/16 00:21",
        "Resolved": "16/Feb/16 00:21",
        "Description": "I ran https://github.com/include-what-you-use/include-what-you-use on the codebase and there are quite a few imprecise or extraneous #includes. We should do this periodically to keep things clean.",
        "Issue Links": []
    },
    "PARQUET-523": {
        "Key": "PARQUET-523",
        "Summary": "Test ColumnReader on a column chunk containing multiple data pages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "12/Feb/16 16:18",
        "Updated": "16/Feb/16 23:35",
        "Resolved": "16/Feb/16 23:35",
        "Description": "Our test cases currently only cover data containing a single data page",
        "Issue Links": []
    },
    "PARQUET-524": {
        "Key": "PARQUET-524",
        "Summary": "In ColumnReader::ReadBatch, continue reading data pages until batch_size reached or EOS",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "12/Feb/16 16:20",
        "Updated": "24/Feb/16 20:57",
        "Resolved": "24/Feb/16 20:57",
        "Description": "This will produce more intuitive behavior for end users and likely yield overall better performance (the only batch with < batch_size values will be the last one).",
        "Issue Links": []
    },
    "PARQUET-525": {
        "Key": "PARQUET-525",
        "Summary": "Test coverage for malformed file failure modes on the read path",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/Feb/16 16:31",
        "Updated": "01/Mar/16 21:31",
        "Resolved": "22/Feb/16 22:08",
        "Description": "These code paths do not have test coverage. We should construct test cases that each possible kind of malformation.",
        "Issue Links": []
    },
    "PARQUET-526": {
        "Key": "PARQUET-526",
        "Summary": "Add more complete unit test coverage for column Scanner implementations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Wes McKinney",
        "Created": "12/Feb/16 16:47",
        "Updated": "24/Feb/16 20:55",
        "Resolved": "24/Feb/16 20:55",
        "Description": "First, we need to be able to at least PLAIN encode all of the primitive types so that test cases can be constructed for all types.",
        "Issue Links": [
            "/jira/browse/PARQUET-481",
            "/jira/browse/PARQUET-502"
        ]
    },
    "PARQUET-527": {
        "Key": "PARQUET-527",
        "Summary": "Provide sandbox option for creating release builds including 3rd-party libraries",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "13/Feb/16 17:13",
        "Updated": "02/Apr/17 16:14",
        "Resolved": "02/Apr/17 16:14",
        "Description": "We are currently set up for debug builds. It would be helpful for 3rd-party users to create out-of-the-box release builds including the dependencies built with optimization and statically linked (e.g. Thrift).",
        "Issue Links": []
    },
    "PARQUET-528": {
        "Key": "PARQUET-528",
        "Summary": "Fix flush() for RecordConsumer and implementations",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "14/Feb/16 14:02",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "05/Mar/16 11:46",
        "Description": "flush() was added in RecordConsumer and MessageColumnIO to help implementing nulls caching.\nHowever, other RecordConsumer implementations should also implements flush() properly. For instance, RecordConsumerLoggingWrapper and ValidatingRecordConsumer should call delegate.flush() in their flush() methods, otherwise data might be mistakenly truncated.\nThis ticket:\n\nmakes flush() abstract in RecordConsumer\nimplements flush() properly for all RecordConsumer subclasses, specifically:\n\t\nRecordConsumerLoggingWrapper\nValidatingRecordConsumer\nConverterConsumer\nExpectationValidatingRecordConsumer",
        "Issue Links": [
            "/jira/browse/PARQUET-401",
            "https://github.com/apache/parquet-mr/pull/325"
        ]
    },
    "PARQUET-529": {
        "Key": "PARQUET-529",
        "Summary": "Avoid evoking job.toString() in ParquetLoader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-pig",
        "Assignee": "Liwei Lin(Inactive)",
        "Reporter": "Liwei Lin(Inactive)",
        "Created": "14/Feb/16 14:05",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "22/Feb/16 02:37",
        "Description": "When ran under hadoop2 environment and log level setting to DEBUG, ParquetLoader would evoke job.toString() in several methods, which might cause the whole application to stop due to :\n\njava.lang.IllegalStateException: Job in state DEFINE instead of RUNNING\n    at org.apache.hadoop.mapreduce.Job.ensureState(Job.java:283)\n    at org.apache.hadoop.mapreduce.Job.toString(Job.java:452)\n    at java.lang.String.valueOf(String.java:2847)\n    at java.lang.StringBuilder.append(StringBuilder.java:128)\n    at org.apache.parquet.pig.ParquetLoader.getSchema(ParquetLoader.java:260)\n    at org.apache.parquet.pig.TestParquetLoader.testSchema(TestParquetLoader.java:54)\n    ...\nThe reason is that in the hadoop 2.x branch, org.apache.hadoop.mapreduce.Job.toString() has added an ensureState(JobState.RUNNING) check; see map-reduce: Job.java#452. In contrast, the hadoop 1.x branch does not contain such checks, so ParquetLoader works well.\nThis ticket simply avoids evoking job.toString() in ParquetLoader.",
        "Issue Links": [
            "/jira/browse/PARQUET-401",
            "https://github.com/apache/parquet-mr/pull/326"
        ]
    },
    "PARQUET-530": {
        "Key": "PARQUET-530",
        "Summary": "Add support for LZO compression",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "14/Feb/16 17:13",
        "Updated": "11/Nov/18 22:12",
        "Resolved": "11/Nov/18 22:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-531": {
        "Key": "PARQUET-531",
        "Summary": "Can't read past first page in a column",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Spiro Michaylov",
        "Created": "14/Feb/16 22:09",
        "Updated": "24/Feb/16 21:03",
        "Resolved": "24/Feb/16 21:03",
        "Description": "Building the code as of 2/14/2015 and adding the obvious three lines of code to serialized-page.cc to enable the newly added CompressionCodec::GZIP:\n\n     case parquet::CompressionCodec::GZIP:\n       decompressor_.reset(new GZipCodec());\n       break;\n\n\nI try to run the parquet_reader example on the column I'm about to attach, which was created by Apache Spark 1.5.0. It works surprisingly well until it hits the end of the first page, where it dies with  \n\nParquet error: Value was non-null, but has not been buffered\nI realize you may be reluctant to look at this because (a) the GZip support is new and (b) I had to modify the code to enable it, but actually things seem to decompress just fine (congratulations: this is awesome!): looking at the problem in the debugger and tracing through a bit it seems to me like the buffering is a bit screwed up in general \u2013 some kind of confusion between the buffering at the Scanner and Reader levels. I can reproduce the problem by reading through just a single column too. \nIt fails after 128 rows, which is suspicious given this line in column/scanner.h:\n\n    DEFAULT_SCANNER_BATCH_SIZE = 128;",
        "Issue Links": []
    },
    "PARQUET-532": {
        "Key": "PARQUET-532",
        "Summary": "Null values detection needs to be fixed and tested",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Deepak Majeti",
        "Created": "14/Feb/16 22:29",
        "Updated": "24/Feb/16 21:05",
        "Resolved": "24/Feb/16 21:05",
        "Description": "The scanner.h code for detecting NULLs has to be fixed and validated with test cases.\nThe below code seems to be wrong.\n\n// Returns true if there is a next value\nbool NextValue(T* val, bool* is_null) {\n....\n*is_null = def_level < rep_level;\n    if (*is_null) {\n      return true;\n    }\n....\n}\n\n\naccording to the spec, a NULL value has a definition level, if any, less than the maximum definition level. If the value is NULL, false must be returned here.",
        "Issue Links": []
    },
    "PARQUET-533": {
        "Key": "PARQUET-533",
        "Summary": "Simplify RandomAccessSource API to combine Seek/Read",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/Feb/16 01:15",
        "Updated": "21/Feb/16 03:27",
        "Resolved": "21/Feb/16 03:27",
        "Description": "In situations where memory-mapping is available, copying bytes into a newly-allocated memory buffer may be unnecessary.\nI propose to generally simplify the interface to random-access capable data sources to instead return a Buffer object (that I'll define) whose subclasses can be responsible for RAII memory-allocation/deallocation if it is necessary. This way, users of RandomAccessSource need not necessarily be responsible for memory allocation and object lifetime management. \nNot an urgent matter but will get a patch together sometime in the next several weeks (most likely at the same time as adding a memory-mapped file input source).\nAs an aside, it would be useful to have this same kind of abstraction available in the context of compressed data pages (note the decompression buffer member variable in ColumnReader)",
        "Issue Links": []
    },
    "PARQUET-534": {
        "Key": "PARQUET-534",
        "Summary": "Add Travis CI and codecov.io badges to README.md",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "16/Feb/16 00:11",
        "Updated": "01/Mar/16 21:30",
        "Resolved": "20/Feb/16 05:32",
        "Description": "This will give users finding parquet-cpp more confidence that it is a reliable piece of software.",
        "Issue Links": []
    },
    "PARQUET-535": {
        "Key": "PARQUET-535",
        "Summary": "Make writeAllFields more efficient in proto-parquet component",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Chen Song",
        "Reporter": "Chen Song",
        "Created": "16/Feb/16 16:17",
        "Updated": "14/Aug/18 15:35",
        "Resolved": null,
        "Description": "This will improve the write performance by 1/3 - 1/4 based on my testing. It makes a huge difference when dealing with really large files (several GBs).\nIn the original implementation, a significant time was spent on creating unnecessary Java objects.",
        "Issue Links": []
    },
    "PARQUET-536": {
        "Key": "PARQUET-536",
        "Summary": "Configure Travis CI caching to preserve built thirdparty in between builds",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "18/Feb/16 19:18",
        "Updated": "11/Nov/18 22:12",
        "Resolved": "11/Nov/18 22:12",
        "Description": "Follow up to PARQUET-471. Will speed up builds",
        "Issue Links": []
    },
    "PARQUET-537": {
        "Key": "PARQUET-537",
        "Summary": "LocalFileSource leaks resources",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "18/Feb/16 21:28",
        "Updated": "02/Mar/16 01:34",
        "Resolved": "02/Mar/16 01:34",
        "Description": "As a result of modifications introduced in PARQUET-497, LocalFileSource never gets deleted and the associated memory and file handle are leaked.",
        "Issue Links": []
    },
    "PARQUET-538": {
        "Key": "PARQUET-538",
        "Summary": "Improve ColumnReader Tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "19/Feb/16 17:58",
        "Updated": "24/Feb/16 18:30",
        "Resolved": "24/Feb/16 18:30",
        "Description": "Scope of this JIRA\n1) Template the ColumnReader tests\n2) Add multiple page tests for OPTIONAL and REQUIRED types\n3) Improve tests with realistic data pages using large number of values",
        "Issue Links": []
    },
    "PARQUET-539": {
        "Key": "PARQUET-539",
        "Summary": "Enable include_order cpplint check",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Feb/16 19:16",
        "Updated": "26/Feb/16 20:03",
        "Resolved": "26/Feb/16 20:03",
        "Description": "This will help keep our includes organized.",
        "Issue Links": []
    },
    "PARQUET-540": {
        "Key": "PARQUET-540",
        "Summary": "Cascading3 module doesn't build when using thrift 0.9.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-cascading",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "19/Feb/16 21:35",
        "Updated": "15/Jul/16 16:57",
        "Resolved": "15/Jul/16 16:57",
        "Description": "The cascading-3 module pulls in 0.7.0 because it doesn't have an explicit dependency on libthrift, although it does require libthrift. This module should have a dependency on libthrift and use the thrift.version property.\nSide note: I also noticed that it inherits SLF4J from thrift 0.7.0 and should instead have a dependency on slf4j-api and a test dependency on slf4j-simple.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/328"
        ]
    },
    "PARQUET-541": {
        "Key": "PARQUET-541",
        "Summary": "Portable build scripts",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitry Bushev",
        "Created": "20/Feb/16 17:08",
        "Updated": "21/Feb/16 03:29",
        "Resolved": "21/Feb/16 03:29",
        "Description": "Shebangs in build scripts should be portable, because some systems doesn't have /bin/bash absolute path (i.e. NixOS)",
        "Issue Links": []
    },
    "PARQUET-542": {
        "Key": "PARQUET-542",
        "Summary": "Support memory allocation from external memory",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Deepak Majeti",
        "Created": "20/Feb/16 17:47",
        "Updated": "16/Mar/16 16:34",
        "Resolved": "16/Mar/16 16:34",
        "Description": "Implement a MemoryPool like class that provides memory allocation/management for external provided memory",
        "Issue Links": [
            "/jira/browse/PARQUET-552"
        ]
    },
    "PARQUET-543": {
        "Key": "PARQUET-543",
        "Summary": "Remove BoundedInt encodings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "22/Feb/16 20:00",
        "Updated": "17/Jul/16 23:28",
        "Resolved": "17/Jul/16 23:28",
        "Description": "The classes in org.apache.parquet.column.values.boundedint aren't used. It looks like this was intended to be the \"right\" way to use the RLE/BitPacking hybrid, but callers ended up instantiating the RLE encoder or writer directly.\nThe ZeroIntegerValuesReader and DevNullValuesWriter are used, but should be relocated. The ZeroIntegerValuesReader is only used when the encoding is RLE (in Encoding.java) and the DevNullValuesWriter actually writes BIT_PACKED values. It would be better to relocate those classes in the rle and bitpacking packages.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/329"
        ]
    },
    "PARQUET-544": {
        "Key": "PARQUET-544",
        "Summary": "ParquetWriter.close() throws NullPointerException on second call, improper implementation of Closeable contract",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Michal Turek",
        "Reporter": "Michal Turek",
        "Created": "24/Feb/16 09:00",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "30/Jun/16 16:45",
        "Description": "org.apache.parquet.hadoop.ParquetWriter implements java.util.Closeable, but its close() method doesn't follow its contract properly. The interface defines \"If the stream is already closed then invoking this method has no effect.\", but ParquetWriter instead throws NullPointerException.\nIt's source is quite obvious, columnStore is set to null and then accessed again. There is no \"if already closed\" condition to prevent it.\n\njava.lang.NullPointerException: null\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:157) ~[parquet-hadoop-1.8.1.jar:1.8.1]\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113) ~[parquet-hadoop-1.8.1.jar:1.8.1]\n\tat org.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:297) ~[parquet-hadoop-1.8.1.jar:1.8.1]\n\n\n\n  private void flushRowGroupToStore()\n      throws IOException {\n    LOG.info(format(\"Flushing mem columnStore to file. allocated memory: %,d\", columnStore.getAllocatedSize()));\n    if (columnStore.getAllocatedSize() > (3 * rowGroupSizeThreshold)) {\n      LOG.warn(\"Too much memory used: \" + columnStore.memUsageString());\n    }\n\n    if (recordCount > 0) {\n      parquetFileWriter.startBlock(recordCount);\n      columnStore.flush();\n      pageStore.flushToFileWriter(parquetFileWriter);\n      recordCount = 0;\n      parquetFileWriter.endBlock();\n      this.nextRowGroupSize = Math.min(\n          parquetFileWriter.getNextRowGroupSize(),\n          rowGroupSizeThreshold);\n    }\n\n    columnStore = null;\n    pageStore = null;\n  }\n\n\nKnown workaround is to prevent the second and other closes explicitly in the application code.\n\n    private final ParquetWriter<V> writer;\n    private boolean closed;\n\n    private void closeWriterOnlyOnce() throws IOException {\n        if (!closed) {\n            closed = true;\n            writer.close();\n        }\n    }",
        "Issue Links": []
    },
    "PARQUET-545": {
        "Key": "PARQUET-545",
        "Summary": "Improve API to support Decimal type",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "24/Feb/16 17:37",
        "Updated": "29/Feb/16 18:04",
        "Resolved": "29/Feb/16 18:04",
        "Description": "Extend the `ColumnDescriptor` API to return `precision` and `scale` values from DecimalMetadata. Implement necessary checks if the `LogicalType` is Decimal.",
        "Issue Links": []
    },
    "PARQUET-546": {
        "Key": "PARQUET-546",
        "Summary": "Implement preconditions for creating PrimitiveNodes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "24/Feb/16 22:18",
        "Updated": "08/May/16 14:33",
        "Resolved": "08/May/16 14:33",
        "Description": "Add to checks to ensure compatibility between LogicalTypes and PrimitiveTypes when creating PrimitiveNodes. Example: LogicalType `DECIMAL` should not be used in conjunction with PrimitiveType `DOUBLE`.",
        "Issue Links": []
    },
    "PARQUET-547": {
        "Key": "PARQUET-547",
        "Summary": "Refactor most templates to use DataType structs rather than the Type::type enum",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Feb/16 15:42",
        "Updated": "30/Apr/16 18:13",
        "Resolved": "30/Apr/16 18:13",
        "Description": "per discussion in https://github.com/apache/parquet-cpp/pull/64 / PARQUET-494",
        "Issue Links": []
    },
    "PARQUET-548": {
        "Key": "PARQUET-548",
        "Summary": "Add Java metadata for PageEncodingStats",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "26/Feb/16 17:41",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "23/Apr/16 00:40",
        "Description": "PARQUET-384 needs to determine whether an entire column chunk is dictionary-encoded, but it is difficult to detect that case based on the set of encodings for a column. For 1.0, this can be done by checking for a PLAIN page because both dictionary pages and dictionary-encoded pages use PLAIN_DICTIONARY and RLE/BIT_PACKING is only used for repetition and definition levels. But for 2.0, dictionary pages might be using PLAIN and there is no way to tell if a column has fallen back.\nPageEncodingStats were added to the format to solve this problem, so we just need to implement them.",
        "Issue Links": [
            "/jira/browse/PARQUET-384",
            "https://github.com/apache/parquet-mr/pull/332"
        ]
    },
    "PARQUET-549": {
        "Key": "PARQUET-549",
        "Summary": "Add scanner and column reader tests for dictionary data pages",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "28/Feb/16 01:53",
        "Updated": "03/Mar/16 05:45",
        "Resolved": "03/Mar/16 05:45",
        "Description": "Extend the existing tests to dictionary pages. Only PLAIN encoding is being tested now.",
        "Issue Links": []
    },
    "PARQUET-550": {
        "Key": "PARQUET-550",
        "Summary": "Large file concerns with fseek/ftell",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Feb/16 18:14",
        "Updated": "03/Jan/17 19:56",
        "Resolved": "03/Jan/17 19:56",
        "Description": "I've read that fseek/ftell can have issues with large files:\nhttps://www.securecoding.cert.org/confluence/display/c/FIO19-C.+Do+not+use+fseek()+and+ftell()+to+compute+the+size+of+a+regular+file\nThis is not urgent, but should be examined in some more detail.",
        "Issue Links": []
    },
    "PARQUET-551": {
        "Key": "PARQUET-551",
        "Summary": "Handle compiler warnings due to disabled DCHECKs in release builds",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "01/Mar/16 01:13",
        "Updated": "07/Jul/16 00:55",
        "Resolved": "06/Jul/16 22:09",
        "Description": "Follow up to PARQUET-519",
        "Issue Links": []
    },
    "PARQUET-552": {
        "Key": "PARQUET-552",
        "Summary": "[C++] Estimate dynamic memory usage using FileMetadata",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "01/Mar/16 16:48",
        "Updated": "16/Aug/19 14:05",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-568",
            "/jira/browse/PARQUET-542"
        ]
    },
    "PARQUET-553": {
        "Key": "PARQUET-553",
        "Summary": "The README instructions for parquet-tools do not run",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jessica Kerr",
        "Created": "01/Mar/16 22:50",
        "Updated": "10/Oct/19 07:42",
        "Resolved": "10/Oct/19 07:42",
        "Description": "The README for parquet-mr/parquet-tools says to run locally:\n`java jar ./parquet-tools-<VERSION>.jar <command> my_parquet_file.lzo.parquet`\nYou need a dash before \"jar\" or it doesn't work.\n```\n$ java jar parquet-tools-1.6.0rc3-SNAPSHOT.jar\nError: Could not find or load main class jar\n```\nAlso, after the 'mvn clean package' instructions given, the jar is in ./target. That part is minor, but the \"jar\" vs \"-jar\" was hard to figure out.\nWhen this did not work, I went looking for other tools. Didn't find any, so came back and tried harder. Please, please add that one character.",
        "Issue Links": [
            "/jira/browse/PARQUET-786"
        ]
    },
    "PARQUET-554": {
        "Key": "PARQUET-554",
        "Summary": "Recursive Thrift Data Structure",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Alfers",
        "Created": "03/Mar/16 08:07",
        "Updated": "03/Mar/16 08:07",
        "Resolved": null,
        "Description": "Thrift recently introduced recursive nested data structures. As I want to store them in parquet, I get an exception: http://stackoverflow.com/questions/35702623/parquet-recursive-thrift-data-structure\nIs it possible to store recursive data structures in parquet?\nThanks,\nSeb",
        "Issue Links": []
    },
    "PARQUET-555": {
        "Key": "PARQUET-555",
        "Summary": "Dictionary page metadata handling inconsistencies",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "03/Mar/16 16:29",
        "Updated": "04/Mar/16 18:18",
        "Resolved": "04/Mar/16 18:18",
        "Description": "See parquet-format re: PLAIN_DICTIONARY / RLE_DICTIONARY distinction \u2014\u00a0our handling of the page metadata is not consistent with the format.",
        "Issue Links": []
    },
    "PARQUET-556": {
        "Key": "PARQUET-556",
        "Summary": "Extend RowGroupStatistics to include \"min\" \"max\" statistics",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "03/Mar/16 20:22",
        "Updated": "13/Mar/16 17:50",
        "Resolved": "13/Mar/16 17:50",
        "Description": "Only `null_count` and `distinct_count` are included as of now.",
        "Issue Links": []
    },
    "PARQUET-557": {
        "Key": "PARQUET-557",
        "Summary": "Enums are incorrectly handled by parquet-avro when using GenericRecords",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Igor Bernstein",
        "Created": "05/Mar/16 06:08",
        "Updated": "05/Mar/16 06:08",
        "Resolved": null,
        "Description": "It appears that enums are handled incorrectly when reading parquet as generic records.\nLooking at the code:\nhttps://github.com/apache/parquet-mr/blob/master/parquet-avro/src/main/java/org/apache/parquet/avro/AvroIndexedRecordConverter.java#L236-L238\nFieldEnumConverter falls back to a string representation when it can't find the corresponding enum class.  This is problematic when trying to read parquet files generically without specific records on the classpath because the records will no longer match the schema. I believe a more correct approach would be to wrap the enums in GenericData.EnumSymbol:\nhttps://github.com/apache/avro/blob/master/lang/java/avro/src/main/java/org/apache/avro/generic/GenericData.java#L397",
        "Issue Links": []
    },
    "PARQUET-558": {
        "Key": "PARQUET-558",
        "Summary": "Support ZSH in build scripts",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Mar/16 08:42",
        "Updated": "06/Mar/16 17:08",
        "Resolved": "06/Mar/16 17:08",
        "Description": "Currently the script thirdparty/set_thirdparty_env.sh uses the variable $BASH_SOURCE which is only available to bash users thus you cannot build/develop as a zsh user.",
        "Issue Links": []
    },
    "PARQUET-559": {
        "Key": "PARQUET-559",
        "Summary": "Enable InputStream as a source to the ParquetFileReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "07/Mar/16 19:59",
        "Updated": "23/Mar/16 01:41",
        "Resolved": "23/Mar/16 01:41",
        "Description": "The current public API only works on a file like source. The scope of this JIRA is to extend the ParquetFileReader API to accept an input stream. \nThe idea is to extend the InputStream class for this purpose similar to the RandomAccessSource class",
        "Issue Links": []
    },
    "PARQUET-560": {
        "Key": "PARQUET-560",
        "Summary": "Incorrect synchronization in SnappyCompressor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "08/Mar/16 01:18",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "25/Apr/16 22:05",
        "Description": "It seems to me that the writes to the finishCalled variable is not synchronized properly in the SnappyCompressor.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/334"
        ]
    },
    "PARQUET-561": {
        "Key": "PARQUET-561",
        "Summary": "ParquetFileReader::Contents PIMPL missing a virtual destructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "13/Mar/16 09:54",
        "Updated": "13/Mar/16 17:45",
        "Resolved": "13/Mar/16 17:45",
        "Description": "As ParquetFileReader::Contents  does not have a virtual destructor, the destructors of the child classes, i.e. SerializedFile are not called and thus all member of it leak memory.",
        "Issue Links": []
    },
    "PARQUET-562": {
        "Key": "PARQUET-562",
        "Summary": "Simplified ZSH support in build scripts",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "13/Mar/16 11:31",
        "Updated": "13/Mar/16 17:47",
        "Resolved": "13/Mar/16 17:47",
        "Description": "The original proposed patch can be implemented in a much simpler way. The complicated approach is only needed when used inside a function. \nAlso some scripts in thirdparty did not support zsh.",
        "Issue Links": [
            "/jira/browse/ARROW-64"
        ]
    },
    "PARQUET-563": {
        "Key": "PARQUET-563",
        "Summary": "Add support for ASAN builds with clang",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "13/Mar/16 16:40",
        "Updated": "11/Nov/18 22:12",
        "Resolved": "11/Nov/18 22:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-564": {
        "Key": "PARQUET-564",
        "Summary": "Add option to run unit tests with valgrind --tool=memcheck",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "13/Mar/16 16:59",
        "Updated": "15/Mar/16 06:33",
        "Resolved": "15/Mar/16 06:33",
        "Description": "As reported in PARQUET-561, we are unable to easily actually test for memory leaks using cmake's ctest. I'll add a configuration option do enable this",
        "Issue Links": []
    },
    "PARQUET-565": {
        "Key": "PARQUET-565",
        "Summary": "Use PATH instead of DIRECTORY in get_filename_component to support CMake<2.8.12",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "16/Mar/16 12:18",
        "Updated": "16/Mar/16 16:36",
        "Resolved": "16/Mar/16 16:36",
        "Description": "In the Find* modules, we use DIRECTORY as the component specifier which was only introduced in 2.8.12 but as CMakeLists.txt specifies that we want to support older CMake versions.\nNote: This is the only thing that needs to be adjusted to get parquet-cpp running on Debian Wheezy (oldstable).",
        "Issue Links": []
    },
    "PARQUET-566": {
        "Key": "PARQUET-566",
        "Summary": "Add method to retrieve the full column path",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Mar/16 19:49",
        "Updated": "27/Mar/16 18:32",
        "Resolved": "27/Mar/16 18:32",
        "Description": "Currently the column description only returns the name of the primitive leaf node. There is no reliable way to the full column path.",
        "Issue Links": []
    },
    "PARQUET-567": {
        "Key": "PARQUET-567",
        "Summary": "C++: Add a \"file iterator\" abstraction and API-compatible interface for scanning directories of Parquet files",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Mar/16 20:40",
        "Updated": "01/May/19 22:42",
        "Resolved": "01/May/19 22:42",
        "Description": "Since many systems write many Parquet files into a particular directory in HDFS, for example, one will want to be able to treat a collection of files from the same dataset semantically as one large table. This will save downstream clients of parquet-cpp from having to write this code themselves.",
        "Issue Links": []
    },
    "PARQUET-568": {
        "Key": "PARQUET-568",
        "Summary": "Read only specified top-level columns in DebugPrint",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Aliaksei Sandryhaila",
        "Reporter": "Aliaksei Sandryhaila",
        "Created": "21/Mar/16 15:23",
        "Updated": "23/Mar/16 15:04",
        "Resolved": "23/Mar/16 01:34",
        "Description": "At the moment, `ParquetFileReader` instantiates Scanners for all columns and reads contents of all columns. We should be able to specify a subset of top-level columns to be read.",
        "Issue Links": [
            "/jira/browse/PARQUET-552"
        ]
    },
    "PARQUET-569": {
        "Key": "PARQUET-569",
        "Summary": "ParquetMetadataConverter offset filter is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "23/Mar/16 20:51",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "23/Apr/16 00:43",
        "Description": "The commit for PARQUET-384 moved block filtering by offset that was happening in the record reader into ParquetMetadataConverter using the same codepath as the existing range filter. This broke the offset filtering because the offsets actually passed to the range filter are row group midpoints.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/337"
        ]
    },
    "PARQUET-570": {
        "Key": "PARQUET-570",
        "Summary": "Write parquet to STDOUT",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Federico Ponzi",
        "Created": "24/Mar/16 12:34",
        "Updated": "30/Mar/16 17:30",
        "Resolved": null,
        "Description": "https://github.com/Parquet/parquet-mr/blob/master/parquet-hadoop/src/main/java/parquet/hadoop/ParquetWriter.java\nI would like to write the parquet to stdout but there is no option to let me do this. Also the file must not already exist, or I' ll get Exception.\nThanks",
        "Issue Links": []
    },
    "PARQUET-571": {
        "Key": "PARQUET-571",
        "Summary": "Fix potential leak in ParquetFileReader.close()",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Nezih Yigitbasi",
        "Reporter": "Nezih Yigitbasi",
        "Created": "24/Mar/16 20:32",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "25/Mar/16 19:20",
        "Description": "If an exception occurs when closing the input stream `f`, the codecs will not\nbe released. This may cause native memory leaks for some codecs.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/338"
        ]
    },
    "PARQUET-572": {
        "Key": "PARQUET-572",
        "Summary": "Rename parquet_cpp namespace to parquet",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Mar/16 23:13",
        "Updated": "28/Mar/16 16:21",
        "Resolved": "28/Mar/16 16:21",
        "Description": "I propose that we do this before it is too late \u2013 I believe we should also scope the Thrift metadata namespace to be parquet::metadata or parquet::thrift to avoid namespace conflicts. Having to use parquet_cpp in thirdparty code so far is feeling a little bit annoying. Thoughts?",
        "Issue Links": []
    },
    "PARQUET-573": {
        "Key": "PARQUET-573",
        "Summary": "C++: Create a public API for reading and writing file metadata",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "27/Mar/16 22:08",
        "Updated": "01/Sep/16 12:43",
        "Resolved": "01/Sep/16 12:43",
        "Description": "Currently, the file metadata is not visible in the public API. This makes some aspects of testing for external users more difficult (for example: examining the size of column chunks and row groups).\nAnalogously, we need to be able to create arbitrary metadata for testing purposes and also for writing fully formed Parquet files. See PARQUET-552, for example.",
        "Issue Links": []
    },
    "PARQUET-574": {
        "Key": "PARQUET-574",
        "Summary": "Boolean format in Plain Decoder",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fabrizio Milo",
        "Created": "28/Mar/16 19:49",
        "Updated": "04/Apr/16 19:16",
        "Resolved": null,
        "Description": "In the encoding.md document is written that the plain encoder for boolean uses [RLE/BitPacking](https://github.com/apache/parquet-format/blob/master/Encodings.md#plain-plain--0) \nWhile in the cpp implementation seems is just using [simple bit decoding back to back.](https://github.com/apache/parquet-cpp/blob/master/src/parquet/encodings/plain-encoding.h#L151)\nWhich one is the right format ?",
        "Issue Links": []
    },
    "PARQUET-575": {
        "Key": "PARQUET-575",
        "Summary": "Different RLE Encoding Specification",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fabrizio Milo",
        "Created": "29/Mar/16 04:38",
        "Updated": "30/Mar/16 19:56",
        "Resolved": null,
        "Description": "In the parquet-format specification https://github.com/Parquet/parquet-format/blob/master/Encodings.md#run-length-encoding--bit-packing-hybrid-rle--3\n is written that the RLE encoding starts with \n```\nrle-bit-packed-hybrid: <length> <encoded-data>\nlength := length of the <encoded-data> in bytes stored as 4 bytes little endian\n```\nwhile in the cpp implementation there is this description https://github.com/apache/parquet-cpp/blob/master/src/parquet/util/rle-encoding.h#L42 and the implementation seems to follow that specification\nwhich  does not include the initial <length> <encoded-data>\nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/util/rle-encoding.h#L272\nSo which one is the correct? seems that the parquet-format is the wrong one.\nDataPage.definitionLevels uses RLE and none of the example format files seem to have that initial <length> <encoded-data> \nAlso the use of both names `literal` and `bit-encoding` is confusing.",
        "Issue Links": []
    },
    "PARQUET-576": {
        "Key": "PARQUET-576",
        "Summary": "C++: Simplify RandomAccessSource reads producing InputStream",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "29/Mar/16 23:26",
        "Updated": "05/May/16 14:27",
        "Resolved": "05/May/16 14:25",
        "Description": "Presently, we have code like \n\n  int64_t bytes_to_read = col.meta_data.total_compressed_size;\n  std::shared_ptr<Buffer> buffer = source_->ReadAt(col_start, bytes_to_read);\n\n  if (buffer->size() < bytes_to_read) {\n    throw ParquetException(\"Unable to read column chunk data\");\n  }\n\n  std::unique_ptr<InputStream> stream(new InMemoryInputStream(buffer));\n\n\nThis seems like an leaky detail (some interfaces expect streams, not buffers) that could be encapsulated in the data source class. This would enable us later to work on \"lazy\" stream instances in a way that does not affect downstream users.",
        "Issue Links": []
    },
    "PARQUET-577": {
        "Key": "PARQUET-577",
        "Summary": "mandatory status of avro columns ignored",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Remek Zajac",
        "Created": "31/Mar/16 08:56",
        "Updated": "31/Mar/16 09:04",
        "Resolved": null,
        "Description": "Avro spec schema [resolution rules ](https://avro.apache.org/docs/1.7.7/spec.html#schema_record) say: \n\"if the reader's record schema has a field with no default value, and writer's schema does not have a field with the same name, an error is signalled.\"\nI can't find the implementation of this aspect in parquet.avro and indeed observe this rule seemingly ignored. I am using 1.6.0 because that's what we can get off maven. \nMy writer's schema:\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"SampleSchema_v1\",\n  \"namespace\" : \"com.xxxx.spark\",\n  \"fields\" : [ {\n    \"name\" : \"stringField\",\n    \"type\" : \"string\",\n    \"doc\"  : \"Sample string field\"\n  },{\n    \"name\" : \"longField\",\n    \"type\" : \"long\",\n    \"doc\"  : \"Sample long field\"\n  } ],\n  \"doc:\" : \"A sample/test schema\"\n}\n\n\nMy reader schema:\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"SampleSchema_newDefaultlessCol\",\n  \"namespace\" : \"com.xxxx.spark\",\n  \"fields\" : [ {\n    \"name\" : \"stringField\",\n    \"type\" : \"string\",\n    \"doc\"  : \"Sample string field\"\n  },{\n    \"name\" : \"longField\",\n    \"type\" : \"long\",\n    \"doc\"  : \"Sample long field\"\n  },{\n    \"name\" : \"mandatoryIntField\",\n    \"type\" : \"int\",\n    \"doc\"  : \"Sample mandatory! int field\"\n  }],\n  \"doc:\" : \"v1 + one extra column that has no default\"\n}\n\n\nThis is my test case:\n\n    \"accept new column w/o a default [schema-evolution, undesired]\" in new MockAvroParquetGrid {\n      //TODO: the behaviour this test case exercises is UNDESIRED, i.e.: a new column with no default value should\n      //TODO: Ticket to track this: https://jira.xxxx.io/browse/ADR-610\n      //constitute an incompatible schema break, instead, this thing uses 0 for the default\n      val inputSampleRecordsV1  = Seq(new SampleSchema_v1(s\"string\", 1))\n      dao.writeParquet[SampleSchema_v1](\n        SparkBase.sc.parallelize(inputSampleRecordsV1),\n        SampleSchema_v1.SCHEMA$,\n        parquetFolder\n      )\n\n      dao\n        .readParquet[SampleSchema_newDefaultlessCol](parquetFolder, SampleSchema_newDefaultlessCol.SCHEMA$)\n        .collect().toSeq.head\n        .getMandatoryIntField must equalTo(0) //TODO: zero is an unwelcome guess\n    }\n\n\nThis is the implementation of writeParquet and readParquet\n\n  def writeParquet[C](source: RDD[C], schema: org.apache.avro.Schema, dstPath: String)\n                     (implicit ctag: ClassTag[C]): Unit = {\n    val hadoopJob = Job.getInstance()\n    ParquetOutputFormat.setWriteSupportClass(hadoopJob, classOf[AvroWriteSupport])\n    ParquetOutputFormat.setCompression(hadoopJob, CompressionCodecName.GZIP)\n    AvroWriteSupport.setSchema(hadoopJob.getConfiguration, schema)\n\n    new PairRDDFunctions[Void,C](\n      source.map(sourceRecord => (null, sourceRecord))\n    ).saveAsNewAPIHadoopFile(\n      bucketDAO.uri(dstPath),\n      classOf[Void],                            //K\n      ctag.runtimeClass.asInstanceOf[Class[C]], //V\n      classOf[AvroParquetOutputFormat],\n      hadoopJob.getConfiguration\n    )\n  }\n\n  def readParquet[C](srcPath: String, schema: org.apache.avro.Schema)(implicit ctag: ClassTag[C]): RDD[C] = {\n    val hadoopJob = Job.getInstance()\n    ParquetInputFormat.setReadSupportClass(hadoopJob, classOf[AvroReadSupport[C]])\n    AvroReadSupport.setAvroReadSchema(hadoopJob.getConfiguration, schema)\n    sc.newAPIHadoopFile(\n      bucketDAO.uri(srcPath),\n      classOf[ParquetInputFormat[C]],\n      classOf[Void],                            //K\n      ctag.runtimeClass.asInstanceOf[Class[C]], //V\n      hadoopJob.getConfiguration\n    ).map { _._2 }\n  }\n\n\nWe use avro-tools to generate java classes from our avro schemas.\njava -jar /path/to/avro-tools-1.8.0.jar compile schema <schema file> <destination>\nThe test case harvests zeroes as values of mandatoryIntField\nNaively, I see a problem in the [indexed revord\u00a0converter](https://git-wip-us.apache.org/repos/asf?p=parquet-mr.git;a=blob;f=parquet-avro/src/main/java/org/apache/parquet/avro/AvroIndexedRecordConverter.java;h=06c66d692571da08298ae1da4f9967446c4864ee;hb=HEAD#l105) in that it cheerfully accepts a condition doomed to fail. The condition being: the reader schema has a column with no default value that is absent in the writer schema.\nI am writing predominantly to confirm my diagnosis and to get the intell on why is it implemented the way it is. Is it fixable (or other depend on it as on a feature)? Can people think of a workaround?",
        "Issue Links": []
    },
    "PARQUET-578": {
        "Key": "PARQUET-578",
        "Summary": "Parquet silently discards decimal values if they do not fit in declared type",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andy Grove",
        "Created": "06/Apr/16 20:48",
        "Updated": "06/Apr/16 20:48",
        "Resolved": null,
        "Description": "If I declare a schema with a DecimalType(7,2) and I try and write a value that will not fit, such as 123,456.00 and then read it back, I get a null value i.e. the data is silently discarded.\nI have written up a full example to demonstrate this, here:\nhttps://github.com/andygrove/spark-parquet-decimal-bug",
        "Issue Links": []
    },
    "PARQUET-579": {
        "Key": "PARQUET-579",
        "Summary": "Add API for writing Column statistics",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Artem Tarasov",
        "Reporter": "Uwe Korn",
        "Created": "11/Apr/16 16:24",
        "Updated": "06/Nov/16 18:21",
        "Resolved": "06/Nov/16 18:21",
        "Description": "In the initial implementation of ParquetFileWriter, there is no support for writing/computing statistics.\n\nWrite an API so that the user can specify statistics on their own.\nAdd a convenience class that calculates the statistics during write.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-580": {
        "Key": "PARQUET-580",
        "Summary": "Potentially unnecessary creation of large int[] in IntList for columns that aren't used",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Piyush Narang",
        "Reporter": "Piyush Narang",
        "Created": "15/Apr/16 01:00",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "17/Apr/16 00:28",
        "Description": "Noticed that for a dataset that we were trying to import that had a lot of columns (few thousand) that weren't being used, we ended up allocating a lot of unnecessary int arrays (each 64K in size) as we create an IntList object for every column. Heap footprint for all those int[]s turned out to be around 2GB or so (and results in some jobs OOMing). This seems unnecessary for columns that might not be used. \nAlso wondering if 64K is the right size to start off with. Wondering if a potential improvement is if we could allocate these int[]s in IntList in a way that slowly ramps up their size. So rather than create arrays of size 64K at a time (which is potentially wasteful if there are only a few hundred bytes), we could create say a 4K int[], then when it fills up an 8K[] and so on till we reach 64K (at which point the behavior is the same as the current implementation).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/339"
        ]
    },
    "PARQUET-581": {
        "Key": "PARQUET-581",
        "Summary": "Min/max row count for page size check are conflated in some places",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Michael MacFadden",
        "Reporter": "Michael MacFadden",
        "Created": "15/Apr/16 21:05",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "17/Apr/16 00:24",
        "Description": "ParquetOutputFormat.java includes two places where minimum and maximum settings of the row count for page size check are conflated.\nPlease consult the PR for details. It's dead simple.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/340"
        ]
    },
    "PARQUET-582": {
        "Key": "PARQUET-582",
        "Summary": "Conversion functions for Parquet enums to Thrift enums",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "16/Apr/16 16:21",
        "Updated": "17/Apr/16 19:43",
        "Resolved": "17/Apr/16 19:43",
        "Description": "Add functions to convert parquet-cpp enums to Thrift enums.",
        "Issue Links": []
    },
    "PARQUET-583": {
        "Key": "PARQUET-583",
        "Summary": "Implement Parquet to Thrift schema conversion",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "16/Apr/16 16:49",
        "Updated": "19/Apr/16 09:02",
        "Resolved": "19/Apr/16 09:02",
        "Description": "Implement the conversion of a Parquet schema tree to std::vector<parquet::format::SchemaElement>.",
        "Issue Links": []
    },
    "PARQUET-584": {
        "Key": "PARQUET-584",
        "Summary": "show proper command usage when there's no arguments",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Kaufman Ng",
        "Reporter": "Kaufman Ng",
        "Created": "18/Apr/16 02:49",
        "Updated": "19/Apr/16 15:27",
        "Resolved": "19/Apr/16 15:27",
        "Description": "parquet-tools command line currently does not show synopsis when no arguments are provided.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/336"
        ]
    },
    "PARQUET-585": {
        "Key": "PARQUET-585",
        "Summary": "Slowly ramp up sizes of int[]s in IntList to keep sizes small when data sets are small",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Piyush Narang",
        "Reporter": "Piyush Narang",
        "Created": "18/Apr/16 23:18",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "21/Apr/16 03:49",
        "Description": "One of the items that I listed in jira: https://issues.apache.org/jira/browse/PARQUET-580 was to ensure that we slowly ramp up the size of the int[] we create in IntList instead of 64K arrays directly. This ensures that we don't allocate large 64K arrays at the start(which is potentially wasteful if there are only a few hundred bytes). \nSo rather than create arrays of size 64K at a time, we could create say a 4K int[], then when it fills up an 8K[] and so on till we reach 64K (at which point the behavior is the same as the current implementation).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/341"
        ]
    },
    "PARQUET-586": {
        "Key": "PARQUET-586",
        "Summary": "Not able to store int or boolean from Pig to Parquet",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-pig",
        "Assignee": null,
        "Reporter": "Jonathan Morales",
        "Created": "19/Apr/16 16:12",
        "Updated": "19/Apr/16 16:12",
        "Resolved": null,
        "Description": "I have a simple Pig schema that contains long and boolean properties. When trying to store the records using `parquet.pig.ParquetStorer();` I get the following error:\n```\njava.io.IOException: parquet.io.ParquetEncodingException: can not write value at 9 in tuple (...) from type 'likeCount: long' to type 'optional int64 likeCount'\n```",
        "Issue Links": []
    },
    "PARQUET-587": {
        "Key": "PARQUET-587",
        "Summary": "Implement BufferReader::Read(int64_t,uint8_t*)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "20/Apr/16 06:56",
        "Updated": "22/Apr/16 14:24",
        "Resolved": "22/Apr/16 14:24",
        "Description": "Method is not yet implemented.",
        "Issue Links": []
    },
    "PARQUET-588": {
        "Key": "PARQUET-588",
        "Summary": "Finalize Parquet 2.0 spec",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "21/Apr/16 03:41",
        "Updated": "21/Apr/16 03:41",
        "Resolved": null,
        "Description": "We should finalize (and document?) the new encodings and format structures in the 2.0 spec.",
        "Issue Links": []
    },
    "PARQUET-589": {
        "Key": "PARQUET-589",
        "Summary": "Implement Chunked InMemoryInputStream for better memory usage",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "21/Apr/16 20:33",
        "Updated": "05/May/16 04:36",
        "Resolved": "05/May/16 04:36",
        "Description": "Parquet-cpp when creating a Column Reader, reads the entire source into a buffer, which is then passed to the InMemoryInputStream. This prohibits reading multiple large parquet files simultaneously. The scope of this JIRA is to implement an InputStream that does not  require the entire source to be read.",
        "Issue Links": []
    },
    "PARQUET-590": {
        "Key": "PARQUET-590",
        "Summary": "Parquet/WriterProperties class to chose serialization options",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:38",
        "Updated": "09/Sep/16 14:45",
        "Resolved": "09/Sep/16 14:45",
        "Description": "We need a class similar to ParquetProperties in parquet-column where we define the basic settings on a global and per-column basis like compression, encodings, page size, ... that will be used during serialization. At the moment we have hard-coded defaults that will not suit everyone's needs.",
        "Issue Links": []
    },
    "PARQUET-591": {
        "Key": "PARQUET-591",
        "Summary": "Page size estimation during writes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:44",
        "Updated": "08/Sep/16 18:35",
        "Resolved": "08/Sep/16 18:35",
        "Description": "Currently we start a new page after 1000 rows. Instead we should split pages by their size. Therefore implement a size estimation algorithm and split pages based on its results.\nSee also in parquet-column/../column/impl/ColumnWriteStoreV2.java:sizeCheck for the Java implementation.",
        "Issue Links": []
    },
    "PARQUET-592": {
        "Key": "PARQUET-592",
        "Summary": "Support compressed writes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Artem Tarasov",
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:46",
        "Updated": "20/Jun/16 12:41",
        "Resolved": "20/Jun/16 12:40",
        "Description": "Implement compression of values and levels in parquet-cpp writes.",
        "Issue Links": []
    },
    "PARQUET-593": {
        "Key": "PARQUET-593",
        "Summary": "Add API for writing Page statistics",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Artem Tarasov",
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:47",
        "Updated": "03/Oct/16 17:41",
        "Resolved": "03/Oct/16 17:41",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-594": {
        "Key": "PARQUET-594",
        "Summary": "[C++] Support CRC checksums in pages",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:48",
        "Updated": "31/May/19 15:34",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-595": {
        "Key": "PARQUET-595",
        "Summary": "Add API for key-value metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "23/Apr/16 17:49",
        "Updated": "29/Apr/17 17:58",
        "Resolved": "29/Apr/17 17:58",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-596": {
        "Key": "PARQUET-596",
        "Summary": "Parquet need to support empty list and empty map",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Yongzhi Chen",
        "Created": "02/May/16 17:17",
        "Updated": "08/Sep/16 16:33",
        "Resolved": "08/Sep/16 16:33",
        "Description": "In current hive upstream 2.1version, when hive tried to insert empty list or empty map to parquet table, it fails with error:\n parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead\nIt seems now parquet only support null value, we should find a way to support empty list and empty map values.",
        "Issue Links": [
            "/jira/browse/HIVE-13632"
        ]
    },
    "PARQUET-597": {
        "Key": "PARQUET-597",
        "Summary": "Add data rates to benchmark output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "02/May/16 19:46",
        "Updated": "04/May/16 00:41",
        "Resolved": "04/May/16 00:41",
        "Description": "Instead of only showing ns, we could also inform GBenchmark about the amount of processed bytes. Then we would get MB/s throughput rates.",
        "Issue Links": []
    },
    "PARQUET-598": {
        "Key": "PARQUET-598",
        "Summary": "[C++] Test writing all primitive data types",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "04/May/16 03:50",
        "Updated": "03/Jun/16 07:35",
        "Resolved": "03/Jun/16 07:35",
        "Description": "As part of helping validate the write path",
        "Issue Links": []
    },
    "PARQUET-599": {
        "Key": "PARQUET-599",
        "Summary": "ColumnWriter::RleEncodeLevels' size estimation might be wrong",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "04/May/16 17:35",
        "Updated": "09/May/16 00:13",
        "Resolved": "09/May/16 00:13",
        "Description": "The size estimation might be too small. Instead of adding a factor of 2 (which I guess we're off), we should add a call to LevelEncoder that redirects either to RleEncoder::MaxBufferSize or computes the value for the BitPacked encoding.",
        "Issue Links": []
    },
    "PARQUET-600": {
        "Key": "PARQUET-600",
        "Summary": "Add benchmarks for RLE-Level encoding",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "04/May/16 17:42",
        "Updated": "17/May/16 06:28",
        "Resolved": "17/May/16 06:28",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-601": {
        "Key": "PARQUET-601",
        "Summary": "Add support in Parquet to configure the encoding used by ValueWriters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-mr",
        "Assignee": "Piyush Narang",
        "Reporter": "Piyush Narang",
        "Created": "05/May/16 19:02",
        "Updated": "17/May/18 14:46",
        "Resolved": "11/Aug/16 20:31",
        "Description": "Parquet is currently structured to choose the appropriate value writer based on the type of the column as well as the Parquet version. Value writers are responsible for writing out values with the appropriate encoding. As an example, for Boolean data types, we use BooleanPlainValuesWriter (v1.0) or RunLengthBitPackingHybridValuesWriter (v2.0). The code to take these decisions is in ParquetProperties. \nThanks to this set up, the writer(s) (and hence encoding) for each data type is hard coded in the Parquet source code. \nWould be nice to support being able to override the encodings per type via config. That allows users to experiment with various encoding strategies manually as well as enables them to override the hardcoded defaults if they don't suit their use case. \nWe can override encodings per data type (int32 / int64 / ...). \nSomething on the lines of:\n\nparquet.writer.encoding-override.<type> = \"encoding1[,encoding2]\"\n\n\nAs an example:\n\n\"parquet.writer.encoding-override.int32\" = \"plain\"\n(Chooses Plain encoding and hence the PlainValuesWriter).\n\n\nWhen a primary + fallback need to be specified, we can do the following:\n\n\"parquet.writer.encoding-override.binary\" = \"rle_dictionary,delta_byte_array\"\n(Chooses RLE_DICTIONARY encoding as the initial encoding and DELTA_BYTE_ARRAY encoding as the fallback and hence creates a FallbackWriter(PlainBinaryDictionaryValuesWriter, DeltaByteArrayWriter). \n\n\nIn such cases we can mandate that the first encoding listed must allow for Fallbacks by implementing RequiresFallback.",
        "Issue Links": [
            "/jira/browse/PARQUET-1302"
        ]
    },
    "PARQUET-602": {
        "Key": "PARQUET-602",
        "Summary": "Add type of Decimal data with 20 Bytes to accomodate dynamic Precision and Scale with data like Oracle Number type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Bhargav Donga",
        "Created": "07/May/16 04:29",
        "Updated": "07/May/16 04:29",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-603": {
        "Key": "PARQUET-603",
        "Summary": "Implement missing information in schema descriptor",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "08/May/16 14:42",
        "Updated": "09/May/16 05:25",
        "Resolved": "09/May/16 05:25",
        "Description": "The scope of this JIRA is to populate the  std::unordered_map<int, const schema::NodePtr> leaf_to_base_ data structure in the schema descriptor. This is handy in reading complex types.",
        "Issue Links": []
    },
    "PARQUET-604": {
        "Key": "PARQUET-604",
        "Summary": "Install writer.h headers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/May/16 20:48",
        "Updated": "09/May/16 00:49",
        "Resolved": "09/May/16 00:49",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-605": {
        "Key": "PARQUET-605",
        "Summary": "Expose schema node in ColumnDescriptor",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/May/16 20:50",
        "Updated": "09/May/16 01:03",
        "Resolved": "09/May/16 01:03",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-606": {
        "Key": "PARQUET-606",
        "Summary": "Travis coverage is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Deepak Majeti",
        "Created": "09/May/16 14:55",
        "Updated": "09/May/16 23:13",
        "Resolved": "09/May/16 23:13",
        "Description": "Pull request #92 (https://github.com/apache/parquet-cpp/pull/92) made changes to the travis build dir. These changes seem to have broken the coverage. \nhttps://coveralls.io/builds/6003456\nErrors can be seen below\nhttps://travis-ci.org/apache/parquet-cpp/jobs/126949580",
        "Issue Links": []
    },
    "PARQUET-607": {
        "Key": "PARQUET-607",
        "Summary": "Public Writer header",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "09/May/16 16:31",
        "Updated": "09/May/16 18:54",
        "Resolved": "09/May/16 18:54",
        "Description": "Add a public header for the writer API",
        "Issue Links": []
    },
    "PARQUET-608": {
        "Key": "PARQUET-608",
        "Summary": "Add thrift.executable property to parquet-format build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/May/16 18:58",
        "Updated": "11/Jul/16 18:03",
        "Resolved": "11/Jul/16 18:03",
        "Description": "Parquet MR has a property to point the thrift maven plugin to a different thrift executable for testing thrift 9 and 7. We should add the same to parquet-format.",
        "Issue Links": []
    },
    "PARQUET-609": {
        "Key": "PARQUET-609",
        "Summary": "Add Brotli compression to Parquet format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.3.1",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/May/16 19:00",
        "Updated": "16/Oct/17 23:56",
        "Resolved": "11/Jul/16 18:01",
        "Description": "To use Brotli with Parquet, we need to add it to the format's compression codec enum.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-521",
            "https://github.com/apache/parquet-format/pull/40"
        ]
    },
    "PARQUET-610": {
        "Key": "PARQUET-610",
        "Summary": "Print ColumnMetaData for each RowGroup",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "10/May/16 19:13",
        "Updated": "12/May/16 16:24",
        "Resolved": "12/May/16 16:24",
        "Description": "The scope of this JIRA is to extend parquet_reader to print the ColumnMetaData in each RowGroup. The current output does not include the compression, encodings and sizes",
        "Issue Links": []
    },
    "PARQUET-611": {
        "Key": "PARQUET-611",
        "Summary": "Use google logging library (glog)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "11/May/16 16:27",
        "Updated": "05/Jul/16 18:54",
        "Resolved": "05/Jul/16 18:54",
        "Description": "The current logging pragmas have some drawbacks.\nMainly, the statements in the logs (eg. DCHECK(condition)) are not used in a release mode build. This  causes plenty of warnings. \nSecondly, we cannot use the logging pragmas directly on code.\neg. DCHECK(foo()) will not call foo() in a release mode build.\nGlog handles all these cases.",
        "Issue Links": []
    },
    "PARQUET-612": {
        "Key": "PARQUET-612",
        "Summary": "Add compression to FileEncodingIT tests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/May/16 17:34",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "30/Jun/16 16:54",
        "Description": "The FileEncodingsIT test validates that pages can be read independently with all encodings, without compression. Pages should not depend on one another for compression to be correct as well, so we should extend this test to use the other compression codecs.\nThis test is already expensive, so I propose adding an environment variable to add more compression codecs. That way this results in no extra build/test time, but we can turn on more validation in Travis CI.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/343"
        ]
    },
    "PARQUET-613": {
        "Key": "PARQUET-613",
        "Summary": "C++: Add conda packaging recipe",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/May/16 16:42",
        "Updated": "14/May/16 18:59",
        "Resolved": "14/May/16 18:59",
        "Description": "Now that Thrift packages are available from the conda-forge community channel, we can also post dev build artifacts of parquet as part of Travis CI (or on demand): https://anaconda.org/conda-forge/thrift-cpp.",
        "Issue Links": []
    },
    "PARQUET-614": {
        "Key": "PARQUET-614",
        "Summary": "C++: Remove unneeded LZ4-related code",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/May/16 22:49",
        "Updated": "13/May/16 17:27",
        "Resolved": "13/May/16 17:27",
        "Description": "The LZ4 algorithm is not used for the Parquet LZO compression type. We should remove the unneeded dependency from the thirdparty toolchain and build.",
        "Issue Links": []
    },
    "PARQUET-615": {
        "Key": "PARQUET-615",
        "Summary": "C++: Building static or shared libparquet should not be mutually exclusive",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "12/May/16 23:26",
        "Updated": "03/Jun/16 07:32",
        "Resolved": "03/Jun/16 07:32",
        "Description": "In practice, many environments may use both the static and dynamic libraries. Whether these are built can be toggled on or off via cmake options",
        "Issue Links": []
    },
    "PARQUET-616": {
        "Key": "PARQUET-616",
        "Summary": "C++: WriteBatch should accept const arrays",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "13/May/16 19:56",
        "Updated": "14/May/16 05:16",
        "Resolved": "13/May/16 20:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-617": {
        "Key": "PARQUET-617",
        "Summary": "C++: Enable conda build to work on systems with non-default C++ toolchains",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "14/May/16 19:31",
        "Updated": "16/May/16 18:34",
        "Resolved": "16/May/16 18:34",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-618": {
        "Key": "PARQUET-618",
        "Summary": "C++: Automatically upload conda build artifacts on commits to master",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "17/May/16 04:24",
        "Updated": "17/May/16 19:43",
        "Resolved": "17/May/16 19:43",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-619": {
        "Key": "PARQUET-619",
        "Summary": "C++: Add OutputStream for local files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/May/16 16:44",
        "Updated": "18/May/16 17:56",
        "Resolved": "18/May/16 17:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-620": {
        "Key": "PARQUET-620",
        "Summary": "C++: Duplicate calls to ParquetFileWriter::Close cause duplicate metdata writes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/May/16 16:47",
        "Updated": "18/May/16 02:12",
        "Resolved": "18/May/16 02:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-621": {
        "Key": "PARQUET-621",
        "Summary": "C++: Uninitialised DecimalMetadata is read",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/May/16 16:50",
        "Updated": "18/May/16 18:02",
        "Resolved": "18/May/16 18:02",
        "Description": "We were always writing out the DecimalMetadata of a schema node even if it was never set, thus getting some Conditional jump or move depends on uninitialised value(s) in valgrind.",
        "Issue Links": []
    },
    "PARQUET-622": {
        "Key": "PARQUET-622",
        "Summary": "Each RowGroup has one DictionaryData",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Xu Chen",
        "Created": "19/May/16 06:23",
        "Updated": "23/May/16 00:22",
        "Resolved": "23/May/16 00:22",
        "Description": "If using dictionary more of duplicate records have more compress rate \uff1f\nWhy do not let one RowGroup has one DictionaryData to make more duplicate records into dictionary .",
        "Issue Links": []
    },
    "PARQUET-623": {
        "Key": "PARQUET-623",
        "Summary": "DeltaByteArrayReader has incorrect skip behaviour",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Johannes Zillmann",
        "Created": "23/May/16 12:38",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "08/Sep/16 21:23",
        "Description": "When reading delta encoded columns (v2 API) and skipping values the read fails (with differing exceptions). This is because the reader needs the last value but the skip logic is not managing this.\nA modified DeltaByteArrayReader which calls readBytes() on skip() does fix the reading. Not sure if there is a more efficient solution possible then simply invoking readBytes().",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/366"
        ]
    },
    "PARQUET-624": {
        "Key": "PARQUET-624",
        "Summary": "Value count used for memSize calculation in ColumnWriterV1 can be skewed based on first 100 values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Piyush Narang",
        "Reporter": "Piyush Narang",
        "Created": "23/May/16 23:37",
        "Updated": "23/May/16 23:45",
        "Resolved": "23/May/16 23:45",
        "Description": "While digging into some OOMs that we were seeing for some of our Parquet writer jobs, I noticed that we were writing out around 250MB+ of data for a single column as one page. Our page size threshold is set to 1MB so this should actually result in a few hundred pages instead of just 1. \nThis seems to be due to the code in: ColumnWriterV1.accountForValueWritten(). We only check if we've crossed the memory threshold if the valueCount exceeds the valueCountForNextSizeCheck. However, valueCountForNextSizeCheck can end up getting skewed substantially if the memSize of the first 100 values of the column is really small:\nFor example, I see this in one of our jobs:\n\n[foo_column] valueCount: 101, memSize: 16, pageSizeThreshold: 1048576\n\nvalueCountForNextSizeCheck = (int)(valueCount + ((float)valueCount * props.getPageSizeThreshold() / memSize)) / 2 + 1;\n\n[foo_column] valueCountForNextSizeCheck = 3309619\n\n\nThis really large new valueCountForNextSizeCheck, results in our job OOMing as we end up seeing more space consuming values much much earlier than the ~3M valueCount point. \nAt this point, I'm thinking of doing something simple which is similar to InternalParquetRecordWriter.checkBlockSizeReached(), basically cap the maximum value of the valueCountForNextSizeCheck:\n\nvalueCountForNextSizeCheck =\n          Math.min(\n            (int)(valueCount + ((float)valueCount * pageSizeThreshold / memSize)) / 2 + 1,\n            valueCount + MAX_COUNT_FOR_SIZE_CHECK // will not look more than max records ahead\n          );\n\n\nOpen to something more sophisticated if people prefer so.",
        "Issue Links": []
    },
    "PARQUET-625": {
        "Key": "PARQUET-625",
        "Summary": "Improve RLE read performance",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "29/May/16 17:38",
        "Updated": "03/Jun/16 18:23",
        "Resolved": "03/Jun/16 18:23",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-626": {
        "Key": "PARQUET-626",
        "Summary": "Fix builds due to unavailable llvm.org apt mirror",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Jun/16 21:35",
        "Updated": "02/Jun/16 20:37",
        "Resolved": "02/Jun/16 20:37",
        "Description": "llvm.org disabled their apt mirror (http://llvm.org/apt/), so our Linux builds are now failing. See https://github.com/travis-ci/apt-source-whitelist/issues/279",
        "Issue Links": []
    },
    "PARQUET-627": {
        "Key": "PARQUET-627",
        "Summary": "Ensure that thrift headers are generated before source compilation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/Jun/16 10:18",
        "Updated": "03/Jun/16 21:18",
        "Resolved": "03/Jun/16 21:18",
        "Description": "parquet_types.h was not generated before it was used if built in release mode with Make.",
        "Issue Links": []
    },
    "PARQUET-628": {
        "Key": "PARQUET-628",
        "Summary": "Link thrift privately",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Jun/16 16:06",
        "Updated": "07/Jun/16 05:55",
        "Resolved": "07/Jun/16 05:55",
        "Description": "Thrift is an implementation detail that should not leak to the library interface.",
        "Issue Links": []
    },
    "PARQUET-629": {
        "Key": "PARQUET-629",
        "Summary": "RowGroupSerializer should only close itself once",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Jun/16 14:19",
        "Updated": "08/Jun/16 23:15",
        "Resolved": "08/Jun/16 23:15",
        "Description": "Otherwise exceptions are thrown multiple times.",
        "Issue Links": []
    },
    "PARQUET-630": {
        "Key": "PARQUET-630",
        "Summary": "C++: Support link flags for older CMake versions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "12/Jun/16 10:05",
        "Updated": "13/Jun/16 19:31",
        "Resolved": "13/Jun/16 19:31",
        "Description": "In older CMake versions, we need to use LINK_PRIVATE and LINK_PUBLIC.",
        "Issue Links": []
    },
    "PARQUET-631": {
        "Key": "PARQUET-631",
        "Summary": "ScroogeWriteSupport can not handle structs with union members",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jeffrey Olchovy",
        "Created": "12/Jun/16 22:39",
        "Updated": "13/Jun/16 02:41",
        "Resolved": "13/Jun/16 02:41",
        "Description": "When attempting to write ThriftStruct instances that contain union members, exceptions are encountered:\n1. When writing a struct that consists solely of a union member, org.apache.parquet.thrift.projection.ThriftProjectionException: No columns have been selected is encountered\n2. When writing a struct that consists of a union member plus other types of members, org.apache.parquet.io.InvalidRecordException: could not get child... is encountered\nGiven the following Thrift definitions:\n\nnamespace java io.narrative.dto\n\nunion SimpleUnion {\n  1: i64 i,\n  2: string s\n}\n\nstruct SimpleDto {\n  1: required i64 i\n}\n\nstruct NestedDto {\n  1: required SimpleDto simpleDto\n}\n\nstruct SimpleUnionDto {\n  1: required SimpleUnion simpleUnion\n}\n\nunion ComplexUnion {\n  1: SimpleDto maybeSimpleDto\n  2: NestedDto maybeNestedDto\n}\n\nstruct NestedWithSimpleUnionDto {\n  1: required SimpleDto simpleDto\n  2: required SimpleUnion simpleUnion\n}\n\nstruct NestedWithComplexUnionDto {\n  1: required SimpleDto simpleDto\n  2: required ComplexUnion complexUnion\n}\n\nstruct NestedWithContainerOfSimpleUnionDto {\n  1: required SimpleDto simpleDto\n  2: required list<SimpleUnion> simpleUnions\n}\n\nstruct NestedWithContainerOfComplexUnionDto {\n  1: required SimpleDto simpleDto\n  2: required list<ComplexUnion> complexUnions\n}\n\n\nAnd the following scalatest specifications:\n\nimport java.io.File\nimport com.twitter.scrooge.ThriftStruct\nimport org.apache.hadoop.fs.Path\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.parquet.hadoop.ParquetWriter\nimport org.apache.parquet.hadoop.metadata.CompressionCodecName\nimport org.apache.parquet.scrooge.ScroogeWriteSupport\nimport org.scalatest.FlatSpec\nimport io.narrative.dto._\n\nclass ScroogeParquetWriterSpec extends FlatSpec {\n\n  import ParquetWriter._\n\n  def write[A <: ThriftStruct : Manifest](a: A): Unit = { \n    val file = File.createTempFile(\"tmp\", \".snappy.parquet\")\n    file.delete()\n    val path = new Path(file.toURI)\n    val writeSupport = new ScroogeWriteSupport[A](manifest[A].runtimeClass.asInstanceOf[Class[A]])\n    val underlying = new ParquetWriter[A](path, writeSupport, CompressionCodecName.SNAPPY, DEFAULT_BLOCK_SIZE, DEFAULT_PAGE_SIZE)\n    try {\n      underlying.write(a)\n    } finally {\n      underlying.close()\n    }   \n  }\n\n  behavior of \"ParquetWriter\"\n\n  it should \"write Scrooge records with simple members\" in {\n    val simpleDto = SimpleDto(42)\n    write(simpleDto)\n  }\n\n  it should \"write Scrooge records with struct members\" in {\n    val simpleDto = SimpleDto(42)\n    val nestedDto = NestedDto(simpleDto)\n    write(nestedDto)\n  }\n\n  it should \"write Scrooge records with a union member that contains simple types\" in {\n    val simpleUnionDto = SimpleUnionDto(SimpleUnion.I(42))\n    write(simpleUnionDto)\n  }\n\n  it should \"write Scrooge records with struct and union members that contain simple types\" in {\n    val simpleDto = SimpleDto(42)\n    val nestedWithSimpleUnion = NestedWithSimpleUnionDto(simpleDto, SimpleUnion.I(42))\n    write(nestedWithSimpleUnion)\n  }\n\n  it should \"write Scrooge records with struct and union members that contain struct types\" in {\n    val simpleDto = SimpleDto(42)\n    val nestedWithComplexUnion = NestedWithComplexUnionDto(simpleDto, ComplexUnion.MaybeSimpleDto(simpleDto))\n    write(nestedWithComplexUnion)\n  }\n\n  it should \"write Scrooge records with a struct member and a list of union members that contain simple types\" in {\n    val simpleDto = SimpleDto(42)\n    val nestedWithSimpleUnions = NestedWithContainerOfSimpleUnionDto(simpleDto, Seq(SimpleUnion.I(42)))\n    write(nestedWithSimpleUnions)\n  }\n\n  it should \"write Scrooge records with a struct member and a list of union members that contain struct types\" in {\n    val simpleDto = SimpleDto(42)\n    val nestedWithComplexUnions = NestedWithContainerOfComplexUnionDto(simpleDto, Seq(ComplexUnion.MaybeSimpleDto(simpleDto)))\n    write(nestedWithComplexUnions)\n  }\n}\n\n\nThe following output is produced:\n\n[info] ScroogeParquetWriterSpec:\n[info] ParquetWriter\n[info] - should write Scrooge records with simple members\n[info] - should write Scrooge records with struct members\n[info] - should write Scrooge records with a union member that contains simple types *** FAILED ***\n[info]   org.apache.parquet.thrift.projection.ThriftProjectionException: No columns have been selected\n[info]   at org.apache.parquet.thrift.ThriftSchemaConvertVisitor.convert(ThriftSchemaConvertVisitor.java:91)\n[info]   at org.apache.parquet.thrift.ThriftSchemaConverter.convert(ThriftSchemaConverter.java:60)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.init(AbstractThriftWriteSupport.java:88)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.<init>(AbstractThriftWriteSupport.java:80)\n[info]   at org.apache.parquet.scrooge.ScroogeWriteSupport.<init>(ScroogeWriteSupport.java:48)\n[info]   at io.narrative.etl.ScroogeParquetWriterSpec.write(ThriftStructWriterSpec.scala:21)\n[info]   at io.narrative.etl.ScroogeParquetWriterSpec$$anonfun$3.apply$mcV$sp(ThriftStructWriterSpec.scala:47)\n[info]   at io.narrative.etl.ScroogeParquetWriterSpec$$anonfun$3.apply(ThriftStructWriterSpec.scala:45)\n[info]   at io.narrative.etl.ScroogeParquetWriterSpec$$anonfun$3.apply(ThriftStructWriterSpec.scala:45)\n[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n[info]   ...\n[info] - should write Scrooge records with struct and union members that contain simple types *** FAILED ***\n[info]   org.apache.parquet.io.InvalidRecordException: could not get child 1 from [GroupColumnIO simpleDto r:0 d:0 [simpleDto]]\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:113)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:217)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:183)\n[info]   ...\n[info]   Cause: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n[info]   at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n[info]   at java.util.ArrayList.get(ArrayList.java:429)\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:111)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   ...\n[info] - should write Scrooge records with struct and union members that contain struct types *** FAILED ***\n[info]   org.apache.parquet.io.InvalidRecordException: could not get child 1 from [GroupColumnIO simpleDto r:0 d:0 [simpleDto]]\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:113)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:217)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:183)\n[info]   ...\n[info]   Cause: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n[info]   at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n[info]   at java.util.ArrayList.get(ArrayList.java:429)\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:111)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   ...\n[info] - should write Scrooge records with a struct member and a list of union members that contain simple types *** FAILED ***\n[info]   org.apache.parquet.io.InvalidRecordException: could not get child 1 from [GroupColumnIO simpleDto r:0 d:0 [simpleDto]]\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:113)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:217)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:183)\n[info]   ...\n[info]   Cause: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n[info]   at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n[info]   at java.util.ArrayList.get(ArrayList.java:429)\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:111)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   ...\n[info] - should write Scrooge records with a struct member and a list of union members that contain struct types *** FAILED ***\n[info]   org.apache.parquet.io.InvalidRecordException: could not get child 1 from [GroupColumnIO simpleDto r:0 d:0 [simpleDto]]\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:113)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:217)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:183)\n[info]   ...\n[info]   Cause: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n[info]   at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n[info]   at java.util.ArrayList.get(ArrayList.java:429)\n[info]   at org.apache.parquet.io.GroupColumnIO.getChild(GroupColumnIO.java:111)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$StructWriteProtocol.<init>(ParquetWriteProtocol.java:322)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol$MessageWriteProtocol.<init>(ParquetWriteProtocol.java:397)\n[info]   at org.apache.parquet.thrift.ParquetWriteProtocol.<init>(ParquetWriteProtocol.java:431)\n[info]   at org.apache.parquet.hadoop.thrift.AbstractThriftWriteSupport.prepareForWrite(AbstractThriftWriteSupport.java:121)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:109)\n[info]   at org.apache.parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:99)\n[info]   at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:272)\n[info]   ...\n[info] Run completed in 1 second, 818 milliseconds.\n[info] Total number of tests run: 7\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 2, failed 5, canceled 0, ignored 0, pending 0\n[info] *** 5 TESTS FAILED ***\n\n\nEvery specification that contained a struct with a union member fails.",
        "Issue Links": []
    },
    "PARQUET-632": {
        "Key": "PARQUET-632",
        "Summary": "Parquet file in invalid state while writing to S3 from EMR",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Peter Halliday",
        "Created": "13/Jun/16 15:40",
        "Updated": "15/Nov/22 11:34",
        "Resolved": null,
        "Description": "I'm writing parquet to S3 from Spark 1.6.1 on EMR.  And when it got to the last few files to write to S3, I received this stacktrace in the log with no other errors before or after it.  It's very consistent.  This particular batch keeps erroring the same way.\n\n2016-06-10 01:46:05,282] WARN org.apache.spark.scheduler.TaskSetManager [task-result-getter-2hread] - Lost task 3737.0 in stage 2.0 (TID 10585, ip-172-16-96-32.ec2.internal): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:414)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:146)\n\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:138)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:195)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:153)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:112)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetRelation.scala:101)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:405)\n\t... 8 more",
        "Issue Links": []
    },
    "PARQUET-633": {
        "Key": "PARQUET-633",
        "Summary": "Add version to WriterProperties",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "16/Jun/16 17:39",
        "Updated": "17/Jun/16 06:28",
        "Resolved": "17/Jun/16 06:28",
        "Description": "PR: https://github.com/apache/parquet-cpp/pull/119",
        "Issue Links": []
    },
    "PARQUET-634": {
        "Key": "PARQUET-634",
        "Summary": "Consistent private linking of dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/Jun/16 06:52",
        "Updated": "17/Jun/16 07:50",
        "Resolved": "17/Jun/16 07:50",
        "Description": "Link all (static) third-party dependencies privately to not expose their API.",
        "Issue Links": []
    },
    "PARQUET-635": {
        "Key": "PARQUET-635",
        "Summary": "[C++] Statically link libstdc++ on Linux in conda recipe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "17/Jun/16 21:33",
        "Updated": "21/Jun/16 20:31",
        "Resolved": "21/Jun/16 20:31",
        "Description": "Since conda artifacts are intended to be portable, we cannot count on the target system to have the same version of gcc that we are building with. This is less of an issue on OS X. \nSee, for example:\nhttps://github.com/apache/arrow/blob/master/cpp/conda.recipe/build.sh#L43",
        "Issue Links": []
    },
    "PARQUET-636": {
        "Key": "PARQUET-636",
        "Summary": "Expose selection for different encodings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "20/Jun/16 16:42",
        "Updated": "22/Jun/16 07:37",
        "Resolved": "22/Jun/16 07:37",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-637": {
        "Key": "PARQUET-637",
        "Summary": "Streamed RLE encoding",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "20/Jun/16 17:01",
        "Updated": "25/Jun/16 11:15",
        "Resolved": "25/Jun/16 11:15",
        "Description": "Adapt the implementation of the RLE Encoder and Decoder to work on an OutputStream to also use them for the RLE encoding of the Data.",
        "Issue Links": []
    },
    "PARQUET-638": {
        "Key": "PARQUET-638",
        "Summary": "[C++] Revert static linking of libstdc++ in conda builds until symbol visibility addressed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Jun/16 00:40",
        "Updated": "06/Jul/16 01:09",
        "Resolved": "23/Jun/16 05:28",
        "Description": "See PARQUET-489. Statically linking libstdc++ before we've made symbols hidden by default can cause conflicts in other Linux environments with incompatible gcc toolchains",
        "Issue Links": [
            "/jira/browse/PARQUET-653"
        ]
    },
    "PARQUET-639": {
        "Key": "PARQUET-639",
        "Summary": "Do not export DCHECK in public headers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "23/Jun/16 07:48",
        "Updated": "25/Jun/16 01:48",
        "Resolved": "25/Jun/16 01:48",
        "Description": "These are private macros, they should not be exported in public headers.",
        "Issue Links": []
    },
    "PARQUET-640": {
        "Key": "PARQUET-640",
        "Summary": "[C++] Force the use of gcc 4.9 in conda builds",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Jun/16 19:57",
        "Updated": "24/Jun/16 01:13",
        "Resolved": "24/Jun/16 01:13",
        "Description": "Follow up to PARQUET-638. I did not realize that using the conda-forge toolchain (which saves us a lot of OS X boilerplate) was globbering our use of gcc 4.9. until we are statically linking the c++ standard library, we need to use gcc 4.9 across the board in our binary testing artifacts",
        "Issue Links": []
    },
    "PARQUET-641": {
        "Key": "PARQUET-641",
        "Summary": "Instantiate stringstream only if needed in SerializedPageReader::NextPage",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "24/Jun/16 14:52",
        "Updated": "25/Jun/16 00:15",
        "Resolved": "25/Jun/16 00:15",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-642": {
        "Key": "PARQUET-642",
        "Summary": "Improve performance of ByteBuffer based read / write paths",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Piyush Narang",
        "Reporter": "Piyush Narang",
        "Created": "24/Jun/16 21:11",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "30/Jun/16 16:52",
        "Description": "While trying out the newest Parquet version, we noticed that the changes to start using ByteBuffers: https://github.com/apache/parquet-mr/commit/6b605a4ea05b66e1a6bf843353abcb4834a4ced8 and https://github.com/apache/parquet-mr/commit/6b24a1d1b5e2792a7821ad172a45e38d2b04f9b8 (mostly avro but a couple of ByteBuffer changes) caused our jobs to slow down a bit. \nRead overhead: 4-6% (in MB_Millis)\nWrite overhead: 6-10% (MB_Millis). \nSeems like this seems to be due to the encoding / decoding of Strings in the Binary class (https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/io/api/Binary.java) - toStringUsingUTF8() - for reads\nencodeUTF8() - for writes\nIn those methods we're using the nio Charsets for encode / decode:\n\n    private static ByteBuffer encodeUTF8(CharSequence value) {\n      try {\n        return ENCODER.get().encode(CharBuffer.wrap(value));\n      } catch (CharacterCodingException e) {\n        throw new ParquetEncodingException(\"UTF-8 not supported.\", e);\n      }\n    }\n  }\n...\n    @Override\n    public String toStringUsingUTF8() {\n      int limit = value.limit();\n      value.limit(offset+length);\n      int position = value.position();\n      value.position(offset);\n      // no corresponding interface to read a subset of a buffer, would have to slice it\n      // which creates another ByteBuffer object or do what is done here to adjust the\n      // limit/offset and set them back after\n      String ret = UTF8.decode(value).toString();\n      value.limit(limit);\n      value.position(position);\n      return ret;\n    }\n\n\nTried out some micro / macro benchmarks and it seems like switching those out to using the String class for the encoding / decoding improves performance:\n\n@Override\n    public String toStringUsingUTF8() {\n      String ret;\n      if (value.hasArray()) {\n        try {\n          ret = new String(value.array(), value.arrayOffset() + offset, length, \"UTF-8\");\n        } catch (UnsupportedEncodingException e) {\n          throw new ParquetDecodingException(\"UTF-8 not supported\");\n        }\n      } else {\n        int limit = value.limit();\n        value.limit(offset+length);\n        int position = value.position();\n        value.position(offset);\n        // no corresponding interface to read a subset of a buffer, would have to slice it\n        // which creates another ByteBuffer object or do what is done here to adjust the\n        // limit/offset and set them back after\n        ret = UTF8.decode(value).toString();\n        value.limit(limit);\n        value.position(position);\n      }\n\n      return ret;\n    }\n...\nprivate static ByteBuffer encodeUTF8(String value) {\n      try {\n        return ByteBuffer.wrap(value.getBytes(\"UTF-8\"));\n      } catch (UnsupportedEncodingException e) {\n        throw new ParquetEncodingException(\"UTF-8 not supported.\", e);\n      }\n    }",
        "Issue Links": []
    },
    "PARQUET-643": {
        "Key": "PARQUET-643",
        "Summary": "Add const modifier to schema pointer reference in ParquetFileWriter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "26/Jun/16 09:20",
        "Updated": "27/Jun/16 16:53",
        "Resolved": "27/Jun/16 16:53",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-644": {
        "Key": "PARQUET-644",
        "Summary": "[C++] Add API for writing boolean values as a packed bitmap",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Shuai Lin",
        "Reporter": "Wes McKinney",
        "Created": "28/Jun/16 00:52",
        "Updated": "15/Aug/19 16:50",
        "Resolved": null,
        "Description": "Presently, the TypedColumnWriter for booleans expects a bool* (i.e. 1 byte per value). Some writers may also use a LSB-ordered bitmap, and we could write that directly rather than having to copy the bits into a temporary bool*",
        "Issue Links": []
    },
    "PARQUET-645": {
        "Key": "PARQUET-645",
        "Summary": "DictionaryFilter incorrectly handles null",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "28/Jun/16 03:18",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "30/Jun/16 16:48",
        "Description": "DictionaryFilter checks whether a column can match a query and filters out row groups that can't match. Equality checks don't currently handle null correctly, which is never in the dictionary and is encoded by the definition level. This is causing row groups to be filtered when they should not be because \"col is null\" is always true.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/348"
        ]
    },
    "PARQUET-646": {
        "Key": "PARQUET-646",
        "Summary": "[C++] Enable easier 3rd-party toolchain clang builds on Linux",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "28/Jun/16 18:43",
        "Updated": "28/Jun/16 22:12",
        "Resolved": "28/Jun/16 22:12",
        "Description": "There's a number of roadblocks to using a 3rd-party gcc toolchain with clang 3.7.x or 3.8.x. I took care of this in Arrow in https://github.com/apache/arrow/commit/a3e3849cde60f611ea47271f510a96c2f36606a7",
        "Issue Links": []
    },
    "PARQUET-647": {
        "Key": "PARQUET-647",
        "Summary": "Null Pointer Exception in Hive upon reading Parquet",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Mahadevan Sudarsanan",
        "Created": "28/Jun/16 23:53",
        "Updated": "08/Sep/16 16:36",
        "Resolved": "08/Sep/16 16:36",
        "Description": "When I write Parquet files from Spark Job, and try to read it in Hive as an External Table , I get Null Pointer Exception. After further analysis , I found I had some Null values in my transformation(used Dataset and DataFrame API's) before saving to parquet. These 2 fields which contains NULL are float data types. When I removed these two columns from the parquet datasets, I was able to read it in hive. Contrastingly , with all NULL columns I was able to read it Hive when I write my job to ORC format.\nWhen a datatype is anything other than String , which is completely empty(NULL) written in parquet is not been able to read by Hive and throws NP Exception.",
        "Issue Links": []
    },
    "PARQUET-648": {
        "Key": "PARQUET-648",
        "Summary": "[C++] Update developer documentation for C++11, clang tools, style guide links, etc.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Jun/16 22:30",
        "Updated": "11/Nov/18 22:13",
        "Resolved": "11/Nov/18 22:13",
        "Description": "The dev documentation has fallen out of date.",
        "Issue Links": []
    },
    "PARQUET-649": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-650": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-651": {
        "Key": "PARQUET-651",
        "Summary": "Parquet-avro fails to decode array of record with a single field name \"element\" correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7.0,                                            1.8.0,                                            1.8.1,                                            1.9.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "01/Jul/16 13:16",
        "Updated": "18/Oct/19 22:04",
        "Resolved": "17/Jul/16 22:00",
        "Description": "Found this issue while investigating SPARK-16344.\nFor the following Parquet schema\n\nmessage root {\n  optional group f (LIST) {\n    repeated group list {\n      optional group element {\n        optional int64 element;\n      }\n    }\n  }\n}\n\n\nparquet-avro decodes it as something like this:\n\nrecord SingleElement {\n  int element;\n}\n\nrecord NestedSingleElement {\n  SingleElement element;\n}\n\nrecord Spark16344Wrong {\n  array<NestedSingleElement> f;\n}\n\n\nwhile correct interpretation should be:\n\nrecord SingleElement {\n  int element;\n}\n\nrecord Spark16344 {\n  array<SingleElement> f;\n}\n\n\nThe reason is that the element syntactic group for LIST in\n\n<list-repetition> group <name> (LIST) {\n  repeated group list {\n    <element-repetition> <element-type> element;\n  }\n}\n\n\nis recognized as a record field named element. The problematic code lies in AvroRecordConverter.isElementType(). We should probably check the standard 3-level layout first before falling back to the legacy 2-level layout.",
        "Issue Links": [
            "/jira/browse/PARQUET-1681",
            "/jira/browse/SPARK-16344",
            "https://github.com/apache/parquet-mr/pull/352"
        ]
    },
    "PARQUET-652": {
        "Key": "PARQUET-652",
        "Summary": "parquet depends on hadoop-lzo, which requires Twitter's Maven repo",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Paul Wais",
        "Created": "04/Jul/16 21:58",
        "Updated": "04/Jul/16 21:59",
        "Resolved": null,
        "Description": "I'm trying to add Parquet to my project:\n\nlibraryDependencies += \"org.apache.parquet\" % \"parquet\" % \"1.8.1\"\nlibraryDependencies += \"org.apache.parquet\" % \"parquet-protobuf\" % \"1.8.1\"\n\n\nBut the project appears to depend on hadoop-lzo, which requires adding Twitter's Maven repo as a resolver:\n\nresolvers += \"twitter-repo\" at \"http://maven.twttr.com\"\n\n\n(now sbt resolves and fetches all dependencies)\n\n# uname -a\nLinux pw-246 3.13.0-85-generic #129-Ubuntu SMP Thu Mar 17 20:50:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n# lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.4 LTS\nRelease:        14.04\nCodename:       trusty\n# sbt sbtVersion\n[info] Loading project definition from xxxxx\n[info] Set current project to xxxx (in build file:xxxx)\n[info] 0.13.11\n\n\nI suggest that Apache host hadoop-lzo along with the main Parquet dependencies.",
        "Issue Links": []
    },
    "PARQUET-653": {
        "Key": "PARQUET-653",
        "Summary": "[C++] Re-enable -static-libstdc++ in dev artifact builds",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Jul/16 01:07",
        "Updated": "06/Jul/16 22:06",
        "Resolved": "06/Jul/16 22:06",
        "Description": "We still have the glibc dependency to deal with (I will look into doing builds with a centos6 docker image on CircleCI), but this will at least address libstdc++ ABI incompatibilities with the system libraries.",
        "Issue Links": [
            "/jira/browse/ARROW-220",
            "/jira/browse/PARQUET-638",
            "/jira/browse/PARQUET-489"
        ]
    },
    "PARQUET-654": {
        "Key": "PARQUET-654",
        "Summary": "Make record-level filtering optional",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Cheng Lian",
        "Created": "08/Jul/16 06:18",
        "Updated": "12/Dec/22 18:11",
        "Resolved": "13/Jul/16 21:52",
        "Description": "For some engines, especially those with vectorized Parquet readers, filter predicate can often be evaluated more efficiently by the engine. In these cases, Parquet record-level filtering may even slow down query execution when filter push-down is enabled. On the other hand, when the data is well prepared, filter push-down can be very valuable due to row group level filtering.\nOne possible improvement here is to add a configuration option that makes record-level filtering optional. In this way, the upper-level engine may leverage both Parquet row group level filtering and faster native record-level filtering.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/353"
        ]
    },
    "PARQUET-655": {
        "Key": "PARQUET-655",
        "Summary": "The LogicalTypes.md link in README.md points to the old Parquet GitHub repository",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "08/Jul/16 11:56",
        "Updated": "08/Sep/16 21:26",
        "Resolved": "08/Sep/16 21:26",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-656": {
        "Key": "PARQUET-656",
        "Summary": "[C++] Revert PARQUET-653",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Jul/16 05:44",
        "Updated": "09/Jul/16 15:59",
        "Resolved": "09/Jul/16 15:59",
        "Description": "Still haven't figured out the cross-Linux build story. Disabling this for now so we can work with an identical toolchain across Parquet and Arrow while the libraries are in development.",
        "Issue Links": []
    },
    "PARQUET-657": {
        "Key": "PARQUET-657",
        "Summary": "[C++] Don't define DISALLOW_COPY_AND_ASSIGN if already defined",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Jul/16 20:26",
        "Updated": "10/Jul/16 19:36",
        "Resolved": "10/Jul/16 19:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-658": {
        "Key": "PARQUET-658",
        "Summary": "ColumnReader has no virtual destructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "10/Jul/16 10:33",
        "Updated": "10/Jul/16 19:36",
        "Resolved": "10/Jul/16 19:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-659": {
        "Key": "PARQUET-659",
        "Summary": "[C++] Instantiated template visibility is broken on clang / OS X",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Jul/16 16:41",
        "Updated": "10/Jul/16 19:37",
        "Resolved": "10/Jul/16 19:37",
        "Description": "This is the source of Arrow build failures currently. Patch forthcoming",
        "Issue Links": []
    },
    "PARQUET-660": {
        "Key": "PARQUET-660",
        "Summary": "Writing Protobuf messages with extensions results in an error or data corruption.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Jakub Kukul",
        "Reporter": "Jakub Kukul",
        "Created": "10/Jul/16 20:20",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "08/Sep/16 21:50",
        "Description": "Currently, in ProtoParquetWriter there's not any support for extended protobuf messages. An attempt to write a protobuf message with extension either:\n\nResults in an uninformative error like this:\n\nparquet.proto.ProtoWriteSupport: Cannot write message \nYOUR_EXTENSION_TYPE cannot be cast to A_BASE_FIELD_TYPE : BASE_FIELD_NAME \n\n\nResults in a data corruption, if an extension type is compatible with a base field type with a corresponding index.\n\nImo there are two possible solutions:\n1. Ignore extension fields.\n2. When an extension field is detected in a message, throw an informative error.",
        "Issue Links": []
    },
    "PARQUET-661": {
        "Key": "PARQUET-661",
        "Summary": "[C++] Do not assume that perl is found in /usr/bin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Jul/16 03:17",
        "Updated": "11/Jul/16 22:46",
        "Resolved": "11/Jul/16 22:46",
        "Description": "Some leaned down Linux environments may not have Perl in /usr/bin (required by ctest / run-test.sh)",
        "Issue Links": []
    },
    "PARQUET-662": {
        "Key": "PARQUET-662",
        "Summary": "[C++] ParquetException must be explicitly exported in dynamic libraries",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Jul/16 20:23",
        "Updated": "12/Jul/16 03:23",
        "Resolved": "12/Jul/16 03:23",
        "Description": "This arose in ARROW-237. Because parquet::ParquetException is header-only, depending on the compiler, 3rd-party libraries may not be able to throw and/or catch exceptions that are recognized as such by all parties (for example, if 3rd-party implements parquet::MemoryAllocator and needs to throw an exception, it may not be recognized). On gcc this is current working but on LLVM / OS X it is not.",
        "Issue Links": []
    },
    "PARQUET-663": {
        "Key": "PARQUET-663",
        "Summary": "Link are Broken in README.md",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "nihed mbarek",
        "Reporter": "nihed mbarek",
        "Created": "13/Jul/16 08:15",
        "Updated": "15/Jul/16 16:52",
        "Resolved": "15/Jul/16 16:52",
        "Description": "Record conversion API and Hadoop API links are broken in README.md",
        "Issue Links": []
    },
    "PARQUET-664": {
        "Key": "PARQUET-664",
        "Summary": "parquet_writer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Gudbergur Erlendsson",
        "Created": "18/Jul/16 18:05",
        "Updated": "20/Apr/17 21:14",
        "Resolved": "20/Apr/17 21:14",
        "Description": "There's an example parquet_reader application in the parquet-cpp examples/ but an example showing parquet writing functionality is missing",
        "Issue Links": []
    },
    "PARQUET-665": {
        "Key": "PARQUET-665",
        "Summary": "Parquet-mr: Protobuf 3 support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Wael Nasreddine",
        "Reporter": "Wael Nasreddine",
        "Created": "19/Jul/16 19:22",
        "Updated": "21/Apr/17 23:12",
        "Resolved": "21/Apr/17 23:08",
        "Description": "Does parquet-mr support Protobuf version 3? I've applied the following patch and the tests are failing mostly due to optional vs required.\n\ndiff --git a/parquet-protobuf/pom.xml b/parquet-protobuf/pom.xml\nindex b3e4e50..aa67423 100644\n--- a/parquet-protobuf/pom.xml\n+++ b/parquet-protobuf/pom.xml\n@@ -31,7 +31,7 @@\n \n   <properties>\n     <elephant-bird.version>4.4</elephant-bird.version>\n-    <protobuf.version>2.5.0</protobuf.version>\n+    <protobuf.version>3.0.0-beta-4</protobuf.version>\n   </properties>\n \n \ndiff --git a/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoInputOutputFormatTest.java b/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoInputOutputFormatTest.java\nindex 5c6ebca..7e2557f 100644\n--- a/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoInputOutputFormatTest.java\n+++ b/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoInputOutputFormatTest.java\n@@ -88,7 +88,7 @@ public class ProtoInputOutputFormatTest {\n \n \n     //test that only requested fields were deserialized\n-    assertTrue(readDocument.hasDocId());\n+    assertTrue(readDocument.getDocId() == 12345);\n     assertTrue(\"Found data outside projection.\", readDocument.getNameCount() == 0);\n   }\n \ndiff --git a/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoRecordConverterTest.java b/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoRecordConverterTest.java\nindex 5318bd2..1cbb972 100644\n--- a/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoRecordConverterTest.java\n+++ b/parquet-protobuf/src/test/java/org/apache/parquet/proto/ProtoRecordConverterTest.java\n@@ -183,16 +183,16 @@ public class ProtoRecordConverterTest {\n     TestProtobuf.InnerMessage third = result.getInner(2);\n \n     assertEquals(\"First inner\", first.getOne());\n-    assertFalse(first.hasTwo());\n-    assertFalse(first.hasThree());\n+    assertEquals(first.getTwo(), \"\");\n+    assertEquals(first.getThree(), \"\");\n \n     assertEquals(\"Second inner\", second.getTwo());\n-    assertFalse(second.hasOne());\n-    assertFalse(second.hasThree());\n+    assertEquals(second.getOne(), \"\");\n+    assertEquals(second.getThree(), \"\");\n \n     assertEquals(\"Third inner\", third.getThree());\n-    assertFalse(third.hasOne());\n-    assertFalse(third.hasTwo());\n+    assertEquals(third.getOne(), \"\");\n+    assertEquals(third.getTwo(), \"\");\n   }\n \n \ndiff --git a/parquet-protobuf/src/test/resources/TestProtobuf.proto b/parquet-protobuf/src/test/resources/TestProtobuf.proto\nindex afa0f63..caf7926 100644\n--- a/parquet-protobuf/src/test/resources/TestProtobuf.proto\n+++ b/parquet-protobuf/src/test/resources/TestProtobuf.proto\n@@ -9,7 +9,7 @@\n //\n //   http://www.apache.org/licenses/LICENSE-2.0\n //\n-// Unless required by applicable law or agreed to in writing,\n+// Unless by applicable law or agreed to in writing,\n // software distributed under the License is distributed on an\n // \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n // KIND, either express or implied.  See the License for the\n@@ -17,6 +17,8 @@\n // under the License.\n //\n \n+syntax = \"proto3\";\n+\n package TestProtobuf;\n \n option java_package = \"org.apache.parquet.proto.test\";\n@@ -25,17 +27,18 @@ option java_package = \"org.apache.parquet.proto.test\";\n // messages but groups were deprecated.\n \n message Document {\n-    required int64 DocId = 1;\n-    optional Links links = 32;\n-    repeated group Name = 24 {\n+    int64 DocId = 1;\n+    Links links = 32;\n+    message Name  {\n         repeated Language name = 4;\n-        optional string url = 5;\n+        string url = 5;\n     }\n+    repeated Name name = 24;\n }\n \n message Language {\n-    required string code = 12;\n-    optional string Country = 14;\n+    string code = 12;\n+    string Country = 14;\n }\n \n message Links {\n@@ -47,42 +50,43 @@ message Links {\n // begin - protocol buffers for ProtoSchemaConverterTest\n \n  message SchemaConverterSimpleMessage {\n-     optional int32 someId = 3;\n+     int32 someId = 3;\n  }\n \n  message SchemaConverterAllDatatypes {\n-     optional double optionalDouble = 1;\n-     optional float optionalFloat = 2;\n-     optional int32 optionalInt32 = 3;\n-     optional int64 optionalInt64 = 4;\n-     optional uint32 optionalUInt32 = 5;\n-     optional uint64 optionalUInt64 = 6;\n-     optional sint32 optionalSInt32 = 7;\n-     optional sint64 optionalSInt64 = 8;\n-     optional fixed32 optionalFixed32 = 9;\n-     optional fixed64 optionalFixed64 = 10;\n-     optional sfixed32 optionalSFixed32 = 11;\n-     optional sfixed64 optionalSFixed64 = 12;\n-     optional bool optionalBool = 13;\n-     optional string optionalString = 14;\n-     optional bytes optionalBytes = 15;\n-     optional SchemaConverterSimpleMessage optionalMessage = 16;\n-     optional group PbGroup  = 17 {\n-       optional int32 groupInt = 2;\n+     double optionalDouble = 1;\n+     float optionalFloat = 2;\n+     int32 optionalInt32 = 3;\n+     int64 optionalInt64 = 4;\n+     uint32 optionalUInt32 = 5;\n+     uint64 optionalUInt64 = 6;\n+     sint32 optionalSInt32 = 7;\n+     sint64 optionalSInt64 = 8;\n+     fixed32 optionalFixed32 = 9;\n+     fixed64 optionalFixed64 = 10;\n+     sfixed32 optionalSFixed32 = 11;\n+     sfixed64 optionalSFixed64 = 12;\n+     bool optionalBool = 13;\n+     string optionalString = 14;\n+     bytes optionalBytes = 15;\n+     SchemaConverterSimpleMessage optionalMessage = 16;\n+     message PbGroup {\n+       int32 groupInt = 2;\n      }\n+     PbGroup pbGroup = 17;\n     enum TestEnum {\n         FIRST = 0;\n         SECOND = 1;\n     }\n-    optional TestEnum optionalEnum = 18;\n+    TestEnum optionalEnum = 18;\n  }\n \n  message SchemaConverterRepetition {\n-     optional int32 optionalPrimitive = 1;\n-     required int32 requiredPrimitive = 2;\n+     int32 optionalPrimitive = 1;\n+     int32 requiredPrimitive = 2;\n      repeated int32 repeatedPrimitive = 3;\n-     optional SchemaConverterSimpleMessage optionalMessage = 7;\n-     required SchemaConverterSimpleMessage requiredMessage = 8;\n+     SchemaConverterSimpleMessage optionalMessage = 7;\n+     SchemaConverterSimpleMessage requiredMessage = 8;\n      repeated SchemaConverterSimpleMessage repeatedMessage = 9;\n  }\n \n@@ -92,22 +96,22 @@ message Links {\n //begin protocol buffers for ProtoInputOutputFormatTest\n \n message InputOutputMsgFormat {\n-    optional int32 someId = 3;\n+    int32 someId = 3;\n }\n \n message IOFormatMessage {\n-    optional double optionalDouble = 1;\n+    double optionalDouble = 1;\n     repeated string repeatedString = 2;\n-    optional InputOutputMsgFormat msg = 3;\n+    InputOutputMsgFormat msg = 3;\n  }\n \n //end protocol buffers for ProtoInputOutputFormatTest\n \n \n message InnerMessage {\n-    optional string one = 1;\n-    optional string two = 2;\n-    optional string three = 3;\n+    string one = 1;\n+    string two = 2;\n+    string three = 3;\n }\n \n message TopMessage {\n@@ -115,7 +119,7 @@ message TopMessage {\n }\n \n message MessageA {\n-    optional InnerMessage inner = 123;\n+    InnerMessage inner = 123;\n }\n \n message RepeatedIntMessage {\n@@ -129,11 +133,11 @@ message HighIndexMessage {\n //custom proto class - ProtoInputOutputFormatTest\n \n message FirstCustomClassMessage {\n-    optional string string = 11;\n+    string string = 11;\n }\n \n message SecondCustomClassMessage {\n-    optional string string = 11;\n+    string string = 11;\n }\n \n //please place your unit test Protocol Buffer definitions here.",
        "Issue Links": []
    },
    "PARQUET-666": {
        "Key": "PARQUET-666",
        "Summary": "PLAIN_DICTIONARY write support",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "27/Jul/16 17:47",
        "Updated": "28/Aug/16 22:40",
        "Resolved": "28/Aug/16 22:40",
        "Description": "Add support for writing columns with PLAIN_DICTIONARY encoding.",
        "Issue Links": []
    },
    "PARQUET-667": {
        "Key": "PARQUET-667",
        "Summary": "Update committers lists to point to apache website",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "None",
        "Assignee": "Alex Levenson",
        "Reporter": "Alex Levenson",
        "Created": "27/Jul/16 18:54",
        "Updated": "27/Jul/16 19:28",
        "Resolved": "27/Jul/16 19:28",
        "Description": "https://github.com/apache/parquet-mr/pull/355#issuecomment-235637473",
        "Issue Links": []
    },
    "PARQUET-668": {
        "Key": "PARQUET-668",
        "Summary": "Provide option to disable auto crop feature in DumpCommand output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Daniel Harper",
        "Created": "31/Jul/16 17:02",
        "Updated": "03/Aug/16 21:15",
        "Resolved": "03/Aug/16 21:15",
        "Description": "Problem\nWhen using the dump command in parquet-tools, the output will sometimes be truncated based on the width of your console, especially on smaller displays.\nExample:\n\nrow group 0\n--------------------------------------------------------------------------------\nid:          INT32 SNAPPY DO:0 FPO:4 SZ:44668/920538/20.61 VC:7240100  [more]...\nname:        BINARY SNAPPY DO:0 FPO:44672 SZ:89464018/1031768430/11.53 [more]...\nevent_time:  INT64 SNAPPY DO:0 FPO:89508690 SZ:43600235/57923935/1.33 VC:7240100 [more]...\n\n    id TV=7240100 RL=0 DL=0 DS: 2 DE:PLAIN_DICTIONARY\n    ----------------------------------------------------------------------------\n    page 0:                      DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLA [more]... SZ:33291\n\n\nThis is especially annoying if you pipe the output to a file as the truncation remains in place. \nProposed fix\nProvide the flag --disable-crop for the dump command. Truncation is enabled by default and will only be disabled when this flag is provided,\nThis will output the full content to standard out, for example:\n\nrow group 0\n--------------------------------------------------------------------------------\nid:          INT32 SNAPPY DO:0 FPO:4 SZ:44668/920538/20.61 VC:7240100 ENC:BIT_PACKED,PLAIN_DICTIONARY\nname:        BINARY SNAPPY DO:0 FPO:44672 SZ:89464018/1031768430/11.53 VC:7240100 ENC:PLAIN,BIT_PACKED\nevent_time:  INT64 SNAPPY DO:0 FPO:89508690 SZ:43600235/57923935/1.33 VC:7240100 ENC:PLAIN,BIT_PACKED,RLE\n\n    id TV=7240100 RL=0 DL=0 DS: 2 DE:PLAIN_DICTIONARY\n    ----------------------------------------------------------------------------\n    page 0:                      DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN_DICTIONARY ST:[min: 0, max: 1, num_nulls: 0] SZ:33291 VC:262146\n    page 1:                      DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN_DICTIONARY ST:[min: 0, max: 1, num_nulls: 0] SZ:33291 VC:262145",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/358"
        ]
    },
    "PARQUET-669": {
        "Key": "PARQUET-669",
        "Summary": "Allow reading file footers from input streams when writing metadata files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Robert Kruszewski",
        "Reporter": "Robert Kruszewski",
        "Created": "31/Jul/16 22:08",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "03/Aug/16 21:26",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-670": {
        "Key": "PARQUET-670",
        "Summary": "Allow for different metadata output path than paths in file footers",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robert Kruszewski",
        "Created": "31/Jul/16 22:09",
        "Updated": "01/Mar/17 14:41",
        "Resolved": "01/Mar/17 14:41",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-671": {
        "Key": "PARQUET-671",
        "Summary": "Improve performance of RLE/bit-packed decoding in parquet-cpp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Eric Daniel",
        "Reporter": "Eric Daniel",
        "Created": "01/Aug/16 16:28",
        "Updated": "02/Aug/16 23:38",
        "Resolved": "02/Aug/16 23:38",
        "Description": "There are steps that can dramatically improve decoding performance:\n\nwhen decoding repeated values in the rle/dictionary encoding, do the dictionary lookup only once\nwhen decoding bit-packed sequences, do the decoding in batches so the bit unpacker's state can be kept in registers (instead of updating members for every decoded value)\nuse Daniel Lemire's fast unpacking routines whenever possible (https://github.com/lemire/FrameOfReference/)\n\nI have a PR ready to implement these changes.",
        "Issue Links": []
    },
    "PARQUET-672": {
        "Key": "PARQUET-672",
        "Summary": "[C++] Build testing conda artifacts in debug mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Aug/16 23:38",
        "Updated": "03/Aug/16 23:45",
        "Resolved": "03/Aug/16 23:45",
        "Description": "The intent of this is to help us fix the broken build being discussed in ARROW-247\nhttps://github.com/apache/arrow/pull/111",
        "Issue Links": []
    },
    "PARQUET-673": {
        "Key": "PARQUET-673",
        "Summary": "No compression for schema prefixed column paths",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "L\u00e9o Gouttefarde",
        "Created": "05/Aug/16 21:13",
        "Updated": "16/Sep/16 06:51",
        "Resolved": "16/Sep/16 06:51",
        "Description": "One issue I noticed in the file-serialize-test : building properties with the column path \"schema.int64\" for compression does not work (data is not getting compressed), I had to remove the \"schema.\" prefix for it to work (even though the GroupNode is created with the \"schema\" prefix).\nSo in parquet-cpp/src/parquet/file/file-serialize-test.cc :\nThis line : WriterProperties::Builder().compression(\"schema.int64\", codec_type)->build();\nNeeds to be : WriterProperties::Builder().compression(\"int64\", codec_type)->build();\nThe test itself is not failing because it does not check that compression is actually happening, it just runs on uncompressed data every time.",
        "Issue Links": []
    },
    "PARQUET-674": {
        "Key": "PARQUET-674",
        "Summary": "Add an abstraction to get the length of a stream",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Aug/16 01:29",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "03/Oct/16 22:05",
        "Description": "PARQUET-400 introduces SeekableInputStream to wrap Hadoop v1 and v2 streams and provide ByteBuffer access transparently. This can also be used as an abstraction to allow Parquet to work without the Hadoop API. The missing component is an abstraction that knows how long the file stream is for reading the footer. This could be done by adding a getLength method to the new stream interface, but I think there is value in adding a higher-level abstraction that carries information about the file and can open streams for it. This abstraction could be passed to a PageReadStore, which could have more complicated logic including parallel streams to read column chunks.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/368"
        ]
    },
    "PARQUET-675": {
        "Key": "PARQUET-675",
        "Summary": "Add INTERVAL_YEAR_MONTH and INTERVAL_DAY_TIME types",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "10/Aug/16 20:26",
        "Updated": "08/Jun/23 04:44",
        "Resolved": null,
        "Description": "For completeness and compatibility with Arrow and SQL types.\nThose are related to the existing INTERVAL type.\nsome references:\n\nhttps://msdn.microsoft.com/en-us/library/ms716506(v=vs.85).aspx\nhttp://www.techrepublic.com/article/sql-basics-datetime-and-interval-data-types/\nhttps://www.postgresql.org/docs/9.3/static/datatype-datetime.html\nhttps://docs.oracle.com/html/E26088_01/sql_elements001.htm\nhttp://www.ibm.com/support/knowledgecenter/SSGU8G_12.1.0/com.ibm.sqlr.doc/ids_sqr_123.htm",
        "Issue Links": [
            "/jira/browse/PARQUET-757",
            "https://github.com/apache/parquet-format/pull/43"
        ]
    },
    "PARQUET-676": {
        "Key": "PARQUET-676",
        "Summary": "MAX_VALUES_PER_LITERAL_RUN causes RLE encoding failure",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Mark Schaefer",
        "Created": "12/Aug/16 15:55",
        "Updated": "03/Sep/16 15:12",
        "Resolved": "03/Sep/16 15:12",
        "Description": "The following code works for NUM_TO_ENCODE <= 400, but fails greater than that with the error:\nCheck failed: (encoded) == (num_buffered_values_)\nIt appears to have to do with how large of an RLE buffer is allocated for buffering, causing Put to fail in levels.cc:78, but there doesn't seem to be recovery from that, or any error indicating what the problem is. I'm assuming MAX_VALUES_PER_LITERAL_RUN is somehow derived from the Parquet spec, but if so, it seems that there ought to be an exception or something generated. This could also be the basis of a writer example.\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n#include <iostream>\n#include <memory>\n#include <list>\n#include <parquet/api/writer.h>\nusing namespace parquet;\nint main(int argc, char** argv) {\n  if (argc != 2) \n{\n    std::cerr << \"Usage: \" << argv[0] << \" <file>\"\n              << std::endl;\n    return -1;\n  }\n\n  std::string filename = argv[1];\n  try {\n    const int NUM_TO_ENCODE = 400;\n    std::shared_ptr<OutputStream> ostream(new LocalFileOutputStream(filename));\n    parquet::schema::NodeVector fields;\n    parquet::schema::NodePtr schema;\n    fields.push_back(parquet::schema::Int32(\"id\", Repetition::REQUIRED));\n    fields.push_back(parquet::schema::ByteArray(\"name\", Repetition::OPTIONAL));\n    schema = parquet::schema::GroupNode::Make(\"schema\", Repetition::REPEATED, fields);\n    std::unique_ptr<ParquetFileWriter> writer = ParquetFileWriter::Open(ostream, std::dynamic_pointer_cast<parquet::schema::GroupNode>(schema));\n    RowGroupWriter* rgBlock = writer->AppendRowGroup(NUM_TO_ENCODE);\n    ColumnWriter* colBlock = rgBlock->NextColumn();\n    Int32Writer* intWriter = static_cast<Int32Writer*>(colBlock);\n    std::vector<int32_t> intbuf;\n    std::vector<int16_t> defbuf;\n    std::vector<ByteArray> strbuf;\n    for (int i = 0; i < NUM_TO_ENCODE; ++i) {\n        intbuf.push_back( i );\n        if (i % 10 == 0) \n{\n            defbuf.push_back(0);\n        }\n else \n{\n            defbuf.push_back(1);\n            uint8_t* buf = new uint8_t[4];\n            ByteArray ba;\n            sprintf((char*)buf,\"%d\",i);\n            ba.ptr = buf;\n            ba.len = strlen((const char*)ba.ptr);\n            strbuf.push_back(ba);\n        }\n    }\n    intWriter->WriteBatch(intbuf.size(), nullptr, nullptr, intbuf.data());\n    intWriter->Close();\n    colBlock = rgBlock->NextColumn();\n    ByteArrayWriter* strWriter = static_cast<ByteArrayWriter*>(colBlock);\n    std::cerr << \"sizes: strings:\" << strbuf.size() << \" definitions: \" << defbuf.size() << std::endl;\n    strWriter->WriteBatch(defbuf.size(), defbuf.data(), nullptr, strbuf.data());\n    strWriter->Close();\n  } catch (const std::exception& e) \n{\n    std::cerr << \"Parquet error: \"\n              << e.what()\n              << std::endl;\n    return -1;\n  }\n\n  return 0;\n}",
        "Issue Links": []
    },
    "PARQUET-677": {
        "Key": "PARQUET-677",
        "Summary": "Quoted identifiers in column names",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Michael Styles",
        "Created": "15/Aug/16 12:40",
        "Updated": "12/Sep/20 21:48",
        "Resolved": null,
        "Description": "Add the ability to quote identifiers for columns in a table. This would allow column names to contain arbitrary characters such as spaces. Hive supports these types of identifiers using backquotes. For example,\ncreate table parquet_table (`Session Token` string) stored as parquetfile;\nHowever, attempting to insert a new row into this table results in an error.\ninsert into parquet_table values ('1234-45')\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: field ended by ';': expected ';' but got 'token' at line 1:   optional string Session Token\nI would suggest using backquotes in Parquet as well.",
        "Issue Links": []
    },
    "PARQUET-678": {
        "Key": "PARQUET-678",
        "Summary": "Allow for custom compression codecs",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Steven Anton",
        "Created": "15/Aug/16 19:55",
        "Updated": "07/Oct/17 00:12",
        "Resolved": null,
        "Description": "I understand that the list of accepted compression codecs is explicity limited to uncompressed, snappy, gzip, and lzo. (See parquet.hadoop.metadata.CompressionCodecName.java) Is there a reason for this? Or is there an easy workaround? On the surface it seems like an unnecessary restriction.\nI ask because I have written a custom codec to implement encryption and I'm unable to use it with Parquet, which is a real shame because it is the main storage format I was hoping to use.\nOther thoughts on how to implement encryption in Parquet with this limitation?",
        "Issue Links": []
    },
    "PARQUET-679": {
        "Key": "PARQUET-679",
        "Summary": "[C++] Build and unit tests support for MSVC on Windows",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Mark Schaefer",
        "Created": "16/Aug/16 16:26",
        "Updated": "02/May/17 13:29",
        "Resolved": "02/May/17 13:29",
        "Description": "Hi, I installed all of the prerequisites for parquet-cpp under Windows and started working though the cmake process. For the most part, I added some search names to the FindXXX scripts, and that worked okay, but now it appears that I'm stuck. The CompilerInfo.cmake script is trying to differentiate between clang and gcc, but has no options for Windows. I know other Apache projects are cross platform, including Thrift, which I was able to build.\nI'm wondering if Windows support is on the short-term roadmap, whether I would be able to potentially contribute some patches, although I'm a  CMake newbie, or whether I should just try to create a solution in VS to build it manually.\nHere is the error:\nC:\\HDFS\\parquet-cpp-master>cmake .\nclang-tidy not found\nclang-format not found\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:946 ] _boost\n_TEST_VERSIONS = 1.61.0;1.61;1.60.0;1.60;1.59.0;1.59;1.58.0;1.58;1.57.0;1.57;1.56.0;1.56;1.55.0;1.55;1.54.0;1.54;1.53.0;1.53;1.52.0;1.52;1.51.0;1.51;1.50.0;1.50;1.49.0;1.49;1.48.0;1.48;1.47.0;1.47;1.46.1;1.46.0;1.46;1.45.0;1.45;1.44.0;1.44;1.43.0;1.43;1.42.0;1.42;1.41.0;1.41;1.40.0;1.40;1.39.0;1.39;1.38.0;1.38;1.37.0;1.37;1.36.1;1.36.0;1.36;1.35.1;1.35.0;1.35;1.34.1;1.34.0;1.34;1.33.1;1.33.0;1.33\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:948 ] Boost_USE_MULTITHREADED = ON\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:950 ] Boost_USE_STATIC_LIBS =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:952 ] Boost_USE_STATIC_RUNTIME =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:954 ] Boost_ADDITIONAL_VERSIONS =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:956 ] Boost_NO_SYSTEM_PATHS =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1024 ] Declared as CMake or Environmental Variables:\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1026 ]   BOOST_ROOT =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1028 ]   BOOST_INCLUDEDIR =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1030 ]   BOOST_LIBRARYDIR =\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1032 ] _boost_TEST_VERSIONS = 1.61.0;1.61;1.60.0;1.60;1.59.0;1.59;1.58.0;1.58;1.57.0;1.57;1.56.0;1.56;1.55.0;1.55;1.54.0;1.54;1.53.0;1.53;1.52.0;1.52;1.51.0;1.51;1.50.0;1.50;1.49.0;1.49;1.48.0;1.48;1.47.0;1.47;1.46.1;1.46.0;1.46;1.45.0;1.45;1.44.0;1.44;1.43.0;1.43;1.42.0;1.42;1.41.0;1.41;1.40.0;1.40;1.39.0;1.39;1.38.0;1.38;1.37.0;1.37;1.36.1;1.36.0;1.36;1.35.1;1.35.0;1.35;1.34.1;1.34.0;1.34;1.33.1;1.33.0;1.33\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1125 ] location of version.hpp: C:/HDFS/boost_1_61_0/boost/version.hpp\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1149 ] version.hpp reveals boost 1.61.0\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1235 ] guessed _boost_COMPILER = -vc140\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1245 ] _boost_MULTITHREADED = -mt\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1288 ] _boost_RELEASE_ABI_TAG = -\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1290 ] _boost_DEBUG_ABI_TAG = -gd\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1344 ] _boost_LIBRARY_SEARCH_DIRS_RELEASE = C:/HDFS/boost_1_61_0/lib;C:/HDFS/boost_1_61_0/../lib;C:/HDFS/boost_1_61_0/stage/lib;PATHS;C:/boost/lib;C:/boost;/sw/local/lib_boost_LIBRARY_SEARCH_DIRS_DEBUG   = C:/HDFS/boost_1_61_0/lib;C:/HDFS/boost_1_61_0/../lib;C:/HDFS/boost_1_61_0/stage/lib;PATHS;C:/boost/lib;C:/boost;/sw/local/lib\n\u2013 [ C:/Program Files/CMake/share/cmake-3.6/Modules/FindBoost.cmake:1595 ] Boost_FOUND = 1\n\u2013 Boost version: 1.61.0\n\u2013 Boost include dir: C:/HDFS/boost_1_61_0\n\u2013 Boost libraries:\n\u2013 THRIFT_HOME: C:/HDFS/ApacheThrift\n\u2013 Thrift version: Thrift version 0.9.3\n\u2013 Thrift include dir: C:/HDFS/ApacheThrift/include\n\u2013 Thrift contrib dir: THRIFT_CONTRIB_DIR-NOTFOUND\n\u2013 Thrift library path: C:/HDFS/ApacheThrift/lib/thriftmdd.lib\n\u2013 Thrift static library: THRIFT_STATIC_LIB_PATH-NOTFOUND/libthrift.a\n\u2013 Thrift compiler: C:/HDFS/ApacheThrift/bin/thrift.exe\n\u2013 Found the Snappy library: C:/HDFS/Snappy/native/snappy64.lib\n\u2013 Found the ZLIB library: C:/HDFS/zlib/lib/zlibstaticd.lib\n\u2013 Found the GTest library: C:/HDFS/gtest/lib/gtest.lib\n\u2013 Build Type: DEBUG\nINFO Microsoft (R) C/C++ Optimizing Compiler Version 19.00.23918 for x86\nCopyright (C) Microsoft Corporation.  All rights reserved.\ncl : Command line warning D9002 : ignoring unknown option '-v'\ncl : Command line error D8003 : missing source filename\nCMake Error at cmake_modules/CompilerInfo.cmake:46 (message):\n  Unknown compiler.  Version info:\n  Microsoft (R) C/C++ Optimizing Compiler Version 19.00.23918 for x86\n  Copyright (C) Microsoft Corporation.  All rights reserved.\n  cl : Command line warning D9002 : ignoring unknown option '-v'\n  cl : Command line error D8003 : missing source filename\nCall Stack (most recent call first):\n  CMakeLists.txt:370 (include)\n\u2013 Configuring incomplete, errors occurred!\nSee also \"C:/HDFS/parquet-cpp-master/CMakeFiles/CMakeOutput.log\".",
        "Issue Links": []
    },
    "PARQUET-680": {
        "Key": "PARQUET-680",
        "Summary": "[C++] Remove TODO file, update READMEs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "16/Aug/16 21:25",
        "Updated": "11/Nov/18 22:14",
        "Resolved": "11/Nov/18 22:13",
        "Description": "These files are outdated",
        "Issue Links": []
    },
    "PARQUET-681": {
        "Key": "PARQUET-681",
        "Summary": "Add tool to scan a parquet file",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "18/Aug/16 23:04",
        "Updated": "05/Sep/16 19:50",
        "Resolved": "05/Sep/16 19:50",
        "Description": "A tool to just scan the parquet file (not print any contents) will be useful to evaluate performance and also check the validity of files.",
        "Issue Links": []
    },
    "PARQUET-682": {
        "Key": "PARQUET-682",
        "Summary": "Configure the encoding used by ValueWriters",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Piyush Narang",
        "Created": "19/Aug/16 02:13",
        "Updated": "06/Oct/16 18:28",
        "Resolved": null,
        "Description": "This was supposed to be tackled by jira: https://issues.apache.org/jira/browse/PARQUET-601 but that ended up being just the work done to refactor the ValuesWriter factory code out of ParquetProperties. As that is now merged, it would be nice to revisit the original purpose - being able to configure which type of ValuesWriters to be used for writing out columns. \nBackground: Parquet is currently structured to choose the appropriate value writer based on the type of the column as well as the Parquet version. Value writers are responsible for writing out values with the appropriate encoding. As an example, for Boolean data types, we use BooleanPlainValuesWriter (v1.0) or RunLengthBitPackingHybridValuesWriter (v2.0). Code to do this is in the DefaultV1ValuesWriterFactory and the DefaultV2ValuesWriterFactory. \nWould be nice to support being able to override the encodings in some way. \nThat allows users to experiment with various encoding strategies manually as well as enables them to override the hardcoded defaults if they don't suit their use case.\nCouple of options I can think of:\nSpecifying encoding by type (or column):\n\nparquet.writer.encoding-override.<type> = \"encoding1[,encoding2]\"\nAs an example:\n\"parquet.writer.encoding-override.int32\" = \"plain\"\n\n\nChooses Plain encoding and hence the PlainValuesWriter.\nWhen a primary + fallback need to be specified, we can do the following:\n\n\"parquet.writer.encoding-override.binary\" = \"rle_dictionary,delta_byte_array\"\n\n\nChooses RLE_DICTIONARY encoding as the initial encoding and DELTA_BYTE_ARRAY encoding as the fallback and hence creates a FallbackWriter(PlainBinaryDictionaryValuesWriter, DeltaByteArrayWriter). \nIn such cases we can mandate that the first encoding listed must allow for Fallbacks by implementing RequiresFallback. \nAnother option suggested by alexlevenson, was to allow overriding of the ValuesWriterFactory using reflection:\n\nparquet.writer.factory-override = \"org.apache.parquet.hadoop.MyValuesWriterFactory\"\n\n\nThis creates a factory, MyValuesWriterFactory which is then invoked for every ColumnDescriptor to get a ValueWriter. This provides the flexibility to the user to implement a ValuesWriterFactory that can read configuration for per type / column encoding overrides. Can also be used to plug-in a more sophisticated approach where we choose the appropriate encoding based on the data being seen. A concern raised by rdblue regarding this approach was that ValuesWriters are supposed to be internal classes in Parquet. So we shouldn't be allowing users to configure the ValuesWriter factories via config.\ncc julienledem / rdblue / alexlevenson for you thoughts / other ideas. We could also explore other ideas based on any other potential use cases.",
        "Issue Links": []
    },
    "PARQUET-683": {
        "Key": "PARQUET-683",
        "Summary": "[C++] Add user documentation for API and utilities",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "22/Aug/16 15:46",
        "Updated": "13/Nov/18 17:31",
        "Resolved": null,
        "Description": "Asciidoc format will be used for the documentation.",
        "Issue Links": []
    },
    "PARQUET-684": {
        "Key": "PARQUET-684",
        "Summary": "[C++] Hardware optimizations for dictionary / RLE encoding/decoding",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "23/Aug/16 03:58",
        "Updated": "16/Sep/16 19:39",
        "Resolved": null,
        "Description": "See discussion in \nhttps://github.com/apache/parquet-cpp/pull/140\nand experiments from Daniel Lemire in \nhttps://github.com/lemire/dictionary",
        "Issue Links": [
            "/jira/browse/PARQUET-688"
        ]
    },
    "PARQUET-685": {
        "Key": "PARQUET-685",
        "Summary": "Deprecated ParquetInputSplit constructor passes parameters in the wrong order.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Julien Le Dem",
        "Created": "23/Aug/16 16:47",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "05/Oct/16 20:22",
        "Description": "https://github.com/apache/parquet-mr/blob/255f10834a67cf13518316de0e2c8a345677ebbf/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputSplit.java#L92\n\n  @Deprecated\n  public ParquetInputSplit(\n      Path path,\n      long start,\n      long length,\n      String[] hosts,\n      List<BlockMetaData> blocks,\n      String requestedSchema,\n      String fileSchema,\n      Map<String, String> extraMetadata,\n      Map<String, String> readSupportMetadata) {\n    this(path, start, length, end(blocks, requestedSchema), hosts, offsets(blocks));\n  }\n\n\nthis() refers to the following:\nhttps://github.com/apache/parquet-mr/blob/255f10834a67cf13518316de0e2c8a345677ebbf/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputSplit.java#L163\n\n  public ParquetInputSplit(\n      Path file, long start, long end, long length, String[] hosts,\n      long[] rowGroupOffsets) {",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/372"
        ]
    },
    "PARQUET-686": {
        "Key": "PARQUET-686",
        "Summary": "Allow for Unsigned Statistics in Binary Type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Andrew Duffy",
        "Created": "23/Aug/16 17:02",
        "Updated": "07/Apr/21 11:36",
        "Resolved": "07/Jun/17 22:39",
        "Description": "BinaryStatistics currently only have a min/max, which are compared as signed byte[]. However, for real UTF8-friendly lexicographic comparison, e.g. for string columns, we would want to calculate the BinaryStatistics based off of a comparator that treats the bytes as unsigned.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-392",
            "/jira/browse/PARQUET-1322",
            "/jira/browse/PARQUET-839",
            "/jira/browse/SPARK-17213",
            "/jira/browse/PARQUET-2016",
            "https://github.com/apache/parquet-format/pull/42",
            "https://github.com/apache/parquet-format/pull/46",
            "https://github.com/apache/parquet-format/pull/55",
            "https://github.com/apache/parquet-mr/pull/362",
            "https://github.com/apache/parquet-mr/pull/367"
        ]
    },
    "PARQUET-687": {
        "Key": "PARQUET-687",
        "Summary": "C++: Switch to PLAIN encoding if dictionary grows too large",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Uwe Korn",
        "Created": "25/Aug/16 11:50",
        "Updated": "15/Sep/16 13:20",
        "Resolved": "15/Sep/16 13:20",
        "Description": "We should add a threshold for the size of the dictionary. Once we go over this threshold, the encoding should be switched back to PLAIN.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-688": {
        "Key": "PARQUET-688",
        "Summary": "SIMD-accelerated dictionary coding",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Daniel Lemire",
        "Created": "26/Aug/16 23:30",
        "Updated": "29/Aug/16 14:07",
        "Resolved": "29/Aug/16 14:07",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-684"
        ]
    },
    "PARQUET-689": {
        "Key": "PARQUET-689",
        "Summary": "C++: Compress DataPages eagerly",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Uwe Korn",
        "Created": "28/Aug/16 09:05",
        "Updated": "16/Sep/16 07:01",
        "Resolved": "16/Sep/16 07:01",
        "Description": "In the write path, we currently compress the data pages when we close the ColumnWriter, not when all elements of a page are written. To have a faster close method, we want to do it more eagerly.",
        "Issue Links": []
    },
    "PARQUET-690": {
        "Key": "PARQUET-690",
        "Summary": "[C++] Investigate / improve performance of Thrift utilities",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Aug/16 02:21",
        "Updated": "04/Jan/19 21:17",
        "Resolved": "04/Jan/19 21:17",
        "Description": "We're currently using Thrift serialization / deserialization functions that allocate and deallocate Thrift C++ objects and memory with each invocation. The Impala codebase (and probably others) have some code we can learn from to be a bit more efficient:\nhttps://github.com/apache/incubator-impala/blob/master/be/src/rpc/thrift-util.h#L42",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3268"
        ]
    },
    "PARQUET-691": {
        "Key": "PARQUET-691",
        "Summary": "[C++] Write ColumnChunk metadata after each column chunk in the file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Aug/16 02:58",
        "Updated": "24/Jan/17 20:26",
        "Resolved": "24/Jan/17 20:26",
        "Description": "While this feature is not frequently utilized (enabling row groups / column chunks to be spread across multiple files), we have not yet implemented it yet.\nhttps://github.com/apache/parquet-format#file-format",
        "Issue Links": []
    },
    "PARQUET-692": {
        "Key": "PARQUET-692",
        "Summary": "Remove file metadata instance from ParquetFileReader class",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "29/Aug/16 04:50",
        "Updated": "01/Sep/16 21:26",
        "Resolved": "01/Sep/16 21:26",
        "Description": "The ParquetFileReader class must re-route metadata queries via the FileMetaData class",
        "Issue Links": []
    },
    "PARQUET-693": {
        "Key": "PARQUET-693",
        "Summary": "C++: Determine a good default page size",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "29/Aug/16 09:15",
        "Updated": "29/Aug/16 09:15",
        "Resolved": null,
        "Description": "Currently we have 1MB as the default data page size in parquet-cpp as in parquet-mr. We should communicate with the other parquet implementations if this is a good value and run benchmarks.",
        "Issue Links": []
    },
    "PARQUET-694": {
        "Key": "PARQUET-694",
        "Summary": "C++: Revert default data page size back to 1M",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "29/Aug/16 09:17",
        "Updated": "01/Sep/16 01:58",
        "Resolved": "01/Sep/16 01:58",
        "Description": "Accidentally increased to 64M",
        "Issue Links": []
    },
    "PARQUET-695": {
        "Key": "PARQUET-695",
        "Summary": "C++: Better default encoding user experience",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "29/Aug/16 12:15",
        "Updated": "11/Sep/16 17:56",
        "Resolved": "11/Sep/16 17:56",
        "Description": "Currently the default encoding is PLAIN. Probably making dictionary encoding the default is the best choice and let the user select an alternative encoding if the dictionary grows too large.\nThe interface should be as follows:\n\nThe user selects on a global and per-column basis if we should attempt dictionary encoding a column. The selection if RLE_DICTIONARY or PLAIN_DICTIONARY is used in the metadata is hidden from the user.\nThe user specifies a fallback (!= dictionary) encoding that is used if either dictionary encoding for a column is not desired or if the dictionary grew exceeded its size limit.\n\nAs a recap the current implement selects the encoding solely on the encoding variable. There is no fallback support implemented if the dictionary grows too large. The only magic at the moment is that the user can supply either PLAIN_DICTIONARY or RLE_DICTIONARY and the enum that is used in the metadata is the one which is suitable for the chosen Parquet version and not the one supplied by the user.",
        "Issue Links": []
    },
    "PARQUET-696": {
        "Key": "PARQUET-696",
        "Summary": "Move travis download from google code (defunct) to github",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "29/Aug/16 18:13",
        "Updated": "29/Aug/16 18:37",
        "Resolved": "29/Aug/16 18:37",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-697": {
        "Key": "PARQUET-697",
        "Summary": "ProtoMessageConverter fails for unknown proto fields",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Kristoffer Sj\u00f6gren",
        "Created": "30/Aug/16 13:18",
        "Updated": "22/Sep/16 11:57",
        "Resolved": null,
        "Description": "Hi\nWe have Spark application that reads parquet files and turns them into a Protobuf RDD like the code below [1]. However, if the parquet schema contain fields that doesn't exist in protobuf class an IncompatibleSchemaModificationException [2] is thrown. \nFor compatibility reasons it would be nice to make it possible to ignore fields instead of throwing an exception. Maybe as an configuration? The fix for ignoring fields is quite easy, just instantiate an empty PrimitiveConverter instead.\nCheers,\n-Kristoffer\n[1]\nJobConf conf = new JobConf(ctx.hadoopConfiguration());\nFileInputFormat.setInputPaths(conf, rawPath);\nProtoReadSupport.setProtobufClass(conf, Msg.class.getName());\nNewHadoopRDD<Void, Msg.Builder> rdd =\n      new NewHadoopRDD(ctx.sc(), ProtoParquetInputFormat.class, void.class, Msg.class, conf);\nrdd.toJavaRDD().foreach(log -> {\n  System.out.println(log._2);\n});\n[2] https://github.com/apache/parquet-mr/blob/master/parquet-protobuf/src/main/java/org/apache/parquet/proto/ProtoMessageConverter.java#L84\n[3] converters[parquetFieldIndex - 1] = new PrimitiveConverter() {};",
        "Issue Links": []
    },
    "PARQUET-698": {
        "Key": "PARQUET-698",
        "Summary": "Max buffer size for RLE encoding too small resulting in CheckBufferFull failure",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Anand Mitra",
        "Created": "30/Aug/16 13:22",
        "Updated": "03/Sep/16 15:25",
        "Resolved": "03/Sep/16 15:25",
        "Description": "I have been serializing dataset of 500 documents with nested schema\nand repeated attributes. I had been batching 50 records to a rowgroup.\nThe check DCHECK_EQ(encoded, num_buffered_values_) in\nColumnWriter::RleEncodeLevels() failed.\nWe are running out of space in the allocated buffer. This seems\nunlikely since we compute the worst case max size required and\nallocate accordingly. \nLooking more closely at the how the max size is computed and comparing\nit with how we write the RLE I noticed the following inconsitency.\nThe worst case space required would be when there are no repeats and\neverything is a literal run with the overhead of the\n\"litteral_indicator\" which is 4 bytes(32-bits). The computation\nassumes that we can get MAX_VALUES_PER_LITERAL_RUN = (1 << 6) * 8 of\nliterals encoded for the overhead of one literal_indicator. \nHowever when I examine the actual code it actually emits literal\nvalues after every 8 literals giving it the overhead of 4 bytes for\nevery 8 literal values. This can be ascertained from the following\nfragments of code.\nRleEncoder::Put() {\n  .....\n  if (++num_buffered_values_ == 8) \n{\n    DCHECK_EQ(literal_count_ % 8, 0);\n    FlushBufferedValues(false);\n  }\n  .....\n}\nas a result of this in RleEncoder::FlushBufferedValues() we will\nencode only one group and not encode the maximum which is\ntheoretically possible with the scheme.\nSuggested fix is to change MAX_VALUES_PER_LITERAL_RUN to 8 to\naccurately calculate the buffer size for the current encoding scheme.",
        "Issue Links": []
    },
    "PARQUET-699": {
        "Key": "PARQUET-699",
        "Summary": "Update parquet.thrift from https://github.com/apache/parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Florian Scheibner",
        "Created": "01/Sep/16 15:14",
        "Updated": "26/May/17 16:55",
        "Resolved": "26/May/17 16:55",
        "Description": "Support logical types TIME_MICROS and TIMESTAMP_MICROS\nAlso the current code was incorrect. Parquet reserved the LogicalTypes 8 and 10, but those were completely omitted in types.h. So types with greater indices were mapped incorrectly.",
        "Issue Links": [
            "/jira/browse/PARQUET-906"
        ]
    },
    "PARQUET-700": {
        "Key": "PARQUET-700",
        "Summary": "C++: Disable dictionary encoding for boolean columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "01/Sep/16 15:21",
        "Updated": "02/Sep/16 15:35",
        "Resolved": "02/Sep/16 15:35",
        "Description": "By accident, we try to encode boolean columns by default with ?_DICTIONARY which is not supported.",
        "Issue Links": []
    },
    "PARQUET-701": {
        "Key": "PARQUET-701",
        "Summary": "C++: Dictionary is written multiple times if close is called multiple times.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "01/Sep/16 15:29",
        "Updated": "01/Sep/16 20:35",
        "Resolved": "01/Sep/16 20:34",
        "Description": "We should check if we already have closed the file in close and not write the dictionary multiple times.",
        "Issue Links": []
    },
    "PARQUET-702": {
        "Key": "PARQUET-702",
        "Summary": "Add a writer + reader example with detailed comments",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "01/Sep/16 21:29",
        "Updated": "15/Nov/16 20:14",
        "Resolved": "15/Nov/16 20:14",
        "Description": "The scope of this jira is to write a complete write + read example with details, which can be used as a reference",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-703": {
        "Key": "PARQUET-703",
        "Summary": "[C++] Validate num_values metadata for columns with nulls",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 19:52",
        "Updated": "06/Sep/16 16:29",
        "Resolved": "06/Sep/16 16:29",
        "Description": "See discussion in https://github.com/apache/parquet-cpp/pull/145",
        "Issue Links": []
    },
    "PARQUET-704": {
        "Key": "PARQUET-704",
        "Summary": "[C++] scan-all.h is not being installed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 20:16",
        "Updated": "05/Sep/16 20:27",
        "Resolved": "05/Sep/16 20:27",
        "Description": "parquet-cpp is broken for 3rd-party users (we should test this in our CI setup)",
        "Issue Links": []
    },
    "PARQUET-705": {
        "Key": "PARQUET-705",
        "Summary": "[C++] Consider renaming schema_descriptor() to schema() in FileMetaData",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 20:25",
        "Updated": "09/Sep/16 13:53",
        "Resolved": "09/Sep/16 13:53",
        "Description": "This used to be called descr() and is now metadata()->schema_descriptor(). This may be simpler for 3rd-party users if called schema()",
        "Issue Links": []
    },
    "PARQUET-706": {
        "Key": "PARQUET-706",
        "Summary": "[C++] Create test case that uses libparquet as a 3rd party library",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 20:26",
        "Updated": "07/Jul/17 20:39",
        "Resolved": "07/Jul/17 20:39",
        "Description": "This will help avoid issues such as PARQUET-704 from recurring",
        "Issue Links": []
    },
    "PARQUET-707": {
        "Key": "PARQUET-707",
        "Summary": "[C++] Clean up SchemaDescriptor public API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 20:33",
        "Updated": "09/Sep/16 13:53",
        "Resolved": "09/Sep/16 13:53",
        "Description": "This is a touch messy right now\n\nBoth schema() and group() functions\nProbably should have a name() method (forwarding schema()->name())",
        "Issue Links": []
    },
    "PARQUET-708": {
        "Key": "PARQUET-708",
        "Summary": "[C++] RleEncoder does not account for \"worst case scenario\" in MaxBufferSize for bit_width > 1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/16 20:50",
        "Updated": "06/Sep/16 20:24",
        "Resolved": "06/Sep/16 20:24",
        "Description": "See discussion in PARQUET-676 and https://github.com/apache/parquet-cpp/commit/fa6e47616e818f33d8d4774ce43ae6d62cdd59bd#commitcomment-18898704",
        "Issue Links": []
    },
    "PARQUET-709": {
        "Key": "PARQUET-709",
        "Summary": "[C++] Fix conda dev binary builds",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Sep/16 00:11",
        "Updated": "06/Sep/16 06:00",
        "Resolved": "06/Sep/16 06:00",
        "Description": "These have been failing for a while. See Travis CI logs",
        "Issue Links": []
    },
    "PARQUET-710": {
        "Key": "PARQUET-710",
        "Summary": "Remove unneeded private member variables from RowGroupReader ABI",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "06/Sep/16 00:49",
        "Updated": "06/Sep/16 20:39",
        "Resolved": "06/Sep/16 20:39",
        "Description": "The current PIMPL idiom exposes private metadata members in its header. With the metadata implementation, this can be removed. \nThis also helps simplify the design and remove virtual functions.",
        "Issue Links": []
    },
    "PARQUET-711": {
        "Key": "PARQUET-711",
        "Summary": "Use metadata builders in parquet writer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "06/Sep/16 01:13",
        "Updated": "09/Sep/16 03:06",
        "Resolved": "09/Sep/16 03:06",
        "Description": "The scope of this JIRA is to use File/RowGroup/ColumnChunk MetaData builders in the parquet writer class.",
        "Issue Links": []
    },
    "PARQUET-712": {
        "Key": "PARQUET-712",
        "Summary": "C++: Read into Arrow memory",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/Sep/16 18:25",
        "Updated": "05/Apr/21 16:31",
        "Resolved": "18/Sep/16 18:01",
        "Description": "We want to reverse the dependency between arrow-cpp and parquet-cpp. Thus parquet-cpp should depend on arrow-cpp and read/write to/from arrow memory.\nThere are two options the implementation could go ahead:\n\nOptionally depend on arrow-cpp and have some methods that read to arrow memory\nStrictly depend on arrow-cpp: This would avoid some code duplication between the two projects, i.e. we could use Arrow's allocator, I/O layer, utility classes instead of having our own ones in parquet-cpp which do exactly the same, just live in another namespace.\n\n(I'll create some more JIRAs so we can proceed a bit more step-by-step but this one should be the one for coordination and discussions)",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-713": {
        "Key": "PARQUET-713",
        "Summary": "parquet-cpp 1.0.0 release",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/Sep/16 18:29",
        "Updated": "15/Mar/17 07:36",
        "Resolved": "15/Mar/17 07:36",
        "Description": "General ticket to accumulate the open issues to get a first release.",
        "Issue Links": [
            "/jira/browse/PARQUET-873",
            "/jira/browse/PARQUET-885",
            "/jira/browse/PARQUET-886",
            "/jira/browse/PARQUET-900",
            "/jira/browse/PARQUET-702",
            "/jira/browse/PARQUET-742",
            "/jira/browse/PARQUET-759",
            "/jira/browse/PARQUET-764",
            "/jira/browse/PARQUET-766",
            "/jira/browse/PARQUET-775",
            "/jira/browse/PARQUET-865",
            "/jira/browse/PARQUET-891",
            "/jira/browse/PARQUET-895",
            "/jira/browse/PARQUET-579",
            "/jira/browse/PARQUET-712",
            "/jira/browse/PARQUET-834",
            "/jira/browse/PARQUET-866",
            "/jira/browse/PARQUET-867",
            "/jira/browse/PARQUET-687",
            "/jira/browse/PARQUET-773",
            "/jira/browse/PARQUET-784",
            "/jira/browse/PARQUET-878",
            "/jira/browse/PARQUET-901"
        ]
    },
    "PARQUET-714": {
        "Key": "PARQUET-714",
        "Summary": "Add arrow as a thirdparty dependency",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/Sep/16 18:37",
        "Updated": "16/Sep/16 07:27",
        "Resolved": "16/Sep/16 07:27",
        "Description": "Add Arrow as a thirdparty dependency with a fixed source hash (later version, once there is a release)",
        "Issue Links": []
    },
    "PARQUET-715": {
        "Key": "PARQUET-715",
        "Summary": "clean up abandoned PRs",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "08/Sep/16 21:58",
        "Updated": "08/Sep/16 21:58",
        "Resolved": null,
        "Description": "parquet-mr: #333\nparquet-format: #38, #33",
        "Issue Links": []
    },
    "PARQUET-716": {
        "Key": "PARQUET-716",
        "Summary": "Website is very out of date",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebb",
        "Created": "09/Sep/16 10:02",
        "Updated": "09/Sep/16 10:02",
        "Resolved": null,
        "Description": "The website does not appear to have been updated for a long while.\nFor example, the footer says \"Copyright 2014\"\nThe parquet.apache.org/adopters/ page does not have any entries since 2013. If the twitter account is not active, perhaps it would be better to drop the page? Might be better to show entries in date order to encourage new listings?\nThe Download page does not mention any of the recent releases. Both 1.70.0 and 1.8.0 were released as a TLP. The download page still mentions the old Maven coordinates.",
        "Issue Links": []
    },
    "PARQUET-717": {
        "Key": "PARQUET-717",
        "Summary": "Implement fallback encoding in Parquet C++",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "11/Sep/16 17:20",
        "Updated": "11/Sep/16 18:09",
        "Resolved": "11/Sep/16 18:09",
        "Description": "Currently, fallback encoding is not supported if the dictionary page grows too large. The scope of this Jira is to support fallback encoding.",
        "Issue Links": []
    },
    "PARQUET-718": {
        "Key": "PARQUET-718",
        "Summary": "Reading boolean pages written by parquet-cpp fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "13/Sep/16 14:52",
        "Updated": "15/Sep/16 13:23",
        "Resolved": "15/Sep/16 13:23",
        "Description": "Seems like we broke something recently. For a failing test, see the upcoming PR.",
        "Issue Links": []
    },
    "PARQUET-719": {
        "Key": "PARQUET-719",
        "Summary": "Fix WriterBatch API to handle NULL values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "15/Sep/16 14:41",
        "Updated": "15/Sep/16 17:08",
        "Resolved": "15/Sep/16 17:08",
        "Description": "https://github.com/apache/parquet-cpp/pull/157\nintroduced batching of values WriteMiniBatch of being written. The implementation missed handling cases where input data has NULL values.",
        "Issue Links": []
    },
    "PARQUET-720": {
        "Key": "PARQUET-720",
        "Summary": "Parquet-cpp fails to link when included in multiple TUs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Florian Scheibner",
        "Reporter": "Florian Scheibner",
        "Created": "16/Sep/16 02:55",
        "Updated": "18/Sep/16 17:01",
        "Resolved": "18/Sep/16 17:01",
        "Description": "When reader.h is included in multiple cpp files, it causes a linker error\nbecause ScanAllValues is defined in multiple TUs\nThe solution is to move the definition of ScanAllValues to a separate cc file.",
        "Issue Links": []
    },
    "PARQUET-721": {
        "Key": "PARQUET-721",
        "Summary": "Performance benchmarks for reading into Arrow structures",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Sep/16 17:39",
        "Updated": "26/Sep/16 02:35",
        "Resolved": "26/Sep/16 02:35",
        "Description": "Simple benchmarks that show per column and repetition type how fast we can read into Arrow memory.",
        "Issue Links": []
    },
    "PARQUET-722": {
        "Key": "PARQUET-722",
        "Summary": "Building with JDK 8 fails over a maven bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Niels Basjes",
        "Created": "21/Sep/16 08:41",
        "Updated": "20/Aug/19 22:59",
        "Resolved": "21/Sep/16 08:55",
        "Description": "When I build parquet on my system I get this error during the build:\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project parquet-generator: Error rendering velocity resource. NullPointerException -> [Help 1]\nAbout a year ago julienledem responded that this is caused due to a bug in Maven in combination with Java 8:\nAt this page http://stackoverflow.com/questions/31229445/build-failure-apache-parquet-mr-source-mvn-install-failure/33360512#33360512 \nNow this bug has been solved at the Maven end in maven-filtering 1.2\nhttps://issues.apache.org/jira/browse/MSHARED-319\nThe problem is that this fix has not yet been integrated into the latest available maven versions yet.\nI'll put up a pull request with a proposed fix for this.",
        "Issue Links": []
    },
    "PARQUET-723": {
        "Key": "PARQUET-723",
        "Summary": "parquet is not storing the type for the column.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Narasimha",
        "Created": "21/Sep/16 17:59",
        "Updated": "26/Oct/16 21:32",
        "Resolved": "26/Oct/16 21:32",
        "Description": "1. Create Text file format table \n\tCREATE EXTERNAL TABLE IF NOT EXISTS emp(\n\tid INT,\n\tfirst_name STRING,\n\tlast_name STRING,\n\tdateofBirth STRING,\n\tjoin_date INT\n\t)\n\tCOMMENT 'This is Employee Table Date Of Birth of type String'\n\tROW FORMAT DELIMITED\n\tFIELDS TERMINATED BY ','\n\tLINES TERMINATED BY '\\n'\n\tSTORED AS TEXTFILE\n\tLOCATION '/user/employee/beforePartition';\n2. Load the data into table\n\tload data inpath '/user/somupoc_timestamp/employeeData_partitioned.csv' into table emp;\n\tselect * from emp;\n3. Create Partitioned table with file format as Parquet (dateofBirth STRING))\n\tcreate external table emp_afterpartition(\n\tid int, first_name STRING, last_name STRING, dateofBirth STRING)\n\tCOMMENT 'Employee partitioned table with dateofBirth of type string'\n\tpartitioned by (join_date int)\n\tSTORED as parquet\n\tLOCATION '/user/employee/afterpartition';\n4.  Fetch the data from Partitioned column\n\tset hive.exec.dynamic.partition=true;  \n\tset hive.exec.dynamic.partition.mode=nonstrict; \n\tinsert overwrite table emp_afterpartition partition (join_date) select * from emp;\n\tselect * from emp_afterpartition;\n5. Create Partitioned table with file format as Parquet (dateofBirth TIMESTAMP))\n\tCREATE EXTERNAL TABLE IF NOT EXISTS employee_afterpartition_timestamp_parq(\n\tid INT,first_name STRING,last_name STRING,dateofBirth TIMESTAMP)\n\tCOMMENT 'employee partitioned table with dateofBirth of type TIMESTAMP'\n\tPARTITIONED BY (join_date INT)\n\tSTORED AS PARQUET\n\tLOCATION '/user/employee/afterpartition';\n\tselect * from employee_afterpartition_timestamp_parq;\n        \u2013 0 records returned\n\timpala ::\talter table employee_afterpartition_timestamp_parq RECOVER PARTITIONS;\n\tHive ::\t\tMSCK REPAIR TABLE employee_afterpartition_timestamp_parq;\n\t\u2013 MSCK works in Hive and  RECOVER PARTITIONS works in Impala \u2013 metastore check command with the repair table option:\n \tselect * from employee_afterpartition_timestamp_parq;\nActual Result :: Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.hive.serde2.io.TimestampWritable\nExpected Result :: Data should display\nNote: if file format is text file instead of Parquet then I am able to fetch the data.\nObservation : Two tables having different column type pointing to same location(HDFS ).\nsample Data\n=========\n1,Joyce,Garza,2016-07-17 14:42:18,201607\n2,Jerry,Ortiz,2016-08-17 21:36:54,201608\n3,Steven,Ryan,2016-09-10 01:32:40,201609\n4,Lisa,Black,2015-10-12 15:05:13,201610\n5,Jose,Turner,2015-011-10 06:38:40,201611\n6,Joyce,Garza,2016-08-02,201608\n7,Jerry,Ortiz,2016-01-01,201601\n8,Steven,Ryan,2016/08/20,201608\n9,Lisa,Black,2016/09/12,201609\n10,Jose,Turner,09/19/2016,201609\n11,Jose,Turner,20160915,201609",
        "Issue Links": []
    },
    "PARQUET-724": {
        "Key": "PARQUET-724",
        "Summary": "Test more advanced properties setting",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Sep/16 20:19",
        "Updated": "22/Sep/16 03:50",
        "Resolved": "22/Sep/16 03:50",
        "Description": "Test that handling of global and column specific is tested and behaving correctly.",
        "Issue Links": []
    },
    "PARQUET-725": {
        "Key": "PARQUET-725",
        "Summary": "Parquet AVRO tests fail when debug logging is enabled",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Niels Basjes",
        "Reporter": "Niels Basjes",
        "Created": "23/Sep/16 10:11",
        "Updated": "12/Dec/17 20:39",
        "Resolved": "12/Dec/17 20:39",
        "Description": "I found that on my machine some of the tests in the parquet-avro fail.\n\nTests run: 25, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 sec\nRunning org.apache.parquet.avro.TestAvroDataSupplier\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec\nRunning org.apache.parquet.avro.TestReadWrite\nTests run: 18, Failures: 4, Errors: 0, Skipped: 0, Time elapsed: 0.414 sec <<< FAILURE!\nRunning org.apache.parquet.avro.TestBackwardCompatibility\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 sec\nRunning org.apache.parquet.avro.TestReadWriteOldListBehavior\nTests run: 16, Failures: 4, Errors: 0, Skipped: 0, Time elapsed: 0.148 sec <<< FAILURE!\nRunning org.apache.parquet.avro.TestInputOutputFormat\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.29 sec\nRunning org.apache.parquet.avro.TestReflectLogicalTypes\nTests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.165 sec\nRunning org.apache.parquet.avro.TestCircularReferences\nTests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec\n\nResults :\n\nFailed tests:   testWriteReflectReadGeneric(org.apache.parquet.avro.TestReflectReadWrite): expected:<{\"myboolean\": true, \"mybyte\": 1, \"myshort\": 1, \"myint\": 1, \"mylong\": 2, \"myfloat\": 3.1, \"mydouble\": 4.1, \"mybytes\": {\"bytes\": \"\\u0001\\u0002\\u0003\\u0004\"}, \"mystring\": \"Hello\", \"myenum\": \"A\", \"mymap\": {\"a\": \"1\", \"b\": \"2\"}, \"myshortarray\": [1, 2], \"myintarray\": [1, 2], \"mystringarray\": [\"a\", \"b\"], \"mylist\": [\"a\", \"b\", \"c\"]}> but was:<{\"myboolean\": true, \"mybyte\": 1, \"myshort\": 1, \"myint\": 1, \"mylong\": 2, \"myfloat\": 3.1, \"mydouble\": 4.1, \"mybytes\": {\"bytes\": \"\"}, \"mystring\": \"Hello\", \"myenum\": \"A\", \"mymap\": {\"a\": \"1\", \"b\": \"2\"}, \"myshortarray\": [1, 2], \"myintarray\": [1, 2], \"mystringarray\": [\"a\", \"b\"], \"mylist\": [\"a\", \"b\", \"c\"]}>\n  testWriteDecimalBytes(org.apache.parquet.avro.TestGenericLogicalTypes): Should read BigDecimals as bytes expected:<[{\"dec\": {\"bytes\": \"\u00f2\\u0096\"}}, {\"dec\": {\"bytes\": \"\\u0000\u00b2\u00e0\u00f8\"}}]> but was:<[{\"dec\": {\"bytes\": \"\"}}, {\"dec\": {\"bytes\": \"\"}}]>\n  testAll[0](org.apache.parquet.avro.TestReadWrite): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAllUsingDefaultAvroSchema[0](org.apache.parquet.avro.TestReadWrite): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAll[1](org.apache.parquet.avro.TestReadWrite): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAllUsingDefaultAvroSchema[1](org.apache.parquet.avro.TestReadWrite): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAll[0](org.apache.parquet.avro.TestReadWriteOldListBehavior): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAllUsingDefaultAvroSchema[0](org.apache.parquet.avro.TestReadWriteOldListBehavior): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAll[1](org.apache.parquet.avro.TestReadWriteOldListBehavior): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n  testAllUsingDefaultAvroSchema[1](org.apache.parquet.avro.TestReadWriteOldListBehavior): expected:<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>\n\n\n\nI see two classes of problems:\n\nThe json with byte arrays appear different.\nSome tests compare the 'toString' of a ByteBuffer. Now for two ByteBuffers that both contain the SAME bytes these tests fail simply because the position field of the ByteBuffer is different. I think these should compare the contents of the ByteBuffer instead.\n\n<java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]> but was:<java.nio.HeapByteBuffer[pos=5 lim=5 cap=5]>",
        "Issue Links": [
            "/jira/browse/AVRO-1885",
            "/jira/browse/PARQUET-765"
        ]
    },
    "PARQUET-726": {
        "Key": "PARQUET-726",
        "Summary": "TestMemoryManager consistently fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "None",
        "Assignee": "Niels Basjes",
        "Reporter": "Niels Basjes",
        "Created": "23/Sep/16 11:24",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "05/Oct/16 20:21",
        "Description": "On my system the test testMemoryManagerUpperLimit consistently fails.\nIt turns out that the difference is 5.7% which is above the configured maximum of 5%.\nI'm unsure why this happens but I suspect the memory management changes in JDK 8 to have something to do with this.",
        "Issue Links": []
    },
    "PARQUET-727": {
        "Key": "PARQUET-727",
        "Summary": "Ensure correct version of thrift is used",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Niels Basjes",
        "Reporter": "Niels Basjes",
        "Created": "23/Sep/16 11:44",
        "Updated": "07/Oct/16 21:29",
        "Resolved": "07/Oct/16 21:29",
        "Description": "I found that if you have the wrong version of thrift in your path during the build the errors you get are very obscure and verbose.",
        "Issue Links": []
    },
    "PARQUET-728": {
        "Key": "PARQUET-728",
        "Summary": "[C++] Bring parquet::arrow up to date with API changes in arrow::io",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Sep/16 21:51",
        "Updated": "25/Sep/16 23:23",
        "Resolved": "25/Sep/16 23:23",
        "Description": "See patches in ARROW-280 and ARROW-267",
        "Issue Links": []
    },
    "PARQUET-729": {
        "Key": "PARQUET-729",
        "Summary": "[C++] Unable to write multi-column tables from parquet_arrow (regression from PARQUET-711)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Sep/16 23:18",
        "Updated": "25/Sep/16 23:47",
        "Resolved": "25/Sep/16 23:47",
        "Description": "Only single-column tables are being written in the test suite. Same goes for file-serialize-test. The row group total bytes size is being improperly accounted for. This was broken here:\nhttps://github.com/apache/parquet-cpp/commit/55604b297f444e95e132658b2ef384870ae1f701#diff-142b09769ea0f63d22c341fbb2aebe21R147\nAs part of ARROW-302 and PARQUET-728 I need to fix this",
        "Issue Links": []
    },
    "PARQUET-730": {
        "Key": "PARQUET-730",
        "Summary": "[C++] Remove redundant total_byte_size calculation",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "26/Sep/16 15:34",
        "Updated": "03/Jul/19 02:28",
        "Resolved": null,
        "Description": "The total_byte_size of a rowgroup is being redundantly computed. Use total_bytes_written_ passed by the writer instead.\nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/file/metadata.cc#L471",
        "Issue Links": []
    },
    "PARQUET-731": {
        "Key": "PARQUET-731",
        "Summary": "[CPP] Add API to return metadata size and Skip reading values",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "26/Sep/16 15:45",
        "Updated": "02/Oct/16 18:52",
        "Resolved": "02/Oct/16 18:52",
        "Description": "1) There is no API to return the size of the metadata.\n2) SQL allows to skip rows via the \"offset\" clause. Adding a method to skip reading values in parquet-cpp will be helpful.",
        "Issue Links": []
    },
    "PARQUET-732": {
        "Key": "PARQUET-732",
        "Summary": "Building a subset of dependencies does not work",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Florian Scheibner",
        "Reporter": "Florian Scheibner",
        "Created": "26/Sep/16 17:10",
        "Updated": "27/Sep/16 07:06",
        "Resolved": "27/Sep/16 07:06",
        "Description": "The command\n\nthirdparty/build_thirdparty.sh thrift zlib snappy\n\n\nfails with:\n\nUnknown module: thrift zlib snappy",
        "Issue Links": []
    },
    "PARQUET-733": {
        "Key": "PARQUET-733",
        "Summary": "[C++] Remove file IO implementations from public API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Oct/16 21:09",
        "Updated": "03/Jan/17 19:52",
        "Resolved": "03/Jan/17 19:52",
        "Description": "parquet-cpp doesn't seem like the best place to maintain cross-platform IO implementations (or hook into boost or some other library providing IO). I suggest we consider dropping these from the public API and leave it to users / library integrators (e.g. people using arrow::io)) to provide their own IO hooks. We can keep the existing code around to assist with unit testing.",
        "Issue Links": []
    },
    "PARQUET-734": {
        "Key": "PARQUET-734",
        "Summary": "[C++] Incorporate upstream Arrow IO API changes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Oct/16 01:39",
        "Updated": "27/Dec/16 17:23",
        "Resolved": "27/Dec/16 17:23",
        "Description": "This must follow ARROW-302",
        "Issue Links": []
    },
    "PARQUET-735": {
        "Key": "PARQUET-735",
        "Summary": "[CPP] Newer bit packing segfaults with batch_size > 128 in Release mode build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "04/Oct/16 04:20",
        "Updated": "06/Jan/17 03:23",
        "Resolved": "06/Jan/17 03:23",
        "Description": "To reproduce\n1) Modify DEFAULT_SCANNER_BATCH_SIZE = 256\n2) Build parquet-cpp in Release mode\n3) Run build/release/parquet_reader parquet_test_gzip.parquet (file attached)",
        "Issue Links": []
    },
    "PARQUET-736": {
        "Key": "PARQUET-736",
        "Summary": "XCode 8.0 breaks builds",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Christopher Aycock",
        "Created": "05/Oct/16 19:51",
        "Updated": "06/Oct/16 05:02",
        "Resolved": "06/Oct/16 05:02",
        "Description": "Exactly the same issues as\nhttps://issues.apache.org/jira/browse/ARROW-313\ncmake_modules/CompilerInfo.cmake has a hard-coded search for \"clang-7\", so it can't find the just-release Xcode 8 tools.\nI can submit the patch if so desired.",
        "Issue Links": []
    },
    "PARQUET-737": {
        "Key": "PARQUET-737",
        "Summary": "Use absolute namespace in macros",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Oct/16 21:18",
        "Updated": "06/Oct/16 11:57",
        "Resolved": "06/Oct/16 11:57",
        "Description": "Relative namespaces in macros will cause problems when included in different hierachies with the same names.",
        "Issue Links": []
    },
    "PARQUET-738": {
        "Key": "PARQUET-738",
        "Summary": "Update arrow version that also supports newer Xcode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Oct/16 21:20",
        "Updated": "06/Oct/16 11:56",
        "Resolved": "06/Oct/16 11:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-739": {
        "Key": "PARQUET-739",
        "Summary": "Rle-decoding uses static buffer that is shared accross threads",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Florian Scheibner",
        "Created": "06/Oct/16 00:00",
        "Updated": "11/Oct/16 02:48",
        "Resolved": "11/Oct/16 02:48",
        "Description": "Reading two parquet files in parallel lead to a memory corruption that caused a crash. The columns are rle dictionary encoded strings in an uncompressed page, created with parquet-mr. \nInitial debugging showed that the indices for the dictionary returned by the rle decoder are garbage. So that data page got corrupted in memory. Reading the files in one thread works.\nI have a ColumnReader for each column and read one element from reach column to get a complete row.\nThe indices are decoded into one global static buffer. So multiple threads all use the same buffer and overwrite each other's indices.",
        "Issue Links": []
    },
    "PARQUET-740": {
        "Key": "PARQUET-740",
        "Summary": "Introduce editorconfig",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Niels Basjes",
        "Reporter": "Niels Basjes",
        "Created": "06/Oct/16 11:57",
        "Updated": "10/Oct/16 20:13",
        "Resolved": "10/Oct/16 20:13",
        "Description": "Editor config is a very easy way of ensuring that developers adhere more closely to the same coding standards when it comes to using tabs/spaces , trailing spaces, end of lines etc.\nQuote from http://editorconfig.org/\n\nEditorConfig helps developers define and maintain consistent coding styles between different editors and IDEs. The EditorConfig project consists of a file format for defining coding styles and a collection of text editor plugins that enable editors to read the file format and adhere to defined styles. EditorConfig files are easily readable and they work nicely with version control systems.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/373"
        ]
    },
    "PARQUET-741": {
        "Key": "PARQUET-741",
        "Summary": "compression_buffer_ is reused although it shouldn't",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Oct/16 14:45",
        "Updated": "07/Oct/16 08:11",
        "Resolved": "07/Oct/16 08:11",
        "Description": "We currently reuse the compression_buffer_ before we have written the page contained in it. Thus with compression turned on, we will write the content of the last page of a row group several times.",
        "Issue Links": []
    },
    "PARQUET-742": {
        "Key": "PARQUET-742",
        "Summary": "Add missing license headers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Oct/16 18:37",
        "Updated": "12/Oct/16 02:27",
        "Resolved": "12/Oct/16 02:27",
        "Description": "Files with unapproved licenses:\n  apache-parquet-cpp-0.1/README.md\n  apache-parquet-cpp-0.1/TODO\n  apache-parquet-cpp-0.1/ci/before_script_travis.sh\n  apache-parquet-cpp-0.1/ci/travis_conda_build.sh\n  apache-parquet-cpp-0.1/ci/travis_script_cpp.sh\n  apache-parquet-cpp-0.1/ci/upload_coverage.sh\n  apache-parquet-cpp-0.1/cmake_modules/clean-all.cmake                                                                                                                                 \napache-parquet-cpp-0.1/conda.recipe/build.sh\n  apache-parquet-cpp-0.1/conda.recipe/meta.yaml\n  apache-parquet-cpp-0.1/dev/README.md\n  apache-parquet-cpp-0.1/setup_build_env.sh\n  apache-parquet-cpp-0.1/src/parquet/symbols.map\n  apache-parquet-cpp-0.1/src/parquet/thrift/util.h\n  apache-parquet-cpp-0.1/src/parquet/util/bpacking.h\n  apache-parquet-cpp-0.1/thirdparty/build_thirdparty.sh\n  apache-parquet-cpp-0.1/thirdparty/download_thirdparty.sh\n  apache-parquet-cpp-0.1/thirdparty/set_thirdparty_env.sh\n  apache-parquet-cpp-0.1/thirdparty/versions.sh",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-743": {
        "Key": "PARQUET-743",
        "Summary": "DictionaryFilters can re-use StreamBytesInput when compressed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Patrick Woody",
        "Created": "07/Oct/16 18:26",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "13/Oct/16 01:06",
        "Description": "When using an And or Or DictionaryFilter, we re-use the BytesInput across reads. This is problematic when compressed because compressed BytesInputs get converted over to StreamBytesInputs which can only be used once.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/376"
        ]
    },
    "PARQUET-744": {
        "Key": "PARQUET-744",
        "Summary": "Clarifications on build instructions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Christopher Aycock",
        "Reporter": "Christopher Aycock",
        "Created": "07/Oct/16 18:29",
        "Updated": "11/Oct/16 17:58",
        "Resolved": "11/Oct/16 17:58",
        "Description": "Under the direction of wesmckinn, I've taken a stab at building Parquet on both Mac and Linux. I have a few observations about the build instructions found in README.md.\n\nlz4 is listed as a dependency, but CMakeLists.txt doesn't check for it.\nzlib is already in macOS, so Homebrew core doesn't have this.\nbuild_thirdparty.sh won't build Thrift on Mac with script, but this isn't documented anywhere.\nThe release build option (-DCMAKE_BUILD_TYPE=Release) isn't listed anywhere.\n\nI can make changes if anyone wants.",
        "Issue Links": []
    },
    "PARQUET-745": {
        "Key": "PARQUET-745",
        "Summary": "TypedRowGroupStatistics fails to PlainDecode min and max in ByteArrayType",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Florian Scheibner",
        "Reporter": "Florian Scheibner",
        "Created": "07/Oct/16 19:14",
        "Updated": "01/Nov/16 02:27",
        "Resolved": "01/Nov/16 02:27",
        "Description": "The TypedRowGroupStatistics tries to PlainDecode the given min and max strings. This fails for BinaryArrayType because it only contains the actual data and not the prepended length.",
        "Issue Links": []
    },
    "PARQUET-746": {
        "Key": "PARQUET-746",
        "Summary": "[C++] Remove conda builds from Travis CI processes",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "07/Oct/16 20:50",
        "Updated": "18/Oct/16 12:10",
        "Resolved": "18/Oct/16 12:10",
        "Description": "I'm moving these builds to conda-forge, so we won't need them here any longer.",
        "Issue Links": []
    },
    "PARQUET-747": {
        "Key": "PARQUET-747",
        "Summary": "[C++] TypedRowGroupStatistics are not being exported in libparquet.so",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Oct/16 02:59",
        "Updated": "11/Oct/16 15:18",
        "Resolved": "11/Oct/16 15:18",
        "Description": "These have to be instantiated and exported like so:\nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/column/reader.h#L258\nsee also https://github.com/apache/parquet-cpp/commit/9930b546ecd52673903b442ca57d89dd4786a59a",
        "Issue Links": []
    },
    "PARQUET-748": {
        "Key": "PARQUET-748",
        "Summary": "Update assembly for parquet tools",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ivan Sadikov",
        "Created": "11/Oct/16 20:43",
        "Updated": "14/Oct/16 20:04",
        "Resolved": "14/Oct/16 20:01",
        "Description": "Small update for parquet-tools module to be able to create distribution including shell scripts and all dependencies. Also documentation update.",
        "Issue Links": []
    },
    "PARQUET-749": {
        "Key": "PARQUET-749",
        "Summary": "[C++] Schema building ParquetFileWriter",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/Oct/16 05:58",
        "Updated": "02/May/19 19:39",
        "Resolved": null,
        "Description": "Sometimes you want to write a Parquet file column-by-column without knowing the whole schema upfront. Format-wise this should be possible if you only have a single RowGroup. The user would write column after column and only at the end (when we need to write the metadata), the whole schema can be assembled.\nCurrently ParquetFileWriter only supports writing files when the schema is known upfront.\nLimitations for the start:\n\nA single RowGroup is written\nNo nestings",
        "Issue Links": []
    },
    "PARQUET-750": {
        "Key": "PARQUET-750",
        "Summary": "Parquet tools cat/head parity, small fixes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ivan Sadikov",
        "Created": "14/Oct/16 20:05",
        "Updated": "14/Oct/16 20:30",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-751": {
        "Key": "PARQUET-751",
        "Summary": "DictionaryFilter patch broke column projection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "14/Oct/16 23:08",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "25/Oct/16 19:09",
        "Description": "The DictionaryFilter patch included a change to reuse a ParquetFileReader rather than creating new ones (and opening new file streams). But by reusing a reader, the projection columns are no longer set. The result is that all columns are read from disk, but only the requested columns are materialized. The fix is to set the requested schema on the file reader when it is fetched from the read context inside InternalParquetRecordReader.",
        "Issue Links": [
            "/jira/browse/PARQUET-392",
            "https://github.com/apache/parquet-mr/pull/379"
        ]
    },
    "PARQUET-752": {
        "Key": "PARQUET-752",
        "Summary": "[C++] Conform parquet_arrow to upstream API changes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "17/Oct/16 14:42",
        "Updated": "18/Oct/16 12:09",
        "Resolved": "18/Oct/16 12:09",
        "Description": "See ARROW-261 https://github.com/apache/arrow/pull/176 \u2013 primarily, ArrowBuilder::Finish can technically fail in some cases, so it now returns Status. There's also some bit utility function renaming in ARROW-317, so this patch will incorporate both changes",
        "Issue Links": []
    },
    "PARQUET-753": {
        "Key": "PARQUET-753",
        "Summary": "GroupType.union() doesn't merge the original type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "17/Oct/16 22:16",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "26/Oct/16 16:50",
        "Description": "When merging two GroupType, the union() method doesn't merge their original type which will be lost after the union.",
        "Issue Links": [
            "/jira/browse/PARQUET-379"
        ]
    },
    "PARQUET-754": {
        "Key": "PARQUET-754",
        "Summary": "Deprecate the \"strict\" argument in MessageType.union()",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "18/Oct/16 00:33",
        "Updated": "18/Oct/16 00:33",
        "Resolved": null,
        "Description": "As discussed in PARQUET-379, non-strict schema merging doesn't really make any sense and we always set to true throughout the code base. Should probably deprecate it and make sure no internal code ever use non-strict schema merging.",
        "Issue Links": []
    },
    "PARQUET-755": {
        "Key": "PARQUET-755",
        "Summary": "create parquet-arrow module with schema converter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "25/Oct/16 20:29",
        "Updated": "09/Nov/16 17:00",
        "Resolved": "09/Nov/16 17:00",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-756": {
        "Key": "PARQUET-756",
        "Summary": "Add Union Logical type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "26/Oct/16 18:26",
        "Updated": "08/Jun/23 19:16",
        "Resolved": null,
        "Description": "Add a union type annotation for Group types that represent a Union rather than a struct.\nModels like Avro or Arrow would make use of it.",
        "Issue Links": [
            "/jira/browse/PARQUET-757",
            "https://github.com/apache/parquet-format/pull/44"
        ]
    },
    "PARQUET-757": {
        "Key": "PARQUET-757",
        "Summary": "Add NULL type to Bring Parquet logical types to par with Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "26/Oct/16 18:53",
        "Updated": "04/Nov/16 17:35",
        "Resolved": "04/Nov/16 17:35",
        "Description": "Missing:\n\nNull\nInterval types\nUnion\nhalf precision float",
        "Issue Links": [
            "/jira/browse/PARQUET-675",
            "/jira/browse/PARQUET-756",
            "/jira/browse/PARQUET-758",
            "https://github.com/apache/parquet-format/pull/45"
        ]
    },
    "PARQUET-758": {
        "Key": "PARQUET-758",
        "Summary": "[Format] HALF precision FLOAT Logical type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "28/Oct/16 15:53",
        "Updated": "15/Jun/23 15:05",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1647",
            "/jira/browse/ARROW-17464",
            "/jira/browse/PARQUET-757"
        ]
    },
    "PARQUET-759": {
        "Key": "PARQUET-759",
        "Summary": "Cannot store columns consisting of empty strings",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "28/Oct/16 16:50",
        "Updated": "01/Nov/16 01:18",
        "Resolved": "01/Nov/16 01:18",
        "Description": "Currently we cannot store string columns that only consist of empty strings. In that case StringArray::data() points to a nullptr thus the call to StringArrays::data()::data() fails with a Segmentation fault in FileWriter::Impl::WriteFlatColumnChunk.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-760": {
        "Key": "PARQUET-760",
        "Summary": "On switching from dictionary to the fallback encoding, an incorrect encoding is set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "30/Oct/16 09:55",
        "Updated": "01/Nov/16 01:25",
        "Resolved": "01/Nov/16 01:25",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-761": {
        "Key": "PARQUET-761",
        "Summary": "C++: Shrink using realloc Buffer instead of doing a memcpy in FlatColumnReader::Impl::TypedReadBatch",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/Nov/16 10:48",
        "Updated": "20/Mar/17 08:48",
        "Resolved": "20/Mar/17 08:48",
        "Description": "See https://issues.apache.org/jira/browse/ARROW-360 This should avoid the two memcpys at the end.",
        "Issue Links": [
            "/jira/browse/ARROW-360"
        ]
    },
    "PARQUET-762": {
        "Key": "PARQUET-762",
        "Summary": "C++: Use optimistic allocation instead of Arrow Builders",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-avro",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/Nov/16 10:51",
        "Updated": "06/Nov/16 19:35",
        "Resolved": "06/Nov/16 19:35",
        "Description": "Instead of using the Arrow builders, optimistically allocate buffers to avoid unncessary memcpys. This should bring us a significant performance improvement.",
        "Issue Links": []
    },
    "PARQUET-763": {
        "Key": "PARQUET-763",
        "Summary": "C++: Expose ParquetFileReader through Arrow reader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/Nov/16 15:12",
        "Updated": "04/Nov/16 07:20",
        "Resolved": "04/Nov/16 07:20",
        "Description": "Make it accessible from the outside so we can access the metadata.",
        "Issue Links": []
    },
    "PARQUET-764": {
        "Key": "PARQUET-764",
        "Summary": "[CPP] Parquet Writer does not write Boolean values correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Deepak Majeti",
        "Created": "04/Nov/16 14:18",
        "Updated": "06/Nov/16 19:18",
        "Resolved": "06/Nov/16 19:18",
        "Description": "The core of the problem is due to https://github.com/apache/parquet-cpp/blob/master/src/parquet/encodings/plain-encoding.h#L203\nThe bit packing happens for every Write(). However, the packing is done at the byte level. If the number of (1-bit) values are not a multiple of 8, it results in padding incorrect values (false for boolean).\nTo reproduce: src/parquet/column/column-writer-test.cc\n\nusing TestBooleanValuesWriter = TestPrimitiveWriter<BooleanType>;\nTEST_F(TestBooleanValuesWriter, AlternateBooleanValues) {\n  this->SetUpSchema(Repetition::REQUIRED);\n  auto writer = this->BuildWriter();\n  for (int i = 0; i < SMALL_SIZE; i++) {\n      bool value = (i % 2 == 0) ? true :  false;\n      writer->WriteBatch(1, nullptr, nullptr, &value);\n  }\n  writer->Close();\n  this->ReadColumn();\n  for (int i = 0; i < SMALL_SIZE; i++) {\n      ASSERT_EQ((i % 2 == 0) ? true :  false, this->values_out_[i]) << i;\n  }\n}",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-765": {
        "Key": "PARQUET-765",
        "Summary": "Upgrade Avro to 1.8.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-avro",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "05/Nov/16 15:33",
        "Updated": "29/Nov/16 08:22",
        "Resolved": "09/Nov/16 17:02",
        "Description": "Unit test failure occurs if using Avro 1.8.1:\n\n[ERROR] Failed to execute goal org.apache.avro:avro-maven-plugin:1.8.1:idl-protocol (schemas) ... \norg.apache.avro.compiler.idl.ParseException: Encountered \" \"date\" \"date \"\" at line 23, column 14.\n[ERROR] Was expecting one of:\n[ERROR] <IDENTIFIER> ...\n[ERROR] \"@\" ...\n[ERROR] \"`\" ...\n[ERROR] -> [Help 1]\n\n\nSee AVRO-1924 about the background.\nIt is not clear whether it is solvable on Avro side or it is more a limitation from Avro 1.8.1. Therefore, I would suggest fixing the issue in the unit test side.",
        "Issue Links": [
            "/jira/browse/AVRO-1924",
            "/jira/browse/PARQUET-725",
            "https://github.com/apache/parquet-mr/pull/382"
        ]
    },
    "PARQUET-766": {
        "Key": "PARQUET-766",
        "Summary": "C++: Expose ParquetFileReader through Arrow reader as const",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Nov/16 18:29",
        "Updated": "06/Nov/16 19:11",
        "Resolved": "06/Nov/16 19:11",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-767": {
        "Key": "PARQUET-767",
        "Summary": "Add release scripts for parquet-cpp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "06/Nov/16 18:39",
        "Updated": "10/Dec/16 08:29",
        "Resolved": "10/Dec/16 08:29",
        "Description": "Add scripts to release parquet-cpp",
        "Issue Links": []
    },
    "PARQUET-768": {
        "Key": "PARQUET-768",
        "Summary": "Add Uwe L. Korn to KEYS",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "07/Nov/16 07:41",
        "Updated": "07/Nov/16 16:39",
        "Resolved": "07/Nov/16 16:39",
        "Description": "To sign parquet-cpp releases that can be verified by others, I would like to have my gpg key appended to the KEYS file. As there is only a single file (https://dist.apache.org/repos/dist/dev/parquet/KEYS) for all parquet RCs/releases at the end, I'm going to do a PR to the parquet-mr repo first and will then later make a PR to have that KEYS file replicated in parquet-cpp.",
        "Issue Links": []
    },
    "PARQUET-769": {
        "Key": "PARQUET-769",
        "Summary": "C++: Add support for Brotli Compression",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "07/Nov/16 14:56",
        "Updated": "26/Nov/16 19:30",
        "Resolved": "26/Nov/16 19:30",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-770": {
        "Key": "PARQUET-770",
        "Summary": "[C++] Implement PARQUET-686 statistics bug fixes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "07/Nov/16 16:55",
        "Updated": "02/Jun/19 01:41",
        "Resolved": "02/Jun/19 01:41",
        "Description": "The statistics written by the parquet-mr / parquet-cpp could be incorrect for certain data types.\nparquet-mr JIRA: PARQUET-686\nIssue Discussion: https://github.com/apache/parquet-mr/pull/362\nparquet-mr patch: https://github.com/apache/parquet-mr/pull/367",
        "Issue Links": []
    },
    "PARQUET-771": {
        "Key": "PARQUET-771",
        "Summary": "C++: Sync KEYS file",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "07/Nov/16 17:25",
        "Updated": "08/Nov/16 15:38",
        "Resolved": "08/Nov/16 15:38",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-772": {
        "Key": "PARQUET-772",
        "Summary": "Test fails if current locale has decimal mark other than .",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "07/Nov/16 17:32",
        "Updated": "26/Jan/17 23:39",
        "Resolved": "26/Jan/17 23:39",
        "Description": "mvn test fails on my machine because of my locale settings:\nFailed tests:   testFloatMinMax(org.apache.parquet.column.statistics.TestStatistics): expected:<min: 0[,00010, max: 553,]59998, num_nulls: 0> but was:<min: 0[.00010, max: 553.]59998, num_nulls: 0>\n  testDoubleMinMax(org.apache.parquet.column.statistics.TestStatistics): expected:<min: 0[,00001, max: 944,]50000, num_nulls: 0> but was:<min: 0[.00001, max: 944.]50000, num_nulls: 0>\n$ printenv | grep LC\nLC_COLLATE=hu_HU.UTF-8\nLC_MEASUREMENT=hu_HU.UTF-8\nLC_CTYPE=hu_HU.UTF-8",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/395"
        ]
    },
    "PARQUET-773": {
        "Key": "PARQUET-773",
        "Summary": "C++: Check licenses with RAT in CI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "08/Nov/16 18:12",
        "Updated": "09/Nov/16 15:13",
        "Resolved": "09/Nov/16 15:13",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-774": {
        "Key": "PARQUET-774",
        "Summary": "Release parquet-cpp 0.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Julien Le Dem",
        "Created": "08/Nov/16 18:22",
        "Updated": "08/Nov/16 18:25",
        "Resolved": "08/Nov/16 18:25",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-775": {
        "Key": "PARQUET-775",
        "Summary": "C++: TrackingAllocator is not thread-safe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "09/Nov/16 15:19",
        "Updated": "10/Nov/16 00:07",
        "Resolved": "10/Nov/16 00:06",
        "Description": "Using parquet-cpp from different threads will use the same default_allocator() and thus we can have races on total_memory_",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-776": {
        "Key": "PARQUET-776",
        "Summary": "C++: Clear out license issues",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "09/Nov/16 20:25",
        "Updated": "21/Apr/17 07:33",
        "Resolved": "21/Apr/17 07:33",
        "Description": "Before the initial release, check that LICENSE and NOTICE contain the relevant information. \n\nAlso, license should be used to record third-party licensed works that are\nincluded in the source distribution. The bit packing code should be in\nthere, rather than in notice. Notice is for required third-party notices\nand isn't the file where third-party licensing information should be\naccumulated.",
        "Issue Links": []
    },
    "PARQUET-777": {
        "Key": "PARQUET-777",
        "Summary": "Add new Parquet CLI tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-cli",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "13/Nov/16 22:48",
        "Updated": "30/Mar/18 21:41",
        "Resolved": "30/Mar/18 21:40",
        "Description": "This issue tracks adding parquet-cli from rdblue/parquet-cli.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/384"
        ]
    },
    "PARQUET-778": {
        "Key": "PARQUET-778",
        "Summary": "Standardize the schema output to match the parquet-mr format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Mike Trinkala",
        "Created": "15/Nov/16 20:58",
        "Updated": "18/Nov/16 17:46",
        "Resolved": "18/Nov/16 17:46",
        "Description": "root node name is preceded by 'message'\nbyte_array type is named 'binary'\ncolumn entries end with a semicolon\nadd logical type output",
        "Issue Links": []
    },
    "PARQUET-779": {
        "Key": "PARQUET-779",
        "Summary": "Export TypedRowGroupStatistics in libparquet",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Artem Tarasov",
        "Reporter": "Artem Tarasov",
        "Created": "17/Nov/16 19:38",
        "Updated": "26/Nov/16 19:32",
        "Resolved": "26/Nov/16 19:32",
        "Description": "This was already addressed by PARQUET-747, but it seems that export attribute is necessary in order to properly expose methods, although GCC produces a warning.",
        "Issue Links": []
    },
    "PARQUET-780": {
        "Key": "PARQUET-780",
        "Summary": "WriterBatch API does not properly handle NULL values for byte array types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Mike Trinkala",
        "Created": "21/Nov/16 22:49",
        "Updated": "26/Nov/16 20:29",
        "Resolved": "26/Nov/16 20:29",
        "Description": "Passing a NULL 'values' parameter into WriteBatch for a *ByteArray type will cause a segfault in the dictionary encoder.\nRelated: https://issues.apache.org/jira/browse/PARQUET-719",
        "Issue Links": []
    },
    "PARQUET-781": {
        "Key": "PARQUET-781",
        "Summary": "ParquetOuptputFormat should support custom OutputCommitter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Mikko Kupsu",
        "Created": "23/Nov/16 13:35",
        "Updated": "23/Nov/16 13:35",
        "Resolved": null,
        "Description": "ParquetOutputFormat should support custom OutputCommitter.\nThere is a need to bypass current Hadoop functionality of writing output data under _temporary folder. Especially with AWS S3, there can be huge overhead of moving the files from _temporary folder to output folder.",
        "Issue Links": []
    },
    "PARQUET-782": {
        "Key": "PARQUET-782",
        "Summary": "C++: Support writing to Arrow sinks",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "24/Nov/16 16:03",
        "Updated": "27/Nov/16 22:24",
        "Resolved": "27/Nov/16 22:24",
        "Description": "Implement a version of parquet::arrow::WriteTable that accepts arrow::io::OutputStream",
        "Issue Links": []
    },
    "PARQUET-783": {
        "Key": "PARQUET-783",
        "Summary": "H2SeekableInputStream does not close its underlying FSDataInputStream, leading to connection leaks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Michael MacFadden",
        "Reporter": "Michael MacFadden",
        "Created": "28/Nov/16 20:37",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "05/Dec/16 23:29",
        "Description": "ParquetFileReader opens a SeekableInputStream to read a footer. In the process, it opens a new FSDataInputStream and wraps it. However, H2SeekableInputStream does not override the close method. Therefore, when ParquetFileReader closes it, the underlying FSDataInputStream is not closed. As a result, these stale connections can exhaust a clusters' data nodes' connection resources and lead to mysterious HDFS read failures in HDFS clients, e.g.\n\norg.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-905337612-172.16.70.103-1444328960665:blk_1720536852_646811517",
        "Issue Links": [
            "/jira/browse/PARQUET-1027",
            "https://github.com/apache/parquet-mr/pull/388"
        ]
    },
    "PARQUET-784": {
        "Key": "PARQUET-784",
        "Summary": "C++: Reference Spark, Kudu and FrameOfReference in LICENSE",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "30/Nov/16 22:09",
        "Updated": "04/Dec/16 15:53",
        "Resolved": "04/Dec/16 15:53",
        "Description": "Mention these three projects as we ship code derived from them.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-785": {
        "Key": "PARQUET-785",
        "Summary": "C++: List conversion for Arrow Schemas",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "01/Dec/16 17:38",
        "Updated": "12/Dec/16 22:22",
        "Resolved": "12/Dec/16 22:22",
        "Description": "Add schema conversions for LIST converted type to/from Arrow.",
        "Issue Links": []
    },
    "PARQUET-786": {
        "Key": "PARQUET-786",
        "Summary": "parquet-tools README incorrectly has 'java jar' instead of 'java -jar'",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Mark Nelson",
        "Reporter": "Mark Nelson",
        "Created": "01/Dec/16 17:48",
        "Updated": "10/Oct/19 07:42",
        "Resolved": "05/Dec/16 23:52",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-553"
        ]
    },
    "PARQUET-787": {
        "Key": "PARQUET-787",
        "Summary": "Add a size limit for heap allocations when reading",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "01/Dec/16 20:49",
        "Updated": "21/Feb/18 17:41",
        "Resolved": "21/Feb/18 17:41",
        "Description": "G1GC allocates humongous objects directly in the old generation to avoid unnecessary copies, which means that these allocations aren't garbage collected until a full GC runs. Humongous objects are objects that are 50% of the region size or more. Region size is at most 32MB (see the table for region size from heap size).\nParquet currently allocates a huge buffer for each contiguous group of column chunks, which in many cases is not garbage collected until a full GC. Adding a size limit for the allocation size should allow users to break row groups across multiple buffers so that buffers get collected when they have been read.",
        "Issue Links": [
            "/jira/browse/PARQUET-1189",
            "https://github.com/apache/parquet-mr/pull/390"
        ]
    },
    "PARQUET-788": {
        "Key": "PARQUET-788",
        "Summary": "[C++] Reference Impala / Apache Impala (incubating) in LICENSE",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Dec/16 15:56",
        "Updated": "05/Dec/16 14:49",
        "Resolved": "05/Dec/16 14:49",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-789": {
        "Key": "PARQUET-789",
        "Summary": "[C++] Catch and translate ParquetException in parquet::arrow::FileReader::{ReadFlatColumn, ReadFlatTable}}",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Dec/16 21:49",
        "Updated": "06/Dec/16 16:43",
        "Resolved": "06/Dec/16 16:43",
        "Description": "Exceptions are mostly uncaught in the implementations of these methods",
        "Issue Links": []
    },
    "PARQUET-790": {
        "Key": "PARQUET-790",
        "Summary": "Close Parquet github account to avoid confusion",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "05/Dec/16 22:46",
        "Updated": "05/Dec/16 23:14",
        "Resolved": null,
        "Description": "The old github repo has significant history (github issues and PRs) that we'd like to maintain.\nBut at the same time it is confusing and people mistake it for the main repo.\nWe need a solution for this.",
        "Issue Links": []
    },
    "PARQUET-791": {
        "Key": "PARQUET-791",
        "Summary": "Predicate pushing down on missing columns should work on UserDefinedPredicate too",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "L. C. Hsieh",
        "Reporter": "L. C. Hsieh",
        "Created": "06/Dec/16 04:20",
        "Updated": "21/Apr/18 12:39",
        "Resolved": "08/Dec/16 17:09",
        "Description": "This is related to PARQUET-389. PARQUET-389 fixes the predicate pushing down on missing columns. But it doesn't fix it for UserDefinedPredicate.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/389"
        ]
    },
    "PARQUET-792": {
        "Key": "PARQUET-792",
        "Summary": "Skip the storage of repetition level and definition level for all-null column",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Li",
        "Created": "06/Dec/16 10:44",
        "Updated": "09/Dec/16 01:52",
        "Resolved": null,
        "Description": "I have a very sparse protobuf message in my project, with thousands of fields.\nIn practise, most of the fields are all null values in one page.\nBut the repetition level and definition level takes lots of storage space.\nCan parquet skip the storage of r level and d level for such all-null columns to save storage space?",
        "Issue Links": []
    },
    "PARQUET-793": {
        "Key": "PARQUET-793",
        "Summary": "[CPP] Do not return incorrect statistics",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "09/Dec/16 17:34",
        "Updated": "12/Feb/17 00:34",
        "Resolved": "12/Feb/17 00:34",
        "Description": "Implement PARQUET-251 and PARQUET-686 patches and prevent returning statistics for known bugs.",
        "Issue Links": []
    },
    "PARQUET-794": {
        "Key": "PARQUET-794",
        "Summary": "Predicate push down fails in case of schema is evolved (alias)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Sunil",
        "Created": "09/Dec/16 21:44",
        "Updated": "09/Dec/16 22:34",
        "Resolved": "09/Dec/16 22:34",
        "Description": "I am trying to read 2 parquet files  in which one file has evolved schema by renaming a column say B from A. Error is thrown when I read them through latest schema and predicate on column B (say B > 10) \nCaused by: java.lang.IllegalArgumentException: Column B does not exist.\n\tat parquet.filter.ColumnRecordFilter$1.bind(ColumnRecordFilter.java:54)",
        "Issue Links": []
    },
    "PARQUET-795": {
        "Key": "PARQUET-795",
        "Summary": "Implement BytesWritable in DataWritableWriter Class in parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Suresh Bahuguna",
        "Created": "12/Dec/16 05:23",
        "Updated": "12/Dec/16 05:29",
        "Resolved": null,
        "Description": "Implement BytesWritable in org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter Class in parquet . \nwritePremitive() api of this class has major Writable but not BytesWritable (Note - it has ByteWritable which is just 1 byte) . So we have to use the binaryWritable class . However binaryWritable does not have the set function which means for each record we have to call new (no of times string data type occurs ).",
        "Issue Links": []
    },
    "PARQUET-796": {
        "Key": "PARQUET-796",
        "Summary": "Delta Encoding is not used when dictionary enabled",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jakub Liska",
        "Created": "12/Dec/16 17:19",
        "Updated": "03/Jan/23 14:15",
        "Resolved": null,
        "Description": "Current code doesn't enable using both Delta Encoding and Dictionary Encoding. If I instantiate ParquetWriter like this : \n\nval writer = new ParquetWriter[Group](outFile, new GroupWriteSupport, codec, blockSize, pageSize, dictPageSize, enableDictionary = true, true, ParquetProperties.WriterVersion.PARQUET_2_0, configuration)\n\n\nThen this piece of code : \nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/values/factory/DefaultValuesWriterFactory.java#L78-L86\nCauses that DictionaryValuesWriter is used instead of the inferred DeltaLongEncodingWriter. \nThe original issue is here : https://github.com/apache/parquet-mr/pull/154#issuecomment-266489768",
        "Issue Links": []
    },
    "PARQUET-797": {
        "Key": "PARQUET-797",
        "Summary": "[C++] Update for API changes in ARROW-418",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/Dec/16 19:17",
        "Updated": "13/Dec/16 19:57",
        "Resolved": "13/Dec/16 19:57",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-798": {
        "Key": "PARQUET-798",
        "Summary": "usage of THRIFT_HOME in build?",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "William Forson",
        "Created": "12/Dec/16 23:22",
        "Updated": "13/Dec/16 23:52",
        "Resolved": "13/Dec/16 23:52",
        "Description": "Hi,\nI posted this issue a while back on the old parquet-cpp github page and forgot about it...until just now.\nFirst of all, to briefly describe my use case: I am using parquet-cpp as a dependency in a project which already has a dependency on thrift. As such, I'd like to be able to build parquet-cpp against the version of thrift which we are already using in the project.\nWhen I first spotted the notes regarding THRIFT_HOME in the readme, I thought this looked like exactly what I wanted \u2013 that is, a way to configure the parquet-cpp build process to use external/pre-built thrift artifacts. So, for starters: am I simply misinterpreting the meaning of THRIFT_HOME? (e.g. does that specify a destination directory for thrift artifacts built during the parquet-cpp 3p build step?) And if so, is there any good way (or bad way) to accomplish what I'm trying to do here?\nOtherwise, when I try to use my own thrift installation (via THRIFT_HOME), the build fails like so:\n\n$ make parquetcpp\ncd parquet-cpp && source ./thirdparty/set_thirdparty_env.sh && make\nmake[1]: Entering directory `parquet-cpp'\nmake[2]: Entering directory `parquet-cpp'\nmake[3]: Entering directory `parquet-cpp'\nScanning dependencies of target parquet_thrift\nmake[3]: Leaving directory `parquet-cpp'\nmake[3]: Entering directory `parquet-cpp'\n[  1%] Building CXX object src/parquet/thrift/CMakeFiles/parquet_thrift.dir/parquet_types.cpp.o\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::Statistics::read(apache::thrift::protocol::TProtocol*)\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:179:3: error: \u2018TInputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TInputRecursionTracker tracker(*iprot);\n   ^\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::Statistics::write(apache::thrift::protocol::TProtocol*) const\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:244:3: error: \u2018TOutputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TOutputRecursionTracker tracker(*oprot);\n   ^\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::SchemaElement::read(apache::thrift::protocol::TProtocol*)\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:357:3: error: \u2018TInputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TInputRecursionTracker tracker(*iprot);\n   ^\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::SchemaElement::write(apache::thrift::protocol::TProtocol*) const\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:471:3: error: \u2018TOutputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TOutputRecursionTracker tracker(*oprot);\n   ^\n<SNIP ... you get the idea>\n   ^\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::FileMetaData::read(apache::thrift::protocol::TProtocol*)\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:2646:3: error: \u2018TInputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TInputRecursionTracker tracker(*iprot);\n   ^\nparquet-cpp/src/parquet/thrift/parquet_types.cpp: In member function \u2018uint32_t parquet::format::FileMetaData::write(apache::thrift::protocol::TProtocol*) const\u2019:\nparquet-cpp/src/parquet/thrift/parquet_types.cpp:2775:3: error: \u2018TOutputRecursionTracker\u2019 is not a member of \u2018apache::thrift::protocol\u2019\n   apache::thrift::protocol::TOutputRecursionTracker tracker(*oprot);\n   ^\nmake[3]: *** [src/parquet/thrift/CMakeFiles/parquet_thrift.dir/parquet_types.cpp.o] Error 1\nmake[3]: Leaving directory `parquet-cpp'\nmake[2]: *** [src/parquet/thrift/CMakeFiles/parquet_thrift.dir/all] Error 2\nmake[2]: Leaving directory `parquet-cpp'\nmake[1]: *** [all] Error 2\nmake[1]: Leaving directory `parquet-cpp'\nmake: *** [<...>/libparquet.a] Error 2\nThis kinda smells like a circular dependency (as all of the types/headers mentioned above look fine), but I haven't dug through the internals of the parquet-cpp build logic enough to diagnose it in more detail. Before doing so, I wanted to post here for a sanity check on my use case, and the intended semantics/usage of THRIFT_HOME.\nAny help/clarification will be much appreciated!",
        "Issue Links": []
    },
    "PARQUET-799": {
        "Key": "PARQUET-799",
        "Summary": "concurrent usage of the file reader API",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Forson",
        "Reporter": "William Forson",
        "Created": "14/Dec/16 17:54",
        "Updated": "19/Dec/16 23:28",
        "Resolved": "19/Dec/16 23:28",
        "Description": "I've recently been debugging a segfault that occurs when concurrently reading (distinct) parquet files from multiple threads.\nI initially assumed this was a reasonable thing to do, since the project README doesn't say anything about concurrency one way or the other. But then I encountered this TODO comment:\n\n// TODO: Parallel processing is not yet safe because of memory-ownership\n// semantics (the PageReader may or may not own the memory referenced by a\n// page)\nAnd it has got me wondering: is parquet-cpp fundamentally NOT thread-safe, even for the use case of reading a single file per thread at any given time? Or is it basically thread-safe with a couple gotchas?\nAlso, jfyi, I'm currently running against a build which incorporates this change.\n(aside: my motivation for recently posting an issue re. THRIFT_HOME was to rule out any ABI weirdness that might result from building parquet-cpp against a different version of thrift than the applications that ultimately consume parquet-cpp)\nThanks!",
        "Issue Links": []
    },
    "PARQUET-800": {
        "Key": "PARQUET-800",
        "Summary": "[C++] Provide public API to access dictionary-encoded indices and values",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "14/Dec/16 18:38",
        "Updated": "16/Aug/19 13:55",
        "Resolved": "16/Aug/19 13:55",
        "Description": "Some tools can operate on dictionary-encoded data, so we may give them the option to request batches of dictionary indices and, separately, the dictionary page values.",
        "Issue Links": [
            "/jira/browse/ARROW-3246"
        ]
    },
    "PARQUET-801": {
        "Key": "PARQUET-801",
        "Summary": "Allow UserDefinedPredicates in DictionaryFilter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Patrick Woody",
        "Reporter": "Patrick Woody",
        "Created": "15/Dec/16 17:11",
        "Updated": "21/Apr/18 12:38",
        "Resolved": "20/Dec/16 22:39",
        "Description": "UserDefinedPredicate is not implemented for dictionary filtering.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/394"
        ]
    },
    "PARQUET-802": {
        "Key": "PARQUET-802",
        "Summary": "The link to \"Google Hangout\" on parquet.apache.org is broken",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lars Volker",
        "Created": "16/Dec/16 16:59",
        "Updated": "12/Jan/17 11:09",
        "Resolved": null,
        "Description": "It 404's, someone might want to remove it.",
        "Issue Links": []
    },
    "PARQUET-803": {
        "Key": "PARQUET-803",
        "Summary": "\"How to contribute\" link is broken on the webpage",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Lars Volker",
        "Created": "16/Dec/16 17:02",
        "Updated": "28/May/18 14:23",
        "Resolved": "28/May/18 14:23",
        "Description": "The webpage links from here to the contribution guidelines, but that link is broken. Can someone fix it?",
        "Issue Links": [
            "/jira/browse/PARQUET-153"
        ]
    },
    "PARQUET-804": {
        "Key": "PARQUET-804",
        "Summary": "parquet-format README.md still links to the old Google group",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "None",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "16/Dec/16 17:16",
        "Updated": "28/Dec/16 20:03",
        "Resolved": "28/Dec/16 20:03",
        "Description": "I guess it should link to the dev@ mailing list instead. I'll send a PR.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/47"
        ]
    },
    "PARQUET-805": {
        "Key": "PARQUET-805",
        "Summary": "C++: Read Int96 into Arrow Timestamp(ns)",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "19/Dec/16 07:01",
        "Updated": "20/Dec/16 21:18",
        "Resolved": "20/Dec/16 21:18",
        "Description": "Although considered deprecated, this is still generated by various engines. Thus we should also support reading this into an Arrow structure. The most similar one is a timestamp with unit nanoseconds.",
        "Issue Links": []
    },
    "PARQUET-806": {
        "Key": "PARQUET-806",
        "Summary": "Parquet-tools silently suppresses error messages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "19/Dec/16 13:57",
        "Updated": "26/Jan/17 23:38",
        "Resolved": "26/Jan/17 23:38",
        "Description": "Without -debug parameter, parquet-tools produces quite cryptic output in case of errors. For example, parquet tools may print the following output and nothing more:\norg/apache/hadoop/conf/Configuration\nIt is not clear from this output that it indicates an error (actually it is not clear what it indicates at all). I don't really understand the reasoning behind hiding exceptions by default, but instead of this unusable message, parquet-tools should at least print:\nNoClassDefFoundError: org/apache/hadoop/conf/Configuration",
        "Issue Links": []
    },
    "PARQUET-807": {
        "Key": "PARQUET-807",
        "Summary": "[C++] Add API to read file metadata only from a file handle",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Dec/16 14:49",
        "Updated": "05/Jan/17 17:21",
        "Resolved": "05/Jan/17 17:20",
        "Description": "To assist with metadata merging",
        "Issue Links": [
            "/jira/browse/PARQUET-808"
        ]
    },
    "PARQUET-808": {
        "Key": "PARQUET-808",
        "Summary": "[C++] Add API to read file given externally-provided FileMetadata",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Dec/16 14:56",
        "Updated": "05/Jan/17 17:21",
        "Resolved": "05/Jan/17 17:20",
        "Description": "When reconciling the metadata in multiple Parquet files, it may often make sense to persist the deserialized metadata in memory rather than reading it twice. The Parquet format also provides for file metadata and row group data living in separate physical files.",
        "Issue Links": [
            "/jira/browse/PARQUET-807"
        ]
    },
    "PARQUET-809": {
        "Key": "PARQUET-809",
        "Summary": "[C++] Add API to determine if two files' schemas are compatible",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Dec/16 00:28",
        "Updated": "05/Jan/17 17:21",
        "Resolved": "05/Jan/17 17:21",
        "Description": "This must provide for both computing an exact match or compatible schema evolution",
        "Issue Links": [
            "/jira/browse/PARQUET-810"
        ]
    },
    "PARQUET-810": {
        "Key": "PARQUET-810",
        "Summary": "[C++] Read from file with schema evolution",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "20/Dec/16 00:29",
        "Updated": "01/May/19 22:45",
        "Resolved": "01/May/19 22:45",
        "Description": "In a large dataset, new optional fields may appear later in the data's lifetime. Assuming we know the current schema and all fields, we must provide an API to read from an older file that is missing some of the optional fields in its metadata.",
        "Issue Links": [
            "/jira/browse/PARQUET-809"
        ]
    },
    "PARQUET-811": {
        "Key": "PARQUET-811",
        "Summary": "[C++] Parquet links against Brotli's shared libraries if they are built",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Dec/16 15:37",
        "Updated": "20/Dec/16 21:17",
        "Resolved": "20/Dec/16 21:17",
        "Description": "We should link statically so that Brotli is not a runtime requirement. see build failure at\nhttps://circleci.com/gh/wesm/pyarrow-feedstock/2",
        "Issue Links": []
    },
    "PARQUET-812": {
        "Key": "PARQUET-812",
        "Summary": "[C++] Failure reading BYTE_ARRAY data from file in parquet-compatibility project",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Dec/16 15:42",
        "Updated": "21/Dec/16 13:56",
        "Resolved": "21/Dec/16 13:55",
        "Description": "I see the odd exception from the Python side:\n\nArrowException: NotImplemented: list<: uint8>\n\n\nThe schema is:\n\n$ debug/parquet-dump-schema ~/Downloads/nation.impala.parquet \nrequired group schema {\n  optional int32 n_nationkey\n  optional byte_array n_name\n  optional int32 n_regionkey\n  optional byte_array n_comment\n}\n\n\nThis may have been introduced by https://github.com/apache/parquet-cpp/commit/8487142f6d5a60d12e3068ac226b2b5dfe178350",
        "Issue Links": [
            "/jira/browse/ARROW-434"
        ]
    },
    "PARQUET-813": {
        "Key": "PARQUET-813",
        "Summary": "C++: Build dependencies using CMake External project",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Dec/16 19:09",
        "Updated": "29/Dec/16 10:06",
        "Resolved": "29/Dec/16 10:06",
        "Description": "Instead of the shell scripts, use CMake's ExternalProject support.",
        "Issue Links": []
    },
    "PARQUET-814": {
        "Key": "PARQUET-814",
        "Summary": "C++: Remove Conda recipes",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Dec/16 22:00",
        "Updated": "21/Dec/16 23:49",
        "Resolved": "21/Dec/16 23:49",
        "Description": "These are now build in conda-forge and the ones in the parquet-cpp repository are deprecated.",
        "Issue Links": []
    },
    "PARQUET-815": {
        "Key": "PARQUET-815",
        "Summary": "Unable to create parquet file for the given data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Navya Krishnappa",
        "Created": "22/Dec/16 10:56",
        "Updated": "04/Jul/17 05:45",
        "Resolved": "23/May/17 04:25",
        "Description": "When i'm trying to read the below mentioned csv source file and creating an parquet file from that throws an java.lang.IllegalArgumentException: Invalid DECIMAL scale: -9 exception.\nThe source file content is \nRow(column name)\n9.03E+12\n1.19E+11\nRefer the given code used read the csv file and creating an parquet file:\n//Read the csv file\nDataset dataset = getSqlContext().read()\n.option(DAWBConstant.HEADER, \"true\")\n.option(DAWBConstant.PARSER_LIB, \"commons\")\n.option(DAWBConstant.INFER_SCHEMA, \"true\")\n.option(DAWBConstant.DELIMITER, \",\")\n.option(DAWBConstant.QUOTE, \"\\\"\")\n.option(DAWBConstant.ESCAPE, \"\n\")\n.option(DAWBConstant.MODE, Mode.PERMISSIVE)\n.csv(sourceFile)\n// create an parquet file\ndataset.write().parquet(\"//path.parquet\")\nStack trace:\nCaused by: java.lang.IllegalArgumentException: Invalid DECIMAL scale: -9\nat org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)\nat org.apache.parquet.schema.Types$PrimitiveBuilder.decimalMetadata(Types.java:410)\nat org.apache.parquet.schema.Types$PrimitiveBuilder.build(Types.java:324)\nat org.apache.parquet.schema.Types$PrimitiveBuilder.build(Types.java:250)\nat org.apache.parquet.schema.Types$Builder.named(Types.java:228)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:412)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:321)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convert$1.apply(ParquetSchemaConverter.scala:313)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convert$1.apply(ParquetSchemaConverter.scala:313)\nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\nat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat org.apache.spark.sql.types.StructType.foreach(StructType.scala:95)\nat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\nat org.apache.spark.sql.types.StructType.map(StructType.scala:95)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter.convert(ParquetSchemaConverter.scala:313)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.init(ParquetWriteSupport.scala:85)\nat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:288)\nat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:262)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetFileFormat.scala:562)\nat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:139)\nat org.apache.spark.sql.execution.datasources.BaseWriterContainer.newOutputWriter(WriterContainer.scala:131)\nat org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:247)\nat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\nat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\nat org.apache.spark.scheduler.Task.run(Task.scala:86)",
        "Issue Links": []
    },
    "PARQUET-816": {
        "Key": "PARQUET-816",
        "Summary": "[C++] Failure decoding sample dict-encoded file from parquet-compatibility project",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Dec/16 21:38",
        "Updated": "25/Dec/16 15:03",
        "Resolved": "25/Dec/16 15:03",
        "Description": "See attached. This throws an exception when read:\n\n$ debug/parquet_reader nation.dict.parquet \nFile statistics:\nVersion: 1\nCreated By: parquet-mr\nTotal rows: 25\nNumber of RowGroups: 1\nNumber of Real Columns: 4\nNumber of Columns: 4\nNumber of Selected Columns: 4\nColumn 0: nation_key (INT32)\nColumn 1: name (BYTE_ARRAY)\nColumn 2: region_key (INT32)\nColumn 3: comment_col (BYTE_ARRAY)\n--- Row Group 0 ---\n--- Total Bytes 0 ---\n  rows: 25---\nColumn 0\n, values: 25  Statistics Not Set\n  compression: UNCOMPRESSED, encodings: \n  uncompressed size: 125, compressed size: 125\nColumn 1\n, values: 25  Statistics Not Set\n  compression: UNCOMPRESSED, encodings: \n  uncompressed size: 322, compressed size: 322\nColumn 2\n, values: 25  Statistics Not Set\n  compression: UNCOMPRESSED, encodings: \n  uncompressed size: 125, compressed size: 125\nColumn 3\n, values: 25  Statistics Not Set\n  compression: UNCOMPRESSED, encodings: \n  uncompressed size: 2002, compressed size: 2002\nnation_key              name                    region_key              comment_col             \n0                       Parquet error: Unexpected end of stream.\n\n\nHowever, I checked that I can read this file with Impala:\n\nIn [13]: hdfs.put('/tmp/nation-dict-test/test.parq', 'nation.dict.parquet')\nOut[13]: '/tmp/nation-dict-test/test.parq'\n\nIn [14]: pf = con.parquet_file('/tmp/nation-dict-test')\n\nIn [15]: pf.execute()\nOut[15]: \n    nation_key            name  region_key  \\\n0            0         ALGERIA           0   \n1            1       ARGENTINA           1   \n2            2          BRAZIL           1   \n3            3          CANADA           1   \n4            4           EGYPT           4   \n5            5        ETHIOPIA           0   \n6            6          FRANCE           3   \n7            7         GERMANY           3   \n8            8           INDIA           2   \n9            9       INDONESIA           2   \n10          10            IRAN           4   \n11          11            IRAQ           4   \n12          12           JAPAN           2   \n13          13          JORDAN           4   \n14          14           KENYA           0   \n15          15         MOROCCO           0   \n16          16      MOZAMBIQUE           0   \n17          17            PERU           1   \n18          18           CHINA           2   \n19          19         ROMANIA           3   \n20          20    SAUDI ARABIA           4   \n21          21         VIETNAM           2   \n22          22          RUSSIA           3   \n23          23  UNITED KINGDOM           3   \n24          24   UNITED STATES           1   \n\n                                          comment_col  \n0    haggle. carefully final deposits detect slyly...  \n1   al foxes promise slyly according to the regula...  \n2   y alongside of the pending deposits. carefully...  \n3   eas hang ironic, silent packages. slyly regula...  \n4   y above the carefully unusual theodolites. fin...  \n5                     ven packages wake quickly. regu  \n6              refully final requests. regular, ironi  \n7   l platelets. regular accounts x-ray: unusual, ...  \n8   ss excuses cajole slyly across the packages. d...  \n9    slyly express asymptotes. regular deposits ha...  \n10  efully alongside of the slyly final dependenci...  \n11  nic deposits boost atop the quickly final requ...  \n12               ously. final, express gifts cajole a  \n13  ic deposits are blithely about the carefully r...  \n14   pending excuses haggle furiously deposits. pe...  \n15  rns. blithely bold courts among the closely re...  \n16      s. ironic, unusual asymptotes wake blithely r  \n17  platelets. blithely pending dependencies use f...  \n18  c dependencies. furiously express notornis sle...  \n19  ular asymptotes are about the furious multipli...  \n20  ts. silent requests haggle. closely express pa...  \n21     hely enticingly express accounts. even, final   \n22   requests against the platelets use never acco...  \n23  eans boost carefully special requests. account...  \n24  y final packages. slow foxes cajole quickly. q...",
        "Issue Links": [
            "/jira/browse/ARROW-434"
        ]
    },
    "PARQUET-817": {
        "Key": "PARQUET-817",
        "Summary": "[C++] Perform Parquet reads and Arrow deserialization concurrently",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "27/Dec/16 17:20",
        "Updated": "12/Jan/17 08:15",
        "Resolved": "12/Jan/17 08:15",
        "Description": "The idea with this issue is to make batch requests from parquet-cpp happen asynchronously in a background thread, so we aren't pausing decoding while converting to Arrow representation",
        "Issue Links": [
            "/jira/browse/PARQUET-820"
        ]
    },
    "PARQUET-818": {
        "Key": "PARQUET-818",
        "Summary": "[C++] Refactor library to share IO, Buffer, and memory management abstractions with Apache Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "28/Dec/16 23:29",
        "Updated": "30/Dec/16 16:36",
        "Resolved": "30/Dec/16 16:36",
        "Description": "As being discussed on the mailing list",
        "Issue Links": []
    },
    "PARQUET-819": {
        "Key": "PARQUET-819",
        "Summary": "C++: Trying to install non-existing parquet/arrow/utils.h",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "30/Dec/16 16:55",
        "Updated": "30/Dec/16 18:04",
        "Resolved": "30/Dec/16 18:04",
        "Description": "This file does no longer exist",
        "Issue Links": []
    },
    "PARQUET-820": {
        "Key": "PARQUET-820",
        "Summary": "C++: Decoders should directly emit arrays with spacing for null entries",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "30/Dec/16 19:00",
        "Updated": "18/Jan/17 00:10",
        "Resolved": "18/Jan/17 00:10",
        "Description": "Currently the decoders all emit the data arrays in the form of the PLAIN encoding, i.e. only the non-null entries with no memory wastage for null entries. But for random access we need to gap the output so that entry i is located at memory address base + i * type_size.",
        "Issue Links": [
            "/jira/browse/PARQUET-817"
        ]
    },
    "PARQUET-821": {
        "Key": "PARQUET-821",
        "Summary": "[C++] zlib download link is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "02/Jan/17 22:50",
        "Updated": "03/Jan/17 12:58",
        "Resolved": "03/Jan/17 12:58",
        "Description": "Appears a zlib 1.2.9 is in the works, but zlib.net/zlib-1.2.8.tar.gz is no longer available.",
        "Issue Links": []
    },
    "PARQUET-822": {
        "Key": "PARQUET-822",
        "Summary": "Upgrade java dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "03/Jan/17 15:12",
        "Updated": "26/Jan/17 23:36",
        "Resolved": "26/Jan/17 23:36",
        "Description": "Upgrade the java dependencies if possible. \nThis one is to track the upgrades that does not require too much code/config modification.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/398"
        ]
    },
    "PARQUET-823": {
        "Key": "PARQUET-823",
        "Summary": "[C++] Update coveralls exclusions from code coverage reports",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "03/Jan/17 21:49",
        "Updated": "04/Jan/17 08:07",
        "Resolved": "04/Jan/17 08:07",
        "Description": "Since changing the thirdparty build details in CI, we have acquired a bunch of additional superfluous code in our code coverage reports. see\nhttps://github.com/apache/parquet-cpp/blob/master/ci/upload_coverage.sh#L29\nhttps://coveralls.io/github/apache/parquet-cpp?branch=master",
        "Issue Links": [
            "/jira/browse/PARQUET-824"
        ]
    },
    "PARQUET-824": {
        "Key": "PARQUET-824",
        "Summary": "[C++] Update coveralls exclusions from code coverage reports",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "03/Jan/17 21:49",
        "Updated": "11/Nov/18 22:10",
        "Resolved": "11/Nov/18 22:10",
        "Description": "Since changing the thirdparty build details in CI, we have acquired a bunch of additional superfluous code in our code coverage reports. see\nhttps://github.com/apache/parquet-cpp/blob/master/ci/upload_coverage.sh#L29\nhttps://coveralls.io/github/apache/parquet-cpp?branch=master",
        "Issue Links": [
            "/jira/browse/PARQUET-823"
        ]
    },
    "PARQUET-825": {
        "Key": "PARQUET-825",
        "Summary": "Static analyzer findings (NPEs, resource leaks)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "04/Jan/17 08:18",
        "Updated": "26/Jan/17 23:33",
        "Resolved": "26/Jan/17 23:33",
        "Description": "Static code analyzer tool found some possible issues:\nNPE:\n\nRecordReaderImplementation:131\nGlobeNode:61,100,139\n\nResource leak:\n\nByteBasedBitPackingGenerator:68\nIntBasedBitPackingGenerator:59\nVersionGenerator:53\n\nDon't catch throwable:\n\nDumpCommand:334\nRegistry:42,57\nPrettyPrintWriter:82,91",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/399"
        ]
    },
    "PARQUET-826": {
        "Key": "PARQUET-826",
        "Summary": "parquet.thrift comments for Statistics are not consistent with parquet-mr and Hive implementations",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "05/Jan/17 18:53",
        "Updated": "26/Jul/17 23:26",
        "Resolved": "26/Jul/17 23:26",
        "Description": "I'm currently working on adding support for writing min/max statistics to Parquet files to Impala (IMPALA-3909). I noticed, that the comments in parquet.thrift#L201 don't seem to match the implementations in parquet-mr and Hive.\nThe comments ask for min/max statistics to be \"encoded in PLAIN encoding\". For strings (BYTE_ARRAY), this should be \"4 byte length stored as little endian, followed by bytes\".\nLooking at BinaryStatistics.java#L61, it seems to return the bytes without a length-prefix. Writing a parquet file with Hive also shows this behavior.\nSimilarly, but less ambiguous, PLAIN encoding for booleans uses bit-packing. It seems to be implied that for a single bit (min/max of a boolean column) it means setting the least significant bit of a single byte. This could be made more clear in the parquet.thrift file, too.",
        "Issue Links": []
    },
    "PARQUET-827": {
        "Key": "PARQUET-827",
        "Summary": "[C++] Incorporate addition of arrow::MemoryPool::Reallocate",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Jan/17 23:29",
        "Updated": "07/Jan/17 20:05",
        "Resolved": "07/Jan/17 20:05",
        "Description": "This was introduced in ARROW-456.",
        "Issue Links": []
    },
    "PARQUET-828": {
        "Key": "PARQUET-828",
        "Summary": "[C++] \"version\" field set improperly in file metadata",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Jan/17 23:18",
        "Updated": "10/Jan/17 07:49",
        "Resolved": "10/Jan/17 07:49",
        "Description": "Version should be 1 for Parquet 1.0, 2 for Parquet 2.0.",
        "Issue Links": []
    },
    "PARQUET-829": {
        "Key": "PARQUET-829",
        "Summary": "C++: Make use of ARROW-469",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "10/Jan/17 14:10",
        "Updated": "11/Jan/17 22:18",
        "Resolved": "11/Jan/17 22:18",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-830": {
        "Key": "PARQUET-830",
        "Summary": "[C++] Add additional configuration options to parquet::arrow::OpenFIle",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/Jan/17 03:00",
        "Updated": "12/Jan/17 07:58",
        "Resolved": "12/Jan/17 07:58",
        "Description": "Since ParquetFileReader/Writer constructors can throw exceptions, it's useful to wrap them in functions that catch exceptions and return arrow::Status in the event of failure. We already have this, but it is missing some new options added (like passing in already-computed metadata).",
        "Issue Links": []
    },
    "PARQUET-831": {
        "Key": "PARQUET-831",
        "Summary": "Corrupt Parquet Files",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Steve Severance",
        "Created": "15/Jan/17 15:09",
        "Updated": "24/Feb/23 23:28",
        "Resolved": null,
        "Description": "I am getting corrupt parquet files as the result of a spark job. The write job completes with no errors but when I read the data again I get the following error:\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://MYPATH/part-r-00004-b5c93a19-2f75-4c04-b798-de9cb463f02f.gz.parquet\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)\nat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:201)\nat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\nat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:128)\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:91)\nat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)\nat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\nat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\nat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\nat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\nat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\nat org.apache.spark.scheduler.Task.run(Task.scala:86)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NegativeArraySizeException\nat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:755)\nat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:494)\nat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:127)\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:208) \nThe job that generates this data partitions and sorts the data in a particular way to achieve better compression. If I don't partition and sort I have not been able to reproduce its behavior. It also only has this behavior on say 25% of the data. Most of the time simply rerunning the write job would cause the read error to go away but I have now run across cases where that was not the case. I am happy to give what data I can, or work with someone to run this down.\nI know this is a sub-optimal report, but I have not been able to randomly generate data to reproduce this issue. The data that trips this bug is typically 5GB+ post compression files.",
        "Issue Links": []
    },
    "PARQUET-832": {
        "Key": "ARROW-3763",
        "Summary": "[C++] Write Parquet ByteArray / FixedLenByteArray reader batches directly into arrow::BinaryBuilder",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "C++",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "18/Jan/17 00:12",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "08/Feb/19 03:03",
        "Description": "As a follow up to PARQUET-820. This may yield some performance benefits.",
        "Issue Links": []
    },
    "PARQUET-833": {
        "Key": "PARQUET-833",
        "Summary": "C++: Provide API to write spaced arrays (e.g. Arrow)",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Jan/17 10:06",
        "Updated": "23/Jan/17 01:08",
        "Resolved": "23/Jan/17 01:08",
        "Description": "Add an API to TypedColumnWriter and the encoders so that we can write null-spaced arrays without an additional memory copy.",
        "Issue Links": []
    },
    "PARQUET-834": {
        "Key": "PARQUET-834",
        "Summary": "C++: Support r/w of arrow::ListArray",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Jan/17 10:24",
        "Updated": "02/Feb/17 21:14",
        "Resolved": "02/Feb/17 21:14",
        "Description": "Add support for reading/wring ListArray consisting of primitive types. This will change the interface of the Parquet Arrow support as we aren't limited anymore to flat columns.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-835": {
        "Key": "PARQUET-835",
        "Summary": "[C++] Add option to parquet::arrow to read columns in parallel using a thread pool",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/17 17:44",
        "Updated": "23/Jan/17 17:55",
        "Resolved": "23/Jan/17 17:55",
        "Description": "For Parquet file reading that is not IO-bound, we can go faster by reading columns in multiple threads (assuming underlying the IO source is threadsafe). The code will be very similar to that in https://github.com/apache/arrow/blob/master/python/src/pyarrow/adapters/pandas.cc#L1193",
        "Issue Links": []
    },
    "PARQUET-836": {
        "Key": "PARQUET-836",
        "Summary": "[C++] Add column selection to parquet::arrow::FileReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Jan/17 18:19",
        "Updated": "24/Jan/17 07:42",
        "Resolved": "24/Jan/17 07:42",
        "Description": "With multithreaded reads coming in PARQUET-835, it would be better to push down the subsetting into the FileReader rather than leaving this work at the application level. One artifact of this is the Python interface in Arrow (which handles subsetting in Python/Cython)",
        "Issue Links": []
    },
    "PARQUET-837": {
        "Key": "PARQUET-837",
        "Summary": "[C++] SerializedFile::ParseMetaData uses Seek, followed by Read, and could have race conditions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Jan/17 01:55",
        "Updated": "23/Jan/17 07:55",
        "Resolved": "23/Jan/17 07:55",
        "Description": "As described in the PR for ARROW-508 https://github.com/apache/arrow/pull/300, the read-at-position should be performed atomically to prevent rare race conditions. I would suggest removing the `Seek` method from the input API altogether",
        "Issue Links": []
    },
    "PARQUET-838": {
        "Key": "PARQUET-838",
        "Summary": "[CPP] Unable to read files written by parquet-cpp from parquet-tools",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "24/Jan/17 20:34",
        "Updated": "02/Jun/20 22:42",
        "Resolved": null,
        "Description": "I could not read files written by parquet-cpp from parquet-tools and Hive.\nSetting field ids in the schema metadata seems to be the problem. We should make setting the field_id optional.",
        "Issue Links": []
    },
    "PARQUET-839": {
        "Key": "PARQUET-839",
        "Summary": "Min-max should be computed based on logical type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "format-2.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Tim Armstrong",
        "Created": "24/Jan/17 21:21",
        "Updated": "07/Jun/17 17:35",
        "Resolved": "07/Jun/17 17:35",
        "Description": "The min/max stats are currently underspecified - it is not clear in any cases from the spec what the expected ordering is.\nThere are some related issues, like PARQUET-686 to fix specific problems, but there seems to be a general assumption that the min/max should be defined based on the primitive type instead of the logical type.\nHowever, this makes the stats nearly useless for some logical types. E.g. consider a DECIMAL encoded into a (variable-length) BINARY. The min-max of the underlying binary type is based on the lexical order of the byte string, but that does not correspond to any reasonable ordering of the decimal values. E.g. 16 (0x1 0x0) will be ordered between 1 (0x0) and (0x2). This makes min-max filtering a lot less effective and would force query engines using parquet to implement workarounds to produce correct results (e.g. custom comparators).",
        "Issue Links": [
            "/jira/browse/PARQUET-686"
        ]
    },
    "PARQUET-840": {
        "Key": "PARQUET-840",
        "Summary": "Min-max for int96 is computed incorrectly in parquet-mr",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tim Armstrong",
        "Created": "24/Jan/17 21:24",
        "Updated": "24/Jan/17 21:24",
        "Resolved": null,
        "Description": "The min-max stats implementation computes the min and max values of an int96 column using the BinaryStatistics class, which uses the lexical ordering of signed bytes. Other integer types order based on the actual integer value.\nThe spec does not specify an ordering, but parquet-mr is internally inconsistent - there is no reason why int96 should be ordered in a completely different way from int64.",
        "Issue Links": []
    },
    "PARQUET-841": {
        "Key": "PARQUET-841",
        "Summary": "[C++] Writing wrong format version when using ParquetVersion::PARQUET_1_0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jan/17 00:35",
        "Updated": "25/Jan/17 03:45",
        "Resolved": "25/Jan/17 03:45",
        "Description": "I'm not sure if this is related to PARQUET-838, but I ran into this while debugging something",
        "Issue Links": []
    },
    "PARQUET-842": {
        "Key": "PARQUET-842",
        "Summary": "[C++] Impala rejects DOUBLE columns if decimal metadata is set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jan/17 01:46",
        "Updated": "25/Jan/17 07:15",
        "Resolved": "25/Jan/17 07:15",
        "Description": "See https://github.com/apache/incubator-impala/blob/757c68b29e21e64fc4586cdf24ee6f9369be460f/be/src/exec/parquet-metadata-utils.cc#L211\nSee problematic logic at \nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/schema/types.cc#L95\nThis causes a problem even though the ConvertedType is not DECIMAL.",
        "Issue Links": []
    },
    "PARQUET-843": {
        "Key": "PARQUET-843",
        "Summary": "[C++] Impala unable to read files created by parquet-cpp",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jan/17 17:31",
        "Updated": "26/Jan/17 02:42",
        "Resolved": "26/Jan/17 02:42",
        "Description": "See attached example file. parquet-tools is able to read this. I have only tested on Impala 2.5.0, with some effort I could check on newer Impala, but it would be good to figure out what is the issue with older versions",
        "Issue Links": []
    },
    "PARQUET-844": {
        "Key": "PARQUET-844",
        "Summary": "[C++] Consolidate encodings, schema, and compression subdirectories into fewer files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jan/17 02:53",
        "Updated": "26/Jan/17 17:02",
        "Resolved": "26/Jan/17 17:02",
        "Description": "The codebase would be simpler to navigate with parquet/compression.h,  parquet/encodings.h, and parquet/schema.h files.",
        "Issue Links": []
    },
    "PARQUET-845": {
        "Key": "PARQUET-845",
        "Summary": "Efficient storage for several INT_8 and INT_16",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fernando Pereira",
        "Created": "26/Jan/17 11:09",
        "Updated": "28/Nov/17 16:47",
        "Resolved": null,
        "Description": "In very large datasets, aggregating several INT8 into INT32 fields (or byte array) can make a big difference.\nIn parquet, efficient algorithms exist for INT32, so if the LogicalType is INT_8 the encoded int might take up only one byte.\nHowever further optimizations could be made by allowing the user to better specify the types.\nWhat about BYTE_ARRAY logical type, backed by FIXED_LEN_BYTE_ARRAY type (or eventually INT_32)?",
        "Issue Links": []
    },
    "PARQUET-846": {
        "Key": "PARQUET-846",
        "Summary": "[CPP] CpuInfo::Init() is not thread safe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "26/Jan/17 19:20",
        "Updated": "26/Jan/17 23:39",
        "Resolved": "26/Jan/17 23:39",
        "Description": "CpuInfo::Init() is called here\nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/encodings/dictionary-encoding.h#L182",
        "Issue Links": []
    },
    "PARQUET-847": {
        "Key": "PARQUET-847",
        "Summary": "CMake try to compile libparquet with a static version of libthrift on Debian",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Gian Lorenzo Meocci",
        "Created": "28/Jan/17 17:44",
        "Updated": "29/Jan/17 17:44",
        "Resolved": "29/Jan/17 17:44",
        "Description": "When we build  parquet-cpp using an extern Thrift (compiled with -fPIC) seems that cmake is trying to compile libparquet.so with libthrift.a.\nThrift version: 0.10.0\nBoost: 1.55.0",
        "Issue Links": []
    },
    "PARQUET-848": {
        "Key": "PARQUET-848",
        "Summary": "[C++] Consolidate libparquet_thrift subcomponent",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Jan/17 01:41",
        "Updated": "29/Jan/17 17:45",
        "Resolved": "29/Jan/17 17:45",
        "Description": "Per discussion in PARQUET-847, this adds complexity that isn't especially useful. I will flatten the directory structure and build the Thrift bits as part of the main parquet_objlib",
        "Issue Links": []
    },
    "PARQUET-849": {
        "Key": "PARQUET-849",
        "Summary": "[C++] Upgrade default Thrift in thirdparty toolchain to 0.9.3 or 0.10",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Julien Lafaye",
        "Reporter": "Wes McKinney",
        "Created": "29/Jan/17 14:12",
        "Updated": "22/Feb/17 14:22",
        "Resolved": "21/Feb/17 21:53",
        "Description": "Thrift 0.9.3 and higher supports CMake, so this may simplify the external project setup",
        "Issue Links": []
    },
    "PARQUET-850": {
        "Key": "PARQUET-850",
        "Summary": "Make MessageType serializable",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Pawel Szulc",
        "Created": "30/Jan/17 16:25",
        "Updated": "30/Jan/17 16:25",
        "Resolved": null,
        "Description": "Background: I'm writing `ParquetRDD` implementation to run on Apache Spark. One of the features I want to provide is the ability to create instance of `ParqeutRDD` with a projection (specify columns to be loaded)\nProblem: Schema information is not serializable. Neither `MessageType` nor any other class that extends `Type` extends `Serializable`.\nQuestion: Is there any specific reason why not to have it the `extends Serializable`? \nIf not, then I will be more then happy to provide PR.",
        "Issue Links": []
    },
    "PARQUET-851": {
        "Key": "PARQUET-851",
        "Summary": "ParquetMetadata.fromJSON method doesn't work",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Vihang Karajgaonkar",
        "Created": "01/Feb/17 01:52",
        "Updated": "01/Feb/17 01:53",
        "Resolved": null,
        "Description": "I wrote a simple testCase to test the fromJSON method below and it fails because ParquetMetadata does not implement default constructors.\n\n \n@Test public void testFromJSON() { \n    MessageType schema = parseMessageType(\"message test { optional binary some_null_field; }\");\n    parquet.hadoop.metadata.FileMetaData fileMetaData =\n      new parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);\n    List<BlockMetaData> blockMetaDataList = new ArrayList<BlockMetaData>();\n    BlockMetaData blockMetaData = new BlockMetaData();\n    blockMetaData.addColumn(createColumnChunkMetaData());\n    blockMetaDataList.add(blockMetaData);\n    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, blockMetaDataList);\n\nString metadataStr = ParquetMetadata.toJSON(metadata);\n System.out.println(ParquetMetadata.toPrettyJSON(metadata));\n ParquetMetadata m2 = ParquetMetadata.fromJSON(metadataStr); \n}",
        "Issue Links": []
    },
    "PARQUET-852": {
        "Key": "PARQUET-852",
        "Summary": "Slowly ramp up sizes of byte[] in ByteBasedBitPackingEncoder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "John Jenkins",
        "Created": "01/Feb/17 14:02",
        "Updated": "24/Apr/18 13:19",
        "Resolved": "12/May/17 22:10",
        "Description": "The current allocation policy for ByteBasedBitPackingEncoder is to allocate 64KB * #bits up-front. As similarly observed in PARQUET-580, this can lead to significant memory overheads for high-fanout scenarios (many columns and/or open files, in my case using BooleanPlainValuesWriter).\nAs done in PARQUET-585, I'll follow up with a PR that starts with a smaller buffer and works its way up to a max.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/467"
        ]
    },
    "PARQUET-853": {
        "Key": "PARQUET-853",
        "Summary": "[C++] Add option to link with shared boost libraries when building Arrow in the thirdparty toolchain",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/17 15:35",
        "Updated": "08/May/17 17:27",
        "Resolved": "08/May/17 17:27",
        "Description": "See discussion in https://github.com/apache/parquet-cpp/pull/231",
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-854": {
        "Key": "PARQUET-854",
        "Summary": "[C++] Statically link Arrow symbols when building libarrow via ExternalProject",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Feb/17 15:37",
        "Updated": "25/Jul/17 03:31",
        "Resolved": "25/Jul/17 03:31",
        "Description": "see discussion in https://github.com/apache/parquet-cpp/pull/231",
        "Issue Links": []
    },
    "PARQUET-855": {
        "Key": "PARQUET-855",
        "Summary": "C++: Run unit tests with AddressSanitizer",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "01/Feb/17 16:12",
        "Updated": "11/Nov/18 22:17",
        "Resolved": "11/Nov/18 22:17",
        "Description": "As valgrind\u00a0is broken for a longer time already on OSX Sierra and memory address checking is really important / helpful in development, we should add an option to run the unittest with AddressSanitizer.",
        "Issue Links": []
    },
    "PARQUET-856": {
        "Key": "PARQUET-856",
        "Summary": "parquet-avro logical type support broke ability to use avro reflection with generics.",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Evan McClain",
        "Reporter": "Evan McClain",
        "Created": "01/Feb/17 21:50",
        "Updated": "02/Feb/17 00:08",
        "Resolved": null,
        "Description": "Impacts 1.9.0 and 1.8.2.\nAssuming the fieldClass is a concrete class (which it is for all of the logical types) is an incorrect assumption when generics are used.\nStatistics<T extends Serializable> {\n  T field1\n  T field2\n}\nSetStatistics extends Statistics<HashSet<String>> {}\nSolution is to check if the fieldClass is abstract/interface/raw object.",
        "Issue Links": []
    },
    "PARQUET-857": {
        "Key": "PARQUET-857",
        "Summary": "[C++] Flatten parquet/encodings directory",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Feb/17 14:44",
        "Updated": "03/Feb/17 16:18",
        "Resolved": "03/Feb/17 16:18",
        "Description": "Help make the codebase flatter, simpler, easier to navigate",
        "Issue Links": []
    },
    "PARQUET-858": {
        "Key": "PARQUET-858",
        "Summary": "[C++] Flatten parquet/column directory, consolidate related code",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Feb/17 16:52",
        "Updated": "26/Jun/17 09:10",
        "Resolved": "26/Jun/17 07:05",
        "Description": "Level encoding would be best consolidated with the column-writer.h code (and decoding into the column-reader.h). The scanner/scan-all code would be best consolidated as well",
        "Issue Links": []
    },
    "PARQUET-859": {
        "Key": "PARQUET-859",
        "Summary": "[C++] Flatten parquet/file directory",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Feb/17 16:53",
        "Updated": "12/Dec/17 19:12",
        "Resolved": "12/Dec/17 19:12",
        "Description": "With appropriate file renaming. Related to PARQUET-858",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/424"
        ]
    },
    "PARQUET-860": {
        "Key": "PARQUET-860",
        "Summary": "ParquetWriter.getDataSize NullPointerException after closed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Mike Mintz",
        "Created": "03/Feb/17 17:40",
        "Updated": "20/Feb/18 19:27",
        "Resolved": null,
        "Description": "When I run ParquetWriter.getDataSize(), it works normally. But after I call ParquetWriter.close(), subsequent calls to ParquetWriter.getDataSize result in a NullPointerException.\n\njava.lang.NullPointerException\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.getDataSize(InternalParquetRecordWriter.java:132)\n\tat org.apache.parquet.hadoop.ParquetWriter.getDataSize(ParquetWriter.java:314)\n\tat FileBufferState.getFileSizeInBytes(FileBufferState.scala:83)\n\n\nThe reason for the NPE appears to be in InternalParquetRecordWriter.getDataSize, where it assumes that columnStore is not null.\nBut the close() method calls flushRowGroupToStore() which sets columnStore = null.\nI'm guessing that once the file is closed, we can just return lastRowGroupEndPos since there should be no more buffered data, but I don't fully understand how this class works.",
        "Issue Links": [
            "/jira/browse/PARQUET-308"
        ]
    },
    "PARQUET-861": {
        "Key": "PARQUET-861",
        "Summary": "Document INT96 timestamps",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/Feb/17 18:39",
        "Updated": "22/Mar/18 22:24",
        "Resolved": null,
        "Description": "Although considered as deprecated, they should be documented as the format is quite special.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/49"
        ]
    },
    "PARQUET-862": {
        "Key": "PARQUET-862",
        "Summary": "Provide defaut cache size values if CPU info probing is not available",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Marc Vertes",
        "Reporter": "Marc Vertes",
        "Created": "04/Feb/17 16:12",
        "Updated": "04/Feb/17 23:30",
        "Resolved": "04/Feb/17 23:29",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-863": {
        "Key": "PARQUET-863",
        "Summary": "[C++] Move SIMD, CPU info, hashing, and other generic utilities into Apache Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Feb/17 19:32",
        "Updated": "07/Jul/17 20:30",
        "Resolved": "07/Jul/17 20:30",
        "Description": "These tools are also needed in Arrow for dictionary encoding. This will ease code maintenance",
        "Issue Links": []
    },
    "PARQUET-864": {
        "Key": "PARQUET-864",
        "Summary": "[C++] Consolidate non-Parquet-specific bit utility code into Apache Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Feb/17 23:24",
        "Updated": "07/Jul/17 20:31",
        "Resolved": "07/Jul/17 20:31",
        "Description": "i.e. parquet/util/bit-util.h",
        "Issue Links": []
    },
    "PARQUET-865": {
        "Key": "PARQUET-865",
        "Summary": "C++: Pass all CXXFLAGS to Thrift ExternalProject",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Feb/17 09:36",
        "Updated": "06/Feb/17 20:36",
        "Resolved": "06/Feb/17 20:36",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-866": {
        "Key": "PARQUET-866",
        "Summary": "[C++] Account for API changes in ARROW-33",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Feb/17 15:13",
        "Updated": "06/Feb/17 22:34",
        "Resolved": "06/Feb/17 22:34",
        "Description": "Patch forthcoming",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-867": {
        "Key": "PARQUET-867",
        "Summary": "[C++] Support writing sliced Arrow arrays",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "06/Feb/17 19:48",
        "Updated": "09/Feb/17 20:26",
        "Resolved": "09/Feb/17 20:26",
        "Description": "Follow up to ARROW-33. We may want to explicitly disallow any non-zero slice offsets until then",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-868": {
        "Key": "PARQUET-868",
        "Summary": "C++: Build snappy with optimizations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "07/Feb/17 14:44",
        "Updated": "08/Feb/17 08:34",
        "Resolved": "08/Feb/17 08:34",
        "Description": "Build snappy with -O1 on OSX and -O2 otherwise to increase the performance. -O2 seems to trigger a compiler bug on OSX, -O1 works though.",
        "Issue Links": []
    },
    "PARQUET-869": {
        "Key": "PARQUET-869",
        "Summary": "Min/Max record counts for block size checks are not configurable",
        "Type": "Improvement",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Pradeep Gollakota",
        "Created": "08/Feb/17 07:48",
        "Updated": "30/Nov/20 06:52",
        "Resolved": null,
        "Description": "While the min/max record counts for page size check are configurable via ParquetOutputFormat.MIN_ROW_COUNT_FOR_PAGE_SIZE_CHECK and ParquetOutputFormat.MAX_ROW_COUNT_FOR_PAGE_SIZE_CHECK configs and via ParquetProperties directly, the min/max record counts for block size check are hard coded inside InternalParquetRecordWriter.\nThese two settings should also be configurable.",
        "Issue Links": [
            "/jira/browse/PARQUET-409",
            "https://github.com/apache/parquet-mr/pull/470",
            "https://github.com/apache/parquet-mr/pull/447",
            "https://github.com/apache/parquet-mr/pull/470"
        ]
    },
    "PARQUET-870": {
        "Key": "PARQUET-870",
        "Summary": "ParquetFileWriter.appendFile does not close file handle - resource leak",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "George Smith",
        "Created": "08/Feb/17 08:57",
        "Updated": "12/Dec/19 10:47",
        "Resolved": null,
        "Description": "Current implementation of the method ParquetFileWriter.appendFile does not close file handle.\n\nParquetFileReader.open(conf, file).appendTo(this);\n\nMy suggested fix is following:\n\ntry (ParquetFileReader reader = ParquetFileReader.open(conf, file)) {\n   reader.appendTo(this);\n}",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/708"
        ]
    },
    "PARQUET-871": {
        "Key": "PARQUET-871",
        "Summary": "[C++] Mechanism to avoid passing -Werror to thirdparty libraries",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "08/Feb/17 21:23",
        "Updated": "21/Feb/17 21:54",
        "Resolved": "21/Feb/17 21:54",
        "Description": "xhochy I noticed after PARQUET-865 that my builds fail because I pass -Werror in $CXXFLAGS. I have\n\nexport CXXFLAGS=\"-Werror -Wall -fno-omit-frame-pointer\"\n\n\nHow do we pass compiler flags to parquet-cpp only and not also the thirdparty libraries?",
        "Issue Links": []
    },
    "PARQUET-872": {
        "Key": "PARQUET-872",
        "Summary": "[C++] Having any $CXXFLAGS set overrides $SNAPPY_CXXFLAGS",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "08/Feb/17 22:05",
        "Updated": "09/Feb/17 14:22",
        "Resolved": "09/Feb/17 14:22",
        "Description": "If there is $CXXFLAGS set in the environment, then $SNAPPY_CXXFLAGS is not respected\nsteps to reproduce:\n\nexport CXXFLAGS=\"\"\ncmake $PARQUET_DIR\nmake snappy_ep",
        "Issue Links": []
    },
    "PARQUET-873": {
        "Key": "PARQUET-873",
        "Summary": "[C++] Reading metadata-only files (0 data rows) in parquet::arrow segfaults",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/17 03:11",
        "Updated": "13/Feb/17 04:17",
        "Resolved": "13/Feb/17 04:17",
        "Description": "See attached. These get written by Spark, for example, and break pyarrow.parquet. Also attaching the gdb backtrace",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-874": {
        "Key": "PARQUET-874",
        "Summary": "[C++] Use default memory allocator from Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/17 15:44",
        "Updated": "11/Feb/17 21:52",
        "Resolved": "11/Feb/17 21:52",
        "Description": "cleanup after ARROW-521",
        "Issue Links": []
    },
    "PARQUET-875": {
        "Key": "PARQUET-875",
        "Summary": "[C++] Fix coveralls build given changes to thirdparty build procedure",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "09/Feb/17 20:24",
        "Updated": "11/Feb/17 14:20",
        "Resolved": "11/Feb/17 14:20",
        "Description": "The coverage reports include a ton of noise that needs appropriate exclusions. see https://coveralls.io/github/apache/parquet-cpp?branch=master",
        "Issue Links": []
    },
    "PARQUET-876": {
        "Key": "PARQUET-876",
        "Summary": "C++: Correct snapshot version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "13/Feb/17 17:29",
        "Updated": "13/Feb/17 18:34",
        "Resolved": "13/Feb/17 18:34",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-877": {
        "Key": "PARQUET-877",
        "Summary": "C++: Update Arrow Hash, update Version in metadata.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "13/Feb/17 18:29",
        "Updated": "14/Feb/17 09:49",
        "Resolved": "14/Feb/17 09:49",
        "Description": "Last release preparations",
        "Issue Links": []
    },
    "PARQUET-878": {
        "Key": "PARQUET-878",
        "Summary": "C++: Remove setup_build_env from rc-verification script",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/Feb/17 09:21",
        "Updated": "14/Feb/17 13:58",
        "Resolved": "14/Feb/17 13:58",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-879": {
        "Key": "PARQUET-879",
        "Summary": "C++: ExternalProject compilation for Thrift fails on older CMake versions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/Feb/17 09:50",
        "Updated": "14/Feb/17 13:45",
        "Resolved": "14/Feb/17 13:45",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-880": {
        "Key": "PARQUET-880",
        "Summary": "[CPP] Prevent destructors from throwing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "14/Feb/17 22:45",
        "Updated": "16/Feb/17 14:34",
        "Resolved": "16/Feb/17 14:34",
        "Description": "In C++, a program can raise abort if destructors throw an exception.\nA scenario is: Pending Exception \"e\" causing the stack to unwind and invoking a destructor that throws again.",
        "Issue Links": []
    },
    "PARQUET-881": {
        "Key": "PARQUET-881",
        "Summary": "C++: Update Arrow hash to 0.2.0-rc2",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "15/Feb/17 18:58",
        "Updated": "16/Feb/17 14:34",
        "Resolved": "16/Feb/17 14:34",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-882": {
        "Key": "PARQUET-882",
        "Summary": "[CPP] Improve Application Version parsing",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "15/Feb/17 21:38",
        "Updated": "21/Feb/17 14:23",
        "Resolved": "21/Feb/17 14:23",
        "Description": "The build details, pre-version are not being parsed.\nThe scope is to use regular expressions from parquet-mr to keep it consistent.",
        "Issue Links": []
    },
    "PARQUET-883": {
        "Key": "PARQUET-883",
        "Summary": "C++: Support non-standard gcc version strings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "16/Feb/17 10:15",
        "Updated": "16/Feb/17 14:35",
        "Resolved": "16/Feb/17 14:35",
        "Description": "There are some GCC versions in the wild that use gcc-Version\u00a0vs gcc version\u00a0in their version string. These are ones built with a different locale.\nNot a blocker as these are very rare occasions and rather more builds of GCC with uncommon/wrong flags.",
        "Issue Links": []
    },
    "PARQUET-884": {
        "Key": "PARQUET-884",
        "Summary": "Add support for Decimal datatype to Parquet-Pig record reader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-pig",
        "Assignee": "Ellen Kletscher",
        "Reporter": "Ellen Kletscher",
        "Created": "16/Feb/17 19:12",
        "Updated": "07/Jun/17 22:42",
        "Resolved": "07/Jun/17 22:23",
        "Description": "parquet.pig.ParquetLoader defaults the Parquet decimal datatype to bytearray.  Would like to add support to convert to BigDecimal instead, which will turn garbage bytearrays into actual numbers.",
        "Issue Links": []
    },
    "PARQUET-885": {
        "Key": "PARQUET-885",
        "Summary": "[C++] Do not search for Thrift in default system paths",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Feb/17 21:47",
        "Updated": "21/Feb/17 14:18",
        "Resolved": "21/Feb/17 14:18",
        "Description": "Because of the likelihood that a Thrift compiled elsewhere may not have been built with -fPIC, I recommend we use NO_DEFAULT_PATH in FindThrift.cmake to force the external project build if the $THRIFT_HOME environment variable is not set",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-886": {
        "Key": "PARQUET-886",
        "Summary": "[C++] Revise build documentation and requirements in README.md",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Feb/17 21:49",
        "Updated": "21/Feb/17 14:32",
        "Resolved": "21/Feb/17 14:32",
        "Description": "The README must list Boost dependencies (and how to install them)",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-887": {
        "Key": "PARQUET-887",
        "Summary": "C++: Fix issues in release scripts arise in RC1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Feb/17 12:13",
        "Updated": "11/Mar/17 15:52",
        "Resolved": "11/Mar/17 15:52",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-888": {
        "Key": "PARQUET-888",
        "Summary": "C++ Memory leak in RowGroupSerializer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Thomas Sanchez",
        "Reporter": "Thomas Sanchez",
        "Created": "21/Feb/17 15:41",
        "Updated": "21/Feb/17 21:59",
        "Resolved": "21/Feb/17 21:59",
        "Description": "The class `::parquet::RowGroupSerializer` (writer-internal.h) inherits from `::parquet::RowGroupWriter::Content` however, the `Content` class does not have a virtual dtor leading to a leak when the `::parquet::RowGroupWriter` is closed.",
        "Issue Links": []
    },
    "PARQUET-889": {
        "Key": "PARQUET-889",
        "Summary": "Fix compilation when PARQUET_USE_SSE is on",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Thomas Sanchez",
        "Reporter": "Thomas Sanchez",
        "Created": "22/Feb/17 09:23",
        "Updated": "22/Feb/17 18:59",
        "Resolved": "22/Feb/17 18:59",
        "Description": "bit-util.h is missing a couple of includes.",
        "Issue Links": []
    },
    "PARQUET-890": {
        "Key": "PARQUET-890",
        "Summary": "C++: Support I/O of DATE columns in parquet_arrow",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "22/Feb/17 13:35",
        "Updated": "07/Mar/17 20:22",
        "Resolved": "07/Mar/17 20:22",
        "Description": "Currently we can only write Timestamp columns but no DATE columns to/from Arrow. This did not occur earlier as the Pandas<->Parquet tests never pass actuall date columns but Timestamp columns to Parquet.",
        "Issue Links": []
    },
    "PARQUET-891": {
        "Key": "PARQUET-891",
        "Summary": "[C++] Do not search for Snappy in default system paths",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Feb/17 18:32",
        "Updated": "23/Feb/17 07:47",
        "Resolved": "23/Feb/17 07:47",
        "Description": "As in PARQUET-885, Linux package managers do not consistently build libsnappy.a with -fPIC, which makes packages such as libsnappy-dev not viable for building the {[libparquet.so}} shared library. Because we can build Snappy ourselves, I will change the CMake module to not search in default system paths for Snappy and respect $SNAPPY_HOME if it's set",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-892": {
        "Key": "PARQUET-892",
        "Summary": "[C++] Clean up link library targets in CMake files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Feb/17 21:13",
        "Updated": "23/Feb/17 08:22",
        "Resolved": "23/Feb/17 08:22",
        "Description": "See mailing list discussion today. \nIn \nhttps://github.com/apache/parquet-cpp/blob/master/CMakeLists.txt#L623\nthe dependencies are being passed with LINK_PRIVATE to parquet_static \u2013 because static libs do not bundle their dependencies, I believe that the link libraries need to be passed to LINK_PUBLIC. I will put up a patch and see if that resolves the issue that the contributor is facing",
        "Issue Links": []
    },
    "PARQUET-893": {
        "Key": "PARQUET-893",
        "Summary": "GroupColumnIO.getFirst() doesn't check for empty groups",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Cheng Lian",
        "Created": "23/Feb/17 07:17",
        "Updated": "23/Feb/17 07:19",
        "Resolved": null,
        "Description": "The following Spark snippet reproduces this issue with Spark 2.1 (with parquet-mr 1.8.1) and Spark 2.2-SNAPSHOT (with parquet-mr 1.8.2):\n\nimport org.apache.spark.sql.types._\n\nval path = \"/tmp/parquet-test\"\n\ncase class Inner(f00: Int)\ncase class Outer(f0: Inner, f1: Int)\n\nval df = Seq(Outer(Inner(1), 1)).toDF()\n\ndf.printSchema()\n// root\n//  |-- f0: struct (nullable = true)\n//  |    |-- f00: integer (nullable = false)\n//  |-- f1: integer (nullable = false)\n\ndf.write.mode(\"overwrite\").parquet(path)\n\nval requestedSchema =\n  new StructType().\n    add(\"f0\", new StructType().\n      // This nested field name differs from the original one\n      add(\"f01\", IntegerType)).\n    add(\"f1\", IntegerType)\n\nprintln(requestedSchema.treeString)\n// root\n//  |-- f0: struct (nullable = true)\n//  |    |-- f01: integer (nullable = true)\n//  |-- f1: integer (nullable = true)\n\nspark.read.schema(requestedSchema).parquet(path).show()\n\n\nIn the above snippet, requestedSchema is compatible with the schema of the written Parquet file, but the following exception is thrown:\n\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/parquet-test/part-00007-d2b0bec1-7be5-4b51-8d53-3642680bc9c2.snappy.parquet\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:243)\n        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)\n        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:184)\n        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n        at org.apache.spark.scheduler.Task.run(Task.scala:99)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.ArrayList.rangeCheck(ArrayList.java:653)\n        at java.util.ArrayList.get(ArrayList.java:429)\n        at org.apache.parquet.io.GroupColumnIO.getFirst(GroupColumnIO.java:102)\n        at org.apache.parquet.io.GroupColumnIO.getFirst(GroupColumnIO.java:102)\n        at org.apache.parquet.io.PrimitiveColumnIO.getFirst(PrimitiveColumnIO.java:102)\n        at org.apache.parquet.io.PrimitiveColumnIO.isFirst(PrimitiveColumnIO.java:97)\n        at org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:277)\n        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:135)\n        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:101)\n        at org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:154)\n        at org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:101)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:140)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:214)\n        ... 21 more\n\n\nAccording to this stack trace, it seems that GroupColumnIO.getFirst() doesn't check for empty groups properly.\nI haven't tried parquet-mr 1.9.0 but it probably suffers from the same issue.",
        "Issue Links": []
    },
    "PARQUET-894": {
        "Key": "PARQUET-894",
        "Summary": "Fix compilation warning",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Thomas Sanchez",
        "Reporter": "Thomas Sanchez",
        "Created": "23/Feb/17 08:50",
        "Updated": "28/Feb/17 18:41",
        "Resolved": "28/Feb/17 18:41",
        "Description": "parquet/encoding.h has a cv-qualifier on a return type that has no effect and raising warning.",
        "Issue Links": []
    },
    "PARQUET-895": {
        "Key": "PARQUET-895",
        "Summary": "Reading of nested columns is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Marc Vertes",
        "Reporter": "Marc Vertes",
        "Created": "23/Feb/17 13:31",
        "Updated": "23/Feb/17 17:37",
        "Resolved": "23/Feb/17 17:37",
        "Description": "Problem occurs when reading a nested column with repeated values, specially when there is much more levels in that column than the number of global rows.\nCiting @peshopetrov, who filed a github pull request identifying the problem and proposing a fix:\nNested repeated columns' count is incorrectly read from row group's metadata. That's correct in cases where there aren't any nested repeated fields but is generally not correct. Instead the num_values from the column's metadata should be used.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-896": {
        "Key": "PARQUET-896",
        "Summary": "[C++] Investigate code sharing opportunities amongst Impala, Kudu, Parquet, Arrow",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Feb/17 18:53",
        "Updated": "12/Nov/18 22:11",
        "Resolved": "12/Nov/18 22:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-897": {
        "Key": "PARQUET-897",
        "Summary": "[C++] Only use designated public headers from libarrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "25/Feb/17 23:11",
        "Updated": "20/Mar/17 14:07",
        "Resolved": "20/Mar/17 14:07",
        "Description": "It would be better to include a arrow/api.h in parquet/arrow instead of including granular headers, so that code can move around in Arrow without breaking Parquet builds",
        "Issue Links": []
    },
    "PARQUET-898": {
        "Key": "PARQUET-898",
        "Summary": "[C++] Change Travis CI OS X image to Xcode 6.4 and fix our thirdparty build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Feb/17 19:11",
        "Updated": "16/Apr/17 19:21",
        "Resolved": "16/Apr/17 19:21",
        "Description": "The unit tests do not build out-of-the-box using our bundled thirdparty builds on Xcode 6.4. This would been caught but our Travis CI setup switched to Xcode 7.3 last fall.",
        "Issue Links": []
    },
    "PARQUET-899": {
        "Key": "PARQUET-899",
        "Summary": "Add metadata field describing the application that wrote the file",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "27/Feb/17 14:42",
        "Updated": "13/Aug/18 12:38",
        "Resolved": "13/Aug/18 12:38",
        "Description": "Although the Parquet library should behave the same regardless of what application uses it, occasionally serious interoperability bugs are introduced in specific applications. For example, data written by a specific application may be unnecessarily adjusted or the calculated statistics may be invalid (both actual problems).\nUnfortunately, currently it is not possible to recognize Parquet files affected by application problems because the metadata does not contain any information about the application using the Parquet library. (The name and version number of the Parquet library is recorded, but that only has limited use, because apart from Impala, the most widespread Parquet writers all use the same Java library.)\nTo allow creating workarounds for future known issues, we should introduce new metadata fields that applications can populate. The simplest approach is to have one field for the application name and another for its version number. A more sophisticated approach suggested by julienledem could also reference a list of earlier issues that are known to be fixed in the application that wrote the Parquet file.",
        "Issue Links": [
            "/jira/browse/PARQUET-352"
        ]
    },
    "PARQUET-900": {
        "Key": "PARQUET-900",
        "Summary": "C++: Fix NOTICE / LICENSE issues",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "28/Feb/17 18:01",
        "Updated": "03/Mar/17 14:31",
        "Resolved": "03/Mar/17 14:31",
        "Description": "cpplint.py has a copyright notice, \u201cCopyright (c) 2009 Google Inc.\u201d\nand a 3-clause BSD license that is missing for LICENSE.txt\nNOTICE.txt contains license information that should only be in\nLICENSE.txt (the inclusion of PFOR code). ASF guidance states: \u201cDo not\nadd anything to NOTICE which is not legally required\u201d so this should\nbe removed from the candidate.\n.travis.yml has no header",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-901": {
        "Key": "PARQUET-901",
        "Summary": "C++: Publish RCs in apache-parquet-VERSION in SVN",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "28/Feb/17 18:24",
        "Updated": "03/Mar/17 14:33",
        "Resolved": "03/Mar/17 14:33",
        "Description": "Update the release script to generate differently named folders.",
        "Issue Links": [
            "/jira/browse/PARQUET-713"
        ]
    },
    "PARQUET-902": {
        "Key": "PARQUET-902",
        "Summary": "[C++] Move compressor interfaces into Apache Arrow",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "01/Mar/17 15:41",
        "Updated": "07/Jul/17 20:30",
        "Resolved": "07/Jul/17 20:30",
        "Description": "We also need to be able to compress data in Arrow",
        "Issue Links": []
    },
    "PARQUET-903": {
        "Key": "PARQUET-903",
        "Summary": "C++: Add option to set RPATH to ORIGIN",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Mar/17 18:52",
        "Updated": "07/Mar/17 00:41",
        "Resolved": "07/Mar/17 00:41",
        "Description": "Made this an option as the actual RPATH value differs between OSX and Linux.",
        "Issue Links": []
    },
    "PARQUET-904": {
        "Key": "PARQUET-904",
        "Summary": "Define INT96 ordering",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "06/Mar/17 19:02",
        "Updated": "06/Mar/17 19:02",
        "Resolved": null,
        "Description": "Currently int96 binary ordering doesn't match its natural ordering.\nWe should either specify this or declare int96 not ordered and link to the type replacing it.",
        "Issue Links": [
            "/jira/browse/PARQUET-323"
        ]
    },
    "PARQUET-905": {
        "Key": "PARQUET-905",
        "Summary": "Add \"Floating Timestamp\" logical type",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "06/Mar/17 19:05",
        "Updated": "28/May/18 13:05",
        "Resolved": "28/May/18 13:03",
        "Description": "Unlike current Parquet Timestamp stored in UTC, a \"floating timestamp\" has no timezone, it is up to the reader to interpret the timestamps based on their timezone.\nThis is the behavior of a Timestamp in the sql standard",
        "Issue Links": [
            "/jira/browse/PARQUET-1253",
            "/jira/browse/ARROW-637"
        ]
    },
    "PARQUET-906": {
        "Key": "PARQUET-906",
        "Summary": "add logical type timestamp with timezone (per SQL)",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "06/Mar/17 19:06",
        "Updated": "10/Oct/17 20:03",
        "Resolved": null,
        "Description": "timestamp with timezone (per SQL)\ntimestamps are adjusted to UTC and stored as integers.\nmetadata in logical types PR:\nSee discussion here: https://github.com/apache/parquet-format/pull/51#discussion_r109667837",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-699",
            "https://github.com/apache/parquet-format/pull/51"
        ]
    },
    "PARQUET-907": {
        "Key": "PARQUET-907",
        "Summary": "Optionally store Page level metadata in the footer to enable predicate pushdowns",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "06/Mar/17 19:08",
        "Updated": "06/Mar/17 19:08",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-908": {
        "Key": "PARQUET-908",
        "Summary": "Fix for PARQUET-890 introduces undefined symbol in libparquet_arrow.so",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Jeff Knupp",
        "Created": "08/Mar/17 15:10",
        "Updated": "09/Mar/17 00:06",
        "Resolved": "09/Mar/17 00:06",
        "Description": "After commit c41a718dae9c60465ea0d8c99d6e3bdca11f802f (the most recent commit for parquet-cpp which fixed PARQUET-890, the following error occurs when pyarrow tries to load libparquet_arrow.so:\n\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pyarrow.parquet\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/pyarrow-0.2.1.dev17+ngb109a24-py2.7-linux-x86_64.egg/pyarrow/parquet.py\", line 20, in <module>\n    from pyarrow._parquet import (ParquetReader, FileMetaData,  # noqa\nImportError: /usr/local/lib/libparquet_arrow.so: undefined symbol: _ZN7parquet19LogicalTypeToStringB5cxx11ENS_11LogicalType4typeE\n\n\nBuilding the library using the previous commit doesn't exhibit this problem.",
        "Issue Links": []
    },
    "PARQUET-909": {
        "Key": "PARQUET-909",
        "Summary": "[CPP]: Reduce buffer allocations (mallocs) on critical path",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "10/Mar/17 17:18",
        "Updated": "17/Mar/17 22:09",
        "Resolved": "17/Mar/17 22:09",
        "Description": "The current implementation allocates and frees memory many times on the critical path. The scope of this JIRA is to reuse buffers where possible.\nOn production systems, too many mallocs/frees can impact performance.",
        "Issue Links": []
    },
    "PARQUET-910": {
        "Key": "PARQUET-910",
        "Summary": "C++: Support TIME logical type in parquet_arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "11/Mar/17 15:53",
        "Updated": "25/Apr/17 21:47",
        "Resolved": "25/Apr/17 21:47",
        "Description": null,
        "Issue Links": [
            "/jira/browse/ARROW-601",
            "/jira/browse/PARQUET-930"
        ]
    },
    "PARQUET-911": {
        "Key": "PARQUET-911",
        "Summary": "C++: Support nested structs in parquet_arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "11/Mar/17 15:53",
        "Updated": "22/Jun/17 16:14",
        "Resolved": "22/Jun/17 16:14",
        "Description": null,
        "Issue Links": [
            "/jira/browse/ARROW-601"
        ]
    },
    "PARQUET-912": {
        "Key": "PARQUET-912",
        "Summary": "C++: Check in release that the correct author is set in git",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "11/Mar/17 17:09",
        "Updated": "11/Mar/17 17:09",
        "Resolved": null,
        "Description": "We should check if the correct author to sign the release tarball is set as the git author. Not sure how we can detect that but I already run twice into the problem that the git author was not the same as I signed the tarball with and the release script aborted half-way. This was not catched in the dry-run mode.",
        "Issue Links": []
    },
    "PARQUET-913": {
        "Key": "PARQUET-913",
        "Summary": "C++: Add apache-parquet-cpp prefix to download URL in the release verification script",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "11/Mar/17 17:14",
        "Updated": "15/Mar/17 14:15",
        "Resolved": "15/Mar/17 14:15",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-914": {
        "Key": "PARQUET-914",
        "Summary": "[C++] Throw more informative exception when user writes too many values to a column in a row group",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "YE",
        "Reporter": "Wes McKinney",
        "Created": "13/Mar/17 22:02",
        "Updated": "05/May/17 06:16",
        "Resolved": "05/May/17 06:16",
        "Description": "In https://github.com/apache/parquet-cpp/blob/5e59bc5c6491a7505585c08fd62aa52f9a6c9afc/src/parquet/column/writer.cc#L159 if the user writes more values than the size of the row group, the message in the exception raised is misleading",
        "Issue Links": []
    },
    "PARQUET-915": {
        "Key": "PARQUET-915",
        "Summary": "Support Arrow Time Types in Schema",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Miki Tebeka",
        "Created": "14/Mar/17 09:44",
        "Updated": "25/Apr/17 06:40",
        "Resolved": "25/Apr/17 06:40",
        "Description": "Support Time with MILLI and MICRO TimeUnit in arrow conversion.\nSee also ARROW-601",
        "Issue Links": [
            "/jira/browse/PARQUET-930"
        ]
    },
    "PARQUET-916": {
        "Key": "PARQUET-916",
        "Summary": "C++: Sync merge script changes from Arrow",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Mar/17 17:26",
        "Updated": "11/Nov/18 22:16",
        "Resolved": "11/Nov/18 22:16",
        "Description": "Apache Arrow has the same script but with Python 3, we should sync the changes.",
        "Issue Links": []
    },
    "PARQUET-917": {
        "Key": "PARQUET-917",
        "Summary": "C++: Build parquet_arrow by default",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "22/Mar/17 07:56",
        "Updated": "23/Mar/17 13:47",
        "Resolved": "23/Mar/17 13:47",
        "Description": "This not being enabled by default is a major hurdle people experience while building pyarrow. As we already hard-depend on Apache Arrow, I would really like to always build the parquet_arrow lib. Having the option to disable it definitely helps packagers and parquet-cpp consumers.\nwesmckinn mdeepak any opinions on this? I would just change the default from OFF to ON.",
        "Issue Links": []
    },
    "PARQUET-918": {
        "Key": "PARQUET-918",
        "Summary": "FromParquetSchema API crashes on nested schemas",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.0.0",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Itai Incze",
        "Reporter": "Itai Incze",
        "Created": "22/Mar/17 08:52",
        "Updated": "10/Apr/17 18:29",
        "Resolved": "10/Apr/17 18:25",
        "Description": "FromParquetSchema@src/parquet/arrow/schema.cc:276 misbehaves by using its column_indices parameter in the second version of the function as indices to the direct schema root fields. \nThis is problematic with nested schema parquet files - the bug crashes the process by accessing the fields vector out of bounds.\nThis bug is masked by another bug in the first version of the FromParquetSchema function which constructs a complete indices list the size of the number of schema fields (instead of the # of columns).\nThe bug is triggered in many significant use-cases, for example when using the arrow::ReadTable API.",
        "Issue Links": []
    },
    "PARQUET-919": {
        "Key": "PARQUET-919",
        "Summary": "[C++] Account for API changes in ARROW-683",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Mar/17 14:54",
        "Updated": "23/Mar/17 13:47",
        "Resolved": "23/Mar/17 13:47",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-920": {
        "Key": "PARQUET-920",
        "Summary": "Getting Vistar Media Added to the Adopters page",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Philip Simmons",
        "Created": "23/Mar/17 18:17",
        "Updated": "23/Mar/17 18:17",
        "Resolved": null,
        "Description": "On this page of the Parquet website - https://parquet.apache.org/adopters/\nIt mentions opening up an issue with a linked tweet to get added to the page.\nWe at Vistar Media have been working with Parquet for a while, and recently put up a tweet and blog post about it  - https://twitter.com/vistarmedia/status/816342773465051137\nI wanted to see if we could be added to the page of Adopters.",
        "Issue Links": []
    },
    "PARQUET-921": {
        "Key": "PARQUET-921",
        "Summary": "[C++] Update conda-forge packages for 1.0.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "23/Mar/17 21:37",
        "Updated": "25/Mar/17 14:29",
        "Resolved": "25/Mar/17 14:29",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-922": {
        "Key": "PARQUET-922",
        "Summary": "Add index pages to the format to support efficient page skipping",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Marcel Kinard",
        "Reporter": "Julien Le Dem",
        "Created": "24/Mar/17 18:45",
        "Updated": "11/Feb/18 02:09",
        "Resolved": "16/Oct/17 23:49",
        "Description": "When a Parquet file is sorted we can define an index consisting of the boundary values for the pages of the columns sorted on as well as the offsets and length of said pages in the file.\nThe goal is to optimize lookup and range scan type queries, using this to read only the pages containing data matching the filter.\nWe'd require the pages to be aligned accross columns.\nmarcelk will add a link to the google doc to discuss the spec",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-1201",
            "/jira/browse/PARQUET-1207",
            "/jira/browse/IMPALA-5840",
            "/jira/browse/IMPALA-5842",
            "https://github.com/apache/parquet-format/pull/72",
            "https://docs.google.com/presentation/d/1UyKy07UT6vRbm4Mo6QSd7zfpMvSJ211TUB0vz-2e6aw/edit?usp=sharing"
        ]
    },
    "PARQUET-923": {
        "Key": "PARQUET-923",
        "Summary": "[C++] Account for Time metadata changes in ARROW-686",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "24/Mar/17 22:00",
        "Updated": "25/Mar/17 13:33",
        "Resolved": "25/Mar/17 13:33",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-924": {
        "Key": "PARQUET-924",
        "Summary": "[C++] Persist original type metadata from Arrow schemas",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Mar/17 00:07",
        "Updated": "09/Dec/19 16:01",
        "Resolved": "16/Aug/19 14:05",
        "Description": "This will enable us to convert back to the original type in some cases (DictionaryArray, Time with seconds)",
        "Issue Links": [
            "/jira/browse/ARROW-3246"
        ]
    },
    "PARQUET-925": {
        "Key": "PARQUET-925",
        "Summary": "[C++] FindArrow.cmake sets the wrong library path after ARROW-648",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "26/Mar/17 16:17",
        "Updated": "27/Mar/17 14:26",
        "Resolved": "26/Mar/17 23:34",
        "Description": "When building the Arrow libraries somewhere other than /usr, they are installed in a lib64\nhttps://github.com/apache/parquet-cpp/blob/master/cmake_modules/FindArrow.cmake#L34\ncc kou",
        "Issue Links": []
    },
    "PARQUET-926": {
        "Key": "PARQUET-926",
        "Summary": "[C++] Use pkg-config to find Apache Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "27/Mar/17 14:25",
        "Updated": "29/Mar/17 06:14",
        "Resolved": "29/Mar/17 06:14",
        "Description": "See also PARQUET-925.",
        "Issue Links": []
    },
    "PARQUET-927": {
        "Key": "PARQUET-927",
        "Summary": "[C++] Specify shared library version of Apache Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "29/Mar/17 14:51",
        "Updated": "29/Mar/17 22:19",
        "Resolved": "29/Mar/17 22:18",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-928": {
        "Key": "PARQUET-928",
        "Summary": "[C++] Support pkg-config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "29/Mar/17 15:28",
        "Updated": "29/Mar/17 22:20",
        "Resolved": "29/Mar/17 22:20",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-929": {
        "Key": "PARQUET-929",
        "Summary": "[C++] Handle arrow::DictionaryArray when writing Arrow data",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "29/Mar/17 19:09",
        "Updated": "17/Sep/17 17:55",
        "Resolved": "17/Sep/17 17:55",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-930": {
        "Key": "PARQUET-930",
        "Summary": "[C++] Account for all Arrow date/time types",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "29/Mar/17 19:09",
        "Updated": "12/May/17 12:57",
        "Resolved": "12/May/17 12:57",
        "Description": "Arrow 0.3 has some additional date / time array and metadata that we need to support more completely",
        "Issue Links": [
            "/jira/browse/PARQUET-972",
            "/jira/browse/PARQUET-915",
            "/jira/browse/PARQUET-910"
        ]
    },
    "PARQUET-931": {
        "Key": "PARQUET-931",
        "Summary": "[C++] Add option to pin thirdparty Arrow version used in ExternalProject",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Mar/17 15:01",
        "Updated": "18/Apr/17 04:28",
        "Resolved": "18/Apr/17 04:28",
        "Description": "This may help in cases where there is API but not ABI stability which building for package managers",
        "Issue Links": []
    },
    "PARQUET-932": {
        "Key": "PARQUET-932",
        "Summary": "[c++] Add option to build parquet library with minimal dependency",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "30/Mar/17 15:59",
        "Updated": "01/Apr/17 18:55",
        "Resolved": "01/Apr/17 18:55",
        "Description": "If users want to build only the parquet library, thirdparty headers are sufficient and GTEST and GBENCHMARK are not required.",
        "Issue Links": []
    },
    "PARQUET-933": {
        "Key": "PARQUET-933",
        "Summary": "[C++] Account for Arrow Table API changes coming in ARROW-728",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "30/Mar/17 16:26",
        "Updated": "30/Mar/17 21:32",
        "Resolved": "30/Mar/17 21:32",
        "Description": "Patch forthcoming",
        "Issue Links": []
    },
    "PARQUET-934": {
        "Key": "PARQUET-934",
        "Summary": "[C++] Support multiarch on Debian",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "31/Mar/17 13:30",
        "Updated": "31/Mar/17 21:31",
        "Resolved": "31/Mar/17 21:31",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-935": {
        "Key": "PARQUET-935",
        "Summary": "[C++] Set shared library version for .deb packages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "31/Mar/17 13:43",
        "Updated": "01/Apr/17 04:04",
        "Resolved": "01/Apr/17 04:04",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-936": {
        "Key": "PARQUET-936",
        "Summary": "[C++] parquet::arrow::WriteTable can enter infinite loop if chunk_size is 0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "YE",
        "Reporter": "Wes McKinney",
        "Created": "31/Mar/17 13:55",
        "Updated": "04/May/17 03:44",
        "Resolved": "04/May/17 03:44",
        "Description": "See also ARROW-723",
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-937": {
        "Key": "PARQUET-937",
        "Summary": "[C++] Support CMake < 3.4 again for Arrow detection",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "01/Apr/17 14:06",
        "Updated": "01/Apr/17 18:56",
        "Resolved": "01/Apr/17 18:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-938": {
        "Key": "PARQUET-938",
        "Summary": "[C++] There is a typo in cmake_modules/FindSnappy.cmake comment",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 05:06",
        "Updated": "02/Apr/17 08:12",
        "Resolved": "02/Apr/17 08:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-939": {
        "Key": "PARQUET-939",
        "Summary": "[C++] Support Thrift_HOME CMake variable like FindSnappy does as Snappy_HOME",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 05:29",
        "Updated": "02/Apr/17 12:17",
        "Resolved": "02/Apr/17 12:17",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-940": {
        "Key": "PARQUET-940",
        "Summary": "[C++] Fix Arrow library path detection",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 05:37",
        "Updated": "02/Apr/17 12:19",
        "Resolved": "02/Apr/17 12:19",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-941": {
        "Key": "PARQUET-941",
        "Summary": "[C++] Stop needless Boost static library detection for CentOS 7 support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 05:42",
        "Updated": "02/Apr/17 12:20",
        "Resolved": "02/Apr/17 12:20",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-942": {
        "Key": "PARQUET-942",
        "Summary": "[C++] Fix wrong variabe use in FindSnappy",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 05:47",
        "Updated": "02/Apr/17 12:22",
        "Resolved": "02/Apr/17 12:22",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-943": {
        "Key": "PARQUET-943",
        "Summary": "[C++] Overflow build error on x86",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "02/Apr/17 06:38",
        "Updated": "02/Apr/17 16:11",
        "Resolved": "02/Apr/17 16:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-944": {
        "Key": "ARROW-760",
        "Summary": "[Python] document differences w.r.t. fastparquet",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "Documentation,                                            Python",
        "Assignee": null,
        "Reporter": "Jeff Reback",
        "Created": "03/Apr/17 12:12",
        "Updated": "11/Jan/23 07:10",
        "Resolved": "23/Mar/18 00:38",
        "Description": "differences in options and/or actual written file formats w.r.t. https://fastparquet.readthedocs.io/en/latest/\n\nnull handling\nnon-supported type handling\noptions that can be passed via top-level functions",
        "Issue Links": []
    },
    "PARQUET-945": {
        "Key": "PARQUET-945",
        "Summary": "[C++] Thrift static libraries are not used with recent patch",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "03/Apr/17 18:16",
        "Updated": "04/Apr/17 20:42",
        "Resolved": "04/Apr/17 20:42",
        "Description": "Bug introduced in https://github.com/apache/parquet-cpp/pull/285",
        "Issue Links": []
    },
    "PARQUET-946": {
        "Key": "PARQUET-946",
        "Summary": "[C++] Refactoring in parquet::arrow::FileReader to be able to read a single row group",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Apr/17 16:07",
        "Updated": "06/Apr/17 07:27",
        "Resolved": "06/Apr/17 07:27",
        "Description": "Some applications may want to read a single row group at a time rather than reading an entire file",
        "Issue Links": []
    },
    "PARQUET-947": {
        "Key": "PARQUET-947",
        "Summary": "[C++] Refactor to account for ARROW-795 Arrow core library consolidation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Apr/17 20:11",
        "Updated": "10/Apr/17 15:05",
        "Resolved": "10/Apr/17 15:05",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-948": {
        "Key": "PARQUET-948",
        "Summary": "[C++] Account for API changes in ARROW-782",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Apr/17 01:10",
        "Updated": "10/Apr/17 15:06",
        "Resolved": "10/Apr/17 15:06",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-949": {
        "Key": "PARQUET-949",
        "Summary": "[C++] Arrow version pinning seems to not be working properly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Apr/17 15:07",
        "Updated": "10/Apr/17 16:06",
        "Resolved": "10/Apr/17 16:06",
        "Description": "I'm not sure why this is not working, going to change the external project to download snapshot tarballs",
        "Issue Links": []
    },
    "PARQUET-950": {
        "Key": "PARQUET-950",
        "Summary": "Mac build parquet failed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jinlian",
        "Created": "12/Apr/17 17:03",
        "Updated": "13/Apr/17 01:24",
        "Resolved": null,
        "Description": "build on macOS Sierra version 10.12 with Xcode 8.3.1 failed, got below message.\n[ 68%] Built target parquet_shared\n[ 68%] Linking CXX static library build/release/libparquet.a\n[ 68%] Built target parquet_static\n[ 68%] Building CXX object benchmarks/CMakeFiles/decode_benchmark.dir/decode_benchmark.cc.o\n[ 68%] Linking CXX executable ../build/release/decode_benchmark\n[ 68%] Built target decode_benchmark\n[ 68%] Building CXX object examples/CMakeFiles/reader-writer.dir/reader-writer.cc.o\n/Users/jinlianchen/EasilyDo/gitProject/new/parquet-cpp/examples/reader-writer.cc:275:14: warning: unused variable\n      'expected_value' [-Wunused-variable]\n        bool expected_value = ((i % 2) == 0) ? true : false;\n             ^\n/Users/jinlianchen/EasilyDo/gitProject/new/parquet-cpp/examples/reader-writer.cc:370:15: warning: unused variable\n      'expected_value' [-Wunused-variable]\n        float expected_value = i * 1.1;\n              ^\n/Users/jinlianchen/EasilyDo/gitProject/new/parquet-cpp/examples/reader-writer.cc:391:16: warning: unused variable\n      'expected_value' [-Wunused-variable]\n        double expected_value = i * 1.1111111;\n               ^\n/Users/jinlianchen/EasilyDo/gitProject/new/parquet-cpp/examples/reader-writer.cc:446:14: warning: unused variable\n      'expected_value' [-Wunused-variable]\n        char expected_value[FIXED_LENGTH] = \n{v, v, v, v, v, v, v, v, v, v}\n;\n             ^\n/Users/jinlianchen/EasilyDo/gitProject/new/parquet-cpp/examples/reader-writer.cc:242:9: warning: unused variable 'num_columns'\n[-Wunused-variable]\n    int num_columns = file_metadata->num_columns();\n        ^ \n5 warnings generated.\n[ 68%] Linking CXX executable ../build/release/reader-writer\nUndefined symbols for architecture x86_64:\n  \"arrow::io::ReadableFile::Open(std::_1::basic_string<char, std::1::char_traits<char>, std::1::allocator<char> > const&, arrow::MemoryPool*, std::_1::shared_ptr<arrow::io::ReadableFile>*)\", referenced from:\n      parquet::ParquetFileReader::OpenFile(std::_1::basic_string<char, std::1::char_traits<char>, std::1::allocator<char> > const&, bool, parquet::ReaderProperties const&, std::_1::shared_ptr<parquet::FileMetaData> const&) in libparquet.a(reader.cc.o)\n  \"arrow::io::FileOutputStream::Open(std::_1::basic_string<char, std::1::char_traits<char>, std::1::allocator<char> > const&, std::_1::shared_ptr<arrow::io::FileOutputStream>*)\", referenced from:\n      _main in reader-writer.cc.o\n  \"arrow::io::MemoryMappedFile::Open(std::_1::basic_string<char, std::1::char_traits<char>, std::1::allocator<char> > const&, arrow::io::FileMode::type, std::_1::shared_ptr<arrow::io::MemoryMappedFile>*)\", referenced from:\n      parquet::ParquetFileReader::OpenFile(std::_1::basic_string<char, std::1::char_traits<char>, std::1::allocator<char> > const&, bool, parquet::ReaderProperties const&, std::_1::shared_ptr<parquet::FileMetaData> const&) in libparquet.a(reader.cc.o)\nld: symbol(s) not found for architecture x86_64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nmake[2]: *** [build/release/reader-writer] Error 1\nmake[1]: *** [examples/CMakeFiles/reader-writer.dir/all] Error 2\nmake: *** [all] Error 2",
        "Issue Links": []
    },
    "PARQUET-951": {
        "Key": "PARQUET-951",
        "Summary": "Missing field id support in parquet metadata",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Qinghui Xu",
        "Created": "12/Apr/17 20:44",
        "Updated": "12/May/21 16:00",
        "Resolved": null,
        "Description": "Field id is essential for some serialization framework such as protobuf, and they are used to keep schema forward/backward compatibility which could not be achieved by using field names. Currently field id is not persisted as file metadata.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/410"
        ]
    },
    "PARQUET-952": {
        "Key": "PARQUET-952",
        "Summary": "Avro union with single type fails with 'is not a group'",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Jarek Jarcec Cecho",
        "Created": "13/Apr/17 00:08",
        "Updated": "19/Jun/18 13:55",
        "Resolved": "19/Jun/18 13:55",
        "Description": "When one uses Avro schema with a union that has only one type specified, the AvroParquetWriter throws an exception. See the following repro test case:\n\n  @Test\n  public void reproCase() throws Exception {\n    System.out.println(\"Parquet version: \" + Version.FULL_VERSION);\n\n    // Schema with a single field 'value' with type of union that have a single item (=string)\n    Schema avroSchema = Schema.parse(\"{\" +\n      \"\\\"type\\\": \\\"record\\\", \" +\n      \"\\\"name\\\": \\\"RandomRecord\\\", \" +\n      \"\\\"fields\\\": [\" +\n      \"{\\\"name\\\": \\\"value\\\", \\\"type\\\": [\\\"string\\\"] }\" +\n      \"]\" +\n      \"}\");\n\n    // Parquet writer\n    ParquetWriter parquetWriter = AvroParquetWriter.builder(path).withSchema(avroSchema)\n      .withConf(new Configuration())\n      .build();\n\n    GenericRecord record = new GenericRecordBuilder(avroSchema)\n      .set(\"value\", \"Surprise!\")\n      .build();\n\n    parquetWriter.write(record);\n  }\n\n\nWill result in:\n\nParquet version: parquet-mr version 1.9.0 (build 38262e2c80015d0935dad20f8e18f2d6f9fbd03c)\n\njava.lang.ClassCastException: required binary value (UTF8) is not a group\n\n\tat org.apache.parquet.schema.Type.asGroupType(Type.java:202)\n\tat org.apache.parquet.avro.AvroWriteSupport.writeValueWithoutConversion(AvroWriteSupport.java:357)\n\tat org.apache.parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:274)\n\tat org.apache.parquet.avro.AvroWriteSupport.writeRecordFields(AvroWriteSupport.java:187)\n\tat org.apache.parquet.avro.AvroWriteSupport.write(AvroWriteSupport.java:161)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\n\tat org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:292)\n\tat net.jarcec.AvroParquet.reproCase(AvroParquet.java:49)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\n\tat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51)\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\n\n\nI'm attaching a small maven project with all the dependencies to make it easier to reproduce locally.\nTrying to isolate the problem further, it seems that the AvroSchemaConverter converts the Avro schema to just required binary value (UTF8); (e.g. primitive type). But then the writer will go based on the Avro schema (which is a union) and tries to call asGroupType() on the primite type.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/459"
        ]
    },
    "PARQUET-953": {
        "Key": "PARQUET-953",
        "Summary": "[C++] Change arrow::FileWriter API to be initialized from a Schema, and provide for writing multiple tables",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "13/Apr/17 03:48",
        "Updated": "13/Apr/17 15:14",
        "Resolved": "13/Apr/17 15:14",
        "Description": "This would also allow for writing metadata-only files, which I need for testing ARROW-528",
        "Issue Links": [
            "/jira/browse/ARROW-528"
        ]
    },
    "PARQUET-954": {
        "Key": "PARQUET-954",
        "Summary": "C++: Use Brolti 0.6 release",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "15/Apr/17 15:42",
        "Updated": "18/Apr/17 17:11",
        "Resolved": "18/Apr/17 17:11",
        "Description": "Now that we finally have a new Brotli release, we should depend on this and not a specific commit hash.",
        "Issue Links": []
    },
    "PARQUET-955": {
        "Key": "PARQUET-955",
        "Summary": "[C++] pkg_check_modules will override $ARROW_HOME if it is set in the environment",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/Apr/17 23:53",
        "Updated": "16/Apr/17 17:34",
        "Resolved": "16/Apr/17 17:34",
        "Description": "If the user has set $ARROW_HOME, it may be ignored if there is a prior system-level Arrow installation that can be detected by pkg_check_modules. This can result in confusing build failures",
        "Issue Links": []
    },
    "PARQUET-956": {
        "Key": "PARQUET-956",
        "Summary": "C++: BUILD_BYPRODUCTS not specified anymore for gtest",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Apr/17 15:20",
        "Updated": "18/Apr/17 17:12",
        "Resolved": "18/Apr/17 17:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-957": {
        "Key": "PARQUET-957",
        "Summary": "[C++] Add optional $PARQUET_BUILD_TOOLCHAIN environment variable option for configuring build environment",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "18/Apr/17 16:03",
        "Updated": "21/Apr/17 01:17",
        "Resolved": "21/Apr/17 01:17",
        "Description": "Some users will install a centralized build toolchain containing all of the build dependencies for Parquet. We should provide an environment variable option to help with this, rather than having to set each thirdparty $FOO_HOME variable separately.\nsee also ARROW-849",
        "Issue Links": []
    },
    "PARQUET-958": {
        "Key": "PARQUET-958",
        "Summary": "[C++] Print Parquet metadata in JSON format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "18/Apr/17 18:34",
        "Updated": "25/Apr/17 06:35",
        "Resolved": "25/Apr/17 06:35",
        "Description": "Extend the current parquet_reader to print metadata in JSON format",
        "Issue Links": []
    },
    "PARQUET-959": {
        "Key": "PARQUET-959",
        "Summary": "[C++] Arrow thirdparty build fails on multiarch systems",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Apr/17 00:08",
        "Updated": "20/Apr/17 00:44",
        "Resolved": "20/Apr/17 00:44",
        "Description": "See build failure: https://circleci.com/gh/wesm/parquet-cpp-feedstock/15. Must set CMAKE_INSTALL_LIBDIR now",
        "Issue Links": []
    },
    "PARQUET-960": {
        "Key": "PARQUET-960",
        "Summary": "Could not find the parquet_arrow library.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Abdul Rahman",
        "Created": "20/Apr/17 01:52",
        "Updated": "11/Nov/18 22:16",
        "Resolved": "11/Nov/18 22:16",
        "Description": "Following through the instructions of building pyarrow with parquet. first built arrow cpp using instruction in readme with the following cmake \ncmake -DARROW_PYTHON=on -DCMAKE_INSTALL_PREFIX=$ARROW_HOME ..\nThen built parquet-cpp (separate location) using readme instructions. Then tried building pyarrow using python setup.py build_ext --with-parquet install  and get the error \nCould not find the parquet library. Could not find the parquet_arrow library. Did you build with -DPARQUET_ARROW=on? Looked in  system search paths.\nCMake Error at CMakeLists.txt:287 (message):\n  Unable to locate Parquet libraries\n$ARROW_HOME is set along with this export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ARROW_HOME/lib\nI can manually see the parquet_arrow libraries under \nparquet-cpp/build/debug/ (libparquet_arrow.so  , libparquet_arrow.so.1, libparquet_arrow.so.1.0.0). I also tried appending the parquet-cpp/build/debug path in LD_LIBRARY_PATH, but that didnt help\nI started to get this problem today after cloning the latest versions of arrow and parquet-cpp . Didnt notice this in the last couple of days",
        "Issue Links": []
    },
    "PARQUET-961": {
        "Key": "PARQUET-961",
        "Summary": "[C++] Strip debug symbols from libparquet libraries in release builds by default",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Apr/17 14:37",
        "Updated": "21/Apr/17 01:33",
        "Resolved": "21/Apr/17 01:33",
        "Description": "This will reduce the size of shipped binaries \u2013 debug symbols can always be restored using $PARQUET_CXXFLAGS",
        "Issue Links": []
    },
    "PARQUET-962": {
        "Key": "PARQUET-962",
        "Summary": "[C++] GTEST_MAIN_STATIC_LIB is not defined in FindGTest.cmake",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "20/Apr/17 20:49",
        "Updated": "21/Apr/17 01:37",
        "Resolved": "21/Apr/17 01:37",
        "Description": "Bug introduced in PR #299",
        "Issue Links": []
    },
    "PARQUET-963": {
        "Key": "PARQUET-963",
        "Summary": "[C++] Disallow reading struct types in Arrow reader for now",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "21/Apr/17 02:21",
        "Updated": "25/Apr/17 13:40",
        "Resolved": "25/Apr/17 13:40",
        "Description": "This bug surfaced in ARROW-601.",
        "Issue Links": []
    },
    "PARQUET-964": {
        "Key": "PARQUET-964",
        "Summary": "Using ProtoParquet with Hive / AWS Athena: ParquetDecodingException: totalValueCount '0' <= 0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Constantin Muraru",
        "Created": "24/Apr/17 22:57",
        "Updated": "26/Apr/17 17:14",
        "Resolved": "26/Apr/17 16:55",
        "Description": "Hi folks!\nWe're working on adding support for ProtoParquet to work with Hive / AWS Athena (Presto) [1]. The problem we've encountered appears whenever we declare a repeated field (array) or a map in the protobuf schema and we then try to convert it to parquet. The conversion works fine, but when we try to query the data with Hive/Presto, we get some freaky errors.\nWe've noticed though that AvroToParquet works great, even when we declare such fields (arrays, maps)! \nComparing the parquet schema generated by protobuf vs avro, we've noticed a few differences.\nTake the simple schema below (protobuf):\n\nmessage ListOfList {\n    string top_field = 1;\n    repeated MyInnerMessage first_array = 2;\n}\n\nmessage MyInnerMessage {\n    int32 inner_field = 1;\n    repeated int32 second_array = 2;\n}\n\n\nAfter using ProtoParquetWriter, the resulting parquet schema is the following:\n\nmessage TestProtobuf.ListOfList {\n  optional binary top_field (UTF8);\n  repeated group first_array {\n    optional int32 inner_field;\n    repeated int32 second_array;\n  }\n}\n\n\nWhen we try to query this data, we get parsing errors from Hive/Athena. The parsing errors are related to the array/map fields.\nHowever, if we create a similar avro schema, the parquet result of the AvroParquetWriter is the following:\n\nmessage TestProtobuf.ListOfList {\n  required binary top_field (UTF8);\n  required group first_array (LIST) {\n    repeated group array {\n      required int32 inner_field;\n      required group second_array (LIST) {\n        repeated int32 array;\n      }\n    }\n  }\n}\n\n\nThis works beautifully with Hive/Athena. Too bad our systems are stuck with protobuf  .\nYou can see the additional wrappers which are missing from protobuf: required group first_array (LIST).\nOur goal is to make the ProtoParquetWriter generate a parquet schema similar to what Avro is doing. We basically want to add these wrappers around lists/maps.\nEverything seemed to work great, until we've bumped into an issue. We tuned ProtoParquetWriter to generate the same parquet schema as AvroParquetWriter. However, one difference between protobuf and avro is that in protobuf we can have a bunch of Optional fields. \n\nmessage TestProtobuf.ListOfList {\n  optional binary top_field (UTF8);\n  required group first_array (LIST) {\n    repeated group array {\n      optional int32 inner_field;\n      required group second_array (LIST) {\n        repeated int32 array;\n      }\n    }\n  }\n}\n\n\nNotice the: optional int32 inner_field (for avro that was required).\nWhen testing with some real proto-parquet data, we get an error every time inner_field is not populated, but the second_array is.\n\nparquet-tools cat /tmp/test23.parquet\n\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/test23.parquet\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:223)\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:122)\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:126)\n\tat org.apache.parquet.tools.command.CatCommand.execute(CatCommand.java:79)\n\tat org.apache.parquet.proto.tools.Main.main(Main.java:214)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\nCaused by: org.apache.parquet.io.ParquetDecodingException: totalValueCount '0' <= 0\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:349)\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:82)\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:77)\n\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:272)\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:145)\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:107)\n\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:155)\n\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:107)\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:194)\n\t... 9 more\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/test23.parquet\n\n\nProcess finished with exit code 1\n\n\n\nBasically this errors occurs whenever the first_array.inner_field is not populated, but first_array.second_array is.\nI'm attaching the code used to generate the parquet files (though keep in mind that we're working on a fork atm).\nGoing through the code, I've noticed that the errors stop and everything seems to work fine, once I change this condition in ColumnReaderImpl: \nFrom:\n\nif (totalValueCount <= 0) {\n      throw new ParquetDecodingException(\"totalValueCount '\" + totalValueCount + \"' <= 0\");\n}\n\n\nTo:\n\nif (totalValueCount < 0) {\n      throw new ParquetDecodingException(\"totalValueCount '\" + totalValueCount + \"' < 0\");\n}\n\n\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnReaderImpl.java#L355\n--->\n\nparquet-tools cat /tmp/test24.parquet\n\n[main] INFO org.apache.parquet.hadoop.InternalParquetRecordReader - RecordReader initialized will read a total of 10 records.\n[main] INFO org.apache.parquet.hadoop.InternalParquetRecordReader - at row 0. reading next block\n[main] INFO org.apache.parquet.hadoop.InternalParquetRecordReader - block read in memory in 27 ms. row count = 10\ntop_field = top_field\nfirst_array:\n.array:\n..second_array:\n...array = 20\n\ntop_field = top_field\nfirst_array:\n.array:\n..second_array:\n...array = 20\n\n\nI am wondering what are your thoughts on this? Should we change this condition to if (totalValueCount < 0)?\nAny feedback is gladly appreciated! Let me know if I missed some information.\nThanks,\nCosti\n[1] https://aws.amazon.com/athena/",
        "Issue Links": []
    },
    "PARQUET-965": {
        "Key": "PARQUET-965",
        "Summary": "[C++] FIXED_LEN_BYTE_ARRAY types are unhandled in the Arrow reader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "YE",
        "Reporter": "Wes McKinney",
        "Created": "26/Apr/17 02:30",
        "Updated": "02/May/17 14:28",
        "Resolved": "02/May/17 13:26",
        "Description": "Currently, a dynamic_cast to a ByteArrayType reader is failing, resulting in a segfault. We should check the Parquet column type and use either a BYTE_ARRAY path or FIXED_LEN_BYTE_ARRAY path.",
        "Issue Links": [
            "/jira/browse/ARROW-901"
        ]
    },
    "PARQUET-966": {
        "Key": "PARQUET-966",
        "Summary": "Store `dictionary entries` of parquet columns that will be used for joins",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            format-2.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Ruslan Dautkhanov",
        "Created": "26/Apr/17 06:53",
        "Updated": "26/Apr/17 06:53",
        "Resolved": null,
        "Description": "It would be great if Parquet would store `dictionary entries` for columns marked to be used for joins. \nWhen a column is used for a join (it could be a surrogate key or a natural key) - the value of a cloumn used for join itself is actually not so important. \nSo we could join directly on `dictionary entries` instead of values \nand save CPU cycles. (no need to decompress etc)\nInspired by Oracle In-memory columnar storage improvements in 12.2",
        "Issue Links": []
    },
    "PARQUET-967": {
        "Key": "PARQUET-967",
        "Summary": "[C++] Combine libparquet/libparquet_arrow libraries",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Apr/17 18:00",
        "Updated": "31/May/17 17:43",
        "Resolved": "31/May/17 17:43",
        "Description": "Since this code needs to use some non-public APIs, it would be simpler to build a single shared / static library. We could still use the PARQUET_ARROW option to omit the extra symbols for users who want that.",
        "Issue Links": []
    },
    "PARQUET-968": {
        "Key": "PARQUET-968",
        "Summary": "Add Hive/Presto support in ProtoParquet",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Constantin Muraru",
        "Reporter": "Constantin Muraru",
        "Created": "29/Apr/17 19:33",
        "Updated": "23/Jul/21 16:34",
        "Resolved": "26/Apr/18 12:54",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/411",
            "https://github.com/apache/parquet-mr/pull/411"
        ]
    },
    "PARQUET-969": {
        "Key": "PARQUET-969",
        "Summary": "Decimal datatype support for parquet-tools output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Fowler",
        "Created": "01/May/17 23:24",
        "Updated": "12/May/17 21:41",
        "Resolved": "12/May/17 21:41",
        "Description": "parquet-tools cat outputs decimal datatypes in binary/bytearray format. I would like to have the decimal datatypes converted to their actual number representation so that when parquet data is output from parquet-tools decimals will be numbers.",
        "Issue Links": []
    },
    "PARQUET-970": {
        "Key": "PARQUET-970",
        "Summary": "Add Add Lz4 and Zstd compression codecs",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "YE",
        "Reporter": "YE",
        "Created": "02/May/17 15:53",
        "Updated": "23/Nov/17 09:23",
        "Resolved": "23/Nov/17 09:23",
        "Description": "https://github.com/facebook/zstd looks quite promising, I'd like to add a compressor in parquet-cpp.\nLz4 and Zstd codecs are added as parquet-format has already added these codecs.",
        "Issue Links": [
            "/jira/browse/PARQUET-1124",
            "https://github.com/apache/parquet-cpp/pull/419"
        ]
    },
    "PARQUET-971": {
        "Key": "PARQUET-971",
        "Summary": "Parquet file can not be read properly when it starts with an underscore",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0,                                            1.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jurgis Pods",
        "Created": "03/May/17 15:07",
        "Updated": "03/May/17 15:07",
        "Resolved": null,
        "Description": "When a Parquet file starts with an underscore,\n\nparquet-tools head <filename> fails\nHive/Impala will not show any data in external tables\n\nIf the underscore is deleted, everything works as expected. I don't know if this is expected behaviour - but if so, it should probably be documented.",
        "Issue Links": []
    },
    "PARQUET-972": {
        "Key": "PARQUET-972",
        "Summary": "C++: Release 1.1.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "03/May/17 18:09",
        "Updated": "22/May/17 16:17",
        "Resolved": "22/May/17 16:17",
        "Description": "Release parquet-cpp-1.1.0",
        "Issue Links": [
            "/jira/browse/PARQUET-987",
            "/jira/browse/PARQUET-992",
            "/jira/browse/PARQUET-936",
            "/jira/browse/PARQUET-976",
            "/jira/browse/PARQUET-985",
            "/jira/browse/PARQUET-853",
            "/jira/browse/PARQUET-930"
        ]
    },
    "PARQUET-973": {
        "Key": "PARQUET-973",
        "Summary": "Corrupt statistics test should include int96",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "03/May/17 22:49",
        "Updated": "03/May/17 22:49",
        "Resolved": null,
        "Description": "int96 are treated as byte arrays internally and were affected by the same bug.\nhttps://github.com/apache/parquet-mr/blob/70f28810a5547219e18ffc3465f519c454fee6e5/parquet-column/src/main/java/org/apache/parquet/CorruptStatistics.java#L56",
        "Issue Links": []
    },
    "PARQUET-974": {
        "Key": "PARQUET-974",
        "Summary": "Add a special ColumnOrder for testing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "05/May/17 16:43",
        "Updated": "27/Jul/17 01:28",
        "Resolved": "27/Jul/17 01:28",
        "Description": "PR #46 introduced ColumnOrder with the limitation that a reader should ignore stats for a column if the corresponding ColumnOrder in FileMetaData contains an unknown value. In order to test this logic, it would be helpful to have a special value InvalidOrder or UnsupportedOrder that would never be supported by a reader. I assume this may be helpful to test other implementations, too.",
        "Issue Links": []
    },
    "PARQUET-975": {
        "Key": "PARQUET-975",
        "Summary": "README.md misses a word",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "05/May/17 16:57",
        "Updated": "09/May/17 19:28",
        "Resolved": "09/May/17 19:28",
        "Description": "It says The size of specified in the header is for all 3 pieces combined. but should say The value of `uncompressed_page_size` specified in the header is for all 3 pieces combined.",
        "Issue Links": []
    },
    "PARQUET-976": {
        "Key": "PARQUET-976",
        "Summary": "[C++] Pass unit test suite with MSVC, build in Appveyor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "05/May/17 19:20",
        "Updated": "09/May/17 19:45",
        "Resolved": "09/May/17 19:45",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-977": {
        "Key": "PARQUET-977",
        "Summary": "Improve MSVC build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Renat Valiullin",
        "Created": "09/May/17 19:19",
        "Updated": "12/May/17 12:56",
        "Resolved": "12/May/17 12:56",
        "Description": "I'm going to improve and cleanup msvc build of parquet-cpp.",
        "Issue Links": []
    },
    "PARQUET-978": {
        "Key": "PARQUET-978",
        "Summary": "[C++] Minimizing footer reads for small(ish) metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "10/May/17 17:40",
        "Updated": "27/May/17 14:32",
        "Resolved": "27/May/17 14:32",
        "Description": "Other Parquet implementations will do a read from the end of the file (e.g. 64K) in case a single read suffices to obtain the metadata, metadata size, and magic end bytes. We don't do this yet: https://github.com/apache/parquet-cpp/blob/master/src/parquet/file/reader-internal.cc#L251",
        "Issue Links": []
    },
    "PARQUET-979": {
        "Key": "PARQUET-979",
        "Summary": "[C++] Limit size of min, max or disable stats for long binary types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "10/May/17 19:01",
        "Updated": "21/May/18 03:19",
        "Resolved": "21/May/18 03:19",
        "Description": "Other Parquet implementations like parquet-mr disable min/max values for long binary types > 4KB. For known logical types comparisons, we could approximate min/max values. We need to implement this.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/465"
        ]
    },
    "PARQUET-980": {
        "Key": "PARQUET-980",
        "Summary": "Cannot read row group larger than 2GB",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.8.1,                                            1.8.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Herman van H\u00f6vell",
        "Created": "11/May/17 20:25",
        "Updated": "12/May/17 16:03",
        "Resolved": null,
        "Description": "Parquet MR 1.8.2 does not support reading row groups which are larger than 2 GB. See:https://github.com/apache/parquet-mr/blob/parquet-1.8.x/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L1064\nWe are seeing this when writing skewed records. This throws off the estimation of the memory check interval in the InternalParquetRecordWriter. The following spark code illustrates this:\n\n/**\n * Create a data frame that will make parquet write a file with a row group larger than 2 GB. Parquet\n * only checks the size of the row group after writing a number of records. This number is based on\n * average row size of the already written records. This is problematic in the following scenario:\n * - The initial (100) records in the record group are relatively small.\n * - The InternalParquetRecordWriter checks if it needs to write to disk (it should not), it assumes\n *   that the remaining records have a similar size, and (greatly) increases the check interval (usually\n *   to 10000).\n * - The remaining records are much larger then expected, making the row group larger than 2 GB (which\n *   makes reading the row group impossible).\n *\n * The data frame below illustrates such a scenario. This creates a row group of approximately 4GB.\n */\nval badDf = spark.range(0, 2200, 1, 1).mapPartitions { iterator =>\n  var i = 0\n  val random = new scala.util.Random(42)\n  val buffer = new Array[Char](750000)\n  iterator.map { id =>\n    // the first 200 records have a length of 1K and the remaining 2000 have a length of 750K.\n    val numChars = if (i < 200) 1000 else 750000\n    i += 1\n\n    // create a random array\n    var j = 0\n    while (j < numChars) {\n      // Generate a char (borrowed from scala.util.Random)\n      buffer(j) = (random.nextInt(0xD800 - 1) + 1).toChar\n      j += 1\n    }\n\n    // create a string: the string constructor will copy the buffer.\n    new String(buffer, 0, numChars)\n  }\n}\nbadDf.write.parquet(\"somefile\")\nval corruptedDf = spark.read.parquet(\"somefile\")\ncorruptedDf.select(count(lit(1)), max(length($\"value\"))).show()\n\n\nThe latter fails with the following exception:\n\njava.lang.NegativeArraySizeException\n\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1064)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:698)\n...\n\n\nThis seems to be fixed by commit https://github.com/apache/parquet-mr/commit/6b605a4ea05b66e1a6bf843353abcb4834a4ced8 in parquet 1.9.x. Is there any chance that we can fix this in 1.8.x?",
        "Issue Links": []
    },
    "PARQUET-981": {
        "Key": "PARQUET-981",
        "Summary": "Repair usage of *_HOME 3rd party dependencies environment variables during Windows build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Max Risuhin",
        "Created": "13/May/17 19:05",
        "Updated": "14/May/17 20:49",
        "Resolved": "14/May/17 20:47",
        "Description": "Setting one of the available *_HOME variables(THRIFT_HOME, SNAPPY_HOME, ZLIB_HOME, BROTLI_HOME, ARROW_HOME) during Windows build doesn't work. It still builds 3rd party libs from the downloaded sources.\nUsage of *_HOME variable also should be compatible with libs packages from conda-forge.",
        "Issue Links": []
    },
    "PARQUET-982": {
        "Key": "PARQUET-982",
        "Summary": "C++: Arrow to Parquet Schema conversion for Decimals",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Uwe Korn",
        "Created": "01/May/16 08:47",
        "Updated": "11/Nov/18 22:10",
        "Resolved": "11/Nov/18 22:10",
        "Description": "Depends on https://issues.apache.org/jira/browse/ARROW-183",
        "Issue Links": []
    },
    "PARQUET-983": {
        "Key": "PARQUET-983",
        "Summary": "C++: Update Thirdparty hash to Arrow 0.3.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/May/17 16:18",
        "Updated": "14/May/17 19:02",
        "Resolved": "14/May/17 19:02",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-984": {
        "Key": "PARQUET-984",
        "Summary": "C++: Add abi and so version to pkg-config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/May/17 21:08",
        "Updated": "15/May/17 14:29",
        "Resolved": "15/May/17 14:29",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-985": {
        "Key": "PARQUET-985",
        "Summary": "[C++] Add change log for the new release",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "14/May/17 22:47",
        "Updated": "12/Nov/18 21:57",
        "Resolved": "12/Nov/18 21:57",
        "Description": "It will be helpful to document what changes went into each release. The idea is to write a brief summary of features added and important bugs fixed.",
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-986": {
        "Key": "PARQUET-986",
        "Summary": "Improve MSVC build - ThirdpartyToolchain - Thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "15/May/17 02:16",
        "Updated": "22/May/17 20:17",
        "Resolved": "22/May/17 20:16",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-987": {
        "Key": "PARQUET-987",
        "Summary": "[C++] Fix regressions caused by PARQUET-981",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/May/17 14:23",
        "Updated": "15/May/17 16:18",
        "Resolved": "15/May/17 16:18",
        "Description": "PARQUET-981 may have caused downstream build failure in Apache Arrow\nhttps://travis-ci.org/apache/arrow/jobs/232424614",
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-988": {
        "Key": "PARQUET-988",
        "Summary": "[C++] Add Linux toolchain-based build to Travis CI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/May/17 14:54",
        "Updated": "15/May/17 20:31",
        "Resolved": "15/May/17 20:31",
        "Description": "We need a Linux build that utilizes external library components rather than using the built-in ExternalProjects. This blind spot in our testing was exposed in PARQUET-987",
        "Issue Links": []
    },
    "PARQUET-989": {
        "Key": "PARQUET-989",
        "Summary": "[C++] Link dynamically to libarrow in toolchain build, set LD_LIBRARY_PATH",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/May/17 21:10",
        "Updated": "15/May/17 22:18",
        "Resolved": "15/May/17 22:18",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-990": {
        "Key": "PARQUET-990",
        "Summary": "More detailed error messages in footer parsing",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Andrew Ash",
        "Reporter": "Andrew Ash",
        "Created": "16/May/17 08:06",
        "Updated": "17/May/17 00:21",
        "Resolved": "17/May/17 00:19",
        "Description": "Include invalid values in exception messages when reading footer for two situations:\n\ntoo-short files (include file length)\nfiles with corrupted footer lengths (include calculated footer start index)",
        "Issue Links": []
    },
    "PARQUET-991": {
        "Key": "PARQUET-991",
        "Summary": "[C++] Fix compiler warnings on MSVC and build with /WX in Appveyor",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "17/May/17 13:43",
        "Updated": "29/May/17 19:56",
        "Resolved": "29/May/17 19:56",
        "Description": "The MSVC build has a ton of compiler warnings. We should fix all of these as they may potentially result in bugs in some circumstances.\nMax Risuhin do you have time to look at this?",
        "Issue Links": []
    },
    "PARQUET-992": {
        "Key": "PARQUET-992",
        "Summary": "[C++] parquet/compression.h leaks zlib.h",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "17/May/17 13:54",
        "Updated": "17/May/17 18:45",
        "Resolved": "17/May/17 18:45",
        "Description": "I encountered this while trying to build the Python-Arrow-Parquet integration on Windows",
        "Issue Links": [
            "/jira/browse/PARQUET-972"
        ]
    },
    "PARQUET-993": {
        "Key": "PARQUET-993",
        "Summary": "C++: Release parquet-cpp 1.2.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/May/17 20:34",
        "Updated": "03/Aug/17 08:39",
        "Resolved": "03/Aug/17 08:39",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-998",
            "/jira/browse/PARQUET-994"
        ]
    },
    "PARQUET-994": {
        "Key": "PARQUET-994",
        "Summary": "C++: release-candidate script should not push to master",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/May/17 20:35",
        "Updated": "16/Jul/17 21:36",
        "Resolved": "16/Jul/17 21:36",
        "Description": "This currently introduces the need for a force push if the vote does not pass.",
        "Issue Links": [
            "/jira/browse/PARQUET-993"
        ]
    },
    "PARQUET-995": {
        "Key": "PARQUET-995",
        "Summary": "[C++] Int96 reader in parquet_arrow uses size of Int96Type instead of Int96",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "18/May/17 13:17",
        "Updated": "22/May/17 20:22",
        "Resolved": "18/May/17 19:49",
        "Description": "This produces a segfault when reading alltypes_plain.parquet with parquet::arrow. I will see if I can reproduce with a test case.",
        "Issue Links": []
    },
    "PARQUET-996": {
        "Key": "PARQUET-996",
        "Summary": "Improve MSVC build - ThirdpartyToolchain - Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "18/May/17 23:56",
        "Updated": "23/May/17 18:30",
        "Resolved": "23/May/17 18:30",
        "Description": "Use CMAKE_GENERATOR instead of harcoded \"NMake Makefiles\"\nFix DEPENDEES for copy_dll_step (it should be 'install' not 'build'), after that ninja generator can be used.\nMerge versioned ExternalProject_Add\nFix bug in FindArrow  which broke \"PARQUET_ARROW_LINKAGE=static\" build",
        "Issue Links": []
    },
    "PARQUET-997": {
        "Key": "PARQUET-997",
        "Summary": "Fix override compiler warnings",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Phillip Cloud",
        "Created": "21/May/17 19:08",
        "Updated": "22/May/17 20:19",
        "Resolved": "22/May/17 20:19",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-998": {
        "Key": "PARQUET-998",
        "Summary": "C++: Release script is not usable",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "22/May/17 15:46",
        "Updated": "16/Jul/17 21:35",
        "Resolved": "16/Jul/17 21:35",
        "Description": "The script in dev/release/release isn't in line with the current way of doing releases. Either re-write or remove it before the next RC.",
        "Issue Links": [
            "/jira/browse/PARQUET-993"
        ]
    },
    "PARQUET-999": {
        "Key": "PARQUET-999",
        "Summary": "Improve MSVC build - Enable PARQUET_BUILD_BENCHMARKS",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "23/May/17 04:04",
        "Updated": "31/May/17 21:24",
        "Resolved": "31/May/17 21:24",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1000": {
        "Key": "PARQUET-1000",
        "Summary": "[C++] Do not build thirdparty Arrow with /WX on MSVC",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Wes McKinney",
        "Created": "23/May/17 15:15",
        "Updated": "25/Jul/17 03:24",
        "Resolved": "25/Jul/17 03:24",
        "Description": "While we should maintain a warning-free Arrow build, failing the Parquet build because of a compiler warning on a thirdparty library seems potentially onerous: https://github.com/apache/parquet-cpp/blob/master/cmake_modules/ThirdpartyToolchain.cmake#L513",
        "Issue Links": []
    },
    "PARQUET-1001": {
        "Key": "PARQUET-1001",
        "Summary": "undefined reference to open function in fileoutputstream",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "yugu",
        "Created": "23/May/17 21:33",
        "Updated": "14/Jul/17 14:51",
        "Resolved": "24/May/17 18:34",
        "Description": "when trying to compile the reader-writer.cc per this https://github.com/apache/parquet-cpp/tree/master/examples\nthe compiler throws undefined reference.\nthat being said, this is probably not a bug with parquet-cpp, rather with the makefile configuration. yet I cannot pinpoint the issue - the linker seems functional.\nusing FileClass = ::arrow::io::FileOutputStream;\n    std::shared_ptr<FileClass> out_file;\n    PARQUET_THROW_NOT_OK(FileClass::Open(PARQUET_FILENAME, &out_file));",
        "Issue Links": []
    },
    "PARQUET-1002": {
        "Key": "PARQUET-1002",
        "Summary": "[C++] Compute statistics based on Logical Types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "25/May/17 18:40",
        "Updated": "11/Sep/17 15:30",
        "Resolved": "11/Sep/17 15:30",
        "Description": "Current implementation computes statistics based on the physical type. We need to consider the logical type first if specified.",
        "Issue Links": []
    },
    "PARQUET-1003": {
        "Key": "PARQUET-1003",
        "Summary": "[C++] Modify DEFAULT_CREATED_BY value for every new release version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "25/May/17 21:43",
        "Updated": "11/Jun/17 18:28",
        "Resolved": "11/Jun/17 18:28",
        "Description": "For every new release version, we should modify the DEFAULT_CREATED_BY value at src/parquet/column/properties.h:88 accordingly.\nWe should automate this update.",
        "Issue Links": [
            "/jira/browse/PARQUET-1012"
        ]
    },
    "PARQUET-1004": {
        "Key": "PARQUET-1004",
        "Summary": "CPP Building fails on windows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "allen tom",
        "Created": "26/May/17 00:01",
        "Updated": "27/May/17 14:15",
        "Resolved": "27/May/17 14:15",
        "Description": "Cannot open C:\\src\\vcpkg\\packages\\zlib_x64-windows\\lib\\zlibstatic.lib. The problem comes when PARQUET_ZLIB_VENDORED is turned on.\nThe reason is because of these four lines \nhttps://github.com/apache/parquet-cpp/blob/bac81a7a6e13aa28f8b828e336dd80f84950cdcc/cmake_modules/FindZLIB.cmake#L69-L72\nHowever with the local libs it's zlib.lib and not zlibstatic.lib",
        "Issue Links": []
    },
    "PARQUET-1005": {
        "Key": "PARQUET-1005",
        "Summary": "Fix DumpCommand parsing to allow column projection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0,                                            1.8.1,                                            1.9.0,                                            2.0.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-cli",
        "Assignee": "Gera Shegalov",
        "Reporter": "Gera Shegalov",
        "Created": "26/May/17 14:24",
        "Updated": "30/Mar/18 21:38",
        "Resolved": "09/Jun/17 18:34",
        "Description": "DumpCommand option for -c is specified as hasArgs() for unlimited\nnumber of arguments following -c. The very description of the option\nshows the real intent of using hasArg() such that multiple columns\ncan be specified as '-c c1 -c c2 ...'. Otherwise, the input path\nis parsed as an argument for -c instead of the command itself.",
        "Issue Links": []
    },
    "PARQUET-1006": {
        "Key": "PARQUET-1006",
        "Summary": "ColumnChunkPageWriter uses only heap memory.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.0,                                            1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Vitalii Diravka",
        "Reporter": "Vitalii Diravka",
        "Created": "26/May/17 14:45",
        "Updated": "12/Apr/23 01:44",
        "Resolved": null,
        "Description": "After PARQUET-160 was resolved, ColumnChunkPageWriter started using ConcatenatingByteArrayCollector. There are all data is collected in the List of byte[], before writing the page. No way to use direct memory for allocating buffers. ByteBufferAllocator is present in the ColumnChunkPageWriter class, but never used.\nUsing of java heap space in some cases can cause OOM exceptions or GC's overhead. \nByteBufferAllocator should be used in the ConcatenatingByteArrayCollector or OutputStream classes.",
        "Issue Links": [
            "/jira/browse/DRILL-7825",
            "/jira/browse/DRILL-7906",
            "/jira/browse/PARQUET-1771",
            "/jira/browse/PARQUET-77",
            "/jira/browse/PARQUET-160",
            "/jira/browse/DRILL-5544"
        ]
    },
    "PARQUET-1007": {
        "Key": "PARQUET-1007",
        "Summary": "[C++ ] Update parquet.thrift from https://github.com/apache/parquet-format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "26/May/17 16:57",
        "Updated": "17/Jun/17 17:34",
        "Resolved": "17/Jun/17 17:34",
        "Description": "Support recent format changes including\n1) PARQUET-906: Add LogicalType annotation (yet to commit)\n2) PARQUET-686: Add Order to store the order used for min/max stat",
        "Issue Links": []
    },
    "PARQUET-1008": {
        "Key": "PARQUET-1008",
        "Summary": "Update TypedColumnReader::ReadBatch method to accept batch_size as int64_t",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Max Risuhin",
        "Created": "27/May/17 18:14",
        "Updated": "08/Jun/17 02:56",
        "Resolved": "08/Jun/17 02:56",
        "Description": "TypedColumnReader::ReadBatch method should take batch_size input param as int64_t type instead of currently used int.",
        "Issue Links": []
    },
    "PARQUET-1009": {
        "Key": "ARROW-1195",
        "Summary": "[C++] CpuInfo doesn't get cache size on Windows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6.0",
        "Component/s": "C++",
        "Assignee": "Max Risuhin",
        "Reporter": "Max Risuhin",
        "Created": "27/May/17 20:58",
        "Updated": "11/Jan/23 07:13",
        "Resolved": "24/Jul/17 21:27",
        "Description": "CpuInfo::cache_sizes_ values are not initialized with actual values on Windows in method CpuInfo::Init(). Default values are used instead.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/9149"
        ]
    },
    "PARQUET-1010": {
        "Key": "PARQUET-1010",
        "Summary": "SKIP FILES STARTS WITH \".\" in parquet.hadoop.ParquetFileReader.readAllFootersInParallel",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0",
        "Fix Version/s": "2.0.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Sandish Kumar HN",
        "Created": "28/May/17 20:02",
        "Updated": "10/Jun/17 06:03",
        "Resolved": "10/Jun/17 06:03",
        "Description": "SKIP FILES NAME STARTS WITH \".\" in parquet.hadoop.ParquetFileReader.readAllFootersInParallel",
        "Issue Links": [
            "/jira/browse/SQOOP-3178"
        ]
    },
    "PARQUET-1011": {
        "Key": "PARQUET-1011",
        "Summary": "bzip2 compression",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rajasekhar Konda",
        "Created": "30/May/17 20:30",
        "Updated": "07/Oct/17 00:13",
        "Resolved": null,
        "Description": "Hi,\nI have a requirement to implement Parquet with bzip2 compression because it's splitable. Right now, we can't provide bzip2 in PIG. \nSET parquet.compression none/gzip/SNAPPY; \nIs there any way to compress to bzip2 on top parquet ?",
        "Issue Links": []
    },
    "PARQUET-1012": {
        "Key": "PARQUET-1012",
        "Summary": "parquet-cpp and parquet-mr version parse inconsistent",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "YE",
        "Created": "31/May/17 08:38",
        "Updated": "13/Jun/17 18:17",
        "Resolved": "13/Jun/17 18:17",
        "Description": "Spark 2.1 uses parquet-mr(common) 1.8.2 which requires created_by to match certain pattern.  I found the following exception when using spark to read parquet file generated by parquet-cpp.\n17/05/31 16:33:53 WARN CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-cpp version 1.0.0\norg.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-cpp version 1.0.0 using format: (.+) version ((.) )?(build ?(.))\nProposal to fix this issue:  set created_by to the certain pattern.",
        "Issue Links": [
            "/jira/browse/PARQUET-1003"
        ]
    },
    "PARQUET-1013": {
        "Key": "PARQUET-1013",
        "Summary": "Fix ZLIB_INCLUDE_DIR",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "31/May/17 23:20",
        "Updated": "01/Jun/17 22:30",
        "Resolved": "01/Jun/17 22:30",
        "Description": "ZLIB_INCLUDE_DIR is defined in FindZLIB, but ZLIB_INCLUDE_DIRS is used in ThirdpartyToolchain",
        "Issue Links": []
    },
    "PARQUET-1014": {
        "Key": "PARQUET-1014",
        "Summary": "Example for multiple row group writer (cpp)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "yugu",
        "Created": "01/Jun/17 19:00",
        "Updated": "12/Nov/18 21:55",
        "Resolved": "12/Nov/18 21:55",
        "Description": "Been looking through the repo and cannot find an example for multiple row group writer.\nProbably missed that. Would be great if you guys can point it out ! : D\nThanks!",
        "Issue Links": []
    },
    "PARQUET-1015": {
        "Key": "PARQUET-1015",
        "Summary": "Object categoricals are not serialized when only None is present",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Marco Neumann",
        "Created": "02/Jun/17 08:16",
        "Updated": "02/Jan/18 15:16",
        "Resolved": "02/Jan/18 15:16",
        "Description": "The following code sample fails with pyarrow.lib.ArrowNotImplementedError: NotImplemented: unhandled type but should not:\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ndf = pd.DataFrame({'x': [None]})\ndf['x'] = df['x'].astype('category')\n\ntable = pa.Table.from_pandas(df)\nbuf = pa.InMemoryOutputStream()\n\npq.write_table(table, buf)",
        "Issue Links": [
            "/jira/browse/ARROW-1286"
        ]
    },
    "PARQUET-1016": {
        "Key": "PARQUET-1016",
        "Summary": "Upgrade thirdparty Arrow to 0.4.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "02/Jun/17 18:50",
        "Updated": "05/Jun/17 11:37",
        "Resolved": "05/Jun/17 11:37",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1017": {
        "Key": "PARQUET-1017",
        "Summary": "parquet-mr must handle statistics in files written by parquet-cpp",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "02/Jun/17 22:00",
        "Updated": "02/Jun/17 22:03",
        "Resolved": null,
        "Description": "shouldIgnoreStatistics in parquet-mr always accepts statistics for files written by other applications including parquet-cpp. We must fix this to correctly handle statistics written by parquet-cpp versions.",
        "Issue Links": []
    },
    "PARQUET-1018": {
        "Key": "PARQUET-1018",
        "Summary": "[C++] parquet.dll has runtime dependencies on one or more libraries in the build toolchain",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Jun/17 00:29",
        "Updated": "15/Jul/17 17:01",
        "Resolved": "15/Jul/17 17:01",
        "Description": "I am not sure which libraries are the problem, but with static linking the intent is to not require transitive runtime dependencies like snappy.dll, etc.",
        "Issue Links": []
    },
    "PARQUET-1019": {
        "Key": "PARQUET-1019",
        "Summary": "[C++] FindZLIB.cmake fails to detect libz.so in the absence of libz.a",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Jun/17 20:57",
        "Updated": "23/Jun/17 20:54",
        "Resolved": "23/Jun/17 20:54",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1020": {
        "Key": "PARQUET-1020",
        "Summary": "Add support for Dynamic Messages in parquet-protobuf",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-protobuf",
        "Assignee": "Alex Buck",
        "Reporter": "Alex Buck",
        "Created": "06/Jun/17 15:44",
        "Updated": "15/Sep/22 14:21",
        "Resolved": "15/Sep/22 14:10",
        "Description": "Hello. We would like to pass in a DynamicMessage rather than using the generated protobuf classes to allow us to make our job very generic. \nI think this could be achieved by setting the descriptor upfront, similarly to how there is a ProtoParquetOutputFormat today.\nIn ProtoWriteSupport in the init method it could then generate the parquet schema created by ProtoSchemaConverter using the passed in descriptor, rather than taking it from the generated proto class.\nWould there be interest in incorporating this change? If so does the approach above sound sensible? I am happy to do a pull request\ninitial PR here: https://github.com/apache/parquet-mr/pull/414",
        "Issue Links": []
    },
    "PARQUET-1021": {
        "Key": "PARQUET-1021",
        "Summary": "[C++] Print more helpful failure message when PARQUET_TEST_DATA environment variable is not set",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "06/Jun/17 18:41",
        "Updated": "23/Apr/18 14:52",
        "Resolved": "23/Apr/18 14:52",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1022": {
        "Key": "PARQUET-1022",
        "Summary": "[C++] Append mode in parquet-cpp",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "yugu",
        "Created": "06/Jun/17 20:39",
        "Updated": "15/Jul/19 21:38",
        "Resolved": "12/Nov/18 21:54",
        "Description": "As said, currently trying to work out a append feature for parquet files in c++.\n(been searching through repo etc, can't find example tho..)\nCurrent solution is to (assume no schema changes that is):\nRead in metadata\nChange metadata based on appended rows+ original rows\nAppend a new row group (or multiple row group writer)\nWrite the new rows.\n\u2014\nThe problem is that, is approached this way, the original last row group may not be complete filled. Was wondering if there is a fix or I'm using the api wrong...\nThanks ! : D",
        "Issue Links": []
    },
    "PARQUET-1023": {
        "Key": "PARQUET-1023",
        "Summary": "[C++] Brotli libraries are not being statically linked on Windows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Wes McKinney",
        "Created": "07/Jun/17 13:27",
        "Updated": "11/Jun/17 18:26",
        "Resolved": "11/Jun/17 18:26",
        "Description": "When building with toolchain Brotli, the DLLs are required to be in the runtime path. I think it's linking to the wrong .lib files\nMax Risuhin could you take a look?",
        "Issue Links": []
    },
    "PARQUET-1024": {
        "Key": "PARQUET-1024",
        "Summary": "allow for case insensitive parquet-xxx prefix in PR title",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "07/Jun/17 22:25",
        "Updated": "30/Mar/18 21:38",
        "Resolved": "09/Jun/17 22:22",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/415"
        ]
    },
    "PARQUET-1025": {
        "Key": "PARQUET-1025",
        "Summary": "Support new min-max statistics in parquet-mr",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Zoltan Ivanfi",
        "Created": "08/Jun/17 13:46",
        "Updated": "03/Apr/18 07:29",
        "Resolved": "13/Jan/18 00:30",
        "Description": "Impala started using new min-max statistics that got specified as part of PARQUET-686. Support for these should be added to parquet-mr as well.",
        "Issue Links": [
            "/jira/browse/PARQUET-1170",
            "https://github.com/apache/parquet-mr/pull/435"
        ]
    },
    "PARQUET-1026": {
        "Key": "PARQUET-1026",
        "Summary": "allow unsigned binary stats when min == max",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "09/Jun/17 01:58",
        "Updated": "30/Mar/18 21:38",
        "Resolved": "09/Jun/17 21:31",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1027",
            "https://github.com/apache/parquet-mr/pull/416"
        ]
    },
    "PARQUET-1027": {
        "Key": "PARQUET-1027",
        "Summary": "release Parquet-mr 1.9.1",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Le Dem",
        "Created": "09/Jun/17 17:21",
        "Updated": "09/Aug/18 07:11",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/SPARK-13127",
            "/jira/browse/PARQUET-783",
            "/jira/browse/PARQUET-1026"
        ]
    },
    "PARQUET-1028": {
        "Key": "PARQUET-1028",
        "Summary": "[JAVA] When reading old Spark-generated files with INT96, stats are reported as valid when they aren't",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Jacques Nadeau",
        "Created": "09/Jun/17 17:47",
        "Updated": "30/Mar/18 21:43",
        "Resolved": "30/Mar/18 21:43",
        "Description": "Found that the condition here is missing a check for INT96. Since INT96 statis are also corrupt with old versions of Parquet, the code here shouldn't short-circuit return.",
        "Issue Links": []
    },
    "PARQUET-1029": {
        "Key": "PARQUET-1029",
        "Summary": "[C++] TypedColumnReader/TypeColumnWriter symbols are no longer being exported",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Saatvik",
        "Created": "13/Jun/17 13:40",
        "Updated": "14/Jun/17 14:31",
        "Resolved": "14/Jun/17 14:31",
        "Description": "This regression was introduced in https://github.com/apache/parquet-cpp/commit/fc5228af3eee2ec8176e404ecb34b7ba985d0e4d. It can be seen with\n((fc5228a...))$ nm -g debug/libparquet.so | c++filt | grep parquet::TypedColumnReader\ncompared with the prior commit. We need to change one of our examples (like the reader-writer example) to use dynamic linking so this problem is caught in the future.",
        "Issue Links": []
    },
    "PARQUET-1030": {
        "Key": "PARQUET-1030",
        "Summary": "[C++] Add installation instructions for use in other projects",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "13/Jun/17 15:35",
        "Updated": "12/Nov/18 21:52",
        "Resolved": "12/Nov/18 21:52",
        "Description": "Apache Arrow headers are required to be able to use libparquet. This is not clear from the README. If users install using the thirdparty Arrow EP, they may have some difficulty using in another project",
        "Issue Links": []
    },
    "PARQUET-1031": {
        "Key": "PARQUET-1031",
        "Summary": "Fix spelling errors, whitespace, GitHub urls",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Anna Szonyi",
        "Reporter": "Lars Volker",
        "Created": "15/Jun/17 15:48",
        "Updated": "11/Oct/17 15:54",
        "Resolved": "11/Oct/17 15:54",
        "Description": "There's a couple of spelling mistakes, whitespaces at the end of a line, and some old url pointing the the Parquet organization instead of apache. We should fix those.\nThere's already a PR for this that needs rebasing.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/59"
        ]
    },
    "PARQUET-1032": {
        "Key": "PARQUET-1032",
        "Summary": "Change link in Encodings.md for variable length encoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Konstantin Shaposhnikov",
        "Reporter": "Lars Volker",
        "Created": "15/Jun/17 15:54",
        "Updated": "16/Oct/17 23:57",
        "Resolved": "06/Oct/17 23:23",
        "Description": "There's a PR for this already: #30\n\nThe spec says that varint-encode() is ULEB-128 encoding but links to VLQ algorithm that is slightly different from ULEB-128",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/69"
        ]
    },
    "PARQUET-1033": {
        "Key": "PARQUET-1033",
        "Summary": "Mismatched Read and Write",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "yugu",
        "Created": "15/Jun/17 15:56",
        "Updated": "19/Jun/17 15:29",
        "Resolved": "19/Jun/17 15:26",
        "Description": "The readbatchspaced reads in more lines than the actual data in file with nulls.\n So I've been trying to write something like [bla.csv] with mixed nulls.\nThe problem is that, when I use writebatchspaced to write and readbatchspaced to read back, \nInstead of getting the correct values, I'm getting less values than I initially wrote and additional nulls in the middle, a brief example as follows\nwritten\n\n-2147483648\n-2147483648\n30\n40\n50\n60\n70\n80\n90\n-2147483648\n-2147483648\n\n\nactual read\n\n-2147483648\n-2147483648\n-2147483648\n-2147483648\n30\n40\n50\n60\n70\n-2147483648\n9\n80\n90\n-2147483648\n-2147483648\n\n\nMy code for reader\n\nUnable to find source-code formatter for language:  c++. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yaml            int64_t rows_read = _c_reader->ReadBatchSpaced(arraysize, definition_level.data(), repetition_level.data(), ivalues.data(), valid_bits.data(), 0, &levels_read, &values_read, &null_count);\n            for (int tmp = 0; tmp < rows_read; tmp ++)\n            {\n              if (definition_level[tmp] < col_rep_type[__c])\n              {\n                ivalues[tmp] = NA_INTEGER;\n\n              }\n              //simply set value\n              if (fsize != 1 && filter[tmp + offset + cur_offset])\n              {\n                //rvec[__c].set(fcnt[__c],0,values[tmp]);\n                dff.set_value(fcnt[__c],0,__c,ivalues[tmp]);\n                fcnt[__c] ++;\n              }\n              else if (fsize == 1)\n              {\n                //rvec[__c].set(tmp,offset+cur_offset,values[tmp]);\n                dff.set_value(tmp,offset+cur_offset,__c,ivalues[tmp]);\n              }\n            }\n\n\nmy code for writer\n\nUnable to find source-code formatter for language:  c++. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yaml        parquet::Int64Writer* int64_writer = static_cast<parquet::Int64Writer*>(rg_writer->NextColumn());\n        IntegerVector tmpvec = df[__c];\n        for (int tmp = 0; tmp < rows_to_write; tmp++)\n        {\n          ivec[tmp] = tmpvec[tmp+offset];\n          if (tmpvec[tmp+offset] == NA_INTEGER)\n          {\n            def_level[tmp]=0;\n          }\n        }\n        int64_writer->WriteBatchSpaced(rows_to_write, def_level.data(), rep_level.data(), valid_bits.data(), 0, ivec.data());",
        "Issue Links": []
    },
    "PARQUET-1034": {
        "Key": "PARQUET-1034",
        "Summary": "Fix link to parquet-mr project in README.md Contributing section",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Anna Szonyi",
        "Reporter": "Lars Volker",
        "Created": "15/Jun/17 15:56",
        "Updated": "15/Sep/17 13:28",
        "Resolved": "15/Sep/17 13:28",
        "Description": "There's a broken link inFix link to parquet-mr project in README.md Contributing section which needs fixing (github.com/Parquet/parquet-mr).",
        "Issue Links": [
            "/jira/browse/PARQUET-1091",
            "https://github.com/apache/parquet-format/pull/60"
        ]
    },
    "PARQUET-1035": {
        "Key": "PARQUET-1035",
        "Summary": "Write Int96 from Arrow Timestamp(ns)",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "colin nichols",
        "Reporter": "colin nichols",
        "Created": "19/Jun/17 23:30",
        "Updated": "16/Jul/17 21:43",
        "Resolved": "16/Jul/17 21:43",
        "Description": "Although considered deprecated, this is still required by various engines.  Supporting it will increase compatibility until broad support for other timestamp formats arrives.",
        "Issue Links": []
    },
    "PARQUET-1036": {
        "Key": "PARQUET-1036",
        "Summary": "parquet file created via pyarrow 0.4.0 ; version 1.0 - incompatible with Spark",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ashima Sood",
        "Created": "20/Jun/17 16:48",
        "Updated": "24/Mar/21 15:24",
        "Resolved": "24/Mar/21 15:24",
        "Description": "using spark sql unable to read parquet file and shows null values. whereas hive reads the values fine.\n17/06/19 17:50:36 WARN CorruptStatistics: Ignoring statistics because created_by could not be parsed (see PARQUET-251): parquet-cpp version 1.0.0\norg.apache.parquet.VersionParser$VersionParseException: Could not parse created_by: parquet-cpp version 1.0.0 using format: (.+) version ((.) )?(build ?(.))\n                at org.apache.parquet.VersionParser.parse(VersionParser.java:112)\n                at org.apache.parquet.CorruptStatistics.shouldIgnoreStatistics(CorruptStatistics.java:60)",
        "Issue Links": []
    },
    "PARQUET-1037": {
        "Key": "PARQUET-1037",
        "Summary": "Allow final RowGroup to be unfilled",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Toby Shaw",
        "Reporter": "Toby Shaw",
        "Created": "22/Jun/17 10:35",
        "Updated": "21/Sep/17 07:34",
        "Resolved": "21/Sep/17 07:29",
        "Description": "When a RowGroup is added with AppendRowGroup, it must be filled with exactly the number of rows specified, or an exception will be thrown.\nIt should be possible go back and modify the RowGroupSize metadata after a column has completed prematurely. This would be useful in scenarios where the total number of rows to be written is not known in advance.",
        "Issue Links": []
    },
    "PARQUET-1038": {
        "Key": "PARQUET-1038",
        "Summary": "Key value metadata should be nullptr if not set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Phillip Cloud",
        "Created": "22/Jun/17 19:39",
        "Updated": "23/Jun/17 04:51",
        "Resolved": "23/Jun/17 04:51",
        "Description": "Key value metadata is initialized with std::make_shared<KeyValueMetadata>() which returns a pointer to an empty instance of KeyValueMetadata. This breaks Arrow's deserialization of metadata because it distinguishes between nullptr and empty metadata.",
        "Issue Links": []
    },
    "PARQUET-1039": {
        "Key": "PARQUET-1039",
        "Summary": "PARQUET-911 Breaks Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Phillip Cloud",
        "Created": "23/Jun/17 04:18",
        "Updated": "23/Jun/17 13:54",
        "Resolved": "23/Jun/17 13:54",
        "Description": "When comparing the results of a parquet file to expected data, after PARQUET-911, a single arrow test is failing: pyarrow/tests/test_parquet.py::test_date_time_types. It's not entirely clear how PARQUET-911 affects this code, but the data in time32 and time64 columns do not compare equal.",
        "Issue Links": []
    },
    "PARQUET-1040": {
        "Key": "PARQUET-1040",
        "Summary": "Missing writer method implementations",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Toby Shaw",
        "Created": "23/Jun/17 08:18",
        "Updated": "05/Jul/17 21:59",
        "Resolved": "05/Jul/17 21:59",
        "Description": "In file/writer.h, these methods exist:\nFor RowGroupWriter:\nnum_rows\nnum_columns\nFor ParquetFileWriter:\nnum_rows\nnum_row_groups\nnum_columns\nWhen using these methods, I get link errors:\nLNK2028\tunresolved token (0A001B98) \"public: int __cdecl parquet::ParquetFileWriter::num_row_groups(void)const \" (?num_row_groups@ParquetFileWriter@parquet@@$$FQEBAHXZ) referenced in ...\nLooking in the source, it's because these methods are not implemented in writer.cc. This should be a trivial fix.",
        "Issue Links": []
    },
    "PARQUET-1041": {
        "Key": "PARQUET-1041",
        "Summary": "C++: Support Arrow's NullArray",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "23/Jun/17 12:43",
        "Updated": "23/Jun/17 16:21",
        "Resolved": "23/Jun/17 16:21",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1042": {
        "Key": "PARQUET-1042",
        "Summary": "C++: Compilation breaks on GCC 4.8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "23/Jun/17 13:59",
        "Updated": "23/Jun/17 15:16",
        "Resolved": "23/Jun/17 15:16",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1043": {
        "Key": "PARQUET-1043",
        "Summary": "[C++] Raise minimum supported CMake version to 3.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Jun/17 14:27",
        "Updated": "23/Jun/17 19:48",
        "Resolved": "23/Jun/17 19:48",
        "Description": "We had discussed this previously and decided to stick with 2.8.x support. This is causing us to accumulate quite a bit of cruft in our build system. While Ubuntu 14.04 LTS ships CMake 2.8 by default, there are many other ways to obtain a newer CMake",
        "Issue Links": []
    },
    "PARQUET-1044": {
        "Key": "PARQUET-1044",
        "Summary": "[C++] Use compression libraries from Apache Arrow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Jun/17 20:51",
        "Updated": "25/Jun/17 15:15",
        "Resolved": "25/Jun/17 15:15",
        "Description": "Follow up work to ARROW-1142",
        "Issue Links": []
    },
    "PARQUET-1045": {
        "Key": "PARQUET-1045",
        "Summary": "[C++] Refactor to account for computational utility code migration in ARROW-1154",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Jun/17 18:12",
        "Updated": "27/Jun/17 21:07",
        "Resolved": "27/Jun/17 21:07",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1046": {
        "Key": "PARQUET-1046",
        "Summary": "Impossible to read thrift object from parquet file if it has List<Enum> field that was removed from thrift schema.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrei Stankevich",
        "Created": "27/Jun/17 22:13",
        "Updated": "27/Jun/17 23:13",
        "Resolved": null,
        "Description": "If thrift class has a field with type List<some_enum> ParquetReader makes list's elements type as enum (type id = 16) but it has to make it Int32.\nWhat happens is all fields that have field type as enum in thrift schema file in java class have field type as Int32. Same is true for List fields if list's elements are enum.\nBut when ParquetReader creates an object it uses type enum for list's elements instead of Int32.\nBecause of this fact we have an issue. We can not remove list field if it has enum elements. If we remove field like this from schema file but it will present in parquet file, when ParquetReader reads this field it tries to skip it because this field is not in the schema and it calls method TProtocolUtil.skip method with type = 15 for list and then it calls same method for each list element with type 16 for enum but TProtocolUtil.skip doesn't have this type in switch-case and it is not skipping list elements and because of this it throws exception later when it tries to skip List end.",
        "Issue Links": []
    },
    "PARQUET-1047": {
        "Key": "PARQUET-1047",
        "Summary": "Anyway to write empty row group?",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.1.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "yugu",
        "Created": "28/Jun/17 20:38",
        "Updated": "29/Jun/17 16:54",
        "Resolved": "29/Jun/17 16:54",
        "Description": "[Error]\nSo I'm trying to write only header information (column names and such).\n\nparquet::RowGroupWriter* rg_writer =\n                file_writer->AppendRowGroup(0);\nfor (i = 0; i < Cols; i++){\nparquet::BoolWriter* writer = static_cast<parquet::BoolWriter*>(rg_writer->NextColumn());\nwriter->WriteBatch(0, nullptr, nullptr, nullptr);\n}\n\n\nThe problem is that when trying to close the file_writer, the program throws:\n\nParquet what error: Parquet Write error: Column 3 is not complete.\n\n\n[Possbility]\nI know this is possible because as per this post an empty row group was written to the back.\nTho the problem was present in parquet-mr instead of parquet-cpp, I guess it's fair to expect some feature like that.\nOr maybe I don't fully understand how the system works.... would be great if you guys can point out where to look it : D",
        "Issue Links": []
    },
    "PARQUET-1048": {
        "Key": "PARQUET-1048",
        "Summary": "[C++] Static linking of libarrow is no longer supported",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "29/Jun/17 02:29",
        "Updated": "10/Jul/17 17:40",
        "Resolved": "10/Jul/17 17:40",
        "Description": "Since the compression libraries were moved to Apache Arrow, static linking requires pulling in the transitive dependencies. Unclear if we want to keep supporting this, or if so the best way to do it (possibly via external project)",
        "Issue Links": []
    },
    "PARQUET-1049": {
        "Key": "PARQUET-1049",
        "Summary": "Make thrift version a property in pom.xml",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "03/Jul/17 13:43",
        "Updated": "16/Oct/17 23:57",
        "Resolved": "31/Jul/17 16:26",
        "Description": "In parquet-mr, the version of the thrift dependency is controlled by a property and can be overridden from the command line, for example `mvn clean verify -Dthrift.version=0.9.0`.\nIn parquet-format, however, the version number of thrift is hard-coded in the pom.xml and a different version can only be used by modifying pom.xml itself.\nThe thrift version should be a property in Parquet-format as well.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/57"
        ]
    },
    "PARQUET-1050": {
        "Key": "PARQUET-1050",
        "Summary": "The comment of Parquet Format Thrift definition file error",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "lynn",
        "Reporter": "lynn",
        "Created": "05/Jul/17 06:10",
        "Updated": "16/Oct/17 23:57",
        "Resolved": "05/Jul/17 06:27",
        "Description": "The comments in the parquet.thrift File are inverse!",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/58"
        ]
    },
    "PARQUET-1051": {
        "Key": "PARQUET-1051",
        "Summary": "Parquet Combine Input format for MapReduce job",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Bing Jiang",
        "Created": "07/Jul/17 08:05",
        "Updated": "07/Jul/17 08:05",
        "Resolved": null,
        "Description": "ParquetInputFormat can only process one parquet file, if there are small files, it will spawn many small map task processing the small file. It is not efficient.\nWe need to provide CombineInputFormat to combine small files together in one map task.",
        "Issue Links": []
    },
    "PARQUET-1052": {
        "Key": "PARQUET-1052",
        "Summary": "[C++] add_compiler_export_flags() throws warning with CMake >= 3.3",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "08/Jul/17 18:20",
        "Updated": "09/Jul/17 16:25",
        "Resolved": "09/Jul/17 16:25",
        "Description": "The following Warning is shown\nCMake Deprecation Warning at /usr/share/cmake-3.5/Modules/GenerateExportHeader.cmake:383 (message):\n  The add_compiler_export_flags function is obsolete.  Use the\n  CXX_VISIBILITY_PRESET and VISIBILITY_INLINES_HIDDEN target properties\n  instead.\nCall Stack (most recent call first):\n  CMakeLists.txt:437 (add_compiler_export_flags)\nSimilar problem in KUDU: https://issues.apache.org/jira/browse/KUDU-1390",
        "Issue Links": []
    },
    "PARQUET-1053": {
        "Key": "PARQUET-1053",
        "Summary": "Fix unused result warnings due to unchecked Statuses",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "10/Jul/17 16:01",
        "Updated": "11/Jul/17 02:04",
        "Resolved": "11/Jul/17 02:03",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1054": {
        "Key": "PARQUET-1054",
        "Summary": "[C++] Account for Arrow API changes in ARROW-1199",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "10/Jul/17 17:56",
        "Updated": "11/Jul/17 20:23",
        "Resolved": "11/Jul/17 20:23",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1055": {
        "Key": "PARQUET-1055",
        "Summary": "Improve the creation of ExecutorService when reading footers",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Benoit Lacelle",
        "Created": "11/Jul/17 10:14",
        "Updated": "30/Mar/18 21:44",
        "Resolved": null,
        "Description": "Doing some benchmarks loading a large set of parquet files (3000+) from the local FS, we observed some inefficiencies in the number of created threads when reading footers.\nBy reading, the read the configuration parallelism in Hadoop configuration (defaulted to 5) and allocate 2 ExecuteService with each 5 threads to read footers. This is especially inefficient if there is less Callable to handle than the configured parallelism.",
        "Issue Links": []
    },
    "PARQUET-1056": {
        "Key": "PARQUET-1056",
        "Summary": "[C++] Add function to snapshot FileMetaData to a separate file, in case writing a row group fails, then prior row groups can be recovered",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "yugu",
        "Created": "12/Jul/17 15:09",
        "Updated": "12/Nov/18 21:51",
        "Resolved": null,
        "Description": "questions are actually twofold:\n1. is there a way to write data column by column ?\nI see a columnwriter but seems that's for arrow..\n2. was wondering if there is a way to preserve the footer information once in a while, in case the process crashes. for now if the proc crash the parquet file become unreadable...\nIn the code the close function (for parquetfilewriter) basically just flushes and close the stream. maybe a separate api for flush footer to target file?\n(or maybe there is an existing way for making things robust I missed)\nthanks!",
        "Issue Links": []
    },
    "PARQUET-1057": {
        "Key": "PARQUET-1057",
        "Summary": "undefined reference for WriteTable",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "yugu",
        "Created": "13/Jul/17 15:00",
        "Updated": "14/Jul/17 16:00",
        "Resolved": "14/Jul/17 16:00",
        "Description": "so i'm trying to write an arrow table using the WriteTable function provided. However, when compiling it throws\nundefined reference to `parquet::arrow::WriteTable(arrow::Table const&, arrow::MemoryPool*, std::shared_ptr<arrow::io::OutputStream> const&, long, std::shared_ptr<parquet::WriterProperties> const&)'\nI'm calling as below\n\n#include <parquet/api/reader.h>\n#include <parquet/api/writer.h>\n#include <parquet/arrow/writer.h>\n#include <vector>\n#include <arrow/io/file.h>\n#include <arrow/api.h>\n#include <arrow/memory_pool.h>\n\n...\n        ::parquet::arrow::WriteTable(*table, ::arrow::default_memory_pool(), m_out_file, rsize, parquet::default_writer_properties());\n...",
        "Issue Links": []
    },
    "PARQUET-1058": {
        "Key": "PARQUET-1058",
        "Summary": "Support enable/disable dictionary for column",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Workaround",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dapeng Sun",
        "Created": "14/Jul/17 01:41",
        "Updated": "17/Aug/17 05:57",
        "Resolved": "17/Aug/17 05:57",
        "Description": "Currently, Parquet support enable/disable dictionary at table level at Hive side, Parquet is better to support enable/disable dictionary for column",
        "Issue Links": [
            "/jira/browse/PARQUET-1062"
        ]
    },
    "PARQUET-1059": {
        "Key": "PARQUET-1059",
        "Summary": "Improve the RLE encoding for Parquet Dictionary IDs",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dapeng Sun",
        "Created": "14/Jul/17 01:44",
        "Updated": "17/Jul/17 08:59",
        "Resolved": null,
        "Description": "The IDs of Parquet Dictionary encoding is using RunLengthBitPackingHybridEncoder.\nRunLengthBitPackingHybridEncoder handles encoding with repeat and bitpacking, we should improve it with the method likes DeltaBinaryPackingWriter",
        "Issue Links": []
    },
    "PARQUET-1060": {
        "Key": "PARQUET-1060",
        "Summary": "Support a better encoding for Parquet Dictionary",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dapeng Sun",
        "Created": "14/Jul/17 01:53",
        "Updated": "19/Jul/17 21:01",
        "Resolved": null,
        "Description": "Currently Parquet is using Plain encoding for the dictionary, it should support a better encoding, such as BitPacking or DeltaBinaryPacking.",
        "Issue Links": []
    },
    "PARQUET-1061": {
        "Key": "PARQUET-1061",
        "Summary": "parquet dictionary filter does not work.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "18/Jul/17 08:54",
        "Updated": "15/Jan/18 10:41",
        "Resolved": "07/Aug/17 08:44",
        "Description": "When perform selective query, we observed that dictionary filter was not applied.  Please see following code snippet. \n    if (rowGroupOffsets != null) {\n      // verify a row group was found for each offset\n      List<BlockMetaData> blocks = reader.getFooter().getBlocks();\n      if (blocks.size() != rowGroupOffsets.length) \n{\n        throw new IllegalStateException(\n            \"All of the offsets in the split should be found in the file.\"\n            + \" expected: \" + Arrays.toString(rowGroupOffsets)\n            + \" found: \" + blocks);\n      }\n\n    } else \n{\n*Why apply data filter when row group offset equal to null? *\n      // apply data filters\n      reader.filterRowGroups(getFilter(configuration));\n    }\n\nI can enable filter after move else block code into second layer if.",
        "Issue Links": []
    },
    "PARQUET-1062": {
        "Key": "PARQUET-1062",
        "Summary": "Add a configurableValueWriterFactory for parquet-hadoop module",
        "Type": "New Feature",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0,                                            1.9.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dapeng Sun",
        "Created": "19/Jul/17 06:35",
        "Updated": "21/Jul/17 08:34",
        "Resolved": null,
        "Description": "Currently, ValuesWriterFactory could be config at ParquetProperties, but ParquetOutputFormat which is used by Hadoop and Hive haven't support to custom a ValuesWriterFactory. We should support update ParquetOutputFormat to support it.",
        "Issue Links": [
            "/jira/browse/PARQUET-1058"
        ]
    },
    "PARQUET-1063": {
        "Key": "PARQUET-1063",
        "Summary": "Parquet website pulls docs from wrong repo",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "20/Jul/17 14:35",
        "Updated": "20/Jul/17 14:35",
        "Resolved": null,
        "Description": "It seems that Parquet website (https://parquet.apache.org/documentation/latest/) pulls documentation from old github repo https://github.com/Parquet/parquet-format instead of the Apache repo https://github.com/apache/parquet-format.",
        "Issue Links": []
    },
    "PARQUET-1064": {
        "Key": "PARQUET-1064",
        "Summary": "Deprecate type-defined sort ordering for INTERVAL type",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "20/Jul/17 17:29",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "09/Jan/18 14:49",
        "Description": "LogicalTypes.md in parquet-format defines the the sort order for INTERVAL to be produced by sorting by the value of months, then days, then milliseconds with unsigned comparison.\nAccording to these rules, 1d0h0s > 0d48h0s, which is counter-intuitive and does not seem to have any practical uses. Unless somebody is aware of an actual use-case in which this makes sense, I think the sort order should be undefined instead. The reference implementation in parquet-mr already considers the ordering to be unknown.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/76"
        ]
    },
    "PARQUET-1065": {
        "Key": "PARQUET-1065",
        "Summary": "Deprecate type-defined sort ordering for INT96 type",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0,                                            format-2.5.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "20/Jul/17 17:37",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "25/Jan/18 15:18",
        "Description": "parquet.thrift in parquet-format defines the the sort order for INT96 to be signed. ParquetMetadataConverter.java in parquet-mr uses unsigned ordering instead. In practice, INT96 is only used for timestamps and neither signed nor unsigned ordering of the numeric values is correct for this purpose. For this reason, the INT96 sort order should be specified as undefined.\n(As a special case, min == max signifies that all values are the same, and can be considered valid even for undefined orderings.)",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/77",
            "https://github.com/apache/parquet-mr/pull/436"
        ]
    },
    "PARQUET-1066": {
        "Key": "PARQUET-1066",
        "Summary": "GC overhead limit exceeded while run a hive task with parquet format hdfs file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "hadu",
        "Created": "24/Jul/17 02:22",
        "Updated": "04/Feb/19 21:21",
        "Resolved": "04/Feb/19 21:21",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1067": {
        "Key": "PARQUET-1067",
        "Summary": "C++: Update arrow hash to 0.5.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "24/Jul/17 12:56",
        "Updated": "24/Jul/17 14:33",
        "Resolved": "24/Jul/17 14:33",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1068": {
        "Key": "PARQUET-1068",
        "Summary": "[C++] Use more vanilla Google C++ code formatting",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jul/17 02:58",
        "Updated": "31/Jul/17 15:15",
        "Resolved": "31/Jul/17 15:15",
        "Description": "This will make our .clang-format much simpler, and also make our codebase more aesthetically like Google codebases (e.g. TensorFlow). See also ARROW-1219",
        "Issue Links": []
    },
    "PARQUET-1069": {
        "Key": "PARQUET-1069",
        "Summary": "C++: ./dev/release/verify-release-candidate is broken due to missing Arrow dependencies",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.2.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "25/Jul/17 05:41",
        "Updated": "26/Jul/17 11:39",
        "Resolved": "26/Jul/17 11:39",
        "Description": "This appeared while testing 1.2.0-rc0",
        "Issue Links": []
    },
    "PARQUET-1070": {
        "Key": "PARQUET-1070",
        "Summary": "Add CPack support to the build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Mike Trinkala",
        "Reporter": "Mike Trinkala",
        "Created": "27/Jul/17 13:50",
        "Updated": "12/Oct/17 18:17",
        "Resolved": "12/Oct/17 18:17",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1071": {
        "Key": "PARQUET-1071",
        "Summary": "[C++] parquet::arrow::FileWriter::Close is not idempotent",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Wes McKinney",
        "Created": "28/Jul/17 01:39",
        "Updated": "28/Mar/18 16:20",
        "Resolved": "28/Mar/18 16:19",
        "Description": "Encountered a segfault when calling multiple times from Python",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/449"
        ]
    },
    "PARQUET-1072": {
        "Key": "PARQUET-1072",
        "Summary": "[C++] Add ARROW_NO_DEPRECATED_API to CI to check for deprecated API use",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "31/Jul/17 23:20",
        "Updated": "06/Aug/17 19:43",
        "Resolved": "06/Aug/17 19:43",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1073": {
        "Key": "PARQUET-1073",
        "Summary": "Hive failed to parse Parquet file generated by Spark SQL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "01/Aug/17 08:37",
        "Updated": "12/Dec/22 18:10",
        "Resolved": "02/Aug/17 00:51",
        "Description": "When load parquet file which generated from sparksql using following SQL:\nCREATE EXTERNAL TABLE IF NOT EXISTS sparksql_tbl\n(\n...\n)\n  STORED AS PARQUET\n  LOCATION '/root/spark-warehouse/sparksql_db.db/sparksql_tbl/'\nparquet-mr throw following exception:\nDiagnostic Messages for this Task:\nError: java.io.IOException: java.lang.reflect.InvocationTargetException\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:271)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:217)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:345)\n        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:695)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:257)\n        ... 11 more\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://bdpe30:9001/root/SQLDataGen/spark-warehouse/sparksql_db.db/sparksql_tbl/part-r-00001-d9a4d43a-e134-4a04-97d4-268dabe26078.parquet\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:222)\n        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:217)\n        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:98)\n        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:59)\n        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:75)\n        at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:99)\n        ... 16 more\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainIntegerDictionary\n        at org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:44)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$BinaryConverter.setDictionary(ETypeConverter.java:291)\n        at org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:346)\n        at org.apache.parquet.column.impl.ColumnReadStoreImpl.newMemColumnReader(ColumnReadStoreImpl.java:82)\n        at org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:77)\n        at org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:270)\n        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:144)\n        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:106)\n        at org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:154)\n        at org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:106)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\n        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)\n        ... 21 more\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask",
        "Issue Links": []
    },
    "PARQUET-1074": {
        "Key": "PARQUET-1074",
        "Summary": "[C++] Switch to long key ids in KEYs file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.1.0",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "02/Aug/17 19:37",
        "Updated": "03/Aug/17 16:35",
        "Resolved": "03/Aug/17 12:33",
        "Description": "PGP keys should be longer than 32bit, as outlined on https://evil32.com/. We should fix the KEYS file in parquet-cpp. I will push a PR shortly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1076",
            "/jira/browse/PARQUET-1077"
        ]
    },
    "PARQUET-1075": {
        "Key": "PARQUET-1075",
        "Summary": "C++: Coverage upload is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Uwe Korn",
        "Created": "03/Aug/17 12:27",
        "Updated": "10/Aug/17 17:54",
        "Resolved": "10/Aug/17 17:54",
        "Description": "++which gcov-4.9\n+coveralls --gcov /usr/bin/gcov-4.9 --gcov-options '\\-l' --root '' --include /home/travis/build/apache/parquet-cpp --exclude /home/travis/build/apache/parquet-cpp/parquet-build/thirdparty --exclude /home/travis/build/apache/parquet-cpp/parquet-build/arrow_ep --exclude /home/travis/build/apache/parquet-cpp/parquet-build/brotli_ep --exclude /home/travis/build/apache/parquet-cpp/parquet-build/brotli_ep-prefix --exclude /home/travis/build/apache/parquet-cpp/parquet-build/gbenchmark_ep --exclude /home/travis/build/apache/parquet-cpp/parquet-build/googletest_ep-prefix --exclude /home/travis/build/apache/parquet-cpp/parquet-build/snappy_ep --exclude /home/travis/build/apache/parquet-cpp/parquet-build/snappy_ep-prefix --exclude /home/travis/build/apache/parquet-cpp/parquet-build/zlib_ep --exclude /home/travis/build/apache/parquet-cpp/parquet-build/zlib_ep-prefix --exclude /home/travis/build/apache/parquet-cpp/build --exclude /home/travis/build/apache/parquet-cpp/src/parquet/thrift --exclude /usr\nTraceback (most recent call last):\n  File \"/usr/local/bin/coveralls\", line 6, in <module>\n    from pkg_resources import load_entry_point\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 3037, in <module>\n    @_call_aside\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 3021, in _call_aside\n    f(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 3050, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 655, in _build_master\n    ws.require(__requires__)\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 969, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 863, in resolve\n    new_requirements = dist.requires(req.extras)[::-1]\n  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2577, in requires\n    \"%s has no such extra feature %r\" % (self, ext)\npkg_resources.UnknownExtra: urllib3 1.7.1 has no such extra feature 'secure'",
        "Issue Links": []
    },
    "PARQUET-1076": {
        "Key": "PARQUET-1076",
        "Summary": "[Format] Switch to long key ids in KEYs file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "03/Aug/17 16:23",
        "Updated": "16/Oct/17 23:56",
        "Resolved": "06/Oct/17 23:26",
        "Description": "PGP keys should be longer than 32bit, as outlined on https://evil32.com/. We should fix the KEYS file in parquet-format. I will push a PR shortly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1074",
            "https://github.com/apache/parquet-format/pull/61"
        ]
    },
    "PARQUET-1077": {
        "Key": "PARQUET-1077",
        "Summary": "[MR] Switch to long key ids in KEYs file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.0.0,                                            1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "03/Aug/17 16:35",
        "Updated": "30/Mar/18 21:39",
        "Resolved": "09/Jan/18 15:03",
        "Description": "PGP keys should be longer than 32bit, as outlined on https://evil32.com/. We should fix the KEYS file in parquet-mr. I will push a PR shortly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1074"
        ]
    },
    "PARQUET-1078": {
        "Key": "PARQUET-1078",
        "Summary": "[C++] Add Arrow writer option to coerce timestamps to milliseconds or microseconds",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Aug/17 17:01",
        "Updated": "07/Aug/17 17:46",
        "Resolved": "07/Aug/17 17:46",
        "Description": "Currently we are performing this coercion on the Arrow side, but we would like to improve the API around this when it comes to writing to Parquet format",
        "Issue Links": [
            "/jira/browse/ARROW-1076"
        ]
    },
    "PARQUET-1079": {
        "Key": "PARQUET-1079",
        "Summary": "[C++] Account for Arrow API change in ARROW-1335",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "08/Aug/17 03:12",
        "Updated": "08/Aug/17 05:13",
        "Resolved": "08/Aug/17 05:13",
        "Description": "This patch for ARROW-1335 corrects an inconsistency in the behavior of PrimitiveArray::raw_values in the Arrow writer path.",
        "Issue Links": []
    },
    "PARQUET-1080": {
        "Key": "PARQUET-1080",
        "Summary": "Empty Parquet Files created as a result of spark jobs fail when read",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Shivam Dalmia",
        "Created": "28/Aug/17 10:28",
        "Updated": "12/Dec/22 18:10",
        "Resolved": "30/Aug/17 09:57",
        "Description": "I have faced an issue intermittently with certain spark jobs writing parquet files which apparently succeed but the written .parquet directory in HDFS is an empty directory (with no _SUCCESS and _metadata parts, even). Surprisingly, no errors are thrown from spark dataframe writer.\nHowever, when attempting to read this written file, spark throws the error:\nUnable\u00a0to\u00a0infer\u00a0schema\u00a0for\u00a0Parquet.\u00a0It\u00a0must\u00a0be\u00a0specified\u00a0manually",
        "Issue Links": [
            "/jira/browse/PARQUET-1081"
        ]
    },
    "PARQUET-1081": {
        "Key": "PARQUET-1081",
        "Summary": "Empty Parquet Files created as a result of spark jobs fail when read again",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Shivam Dalmia",
        "Created": "28/Aug/17 16:39",
        "Updated": "28/Aug/17 16:40",
        "Resolved": "28/Aug/17 16:39",
        "Description": "I have faced an issue intermittently with certain spark jobs writing parquet files which apparently succeed but the written .parquet directory in HDFS is an empty directory (with no _SUCCESS and _metadata parts, even). Surprisingly, no errors are thrown from spark dataframe writer.\nHowever, when attempting to read this written file, spark throws the error:\nUnable\u00a0to\u00a0infer\u00a0schema\u00a0for\u00a0Parquet.\u00a0It\u00a0must\u00a0be\u00a0specified\u00a0manually",
        "Issue Links": [
            "/jira/browse/PARQUET-1080"
        ]
    },
    "PARQUET-1082": {
        "Key": "PARQUET-1082",
        "Summary": "[C++] Account for API deprecation in ARROW-1402",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "28/Aug/17 19:45",
        "Updated": "12/Dec/17 20:01",
        "Resolved": "12/Dec/17 20:01",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1083": {
        "Key": "PARQUET-1083",
        "Summary": "[C++] Refactor core logic in parquet-scan.cc so that it can be used as a library function for benchmarking",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Aug/17 22:38",
        "Updated": "30/Aug/17 07:49",
        "Resolved": "30/Aug/17 07:49",
        "Description": null,
        "Issue Links": [
            "/jira/browse/ARROW-1377"
        ]
    },
    "PARQUET-1084": {
        "Key": "PARQUET-1084",
        "Summary": "Parquet-C++ doesn't selectively read columns with mmap'ed files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "cpp-1.0.0,                                            cpp-1.2.0",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Jim Pivarski",
        "Created": "30/Aug/17 14:53",
        "Updated": "17/Jan/18 15:40",
        "Resolved": "16/Jan/18 21:49",
        "Description": "I first saw this reported in a [review of file formats for C++](https://indico.cern.ch/event/567550/contributions/2628878/attachments/1511966/2358123/hep-file-formats.pdf), which showed that an attempt to read two columns from a Parquet file in C++ resulted in the whole file\u2014 26 columns\u2014 being read (18th page of the PDF, \"15 / 25\" in the bottom-right corner). That test used Parquet-C++ version 1.2.0.\nTo check this, I pip-installed pyarrow (version 0.6.0), which comes with Parquet-C++ version 1.0.0. I used [vmtouch](https://hoytech.com/vmtouch/) to identify the fraction of pages touched, and double-checked by measuring the time-to-load. The fact that it's a slow disk makes it obvious whether it's reading one column or all columns.\nI'm using the same files as the presenter of that talk: [B2HHH.parquet-inflated](https://cernbox.cern.ch/index.php/s/ub43DwvQIFwxfxs/download?path=%2F&files=B2HHH.parquet-inflated) and [B2HHH.parquet-deflated](https://cernbox.cern.ch/index.php/s/ub43DwvQIFwxfxs/download?path=%2F&files=B2HHH.parquet-deflated). They have 20 double-precision columns and 6 int32 columns with no nesting, 500 rows per group * 17113 row groups = 8556118 rows = 1.5 GB for the inflated (uncompressed) file. Each column within a row group should be 4000 or 2000 bytes, so reading one column should be one or two 4k disk pages per row group out of 769 disk pages per row group, depending on alignment\u2014 granularity should not be a problem, as it would be if the row groups were too small.\nProcedure:\n\nI evicted the uncompressed file from VM cache to force reads to come from disk.\nI imported pyarrow.parquet in Python and called read_table(\"data/B2HHH-inflated.parquet\", [\"h1_px\"]) (one column).\nI checked to see how much of the file has been loaded into VM cache.\nI also checked the time-to-load of one column from cold cache versus all columns from cold cache.\n\nThe result is that the entire file get loaded into VM cache and the file takes 14.6 seconds to read regardless of whether I read one column or the whole file. (From warm cache is 4.7 seconds, so we're clearly seeing the effect of disk speed.) Both methods agree that the file is not being selectively read, as I think it should be.\nIs there a setting that the presenter of the talk (using Parquet-C++ version 1.2.0 in C+) and I (using pyarrow with Parquet-C+ 1.0.0 in Python) are both missing? Is this a future feature? I would consider it to be a performance bug, since a major reason for having a columnar data format is to read columns selectively.",
        "Issue Links": []
    },
    "PARQUET-1085": {
        "Key": "PARQUET-1085",
        "Summary": "[C++] Backwards compatibility from macro cleanup in transitive dependencies in ARROW-1452",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "02/Sep/17 22:43",
        "Updated": "03/Sep/17 17:01",
        "Resolved": "03/Sep/17 17:01",
        "Description": "This is a little bit annoying (we are relying on UNLIKELY, LIKELY, UNUSED being defined someplace in the Arrow headers). I will add defines so that we can gracefully transition to more-uniquely-named macros",
        "Issue Links": []
    },
    "PARQUET-1086": {
        "Key": "PARQUET-1086",
        "Summary": "[C++] Remove usage of arrow/util/compiler-util.h after 1.3.0 release",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "03/Sep/17 17:49",
        "Updated": "16/Jan/18 03:31",
        "Resolved": "16/Jan/18 03:31",
        "Description": "Follow up to ARROW-1452 so that we do not need to keep this header file around",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/428"
        ]
    },
    "PARQUET-1087": {
        "Key": "PARQUET-1087",
        "Summary": "[C++] Add wrapper for ScanFileContents in parquet::arrow that catches exceptions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Sep/17 00:36",
        "Updated": "05/Sep/17 19:45",
        "Resolved": "05/Sep/17 19:44",
        "Description": "Needed to make ARROW-1377 easier",
        "Issue Links": []
    },
    "PARQUET-1088": {
        "Key": "PARQUET-1088",
        "Summary": "[CPP] remove parquet_version.h from version control since it gets auto generated",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "05/Sep/17 14:41",
        "Updated": "06/Sep/17 07:30",
        "Resolved": "06/Sep/17 07:30",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1089": {
        "Key": "PARQUET-1089",
        "Summary": "A NullPointerException in DictionaryValuesWriter when writing Parquet",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1,                                            1.8.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Irina Truong",
        "Created": "05/Sep/17 20:39",
        "Updated": "02/Mar/18 20:07",
        "Resolved": null,
        "Description": "A NullReference Exception happens when writing a large partitioned Spark dataframe to Parquet.\nThe job is running in AWS EMR 5.8.0 (Spark 2.1.1).\nException:\n\n17/09/05 16:40:33 ERROR Utils: Aborting task\njava.lang.NullPointerException\n\tat org.apache.parquet.it.unimi.dsi.fastutil.objects.Object2IntLinkedOpenHashMap.getInt(Object2IntLinkedOpenHashMap.java:590)\n\tat org.apache.parquet.column.values.dictionary.DictionaryValuesWriter$PlainFixedLenArrayDictionaryValuesWriter.writeBytes(DictionaryValuesWriter.java:307)\n\tat org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:162)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:201)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:467)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$10.apply(ParquetWriteSupport.scala:184)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$10.apply(ParquetWriteSupport.scala:172)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields$1.apply$mcV$sp(ParquetWriteSupport.scala:124)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$consumeField(ParquetWriteSupport.scala:437)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields(ParquetWriteSupport.scala:123)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$write$1.apply$mcV$sp(ParquetWriteSupport.scala:114)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:425)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:113)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:51)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:180)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:46)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2.apply(FileFormatWriter.scala:465)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2.apply(FileFormatWriter.scala:440)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.catalyst.util.AbstractScalaRowIterator.foreach(AbstractScalaRowIterator.scala:26)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.execute(FileFormatWriter.scala:440)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)}}",
        "Issue Links": []
    },
    "PARQUET-1090": {
        "Key": "PARQUET-1090",
        "Summary": "[C++] Fix int32 overflow in Arrow table writer, add max row group size property",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Sep/17 16:03",
        "Updated": "06/Sep/17 23:37",
        "Resolved": "06/Sep/17 23:37",
        "Description": "This fixes the underlying cause of ARROW-1446",
        "Issue Links": [
            "/jira/browse/ARROW-1446"
        ]
    },
    "PARQUET-1091": {
        "Key": "PARQUET-1091",
        "Summary": "Wrong and broken links in README",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "07/Sep/17 22:23",
        "Updated": "16/Oct/17 23:57",
        "Resolved": "13/Sep/17 02:03",
        "Description": "Multiple links in README.md still point to the old Parquet/parquet-format repository, which is now removed.",
        "Issue Links": [
            "/jira/browse/PARQUET-1034"
        ]
    },
    "PARQUET-1092": {
        "Key": "PARQUET-1092",
        "Summary": "[C++] Write Arrow tables with chunked columns",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "09/Sep/17 02:42",
        "Updated": "17/Dec/17 11:13",
        "Resolved": "17/Dec/17 11:13",
        "Description": "Requires incoming patch in ARROW-232",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/426"
        ]
    },
    "PARQUET-1093": {
        "Key": "PARQUET-1093",
        "Summary": "C++: Improve Arrow level generation error message",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "09/Sep/17 10:01",
        "Updated": "11/Sep/17 15:27",
        "Resolved": "11/Sep/17 15:27",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1094": {
        "Key": "PARQUET-1094",
        "Summary": "C++: Add benchmark for boolean Arrow column I/O",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "09/Sep/17 10:41",
        "Updated": "17/Sep/17 17:55",
        "Resolved": "17/Sep/17 17:55",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1095": {
        "Key": "PARQUET-1095",
        "Summary": "[C++] Read and write Arrow decimal values",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Wes McKinney",
        "Created": "10/Sep/17 00:10",
        "Updated": "20/Nov/17 04:20",
        "Resolved": "20/Nov/17 04:19",
        "Description": "We now have 16-byte decimal values in Arrow which have been validated against the Java implementation. We need to be able to read and write these to Parquet format. \nTo make these values readable by Impala or some other Parquet readers may require some work. It expects the storage size to match the decimal precision exactly. So in parquet-cpp we will need to write the correct non-zero bytes into a FIXED_LEN_BYTE_ARRAY of the appropriate size.\nWe should validate this against Java Parquet implementations",
        "Issue Links": [
            "/jira/browse/ARROW-1398",
            "https://github.com/apache/parquet-cpp/pull/403"
        ]
    },
    "PARQUET-1096": {
        "Key": "PARQUET-1096",
        "Summary": "C++: Update sha{1, 256, 512} checksums per latest ASF release policy",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "10/Sep/17 06:02",
        "Updated": "11/Sep/17 15:31",
        "Resolved": "11/Sep/17 15:31",
        "Description": "See http://www.apache.org/dev/release-distribution#sigs-and-sums",
        "Issue Links": []
    },
    "PARQUET-1097": {
        "Key": "PARQUET-1097",
        "Summary": "[C++] Account for Arrow API deprecation in ARROW-1511",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Sep/17 15:47",
        "Updated": "13/Jan/18 12:06",
        "Resolved": "13/Jan/18 12:06",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1098": {
        "Key": "PARQUET-1098",
        "Summary": "[C++] Install new header in parquet/util",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Sep/17 16:23",
        "Updated": "11/Sep/17 16:27",
        "Resolved": "11/Sep/17 16:27",
        "Description": "Follow up to PARQUET-1002",
        "Issue Links": []
    },
    "PARQUET-1099": {
        "Key": "PARQUET-1099",
        "Summary": "[C++] Add Travis CI entry that uses parquet-cpp as a library",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "11/Sep/17 16:25",
        "Updated": "27/Feb/18 16:42",
        "Resolved": "27/Feb/18 16:42",
        "Description": "We could use pyarrow to create the build entry",
        "Issue Links": []
    },
    "PARQUET-1100": {
        "Key": "PARQUET-1100",
        "Summary": "[C++] Reading repeated types should decode number of records rather than number of values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Jarno Seppanen",
        "Created": "31/Aug/17 08:50",
        "Updated": "05/Oct/17 13:09",
        "Resolved": "20/Sep/17 01:39",
        "Description": "Reading the attached parquet file into pandas dataframe and then using the dataframe segfaults.\n\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import pyarrow\n>>> import pyarrow.parquet as pq\n>>> pyarrow.__version__\n'0.6.0'\n>>> import pandas as pd\n>>> pd.__version__\n'0.19.0'\n>>> df = pq.read_table('part-00000-6570e34b-b42c-4a39-8adf-21d3a97fb87d.snappy.parquet') \\\n...        .to_pandas()\n>>> len(df)\n69\n>>> df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 69 entries, 0 to 68\nData columns (total 6 columns):\nlabel               69 non-null int32\naccount_meta        69 non-null object\nfeatures_type       69 non-null int32\nfeatures_size       69 non-null int32\nfeatures_indices    1 non-null object\nfeatures_values     1 non-null object\ndtypes: int32(3), object(3)\nmemory usage: 2.5+ KB\n>>> \n>>> pd.concat([df, df])\nSegmentation fault (core dumped)\n\n\nActually just print(df) is enough to trigger the segfault",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/1043"
        ]
    },
    "PARQUET-1101": {
        "Key": "PARQUET-1101",
        "Summary": "[C++] Build against arrow master in CI",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "12/Sep/17 17:04",
        "Updated": "02/Jan/18 15:15",
        "Resolved": "02/Jan/18 15:15",
        "Description": "From this appveyor build it looks like we're testing against a five-week-old version of arrow. I've got a PR in the works to fix this.",
        "Issue Links": []
    },
    "PARQUET-1102": {
        "Key": "PARQUET-1102",
        "Summary": "Travis CI builds are failing for parquet-format PRs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Cheng Lian",
        "Reporter": "Cheng Lian",
        "Created": "13/Sep/17 00:35",
        "Updated": "16/Oct/17 23:55",
        "Resolved": "13/Sep/17 01:46",
        "Description": "Travis CI builds are failing for parquet-format PRs, probably due to the migration from Ubuntu precise to trusty on Sep 1 according to this Travis official blog post.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/66"
        ]
    },
    "PARQUET-1103": {
        "Key": "PARQUET-1103",
        "Summary": "Update parquet-thrift's thrift version to 0.9.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-thrift",
        "Assignee": null,
        "Reporter": "Ryan Blue",
        "Created": "13/Sep/17 16:37",
        "Updated": "21/Nov/17 17:30",
        "Resolved": "21/Nov/17 17:30",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1135"
        ]
    },
    "PARQUET-1104": {
        "Key": "PARQUET-1104",
        "Summary": "[C++] Upgrade to Apache Arrow 0.7.0 RC0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "13/Sep/17 16:37",
        "Updated": "13/Sep/17 21:35",
        "Resolved": "13/Sep/17 21:35",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1105": {
        "Key": "PARQUET-1105",
        "Summary": "[CPP] Remove libboost_system dependency",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "13/Sep/17 20:10",
        "Updated": "28/Sep/17 13:44",
        "Resolved": "28/Sep/17 13:44",
        "Description": "Arrow added an additional libboost_system dependency. We now transitively added the dependency to parquet for static linking. Remove it from Parquet when Arrow does ARROW-1536",
        "Issue Links": [
            "/jira/browse/ARROW-1536"
        ]
    },
    "PARQUET-1106": {
        "Key": "PARQUET-1106",
        "Summary": "[C++] Add include-what-you-use setup, fix IWYU warnings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Sep/17 16:23",
        "Updated": "11/Nov/18 22:08",
        "Resolved": "11/Nov/18 22:08",
        "Description": "Like ARROW-1413 https://github.com/apache/arrow/commit/75d1f613cf99a822fa61859a7d081c8527c95500#diff-2c5f2cba9af7f92986d3de1e753f3faa",
        "Issue Links": []
    },
    "PARQUET-1107": {
        "Key": "PARQUET-1107",
        "Summary": "[C++] Expose key_value_metadata in parquet::ColumnChunkMetaData",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "20/Sep/17 12:20",
        "Updated": "21/Sep/17 05:48",
        "Resolved": null,
        "Description": "This is available already at the file level:\nhttps://github.com/apache/parquet-cpp/blob/master/src/parquet/file/metadata.h#L177\nbut not at the ColumnChunk level",
        "Issue Links": []
    },
    "PARQUET-1108": {
        "Key": "PARQUET-1108",
        "Summary": "[C++] Fix Int96 comparators",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "20/Sep/17 17:17",
        "Updated": "21/Sep/17 07:31",
        "Resolved": "21/Sep/17 07:31",
        "Description": "As discussed here https://github.com/apache/parquet-format/pull/55/files\nThe bytes must be compared in the reverse order.",
        "Issue Links": []
    },
    "PARQUET-1109": {
        "Key": "PARQUET-1109",
        "Summary": "C++: Update release verification script to SHA512",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Sep/17 11:49",
        "Updated": "25/Sep/17 12:43",
        "Resolved": "25/Sep/17 12:43",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1110": {
        "Key": "PARQUET-1110",
        "Summary": "[C++] Release verification script for Windows",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "22/Sep/17 01:26",
        "Updated": "25/Sep/17 12:42",
        "Resolved": "25/Sep/17 12:42",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1111": {
        "Key": "PARQUET-1111",
        "Summary": "dev/release/verify-release-candidate has stale help",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.0",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "22/Sep/17 05:56",
        "Updated": "25/Sep/17 12:43",
        "Resolved": "25/Sep/17 12:43",
        "Description": "The help of dev/release/verify-release-candidate is stale, it takes two parameters but only says it needs one. I'll push a PR momentarily.",
        "Issue Links": []
    },
    "PARQUET-1112": {
        "Key": "PARQUET-1112",
        "Summary": "ParquetFileReader::OpenFile(string) not available in conda binary",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Ehsan Totoni",
        "Created": "22/Sep/17 23:04",
        "Updated": "24/Sep/17 17:05",
        "Resolved": "24/Sep/17 16:46",
        "Description": "When I install parquet-cpp through conda, the ParquetFileReader::OpenFile(string) function is not available in the library binary. 'nm' only shows the version of the function with (ReaderProperties, FileMetaData) input.\nThe function is available when building parquet-cpp from source though.",
        "Issue Links": []
    },
    "PARQUET-1113": {
        "Key": "PARQUET-1113",
        "Summary": "[C++] Incorporate fix from ARROW-1601 on bitmap read path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Rene Sugar",
        "Reporter": "Wes McKinney",
        "Created": "24/Sep/17 15:26",
        "Updated": "13/Jan/18 12:10",
        "Resolved": "13/Jan/18 12:10",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1114": {
        "Key": "PARQUET-1114",
        "Summary": "Apply fix for ARROW-1601 and ARROW-1611 to parquet-cpp",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Rene Sugar",
        "Reporter": "Rene Sugar",
        "Created": "25/Sep/17 22:43",
        "Updated": "28/Sep/17 19:45",
        "Resolved": "27/Sep/17 04:29",
        "Description": "See ARROW-1601 and ARROW-1611 on how to reproduce the crash. Fix for ARROW-1601 and ARROW-1611 need to be applied to parquet-cpp.\nOld pull request (close):\nhttps://github.com/apache/parquet-cpp/pull/404\nNew pull request with suggested changes:\nhttps://github.com/apache/parquet-cpp/pull/405",
        "Issue Links": []
    },
    "PARQUET-1115": {
        "Key": "PARQUET-1115",
        "Summary": "Warn users when misusing parquet-tools merge",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "27/Sep/17 17:16",
        "Updated": "25/Aug/22 01:29",
        "Resolved": "07/Nov/17 14:44",
        "Description": "To prevent users from using parquet-tools merge in scenarios where its use is not practical, we should describe its limitations in the help text of this command. Additionally, we should add a warning to the output of the merge command if the size of the original row groups are below a threshold.\nReasoning:\nMany users are tempted to use the new parquet-tools merge functionality, because they want to achieve good performance and historically that has been associated with large Parquet files. However, in practice Hive performance won't change significantly after using parquet-tools merge, but Impala performance will be much worse. The reason for that is that good performance is not a result of large files but large rowgroups instead (up to the HDFS block size).\nHowever, parquet-tools merge does not merge rowgroups, it just places them one after the other. It was intended to be used for Parquet files that are already arranged in row groups of the desired size. When used to merge many small files, the resulting file will still contain small row groups and one loses most of the advantages of larger files (the only one that remains is that it takes a single HDFS operation to read them).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/433"
        ]
    },
    "PARQUET-1116": {
        "Key": "PARQUET-1116",
        "Summary": "Add Yetus InterfaceAudience annotations to Parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "27/Sep/17 17:32",
        "Updated": "27/Sep/17 17:32",
        "Resolved": null,
        "Description": "Parquet should use Yetus InterfaceAudience annotations to specify its API's intended audience (public/private) and optionally stability (unstable/evolving/stable) in code annotations both for the benefit of its users and also for ensuring that no breaking changes happen in the public API.",
        "Issue Links": []
    },
    "PARQUET-1117": {
        "Key": "PARQUET-1117",
        "Summary": "ParquetRecordWriter does not provide interface like getRowCount(),getRawDataSize() like org.apache.orc.Writer",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "liyunzhang",
        "Created": "28/Sep/17 03:43",
        "Updated": "28/Sep/17 03:43",
        "Resolved": null,
        "Description": "Hive with orc can update the statistics like rowCount,rawDataSize after loading data to table. Hive with parquet cannot and need to use analyze command like \"analyze table xxx compute statistics noscan\" to update these two statistics info.  The reason is ParquetRecordWriter used in hive does not provide interfaces like getRowCount(),getRawDataSize(). While org.apache.orc.Writer  provides these two interfaces.  Anyone knows how to get rowCount and rawDataSize in ParquetRecordWriter?",
        "Issue Links": []
    },
    "PARQUET-1118": {
        "Key": "PARQUET-1118",
        "Summary": "Build a corpus of Parquet files that client implementations can use for validation",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Lars Volker",
        "Created": "28/Sep/17 23:43",
        "Updated": "28/Jan/19 14:06",
        "Resolved": null,
        "Description": "We should build a corpus of Parquet files that client implementations can use for validation. In addition to the input files, it should contain a description or a verbatim copy of the data in each file, so that readers can validate their results.\nAs a starting point we can look at the old parquet-compatibility repo and Impala's test data, in particular the Parquet files it contains.\n\n$\u00a0find testdata | grep -i parq\ntestdata/workloads/tpch/queries/insert_parquet.test\ntestdata/workloads/functional-planner/queries/PlannerTest/parquet-filtering.test\ntestdata/workloads/functional-planner/queries/PlannerTest/parquet-stats-agg.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-filtering.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-zero-rows.test\ntestdata/workloads/functional-query/queries/QueryTest/insert_parquet_invalid_codec.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-corrupt-rle-counts-abort.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-ambiguous-list-legacy.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-stats-agg.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-deprecated-stats.test\ntestdata/workloads/functional-query/queries/QueryTest/nested-types-parquet-stats.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-resolution-by-name.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-abort-on-error.test\ntestdata/workloads/functional-query/queries/QueryTest/mt-dop-parquet.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-corrupt-rle-counts.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-continue-on-error.test\ntestdata/workloads/functional-query/queries/QueryTest/mt-dop-parquet-nested.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-ambiguous-list-modern.test\ntestdata/workloads/functional-query/queries/QueryTest/parquet-stats.test\ntestdata/max_nesting_depth/int_map/file.parq\ntestdata/max_nesting_depth/struct/file.parq\ntestdata/max_nesting_depth/struct_map/file.parq\ntestdata/max_nesting_depth/int_array/file.parq\ntestdata/max_nesting_depth/struct_array/file.parq\ntestdata/parquet_nested_types_encodings\ntestdata/parquet_nested_types_encodings/README\ntestdata/parquet_nested_types_encodings/UnannotatedListOfGroups.parquet\ntestdata/parquet_nested_types_encodings/AmbiguousList_Modern.parquet\ntestdata/parquet_nested_types_encodings/UnannotatedListOfPrimitives.parquet\ntestdata/parquet_nested_types_encodings/AmbiguousList.json\ntestdata/parquet_nested_types_encodings/AvroPrimitiveInList.parquet\ntestdata/parquet_nested_types_encodings/ThriftPrimitiveInList.parquet\ntestdata/parquet_nested_types_encodings/bad-avro.parquet\ntestdata/parquet_nested_types_encodings/AmbiguousList.avsc\ntestdata/parquet_nested_types_encodings/SingleFieldGroupInList.parquet\ntestdata/parquet_nested_types_encodings/ThriftSingleFieldGroupInList.parquet\ntestdata/parquet_nested_types_encodings/AvroSingleFieldGroupInList.parquet\ntestdata/parquet_nested_types_encodings/AmbiguousList_Legacy.parquet\ntestdata/parquet_nested_types_encodings/bad-thrift.parquet\ntestdata/ComplexTypesTbl/nonnullable.parq\ntestdata/ComplexTypesTbl/nullable.parq\ntestdata/bad_parquet_data\ntestdata/bad_parquet_data/README\ntestdata/bad_parquet_data/dict-encoded-out-of-bounds.parq\ntestdata/bad_parquet_data/plain-encoded-negative-len.parq\ntestdata/bad_parquet_data/plain-encoded-out-of-bounds.parq\ntestdata/bad_parquet_data/dict-encoded-negative-len.parq\ntestdata/parquet_schema_resolution\ntestdata/parquet_schema_resolution/README\ntestdata/parquet_schema_resolution/switched_map.json\ntestdata/parquet_schema_resolution/switched_map.avsc\ntestdata/parquet_schema_resolution/switched_map.parq\ntestdata/src/main/java/org/apache/impala/datagenerator/JsonToParquetConverter.java\ntestdata/LineItemMultiBlock/lineitem_one_row_group.parquet\ntestdata/LineItemMultiBlock/lineitem_sixblocks.parquet\ntestdata/data/zero_rows_zero_row_groups.parquet\ntestdata/data/chars-formats.parquet\ntestdata/data/multiple_rowgroups.parquet\ntestdata/data/bad_parquet_data.parquet\ntestdata/data/bad_metadata_len.parquet\ntestdata/data/huge_num_rows.parquet\ntestdata/data/bad_compressed_size.parquet\ntestdata/data/zero_rows_one_row_group.parquet\ntestdata/data/bad_rle_repeat_count.parquet\ntestdata/data/bad_column_metadata.parquet\ntestdata/data/alltypesagg_hive_13_1.parquet\ntestdata/data/bad_dict_page_offset.parquet\ntestdata/data/bad_rle_literal_count.parquet\ntestdata/data/bad_magic_number.parquet\ntestdata/data/repeated_values.parquet\ntestdata/data/schemas/malformed_decimal_tiny.parquet\ntestdata/data/schemas/alltypestiny.parquet\ntestdata/data/schemas/nested/modern_nested.parquet\ntestdata/data/schemas/nested/legacy_nested.parquet\ntestdata/data/schemas/enum/enum.parquet\ntestdata/data/schemas/decimal.parquet\ntestdata/data/schemas/zipcode_incomes.parquet\ntestdata/data/repeated_root_schema.parquet\ntestdata/data/long_page_header.parquet\ntestdata/data/deprecated_statistics.parquet\ntestdata/data/kite_required_fields.parquet\ntestdata/data/out_of_range_timestamp.parquet\n\n\nImpala also has a tool to generate Parquet files from JSON files: https://github.com/apache/incubator-impala/blob/master/testdata/src/main/java/org/apache/impala/datagenerator/JsonToParquetConverter.java\nArrow has a similar tool: https://github.com/apache/arrow/blob/master/integration/integration_test.py",
        "Issue Links": [
            "/jira/browse/PARQUET-1241"
        ]
    },
    "PARQUET-1119": {
        "Key": "PARQUET-1119",
        "Summary": "Reduce memory footprint for nulls caching",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Boris Molodenkov",
        "Created": "01/Oct/17 20:54",
        "Updated": "01/Oct/17 20:55",
        "Resolved": null,
        "Description": "Scenario:\nThere is a schema with many optional groups, e.g.\n\nmessage example {\n  required binary id (UTF8);\n  optional group a1 (LIST) {\n    repeated int64 array;\n  }\n  optional group a2 (LIST) {\n    repeated int64 array;\n  }\n...\n  optional group aN (LIST) {\n    repeated int64 array;\n  }\n}\n\n\nMany records without optional parameters are written.\nIn this case groupNullCache will contain many elements which all are zeros.",
        "Issue Links": [
            "/jira/browse/PARQUET-343"
        ]
    },
    "PARQUET-1120": {
        "Key": "PARQUET-1120",
        "Summary": "Parquet-mr project build fails with parquet format 2.3.2-SNAPSHOT",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Krishnaprasad A S",
        "Created": "03/Oct/17 07:21",
        "Updated": "18/Apr/18 13:34",
        "Resolved": null,
        "Description": "current parquet-mr (1.9.1-SNAPSHOT) project build fails with parquet-format 2.3.2-SNAPSHOT. Which blocks to incorporate the parquet-format related changes into parquet-mr project.",
        "Issue Links": []
    },
    "PARQUET-1121": {
        "Key": "PARQUET-1121",
        "Summary": "C++: DictionaryArrays of NullType cannot be written",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.0",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "05/Oct/17 11:38",
        "Updated": "07/Oct/17 20:28",
        "Resolved": "07/Oct/17 20:28",
        "Description": "Current exception: \n\nArrowNotImplementedError: No cast implemented from dictionary<values=null, indices=int8, ordered=0> to null",
        "Issue Links": []
    },
    "PARQUET-1122": {
        "Key": "PARQUET-1122",
        "Summary": "[C++] Support 2-level list encoding in Arrow decoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Luke Higgins",
        "Created": "05/Oct/17 15:06",
        "Updated": "03/Apr/21 03:40",
        "Resolved": "03/Apr/21 03:40",
        "Description": "While trying to read a parquetfile (written by nifi) I am getting an error.\ncode:\nimport pyarrow.parquet as pq\nt = pq.read_table('test.parq')\nerror:\nTraceback (most recent call last):\n  File \"parquet_reader.py\", line 2, in <module>\n    t = pq.read_table('test.parq')\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py\", line 823, in read_table\n    use_pandas_metadata=use_pandas_metadata)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py\", line 119, in read\n    nthreads=nthreads)\n  File \"pyarrow/_parquet.pyx\", line 466, in pyarrow._parquet.ParquetReader.read_all (/arrow/python/build/temp.linux-x86_64-3.6/_parquet.cxx:9181)\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status (/arrow/python/build/temp.linux-x86_64-3.6/lib.cxx:8115)\npyarrow.lib.ArrowNotImplementedError: No support for reading columns of type list<array: string not null>",
        "Issue Links": []
    },
    "PARQUET-1123": {
        "Key": "PARQUET-1123",
        "Summary": "[C++] Update parquet-cpp to use Arrow's AssertArraysEqual",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "06/Oct/17 20:48",
        "Updated": "07/Oct/17 19:47",
        "Resolved": "07/Oct/17 19:47",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1124": {
        "Key": "PARQUET-1124",
        "Summary": "Add new compression codecs to the Parquet spec",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "07/Oct/17 00:17",
        "Updated": "23/Nov/17 00:05",
        "Resolved": "10/Oct/17 20:02",
        "Description": "After recent tests, I think we should add Zstd to the spec.\nI'm also proposing we add LZ4 because it is widely available and outperforms snappy. As a successor for fast compression but not necessarily good compression ratios, I think it makes sense to have it.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/PARQUET-970",
            "https://docs.google.com/spreadsheets/d/1MAPrKHJn1li4MEbtQ9-T1Myu-AI0AshTPSC6C0ttuIw/edit#gid=1749919921",
            "https://github.com/apache/parquet-format/pull/70"
        ]
    },
    "PARQUET-1125": {
        "Key": "PARQUET-1125",
        "Summary": "Add UUID logical type",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "07/Oct/17 00:28",
        "Updated": "14/Jan/21 11:51",
        "Resolved": "10/Oct/17 19:53",
        "Description": "I think we should add a UUID logical type that is stored in a 16-byte fixed. The common string representation is 36 bytes instead of the 16 required. UUIDs are commonly used as unique identifiers, so it makes sense to have a good support. A binary representation will reduce memory when writing or building bloom filters and will reduce cycles needed to compare values.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "/jira/browse/DRILL-7825",
            "/jira/browse/DRILL-7829",
            "https://github.com/apache/parquet-format/pull/71"
        ]
    },
    "PARQUET-1126": {
        "Key": "PARQUET-1126",
        "Summary": "make it easy to read and write parquet files in java without depending on hadoop",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Oscar Boykin",
        "Created": "07/Oct/17 01:43",
        "Updated": "15/Dec/20 19:04",
        "Resolved": null,
        "Description": "I am happy to help with this but I'd love some guidance on:\n1) likelihood of being accepted as a patch.\n2) how critical it is to maintain backwards compatibility in APIs.\nFor instance, we probably want to introduce a new artifact that lives under the existing hadoop depending artifact, and move as much code as possible to that, keeping the hadoop apis in the old artifact.\nWelcome comments on solving this issue.",
        "Issue Links": [
            "/jira/browse/BEAM-4379",
            "/jira/browse/PARQUET-1142",
            "/jira/browse/PARQUET-1953",
            "/jira/browse/PARQUET-1776"
        ]
    },
    "PARQUET-1127": {
        "Key": "ARROW-3773",
        "Summary": "[C++] Remove duplicated AssertArraysEqual code in parquet/arrow/arrow-reader-writer-test.cc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "C++",
        "Assignee": "Wes McKinney",
        "Reporter": "Phillip Cloud",
        "Created": "08/Oct/17 00:27",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "20/Nov/18 02:03",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2999"
        ]
    },
    "PARQUET-1128": {
        "Key": "PARQUET-1128",
        "Summary": "[Java] Upgrade the Apache Arrow version to 0.8.0 for SchemaConverter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "09/Oct/17 13:05",
        "Updated": "21/Apr/18 14:00",
        "Resolved": "21/Apr/18 13:59",
        "Description": "When I converted parquet(1.9.1-SNAPSHOT) schema to arrow(0.4.0) with SchemaConverter, this exception raised.\n\n\r\njava.lang.NoClassDefFoundError: org/apache/arrow/vector/types/pojo/ArrowType$Struct_\r\n\r\n\tat net.wrap_trap.parquet_arrow.ParquetToArrowConverter.convertToArrow(ParquetToArrowConverter.java:67)\r\n\tat net.wrap_trap.parquet_arrow.ParquetToArrowConverter.convertToArrow(ParquetToArrowConverter.java:40)\r\n\tat net.wrap_trap.parquet_arrow.ParquetToArrowConverterTest.parquetToArrowConverterTest(ParquetToArrowConverterTest.java:27)\r\n\n\nThis reason is that SchemaConverter refer to Apache Arrow 0.1.0.\nI upgrade the Apache Arrow version to 0.8.0 for SchemaConverter.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/443"
        ]
    },
    "PARQUET-1129": {
        "Key": "PARQUET-1129",
        "Summary": "Cannot build parquet-tools as per instructions",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sam",
        "Created": "10/Oct/17 10:37",
        "Updated": "16/Jan/18 21:25",
        "Resolved": null,
        "Description": "If I follow https://github.com/nielsbasjes/parquet-mr/tree/master/parquet-tools\nI.e. clone and run\n\n\r\ncd parquet-tools && mvn clean package -Plocal \r\n\n\nthen I get\n\n\r\n[INFO] Scanning for projects...\r\n[INFO]                                                                         \r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Building Apache Parquet Tools 1.9.1-SNAPSHOT\r\n[INFO] ------------------------------------------------------------------------\r\n[WARNING] The POM for org.apache.parquet:parquet-hadoop:jar:1.9.1-SNAPSHOT is missing, no dependency information available\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 0.846s\r\n[INFO] Finished at: Tue Oct 10 10:36:37 UTC 2017\r\n[INFO] Final Memory: 11M/483M\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal on project parquet-tools: Could not resolve dependencies for project org.apache.parquet:parquet-tools:jar:1.9.1-SNAPSHOT: Failure to find org.apache.parquet:parquet-hadoop:jar:1.9.1-SNAPSHOT in http://repository.apache.org/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots has elapsed or updates are forced -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException",
        "Issue Links": []
    },
    "PARQUET-1130": {
        "Key": "PARQUET-1130",
        "Summary": "Parquet-cli convert-csv : Missing option -o for output file path in examples",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Christophe SEBILLE",
        "Created": "10/Oct/17 10:58",
        "Updated": "10/Oct/17 20:00",
        "Resolved": null,
        "Description": "Examples fail to launch with the current parameters as output file name is not preceded by -o option\n{{        \"# Create a Parquet file from a CSV file\",\n        \"sample.csv -o sample.parquet --schema schema.avsc\",\n        \"# Create a Parquet file in HDFS from local CSV\",\n        \"path/to/sample.csv -o hdfs:/user/me/sample.parquet --schema schema.avsc\",\n        \"# Create an Avro file from CSV data in S3\",\n        \"s3:/data/path/sample.csv -o sample.avro --format avro --schema s3:/schemas/schema.avsc\"}}\nI have checked the first example with the -o, it works.\nI have not checked the second and third examples.\nBest regards,\nChristophe",
        "Issue Links": []
    },
    "PARQUET-1131": {
        "Key": "PARQUET-1131",
        "Summary": "Parquet-cli convert-csv : Unknown --format option in example",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Christophe SEBILLE",
        "Created": "10/Oct/17 11:08",
        "Updated": "10/Oct/17 11:08",
        "Resolved": null,
        "Description": "In the following example :\n        \"# Create an Avro file from CSV data in S3\",\n        \"s3:/data/path/sample.csv -o sample.avro --format avro --schema s3:/schemas/schema.avsc\"\nthe option --format doesn't exist (or doesn't exist anymore)\nIt seems it has been replaced by the to-avro command of parquet-cli \nSo, example could simply be :\n        \"# Create a parquet file from CSV data in S3\",\n        \"s3:/data/path/sample.csv -o sample.parquet --schema s3:/schemas/schema.avsc\"\nBest regards,\nChristophe",
        "Issue Links": []
    },
    "PARQUET-1132": {
        "Key": "PARQUET-1132",
        "Summary": "parquet-tools merge does not work nor give any error message",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sam",
        "Created": "10/Oct/17 13:19",
        "Updated": "10/Oct/17 13:19",
        "Resolved": null,
        "Description": "When I run \n\n\r\njava -jar ./parquet-mr/parquet-tools/target/parquet-tools-1.9.1-SNAPSHOT.jar merge $inpath $outpath\r\n\n\nIt does not work, no matter what combination of $inpath or $outpath I use.\nIt gives exit code 1, but never produces an error message or any indication of what is wrong.",
        "Issue Links": []
    },
    "PARQUET-1133": {
        "Key": "PARQUET-1133",
        "Summary": "INT96 types and Maps without OriginalType cause exceptions in PigSchemaConverter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "1.9.0",
        "Component/s": "parquet-pig",
        "Assignee": "Addisu Feyissa",
        "Reporter": "Addisu Feyissa",
        "Created": "10/Oct/17 19:30",
        "Updated": "10/Oct/17 23:22",
        "Resolved": "10/Oct/17 23:22",
        "Description": "Trying to load parquet files in Pig, that have the following causes an exception and parsing to fail:\n\nINT96 fields, for example:\n\nmessage spark_schema {\r\n  optional int96 datetime;\r\n}\r\n\n\nThe Exception thrown is:\n\nFailed to parse: can't convert optional int96 myInt96\r\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:201)\r\n\tat org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1791)\r\n\tat org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1764)\r\n\tat org.apache.pig.PigServer.registerQuery(PigServer.java:707)\r\n\tat org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1075)\r\n\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:505)\r\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:231)\r\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:206)\r\n\tat org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)\r\n\tat org.apache.pig.Main.run(Main.java:564)\r\n\tat org.apache.pig.Main.main(Main.java:176)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:234)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\r\nCaused by: org.apache.parquet.pig.SchemaConversionException: can't convert optional int96 myInt96\r\n\tat org.apache.parquet.pig.PigSchemaConverter.convertFields(PigSchemaConverter.java:202)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.convert(PigSchemaConverter.java:178)\r\n\tat org.apache.parquet.pig.TupleReadSupport.getPigSchemaFromMultipleFiles(TupleReadSupport.java:95)\r\n\tat org.apache.parquet.pig.ParquetLoader.initSchema(ParquetLoader.java:300)\r\n\tat org.apache.parquet.pig.ParquetLoader.setInput(ParquetLoader.java:183)\r\n\tat org.apache.parquet.pig.ParquetLoader.getSchema(ParquetLoader.java:285)\r\n\tat org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:175)\r\n\tat org.apache.pig.newplan.logical.relational.LOLoad.<init>(LOLoad.java:89)\r\n\tat org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:901)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3568)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1625)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)\r\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)\r\n\t... 16 more\r\nCaused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: NYI\r\n\tat org.apache.parquet.pig.PigSchemaConverter$1.convertINT96(PigSchemaConverter.java:242)\r\n\tat org.apache.parquet.pig.PigSchemaConverter$1.convertINT96(PigSchemaConverter.java:214)\r\n\tat org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName$7.convert(PrimitiveType.java:223)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.getSimpleFieldSchema(PigSchemaConverter.java:213)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.getFieldSchema(PigSchemaConverter.java:320)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.convertFields(PigSchemaConverter.java:193)\r\n\t... 30 more\r\n\n\n\n\nMap Types without OriginalType, for example:\n \nmessage spark_schema {\r\n  optional binary a;\r\n  optional group b (MAP) {\r\n    repeated group map {\r\n      required binary key;\r\n      optional group value {\r\n        optional fixed_len_byte_array(5) c;\r\n        optional fixed_len_byte_array(7) d;\r\n      }\r\n    }\r\n  }\r\n}\r\n\n\nThe Exception thrown is:\n\nERROR 1200: null\r\n\r\nFailed to parse: null\r\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:201)\r\n\tat org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1791)\r\n\tat org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1764)\r\n\tat org.apache.pig.PigServer.registerQuery(PigServer.java:707)\r\n\tat org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1075)\r\n\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:505)\r\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:231)\r\n\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:206)\r\n\tat org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)\r\n\tat org.apache.pig.Main.run(Main.java:564)\r\n\tat org.apache.pig.Main.main(Main.java:176)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:234)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.parquet.pig.PigSchemaConverter.getComplexFieldSchema(PigSchemaConverter.java:281)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.getFieldSchema(PigSchemaConverter.java:322)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.convertFields(PigSchemaConverter.java:193)\r\n\tat org.apache.parquet.pig.PigSchemaConverter.convert(PigSchemaConverter.java:178)\r\n\tat org.apache.parquet.pig.TupleReadSupport.getPigSchemaFromMultipleFiles(TupleReadSupport.java:95)\r\n\tat org.apache.parquet.pig.ParquetLoader.initSchema(ParquetLoader.java:300)\r\n\tat org.apache.parquet.pig.ParquetLoader.setInput(ParquetLoader.java:183)\r\n\tat org.apache.parquet.pig.ParquetLoader.getSchema(ParquetLoader.java:285)\r\n\tat org.apache.pig.newplan.logical.relational.LOLoad.getSchemaFromMetaData(LOLoad.java:175)\r\n\tat org.apache.pig.newplan.logical.relational.LOLoad.<init>(LOLoad.java:89)\r\n\tat org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:901)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3568)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1625)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)\r\n\tat org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)\r\n\tat org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)\r\n\t... 16 more",
        "Issue Links": []
    },
    "PARQUET-1134": {
        "Key": "PARQUET-1134",
        "Summary": "Release Parquet format 2.4.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.4.0",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "10/Oct/17 19:58",
        "Updated": "13/Nov/17 16:51",
        "Resolved": "13/Nov/17 16:51",
        "Description": "This is an umbrella issue for the 2.4.0 release. Please add any issues you'd like to get into that release as blockers.",
        "Issue Links": [
            "/jira/browse/PARQUET-1143",
            "/jira/browse/PARQUET-412",
            "/jira/browse/PARQUET-686",
            "/jira/browse/PARQUET-1136",
            "/jira/browse/PARQUET-371",
            "/jira/browse/PARQUET-609",
            "/jira/browse/PARQUET-922",
            "/jira/browse/PARQUET-1124",
            "/jira/browse/PARQUET-1125",
            "/jira/browse/PARQUET-1144",
            "/jira/browse/PARQUET-906"
        ]
    },
    "PARQUET-1135": {
        "Key": "PARQUET-1135",
        "Summary": "upgrade thrift and protobuf dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0,                                            1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "10/Oct/17 22:30",
        "Updated": "13/Nov/19 12:15",
        "Resolved": "10/Mar/18 00:17",
        "Description": "thrift 0.7.0 -> 0.9.3\n protobuf 3.2 -> 3.5.1",
        "Issue Links": [
            "/jira/browse/PARQUET-1103",
            "https://github.com/apache/parquet-mr/pull/427",
            "https://github.com/apache/parquet-mr/pull/488"
        ]
    },
    "PARQUET-1136": {
        "Key": "PARQUET-1136",
        "Summary": "Makefile is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "11/Oct/17 15:55",
        "Updated": "12/Oct/17 16:05",
        "Resolved": "12/Oct/17 16:05",
        "Description": "The path to the parquet.thrift file in the Makefile is broken. I'll send a PR shortly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "https://github.com/apache/parquet-format/pull/73"
        ]
    },
    "PARQUET-1137": {
        "Key": "PARQUET-1137",
        "Summary": "Check Makefile in Travis build",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "12/Oct/17 20:44",
        "Updated": "12/Oct/17 20:44",
        "Resolved": null,
        "Description": "We recently figured out that the Makefile was broken and it would be best to check it during the travis tests. I have a fix locally that I'll rebase and push once PR #72 has been merged.",
        "Issue Links": []
    },
    "PARQUET-1138": {
        "Key": "PARQUET-1138",
        "Summary": "[C++] Fix compilation with Arrow 0.7.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/Oct/17 23:49",
        "Updated": "16/Oct/17 18:58",
        "Resolved": "16/Oct/17 18:58",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1139": {
        "Key": "PARQUET-1139",
        "Summary": "Add license to cmake_modules/parquet-cppConfig.cmake.in",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.0",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "16/Oct/17 16:52",
        "Updated": "17/Oct/17 18:08",
        "Resolved": "17/Oct/17 18:08",
        "Description": "The file misses a license header, RAT complains about it. I'll push a PR shortly.",
        "Issue Links": []
    },
    "PARQUET-1140": {
        "Key": "PARQUET-1140",
        "Summary": "[C++] Fail on RAT errors in CI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "16/Oct/17 17:05",
        "Updated": "20/Oct/17 17:05",
        "Resolved": "20/Oct/17 17:05",
        "Description": "See relevant bits in CI scripts for Apache Arrow or Apache Kudu",
        "Issue Links": []
    },
    "PARQUET-1141": {
        "Key": "PARQUET-1141",
        "Summary": "IDs are dropped in metadata conversion",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0,                                            1.8.2",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "17/Oct/17 16:55",
        "Updated": "30/Mar/18 21:39",
        "Resolved": "04/Jan/18 18:34",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/428"
        ]
    },
    "PARQUET-1142": {
        "Key": "PARQUET-1142",
        "Summary": "Avoid leaking Hadoop API to downstream libraries",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "17/Oct/17 17:09",
        "Updated": "01/Mar/19 14:59",
        "Resolved": "13/Dec/17 19:29",
        "Description": "Parquet currently leaks the Hadoop API by requiring callers to pass Path and Configuration instances, and by using Hadoop codecs. InputFile and SeekableInputStream add alternatives to Hadoop classes in some parts of the read path, but this needs to be extended to the write path and to avoid passing options through Configuration.",
        "Issue Links": [
            "/jira/browse/PARQUET-1126",
            "https://github.com/apache/parquet-mr/pull/429"
        ]
    },
    "PARQUET-1143": {
        "Key": "PARQUET-1143",
        "Summary": "Update Java for format 2.4.0 changes",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0,                                            1.8.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "17/Oct/17 17:31",
        "Updated": "04/Apr/21 20:41",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "https://github.com/apache/parquet-mr/pull/430",
            "https://github.com/apache/parquet-mr/pull/430"
        ]
    },
    "PARQUET-1144": {
        "Key": "PARQUET-1144",
        "Summary": "Parquet format should not depend on slf4j-nop",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.4.0",
        "Fix Version/s": "format-2.4.0",
        "Component/s": "parquet-format",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "17/Oct/17 19:17",
        "Updated": "13/Nov/17 16:52",
        "Resolved": "13/Nov/17 16:52",
        "Description": "Format 2.4.0 no longer shades slf4j-api and slf4j-nop. The no-op logger implementation should be removed. It was previously included to avoid stderr messages complaining that there was no log implementation for the shaded SLF4J. Now that the user's log implementation will be used, it is no longer necessary and is dangerous because it could make it to the runtime classpath and discard logs.",
        "Issue Links": [
            "/jira/browse/PARQUET-1134",
            "https://github.com/apache/parquet-format/pull/74"
        ]
    },
    "PARQUET-1145": {
        "Key": "PARQUET-1145",
        "Summary": "Add license to .gitignore and .travis.yml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.3.1,                                            format-2.4.0",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "None",
        "Assignee": "Lars Volker",
        "Reporter": "Lars Volker",
        "Created": "17/Oct/17 22:42",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "13/Nov/17 12:57",
        "Description": "The .gitignore file could have the ASF license. I'll post a PR momentarily.",
        "Issue Links": []
    },
    "PARQUET-1146": {
        "Key": "PARQUET-1146",
        "Summary": "C++: Add macOS-compatible sha512sum call to release verify script",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "22/Oct/17 11:39",
        "Updated": "19/Nov/17 15:33",
        "Resolved": "19/Nov/17 15:33",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/414"
        ]
    },
    "PARQUET-1147": {
        "Key": "PARQUET-1147",
        "Summary": "[C++] Account for API deprecation / change in ARROW-1671",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "23/Oct/17 01:41",
        "Updated": "13/Jan/18 12:08",
        "Resolved": "13/Jan/18 12:08",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1148": {
        "Key": "PARQUET-1148",
        "Summary": "[C++] Code coverage has been broken since June 23",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Wes McKinney",
        "Created": "24/Oct/17 13:37",
        "Updated": "19/Nov/18 19:30",
        "Resolved": "19/Nov/18 19:30",
        "Description": "PR #357 is the smoking gun, but we'll need to investigate",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/415"
        ]
    },
    "PARQUET-1149": {
        "Key": "PARQUET-1149",
        "Summary": "Upgrade Avro dependancy to 1.8.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fokko Driesprong",
        "Created": "27/Oct/17 15:25",
        "Updated": "30/Mar/18 21:39",
        "Resolved": "10/Nov/17 10:38",
        "Description": "I would like to update the Avro dependancy to 1.8.2.",
        "Issue Links": []
    },
    "PARQUET-1150": {
        "Key": "PARQUET-1150",
        "Summary": "C++: Hide statically linked boost symbols",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "28/Oct/17 14:10",
        "Updated": "30/Oct/17 07:56",
        "Resolved": "30/Oct/17 07:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1151": {
        "Key": "PARQUET-1151",
        "Summary": "[C++] Add build options / configuration to use static runtime libraries with MSVC",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Wes McKinney",
        "Created": "30/Oct/17 13:57",
        "Updated": "24/Jan/18 22:23",
        "Resolved": "24/Jan/18 22:23",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/429"
        ]
    },
    "PARQUET-1152": {
        "Key": "PARQUET-1152",
        "Summary": "Parquet-thrift doesn't compile with Thrift 0.9.3",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "30/Oct/17 16:33",
        "Updated": "30/Mar/18 21:40",
        "Resolved": "09/Nov/17 10:27",
        "Description": "Parquet-thrift doesn't compile with Thrift 0.9.3, because TBinaryProtocol#setReadLength method was removed.\nPARQUET-180 already addressed the problem, but only in runtime.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/432"
        ]
    },
    "PARQUET-1153": {
        "Key": "PARQUET-1153",
        "Summary": "Parquet-thrift doesn't compile with Thrift 0.10.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "02/Nov/17 09:23",
        "Updated": "30/Mar/18 21:40",
        "Resolved": "09/Nov/17 16:18",
        "Description": "Parquet-thrift doesn't compile with Thrift 0.10.0 due to THRIFT-2263. The default generator parameter used for --gen argument by Thrift Maven plugin is no longer supported, this can be fixed with an additional <generator>java</generator> parameter to Thrift Maven plugin.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/434"
        ]
    },
    "PARQUET-1154": {
        "Key": "PARQUET-1154",
        "Summary": "[C++] Add function to concatenate a collection of Parquet files to create a new single file",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "04/Nov/17 19:25",
        "Updated": "04/Nov/17 23:33",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1155": {
        "Key": "PARQUET-1155",
        "Summary": "Support for GDPR erase requirements",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Machiel Groeneveld",
        "Created": "06/Nov/17 14:20",
        "Updated": "21/Dec/19 11:42",
        "Resolved": null,
        "Description": "As understand it Parquet is a write once thing. So mutating data inside Parquet files is not an option. Now there is a new cross EU law coming in effect May 2018 that requires companies to delete data pertaining a customer if being asked to do so.\nOur case is quite simple, our biggest parquet tables collect 7.5 billion rows a month. So removing data by duplicating this table whilst filtering out the unwanted customer data is not feasible. \nPerhaps there is some way to remove particular data? Or perhaps there is an efficient way to do read/filter/write? Perhaps zeroing the data is an idea to not change the layout of the files. \nNot sure if this is the right platform to start this discussion but I think more people will have this issue once it becomes clear that data needs to be deleted in all places, also in parquet files. Companies fase multi million dollar fines if they don't comply with GDPR.",
        "Issue Links": []
    },
    "PARQUET-1156": {
        "Key": "PARQUET-1156",
        "Summary": "dev/merge_parquet_pr.py problems",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0,                                            format-2.5.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "07/Nov/17 10:57",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "09/Jan/18 14:45",
        "Description": "I have run into several issues while trying to run dev/merge_parquet_pr.py according to the instructions:\n\nThe optional import jira.client is only checked for the resolve_jira() call, but the script fails much earlier if jira.client is not available in check_jira(), so a check should be added there as well. In fact, the script shouldn't even ask for JIRA_USERNAME and JIRA_PASSWORD if jira.client is not available.\nI had to issue pip install jira instead of pip install jira-python that was suggested by the script.\nOnce you have jira.client installed, the script still fails when following the instructions, because the instructions specify a remote named github-apache but the script tries to use apache-github instead. Either the instructions or the script should be updated.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/commit/81f480149054399d28b0609482c978788d9f5895",
            "https://github.com/apache/parquet-format/pull/78",
            "https://github.com/apache/parquet-mr/pull/437"
        ]
    },
    "PARQUET-1157": {
        "Key": "PARQUET-1157",
        "Summary": "Parquet Write bug - parquet data unreadable by hive or presto or spark 2.1",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Costas Piliotis",
        "Created": "09/Nov/17 04:13",
        "Updated": "02/Oct/19 09:00",
        "Resolved": null,
        "Description": "In our paradigm, we have a mapreduce output parquet data to s3, and then we use a spark job to consolidate these files from our staging area into target tables and add the partitions and modify tables as need be.\nWe have implemented and are using parquet schema merging in hive.    \nThe data written from our mapreduce task shows for this column the following metadata (written as parquet-avro):\n\n\r\n  optional group playerpositions_ai (LIST) {\r\n    repeated int32 array;\r\n  }\r\n\n\nHowever when spark writes it out it is converted.   We have tried both the legacy parquet format on and off.\nWith legacy \n\n\r\n  optional group playerpositions_ai (LIST) {\r\n    repeated group list {\r\n      optional int32 element;\r\n    }\r\n  }\r\n\n\nand with legacy:\n\n\r\n  optional group playerpositions_ai (LIST) {\r\n    repeated group bag {\r\n      optional int32 array;\r\n    }\r\n  }\r\n\n\nFrom what I've been reading in the spec the latter seems valid.\nSporadically we see some array columns producing odd failures in this parquet format on read:\n\n\r\nQuery 20171108_224243_00083_ec9ww failed: com.facebook.presto.spi.PrestoException\r\nCan not read value at 28857 in block 0 in file s3://.....\r\ncom.facebook.presto.hive.parquet.ParquetHiveRecordCursor.advanceNextPosition(ParquetHiveRecordCursor.java:232)\r\ncom.facebook.presto.hive.HiveCoercionRecordCursor.advanceNextPosition(HiveCoercionRecordCursor.java:98)\r\ncom.facebook.presto.hive.HiveRecordCursor.advanceNextPosition(HiveRecordCursor.java:179)\r\ncom.facebook.presto.spi.RecordPageSource.getNextPage(RecordPageSource.java:99)\r\ncom.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:247)\r\ncom.facebook.presto.operator.Driver.processInternal(Driver.java:378)\r\ncom.facebook.presto.operator.Driver.processFor(Driver.java:301)\r\ncom.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:622)\r\ncom.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:534)\r\ncom.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:670)\r\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\njava.lang.Thread.run(Thread.java:745)\r\n\n\nAnd in spark reading this file:\n\n\r\njava.lang.IllegalArgumentException: Reading past RLE/BitPacking stream.\r\n\tat org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)\r\n\tat org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readNext(RunLengthBitPackingHybridDecoder.java:82)\r\n\tat org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder.readInt(RunLengthBitPackingHybridDecoder.java:64)\r\n\tat org.apache.parquet.column.values.dictionary.DictionaryValuesReader.readInteger(DictionaryValuesReader.java:112)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl$2$3.read(ColumnReaderImpl.java:243)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.readValue(ColumnReaderImpl.java:464)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:370)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:405)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:218)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\n\nI'm perhaps hopeful that this bug has been fixed and is related to PARQUET-511\nFor giggles I also took this parquet data and loaded it into Amazon Athena (which is basically presto anyway) in hopes that it was corruption on our end and Athena is throwing the same thing \n\n\r\nHIVE_CURSOR_ERROR: Can not read value at 28857 in block 0 in file \r\n\n\nThe integer value isn't particularly interesting; it's a 0.\nThe parquet write command we used in spark is not particularly interesting.  \n\n\r\n      data.repartition(((data.count() / 10000000) + 1).toInt).write.format(\"parquet\")\r\n        .mode(\"append\")\r\n        .partitionBy(partitionColumns: _*)\r\n        .save(path)\r\n\n\nCurrently our vendor has not been successful in moving our libraries to parquet 1.9 at this time.   I believe this issue if it's related to PARQUET-511 should be resolved by our vendor, but I'm seeking clarification if this is in fact the case.\nMy version of parquet tools on my desktop:\n\ncan totally dump the contents of that column without error\nis on parquet 1.9\n\nAt this point I'm stumped and I believe this to be a bug somewhere.   \nIf this is a duplicate of PARQUET-511, cool, but if hive, presto, and spark are all struggling to read this file written out by spark I'm inclined to believe it's either spark or parquet library itself.",
        "Issue Links": []
    },
    "PARQUET-1158": {
        "Key": "PARQUET-1158",
        "Summary": "[C++] Basic RowGroup filtering",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "10/Nov/17 16:27",
        "Updated": "22/Aug/22 11:37",
        "Resolved": null,
        "Description": "See https://github.com/dask/fastparquet/blob/master/fastparquet/api.py#L296-L300\nWe should be able to translate this into C++ enums and apply in the Arrow read methods methods.",
        "Issue Links": [
            "/jira/browse/PARQUET-1392",
            "/jira/browse/ARROW-3774",
            "/jira/browse/ARROW-1796"
        ]
    },
    "PARQUET-1159": {
        "Key": "PARQUET-1159",
        "Summary": "[C++] Compatibility with C++ iterators",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-9.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Phillip Cloud",
        "Created": "10/Nov/17 23:19",
        "Updated": "28/Jun/22 10:24",
        "Resolved": "28/Jun/22 10:24",
        "Description": "There are some places where it would clean up the code quite a bit to use C++ STL iterators and be compatible with their APIs.\nAdditionally, in this PR (https://github.com/apache/parquet-cpp/pull/403) I had to allocate a separate vector to hold byte swapped values, when what I really want to do is iterate over the existing values in reverse (starting at the last valid byte) so I don't have to copy them into a separate container. This can be done with a std::reverse_iterator which allows one to use the ++ operator everywhere.",
        "Issue Links": []
    },
    "PARQUET-1160": {
        "Key": "PARQUET-1160",
        "Summary": "[C++] Implement BYTE_ARRAY-backed Decimal reads",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Ted Haining",
        "Reporter": "Phillip Cloud",
        "Created": "15/Nov/17 16:38",
        "Updated": "30/Sep/18 08:59",
        "Resolved": "30/Sep/18 08:58",
        "Description": "These are valid in the parquet spec, but it seems like no system in use today implements a writer for this type.\nWhat systems support writing Decimals with this underlying type?",
        "Issue Links": [
            "https://github.com/apache/parquet-testing/pull/1",
            "https://github.com/apache/parquet-cpp/pull/495",
            "https://github.com/apache/arrow/pull/2646"
        ]
    },
    "PARQUET-1161": {
        "Key": "PARQUET-1161",
        "Summary": "Broken links and formatting problems in Parquet documentation",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Roland Crosby",
        "Created": "17/Nov/17 19:26",
        "Updated": "17/Nov/17 19:26",
        "Resolved": null,
        "Description": "The links to LogicalTypes.md and Encodings.md in the Parquet documentation are broken. That page also has what appear to be some Markdown formatting issues:\n\nmultiple lists are rendering incorrectly: the list of types under the \"Types\" header, the list of data under the \"Data Pages\" header, and the list of extension points under the \"Extensibility\" header\nsome underscores that should be escaped are instead rendering as italics - see the last line under the \"Nested Encoding\" header",
        "Issue Links": []
    },
    "PARQUET-1162": {
        "Key": "PARQUET-1162",
        "Summary": "C++: Update dev/README after migration to Gitbox",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "19/Nov/17 15:37",
        "Updated": "19/Nov/17 22:43",
        "Resolved": "19/Nov/17 22:43",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/417"
        ]
    },
    "PARQUET-1163": {
        "Key": "PARQUET-1163",
        "Summary": "[C++] Add test function to compare underlying values of Arrow arrays, not including type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "19/Nov/17 19:20",
        "Updated": "19/Nov/17 23:49",
        "Resolved": null,
        "Description": "For unsigned integer typed Arrow arrays read in from parquet files we want to compare the values (which are stored as signed integers) but we don't want to fail an assertion because the types of the Arrow arrays are different.",
        "Issue Links": []
    },
    "PARQUET-1164": {
        "Key": "PARQUET-1164",
        "Summary": "[C++] Follow API changes in ARROW-1808",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/Nov/17 16:38",
        "Updated": "22/Nov/17 14:32",
        "Resolved": "22/Nov/17 14:32",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/418"
        ]
    },
    "PARQUET-1165": {
        "Key": "PARQUET-1165",
        "Summary": "[C++] Pin clang-format version to 4.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Wes McKinney",
        "Created": "20/Nov/17 16:45",
        "Updated": "11/Dec/17 20:21",
        "Resolved": "11/Dec/17 20:21",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/420"
        ]
    },
    "PARQUET-1166": {
        "Key": "PARQUET-1166",
        "Summary": "[API Proposal] Add GetRecordBatchReader in parquet/arrow/reader.h",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "YE",
        "Reporter": "YE",
        "Created": "27/Nov/17 05:59",
        "Updated": "24/Mar/18 02:46",
        "Resolved": "23/Mar/18 20:03",
        "Description": "Hi, I'd like to proposal a new API to better support splittable reading for Parquet File.\nThe intent for this API is that we can selective reading RowGroups(normally be contiguous, but can be arbitrary as long as the row_group_idxes are sorted and unique, [1, 3, 5] for example). \nThe proposed API would be something like this:\n\n\r\n::arrow::Status GetRecordBatchReader(const std::vector<int>& row_group_indices,\r\n                                                                std::shared_ptr<::arrow::RecordBatchReader>* out);\r\n                \r\n::arrow::Status GetRecordBatchReader(const std::vector<int>& row_group_indices,\r\n                                                                const std::vector<int>& column_indices,\r\n                                                                std::shared_ptr<::arrow::RecordBatchReader>* out);\r\n\r\n\n\nWith new API, we can split Parquet file into RowGroups and can be processed by multiple tasks(maybe be on different hosts, like the Map task in MapReduce)\nwesmckinnxhochy What do you think?",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/445"
        ]
    },
    "PARQUET-1167": {
        "Key": "PARQUET-1167",
        "Summary": "[C++] FieldToNode function should return a status when throwing an exception",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "03/Dec/17 16:59",
        "Updated": "04/Dec/17 07:59",
        "Resolved": "03/Dec/17 21:07",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/421"
        ]
    },
    "PARQUET-1168": {
        "Key": "PARQUET-1168",
        "Summary": "[C++] Table::Make no longer exists in Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "cpp-1.2.0",
        "Fix Version/s": "cpp-1.3.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "03/Dec/17 18:42",
        "Updated": "04/Dec/17 19:30",
        "Resolved": "04/Dec/17 19:30",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/422"
        ]
    },
    "PARQUET-1169": {
        "Key": "PARQUET-1169",
        "Summary": "[C++] Segment fault when using NextBatch of parquet::arrow::ColumnReader in parquet-cpp",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Jian Fang",
        "Created": "06/Dec/17 02:34",
        "Updated": "16/Aug/19 14:04",
        "Resolved": "16/Aug/19 14:04",
        "Description": "When I running the below code, I consistently get segment fault, not sure whether this is a bug or I did something wrong. Anyone here could help me take a look?\n\n\r\n#include <iostream>\r\n#include <string>\r\n\r\n#include \"arrow/array.h\"\r\n#include \"arrow/io/file.h\"\r\n#include \"arrow/test-util.h\"\r\n#include \"parquet/arrow/reader.h\"\r\n\r\nusing arrow::Array;\r\nusing arrow::default_memory_pool;\r\nusing arrow::io::FileMode;\r\nusing arrow::io::MemoryMappedFile;\r\nusing parquet::arrow::ColumnReader;\r\nusing parquet::arrow::FileReader;\r\nusing parquet::arrow::OpenFile;\r\n\r\nint main(int argc, char** argv) {\r\n  if (argc > 1) {\r\n    std::string file_name = argv[1];\r\n    std::shared_ptr<MemoryMappedFile> file;\r\n    ABORT_NOT_OK(MemoryMappedFile::Open(file_name, FileMode::READ, &file));\r\n    std::unique_ptr<FileReader> file_reader;\r\n    ABORT_NOT_OK(OpenFile(file, default_memory_pool(), &file_reader));\r\n    std::unique_ptr<ColumnReader> column_reader;\r\n    ABORT_NOT_OK(file_reader->GetColumn(0, &column_reader));\r\n\r\n    std::shared_ptr<Array> array1;\r\n    ABORT_NOT_OK(column_reader->NextBatch(1, &array1));\r\n    std::cout << \"length \" << array1->length() << std::endl;\r\n\r\n    std::shared_ptr<Array> array2;\r\n    // segment fault\r\n    ABORT_NOT_OK(column_reader->NextBatch(1, &array2));\r\n    std::cout << \"length \" << array2->length() << std::endl;\r\n  }\r\n  return 0;\r\n}\r\n\n\nCommand to compile this program:\n\n\r\ng++ test.c -I/usr/local/include/arrow -I/usr/local/include/parquet --std=c++11 -lparquet -larrow -lgtest -o parquet_test\r\n\n\nCommand to run the program\n\n\r\n./parquet_test test.parquet",
        "Issue Links": []
    },
    "PARQUET-1170": {
        "Key": "PARQUET-1170",
        "Summary": "Logical-type-based toString for proper representeation in tools/logs",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "06/Dec/17 12:18",
        "Updated": "22/Jan/18 16:22",
        "Resolved": "22/Jan/18 16:22",
        "Description": "Currently, we write the string representation of the values based on the primitive types only.\nFor example parquet-tools displays the min-max stats as signed numbers while UINT types are used which is misleading (min value might seem to be larger than the max value). Similar problem at displaying decimals binary values as UTF8 strings.",
        "Issue Links": [
            "/jira/browse/PARQUET-1025",
            "https://github.com/apache/parquet-mr/pull/448"
        ]
    },
    "PARQUET-1171": {
        "Key": "PARQUET-1171",
        "Summary": "[C++] Clarify valid uses for RLE, BIT_PACKED encodings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "06/Dec/17 14:24",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "10/Jan/18 03:05",
        "Description": "Currently we only support these encodings for levels but not for data.",
        "Issue Links": []
    },
    "PARQUET-1172": {
        "Key": "PARQUET-1172",
        "Summary": "Question on pig loader read parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0,                                            1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr,                                            parquet-pig",
        "Assignee": null,
        "Reporter": "abel_ke",
        "Created": "07/Dec/17 03:45",
        "Updated": "07/Dec/17 03:47",
        "Resolved": null,
        "Description": "When I use spark save parquet file, schema like this\n\noptional group attref (LIST) {\r\n       repeated group list {\r\n         optional group element {\r\n           optional binary nid (UTF8);\r\n           optional binary nss (UTF8);\r\n         }\r\n       }\r\n     }\r\n\n\nAnd then use parquet-pig-bundle to read this file, the read function can work, but when i need to access \"nid\" it have some problem\nIf I read other file save by pig-storer, and need nid list, pig command is:\n\n \r\nB = foreach A generate value.addr.clientIp_bag.clientIp, value.guid , value.attref.nid;\r\n\n\nbut read spark save version I need use this:\n\nB = foreach M generate value.addr.clientIp, value.guid , flatten(value.attref);\r\nC = foreach B generate clientIp, guid, attref::element.nid; \r\n\n\nand this command will flatten column \nMy question is pig loader have some problem when loading parquet file(save by spark)",
        "Issue Links": []
    },
    "PARQUET-1173": {
        "Key": "PARQUET-1173",
        "Summary": "com.fasterxml.jackson.core.jackson dependency harmonization",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Davide Gesino",
        "Created": "07/Dec/17 13:56",
        "Updated": "22/Mar/18 12:03",
        "Resolved": null,
        "Description": "Parquet as a whole depends on many jackson versions, also legacy release.\nthere are 2 overlapping dependencies on com.fasterxml.jackson.core bundles:\n2.7.1, 2.3.1 and 2.3.0 \n[INFO] +- org.apache.arrow:arrow-vector:jar:0.1.0:compile\n[INFO] |  +- joda-time:joda-time:jar:2.9:compile   \n[INFO] |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.7.1:compile \n[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.7.1:compile\n[INFO] |  |  - com.fasterxml.jackson.core:jackson-core:jar:2.7.1:compile\nand \n[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.3.1:compile\n[INFO] |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.3.0:compile\n[INFO] |  - com.fasterxml.jackson.core:jackson-core:jar:2.3.1:compile\nIt would be better to have only 1 non overlapping dependency tree from jackson.\nThen other submodules of Parquet depend on old \"codehaus\" release. These should be harmonized as well, at least those that do not need that version to cope with other third party libraries that need the old one.\nI spotted this one.\nParquet jackson\n[INFO] org.apache.parquet:parquet-jackson:jar:1.9.1-SNAPSHOT\n[INFO] +- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile\n[INFO] +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile",
        "Issue Links": [
            "/jira/browse/AVRO-1605"
        ]
    },
    "PARQUET-1174": {
        "Key": "PARQUET-1174",
        "Summary": "Concurrent read micro benchmarks",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Takeshi Yoshimura",
        "Created": "08/Dec/17 08:42",
        "Updated": "30/Mar/18 21:41",
        "Resolved": null,
        "Description": "parquet-benchmarks only contain read and write benchmarks with a single thread.\nI add concurrent Parquet file scans like typical data-parallel computing.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/440"
        ]
    },
    "PARQUET-1175": {
        "Key": "PARQUET-1175",
        "Summary": "[C++] Fix usage of deprecated Arrow API",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "10/Dec/17 22:27",
        "Updated": "11/Dec/17 20:21",
        "Resolved": "11/Dec/17 20:21",
        "Description": "FAILED: CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o \r\nccache /usr/bin/clang++-4.0  -DHAVE_INTTYPES_H -DHAVE_NETDB_H -DHAVE_NETINET_IN_H -I../src -isystem /home/wesm/cpp-toolchain/include -isystem /home/wesm/cpp-toolchain/include/thrift -isystem /home/wesm/local/include -Werror -DARROW_NO_DEPRECATED_API  -ggdb -O0 -fno-strict-aliasing -Wall -Qunused-arguments  -g -fPIC   -fPIC -std=gnu++11 -MD -MT CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o -MF CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o.d -o CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o -c ../src/parquet/arrow/reader.cc\r\n../src/parquet/arrow/reader.cc:871:39: error: no member named 'ShallowCopy' in 'arrow::ArrayData'\r\n      auto new_data = (*out)->data()->ShallowCopy();\r\n                      ~~~~~~~~~~~~~~  ^\r\n1 error generated.\r\n[15/25] Building CXX object CMakeFiles/parquet_objlib.dir/src/parquet/file/metadata.cc.o\r\nninja: build stopped: subcommand failed.\r\n[7/12] Building CXX object CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o\r\nFAILED: CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o \r\nccache /usr/bin/clang++-4.0  -DHAVE_INTTYPES_H -DHAVE_NETDB_H -DHAVE_NETINET_IN_H -I../src -isystem /home/wesm/cpp-toolchain/include -isystem /home/wesm/cpp-toolchain/include/thrift -isystem /home/wesm/local/include -Werror -DARROW_NO_DEPRECATED_API  -ggdb -O0 -fno-strict-aliasing -Wall -Qunused-arguments  -g -fPIC   -fPIC -std=gnu++11 -MD -MT CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o -MF CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o.d -o CMakeFiles/parquet_objlib.dir/src/parquet/arrow/reader.cc.o -c ../src/parquet/arrow/reader.cc\r\n../src/parquet/arrow/reader.cc:871:39: error: no member named 'ShallowCopy' in 'arrow::ArrayData'\r\n      auto new_data = (*out)->data()->ShallowCopy();\r\n                      ~~~~~~~~~~~~~~  ^\r\n1 error generated.\r\n[8/12] Building CXX object CMakeFiles/parquet_objlib.dir/src/parquet/parquet_types.cpp.o\r\nninja: build stopped: subcommand failed.\r\n~/code/arrow/python ~/code/parquet-cpp/library-build ~/code/arrow/python",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/423"
        ]
    },
    "PARQUET-1176": {
        "Key": "PARQUET-1176",
        "Summary": "Occasional corruption of parquet files , parquet writer might not be calling ParquetFileWriter->end()",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6.0,                                            1.7.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "venkata yerubandi",
        "Created": "11/Dec/17 23:25",
        "Updated": "16/Jan/18 19:52",
        "Resolved": null,
        "Description": "We have a high volume streaming service which works most of the time . But off late we have been observing that some of the parquet files written out by write flow are getting corrupted. This is manifested in our reading flow with the following exception\nWriter version - 1.6.0 , Reader version - 1.7.0\nCaused by: java.lang.RuntimeException: hdfs://Ingest/ingest/jobs/2017-11-30/00-05/part4139 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [-28, -126, 1, 1]\n    at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:422)\n    at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\n    at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:157)\n    at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)\n    at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.<init>(SqlNewHadoopRDD.scala:180)\n    at org.apache.spark.rdd.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:126)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n    at org.apache.spark.scheduler.Task.run(Task.scala:89)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)```\nAfter looking at the code , i can see that one of the possible causes is/are\n1] footer not being serialized in the writer due to end not being called\nbut we are not seeing any exceptions on the writer. \n2] data size - does data size has impact ? There will be cases when row group sizes will be huge as it is activity data of a user \nWe are using default parquet block size and hdfs block size . Other than upgrading to the latest version and re-test , what are the options we have to debug a issue like this",
        "Issue Links": []
    },
    "PARQUET-1177": {
        "Key": "PARQUET-1177",
        "Summary": "[C++] Add more extensive compiler warnings when using Clang",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Dec/17 23:45",
        "Updated": "13/Dec/17 22:19",
        "Resolved": "13/Dec/17 22:19",
        "Description": "We should add a \"build warning level\" flag like Apache Arrow, see\nhttps://github.com/apache/arrow/blob/master/cpp/cmake_modules/SetupCxxFlags.cmake#L73\nThis will help us keep things clean",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/425"
        ]
    },
    "PARQUET-1178": {
        "Key": "PARQUET-1178",
        "Summary": "Parquet modular encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Dec/17 15:29",
        "Updated": "17/May/21 03:02",
        "Resolved": "26/Mar/21 08:41",
        "Description": "A mechanism for modular encryption and decryption of Parquet files. Allows to keep data fully encrypted in the storage - while enabling efficient analytics on the data, via reader-side extraction / authentication / decryption of data subsets required by columnar projection and predicate push-down.\nEnables fine-grained access control to column data by encrypting different columns with different keys.\nSupports a number of encryption algorithms, to account for different security and performance requirements.",
        "Issue Links": [
            "/jira/browse/SPARK-25858",
            "/jira/browse/PARQUET-1300",
            "/jira/browse/PARQUET-1906",
            "/jira/browse/PARQUET-1376",
            "/jira/browse/PARQUET-1457",
            "/jira/browse/PARQUET-1373",
            "/jira/browse/PARQUET-1568",
            "/jira/browse/PARQUET-1397",
            "/jira/browse/HIVE-25119",
            "https://docs.google.com/document/d/1T89G7xR0zHFV1f2pjTO28jtfVm8qoNVGEJQ70Rsk-bY/edit?usp=sharing",
            "https://docs.google.com/document/d/1WasYsgYFXJPkZUWtERF_NiqNMJECor1eo_kFi3Og_Qg/edit?usp=sharing"
        ]
    },
    "PARQUET-1179": {
        "Key": "PARQUET-1179",
        "Summary": "[C++] Support Apache Thrift 0.11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Stephen Carman",
        "Created": "20/Dec/17 18:50",
        "Updated": "18/Apr/18 09:28",
        "Resolved": "28/Jan/18 14:30",
        "Description": "I am not sure if this is an OSX specific issue or something with a new version of Boost, but parquet does not seem to build with the current setup.\n\n\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/schema.cc:28:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:105:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(tmem_transport);\r\n                                 ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/schema.cc:28:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:128:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(mem_buffer);\r\n                                 ^~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\n2 errors generated.\r\nmake[2]: *** [CMakeFiles/parquet_objlib.dir/src/parquet/schema.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/writer-internal.cc:28:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:105:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(tmem_transport);\r\n                                 ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/writer-internal.cc:28:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:128:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(mem_buffer);\r\n                                 ^~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\n2 errors generated.\r\nmake[2]: *** [CMakeFiles/parquet_objlib.dir/src/parquet/file/writer-internal.cc.o] Error 1\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/reader-internal.cc:32:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:105:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(tmem_transport);\r\n                                 ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/reader-internal.cc:32:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:128:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(mem_buffer);\r\n                                 ^~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\n2 errors generated.\r\nmake[2]: *** [CMakeFiles/parquet_objlib.dir/src/parquet/file/reader-internal.cc.o] Error 1\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/metadata.cc:26:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:105:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(tmem_transport);\r\n                                 ^~~~~~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\nIn file included from /Users/steve_carman/software/parquet-cpp/src/parquet/file/metadata.cc:26:\r\n/Users/steve_carman/software/parquet-cpp/src/parquet/thrift.h:128:34: error: no viable conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'stdcxx::shared_ptr<TTransport>'\r\n      tproto_factory.getProtocol(mem_buffer);\r\n                                 ^~~~~~~~~~\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3900:23: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'nullptr_t' for 1st argument\r\n    _LIBCPP_CONSTEXPR shared_ptr(nullptr_t) _NOEXCEPT;\r\n                      ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3914:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'const std::__1::shared_ptr<apache::thrift::transport::TTransport> &' for 1st argument\r\n    shared_ptr(const shared_ptr& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3922:5: note: candidate constructor not viable: no known conversion from 'boost::shared_ptr<apache::thrift::transport::TMemoryBuffer>' to 'std::__1::shared_ptr<apache::thrift::transport::TTransport> &&' for 1st argument\r\n    shared_ptr(shared_ptr&& __r) _NOEXCEPT;\r\n    ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3917:9: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n        shared_ptr(const shared_ptr<_Yp>& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3923:52: note: candidate template ignored: could not match 'std::__1::shared_ptr' against 'boost::shared_ptr'\r\n    template<class _Yp> _LIBCPP_INLINE_VISIBILITY  shared_ptr(shared_ptr<_Yp>&& __r,\r\n                                                   ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3931:9: note: candidate template ignored: could not match 'auto_ptr' against 'shared_ptr'\r\n        shared_ptr(auto_ptr<_Yp>&& __r,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3940:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/memory:3949:9: note: candidate template ignored: could not match 'unique_ptr' against 'shared_ptr'\r\n        shared_ptr(unique_ptr<_Yp, _Dp>&&,\r\n        ^\r\n/usr/local/include/thrift/protocol/TCompactProtocol.h:242:76: note: passing argument to parameter 'trans' here\r\n  stdcxx::shared_ptr<TProtocol> getProtocol(stdcxx::shared_ptr<TTransport> trans) {\r\n                                                                           ^\r\n2 errors generated.\r\nmake[2]: *** [CMakeFiles/parquet_objlib.dir/src/parquet/file/metadata.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/parquet_objlib.dir/all] Error 2\r\nmake: *** [all] Error 2",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/433"
        ]
    },
    "PARQUET-1180": {
        "Key": "PARQUET-1180",
        "Summary": "C++: Fix behaviour of num_children element of primitive nodes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Uwe Korn",
        "Created": "29/Dec/17 16:39",
        "Updated": "29/Dec/17 16:43",
        "Resolved": "29/Dec/17 16:43",
        "Description": "Per the parquet.thift spec, for primitive nodes the num_children schema\nattibute should remain unset. This is implemeted correctly in parquet-mr\nsee [1]. However currently parquet-cpp does set the num_children\nattribute to 0 if it is a primitive node. This pull requests fixes this\nissue and the tests that were relying on this behavior",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/427"
        ]
    },
    "PARQUET-1181": {
        "Key": "PARQUET-1181",
        "Summary": "[C++] Devise way to store Arrow logical types not available in the Parquet metadata",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "29/Dec/17 18:57",
        "Updated": "29/Dec/17 18:57",
        "Resolved": null,
        "Description": "As an example example: storing 64-bit nanosecond timestamps without any conversion to another unit or without using INT96. These could be embedded in an INT64 column with some additional custom metadata. A similar approach of using custom metadata could be used for other type of data not found in the Parquet logical types",
        "Issue Links": []
    },
    "PARQUET-1182": {
        "Key": "PARQUET-1182",
        "Summary": "Parquet-cpp version 1.3.1 not tagged in git repo",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "cpp-1.3.1",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Ladislav Skokan",
        "Created": "03/Jan/18 12:46",
        "Updated": "04/Jan/18 08:23",
        "Resolved": "03/Jan/18 15:11",
        "Description": "The released version of parquet-cpp 1.3.1 is not tagged in git repo\n$ git remote -v\norigin\thttps://github.com/apache/parquet-cpp (fetch)\norigin\thttps://github.com/apache/parquet-cpp (push)\n$ git pull; git tag\nAlready up-to-date.\napache-parquet-cpp-1.0.0\napache-parquet-cpp-1.1.0\napache-parquet-cpp-1.2.0\napache-parquet-cpp-1.2.0-rc0\napache-parquet-cpp-1.2.0-rc1\napache-parquet-cpp-1.3.0\napache-parquet-cpp-1.3.0-rc0\napache-parquet-cpp-1.3.1-rc0\nrel/1.0.0-rc1",
        "Issue Links": []
    },
    "PARQUET-1183": {
        "Key": "PARQUET-1183",
        "Summary": "AvroParquetWriter needs OutputFile based Builder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Blue",
        "Reporter": "Werner Daehn",
        "Created": "04/Jan/18 09:51",
        "Updated": "30/Mar/18 22:25",
        "Resolved": "30/Mar/18 22:25",
        "Description": "The ParquetWriter got a new Builder(OutputFile). \nBut it cannot be used by the AvroParquetWriter as there is no matching Builder/Constructor.\nChanges are quite simple:\npublic static <T> Builder<T> builder(OutputFile file) \n{\r\n  return new Builder<T>(file)\r\n}\n\nand in the static Builder class below\nprivate Builder(OutputFile file) \n{\r\n  super(file);\r\n}\n\nNote: I am not good enough with builds, maven and git to create a pull request yet. Sorry. Will try to get better here.\nSee: https://issues.apache.org/jira/browse/PARQUET-1142",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/446",
            "https://github.com/apache/parquet-mr/pull/460"
        ]
    },
    "PARQUET-1184": {
        "Key": "PARQUET-1184",
        "Summary": "Make DelegatingPositionOutputStream a concrete class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Werner Daehn",
        "Created": "04/Jan/18 09:56",
        "Updated": "30/Mar/18 21:58",
        "Resolved": "30/Mar/18 21:58",
        "Description": "I fail to understand why this is an abstract class. In my example I want to write the Parquet file to a java.io.FileOutputStream, hence have to extend the DelegatingPositionOutputStream and store the pos information, increase it in all write(..) methods and return its value in getPos().\nDoable of course, but useful? Previously yes but now with the OutputFile changes to decouple it from Hadoop more, I believe no.\nrelated to: https://issues.apache.org/jira/browse/PARQUET-1142",
        "Issue Links": []
    },
    "PARQUET-1185": {
        "Key": "PARQUET-1185",
        "Summary": "TestBinary#testBinary unit test fails after PARQUET-1141",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "05/Jan/18 09:57",
        "Updated": "10/Jan/18 13:56",
        "Resolved": "10/Jan/18 13:56",
        "Description": "TestBinary#testBinary fails after PARQUET-1141",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/444"
        ]
    },
    "PARQUET-1186": {
        "Key": "ARROW-3775",
        "Summary": "[C++] Handling Parquet Arrow reads that overflow a BinaryArray capacity",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "C++",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "05/Jan/18 16:07",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "20/Nov/18 17:25",
        "Description": "See comment thread in https://stackoverflow.com/questions/48115087/converting-parquetfile-to-pandas-dataframe-with-a-column-with-a-set-of-string-in",
        "Issue Links": []
    },
    "PARQUET-1187": {
        "Key": "PARQUET-1187",
        "Summary": "[C++] Add abi-compliance-checker to the CI build",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "07/Jan/18 19:59",
        "Updated": "11/Nov/18 22:09",
        "Resolved": "11/Nov/18 22:09",
        "Description": "I would like to check our baseline modules with https://lvc.github.io/abi-compliance-checker/ to ensure that version upgrades are much smoother and that we don\u2018t break the ABI in patch releases. \nAs we\u2018re pre-1.0 yet, I accept that there will be breakage but I would like to keep them to a minimum. Currently the biggest pain with Arrow is you need to pin it in Python always with ==0.x.y, otherwise segfaults are inevitable.",
        "Issue Links": []
    },
    "PARQUET-1188": {
        "Key": "PARQUET-1188",
        "Summary": "Off heap memory leaks with large binary fields using Snappy",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Remi Dettai",
        "Created": "09/Jan/18 09:51",
        "Updated": "09/Jan/18 09:51",
        "Resolved": null,
        "Description": "When I write a large pages (~100MB) that contains large binary fields (~1MB), the java application uses an unexpected amount of off-heap memory (1.2GB) \nThis problem was identified when using the AvroParquetWriter but its source lies in the  parquet-hadoop submodule.\nDiving a little bit deeper shows the following:\n\nwriting fields into the ParquetWriter creates a SequenceBytesIn which is actually just a list of BytesInput for each field. When calling bytes.writeAllTo(cos) in the CodecFactory, it actually writes one ByteInput (which contains a single field) at a time.\nthe SnappyCompressor receives the data in setInput one large field at a time. This calls ByteBuffer.allocateDirect each time with a growing size. But as the memory is actually allocated off-heap, this does not trigger the garbage collector which only sees small objects on the heap. The actual memory associated with the object is the size of all the fields added to the page until then, so off-heap the memory is growing quadratically.\n\nI did not attach a pull request to this issue because I see multiple mitigation to the issue but I'm not really delighted by any of them:\n\nmerge all the fields into one byte array before pushing them down to the SnappyCompressor. For instance we could replace the previous statement in the CodecFactory with BytesInput.from(bytes.toByteArray()).writeAllTo(cos). But this generates an extra on-heap allocation the size of the whole page.\nforce the DirectBuffer to be cleaned up with something like ((DirectBuffer)inputBuffer).cleaner().clean() after having copied it to the new bigger buffer. The issue here would be that DirectBuffer is part of the internal API and is likely to be moved. Using reflexion could make the solution more resilient but is even \"hackier\" IMHO.",
        "Issue Links": []
    },
    "PARQUET-1189": {
        "Key": "PARQUET-1189",
        "Summary": "Release Parquet Java 1.10",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "09/Jan/18 17:56",
        "Updated": "20/Apr/18 18:28",
        "Resolved": "20/Apr/18 18:28",
        "Description": "Please link needed issues as blockers.",
        "Issue Links": [
            "/jira/browse/PARQUET-1208",
            "/jira/browse/PARQUET-1246",
            "/jira/browse/PARQUET-787",
            "/jira/browse/PARQUET-1215",
            "/jira/browse/PARQUET-1264"
        ]
    },
    "PARQUET-1190": {
        "Key": "PARQUET-1190",
        "Summary": "Use the same default page size across different language bindings",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "10/Jan/18 14:13",
        "Updated": "10/Jan/18 14:13",
        "Resolved": null,
        "Description": "Currently there are many different page size recommandations/defaults in use:\n\nparquet-format recommends 8 KB.\nparquet-mr uses 1 MB.\nImpala uses 64 KB.\n\nThese values (and other language bindings not listed above) should be consistent.\nTo pick a sensible new value, we may need to do some measurements. Because of this, we shall wait for column indexes to be implemented before picking a new value.\nThe new default page size does not necessarily have to be a single value any more, we have several options:\n\nA single default page size, as before.\nDifferent page size defaults depending on the type.\nUsing a specified number of values instead of data size (e.g., every page contains 10000 values).",
        "Issue Links": []
    },
    "PARQUET-1191": {
        "Key": "PARQUET-1191",
        "Summary": "Type.hashCode() takes originalType into account but Type.equals() does not",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Jan/18 14:43",
        "Updated": "26/Jan/18 09:48",
        "Resolved": "19/Jan/18 15:54",
        "Description": "Taking originalType into account in Type.hashCode() but ignoring it in Type.equals() is inconsistent and violates hashCode-equals contract.\nIf two Type instances that are equal according to equals() but have different logical types were ever put in a hash map, then both of the following cases are possible:\n\nThe two instances may accidentally have the same hash, which is consistent with the equals() method but is pure coincidence and has a very low probablility.\nThe two instances may have different hashes and end up in different buckets of a hash map, leading to a situation where we can't find a value in the hash map despite that it's equals() would return true.\n\nWe should decide whether originalType is needed for an equality check or not. If it is, then it should be added to equals(). Otherwise it should be removed from hashCode().",
        "Issue Links": [
            "/jira/browse/HIVE-18554",
            "https://github.com/apache/parquet-mr/pull/450"
        ]
    },
    "PARQUET-1192": {
        "Key": "PARQUET-1192",
        "Summary": "Parquet Pushdown",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-pig",
        "Assignee": null,
        "Reporter": "Rana Faisal Munir",
        "Created": "11/Jan/18 16:20",
        "Updated": "11/Jan/18 16:20",
        "Resolved": null,
        "Description": "Hi,\nI am doing some experiments with Apache Parquet to test Predicate pushdown and effect of different row group sizes. My assumptions are:\n1) Parquet reader first read the metadata to filter out row groups and data pages\n2) Then, it reads only those row groups and data pages which match the filter. \n3) The total size of read should be the sum of row group size and size of meta data.\nI have a wide table with 1184 columns. 2 columns are long type and remaining columns are binary. One of the long column is sorted and unique. I disabled dictionary encoding and compression. My file size is 34GB in CSV. I converted it to Parquet. I tried with two options\n1) Generate only 1 File of Parquet (i.e. 43GB)\n2) Generate multiple files of Parquet (i.e., overall size 43GB). \nI allow only 1 Mapper to eliminate the effect of parallelism. \nI have a query to search 1 record from the sorted column. The results are for row group 16MB and data page size of 1MB\nWhen there is only 1 file of Parquet.\nInput(s):\nSuccessfully read 1 records (22135659519 bytes) from: \"/output/wide/16777216/1048576\"\nWhen there is multiple file of Parquet\nInput(s):\nSuccessfully read 1 records (800413428 bytes) from: \"/output/wide/16777216/1048576\"\nMy questions are:\n1) Why there is big difference. In one file, I am reading 22GB and with multiple file, It is reading 800MB. This is a bug or what?\n2) Why it is not reading 16MB + Size of meta data (which is 252MB). Why it is reading more than that?\n3) Can I rely on the pig statistics for estimating bytes read? \n4) My assumptions are correct or am I missing something?\nCould you please have a look into this problem and guide me if it is a bug ?\nLogs are attached with this email. \nThank you\nRegards\nRana Faisal",
        "Issue Links": []
    },
    "PARQUET-1193": {
        "Key": "PARQUET-1193",
        "Summary": "[CPP] Implement ColumnOrder to support min_value and max_value",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "12/Jan/18 03:59",
        "Updated": "24/Jan/18 22:22",
        "Resolved": "24/Jan/18 22:21",
        "Description": "Use ColumnOrder to set min_value and max_value statistics.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/430"
        ]
    },
    "PARQUET-1194": {
        "Key": "PARQUET-1194",
        "Summary": "org.apache.commons.codec.binary.Base64 clash",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "JaredLi",
        "Created": "15/Jan/18 13:01",
        "Updated": "12/Feb/19 19:25",
        "Resolved": null,
        "Description": "The class \"org.apache.commons.codec.binary.Base64\" in parquet-tools.jar version 1.9.0 is same at package and class name to which is in commons-codec.jar(all version) ,but its implement is much older.\nThat makes a jar-hell when both parquet-tools.jar and commons-codec.jar are used .\ne.g.\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.commons.codec.binary.Base64.encodeBase64String([B)Ljava/lang/String;",
        "Issue Links": [
            "/jira/browse/PARQUET-1195",
            "/jira/browse/PARQUET-1530"
        ]
    },
    "PARQUET-1195": {
        "Key": "PARQUET-1195",
        "Summary": "org.apache.commons.codec.binary.Base64 clash #449",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "JaredLi",
        "Created": "15/Jan/18 13:01",
        "Updated": "28/May/18 09:50",
        "Resolved": "28/May/18 09:50",
        "Description": "The class \"org.apache.commons.codec.binary.Base64\" in parquet-tools.jar version 1.9.0 is same at package and class name to which is in commons-codec.jar(all version) ,but its implement is much older.\nThat makes a jar-hell when both parquet-tools.jar and commons-codec.jar are used .\ne.g.",
        "Issue Links": [
            "/jira/browse/PARQUET-1194"
        ]
    },
    "PARQUET-1196": {
        "Key": "PARQUET-1196",
        "Summary": "[C++] Provide a parquet_arrow example project incl. CMake setup",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/Jan/18 15:05",
        "Updated": "15/Feb/18 17:42",
        "Resolved": "15/Feb/18 17:42",
        "Description": "Currently we only provide a simple example for the low-level interface, we should also provide an example for the prefered high-level parquet_arrow interface. Additionally, we should provide a CMake project that shows how to use parquet as a lib. This can then also be used in CI",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/436"
        ]
    },
    "PARQUET-1197": {
        "Key": "PARQUET-1197",
        "Summary": "Log rat failures",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0,                                            format-2.5.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "18/Jan/18 09:20",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "18/Jan/18 13:41",
        "Description": "Currently, rat plugin does not log anything useful in case of failure (e.g. found files without a license). The details can be found only in the generated\u00a0rat.txt\u00a0file. This can make hard to find the problem in environments where the build workspace is not easily accessible (e.g. Jenkins).\nThe rat plugin should be configured to log the erroneous files so accessing\u00a0rat.txt\u00a0is not required to fix the issues.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/451"
        ]
    },
    "PARQUET-1198": {
        "Key": "PARQUET-1198",
        "Summary": "Bump java source and target to java8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "19/Jan/18 07:25",
        "Updated": "30/Jan/18 18:09",
        "Resolved": "30/Jan/18 18:09",
        "Description": "java7 is already reached EOL. Use java8 for compile/run parquet-mr.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/452"
        ]
    },
    "PARQUET-1199": {
        "Key": "PARQUET-1199",
        "Summary": "[C++] Support writing (and test reading) boolean values with RLE encoding",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "19/Jan/18 17:52",
        "Updated": "22/Aug/22 11:36",
        "Resolved": null,
        "Description": "This is supported by the Parquet specification, we should ensure that we are able to read such data",
        "Issue Links": []
    },
    "PARQUET-1200": {
        "Key": "PARQUET-1200",
        "Summary": "[C++]\u00a0Support reading a single Arrow column from a Parquet file",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Jan/18 16:41",
        "Updated": "13/Feb/18 15:08",
        "Resolved": "13/Feb/18 15:08",
        "Description": "A small convenience for consumers that read in columns with\u00a0separate function contexts.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/434"
        ]
    },
    "PARQUET-1201": {
        "Key": "PARQUET-1201",
        "Summary": "Column indexes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "format-2.5.0,                                            1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Jan/18 13:22",
        "Updated": "08/Jan/20 10:57",
        "Resolved": "18/Oct/18 12:11",
        "Description": "Write the column indexes described in PARQUET-922.\n This is the first phase of implementing the whole feature. The implementation is done in the following steps:\n\nUtility to read/write indexes in parquet-format\nWriting indexes in the parquet file\nExtend parquet-tools and parquet-cli to show the indexes\nLimit index size based on parquet properties\nTrim min/max values where possible based on parquet properties\nFiltering based on column indexes\n\nThe work is done on the feature branch\u00a0column-indexes. This JIRA will be resolved after the branch has been merged to\u00a0master.",
        "Issue Links": [
            "/jira/browse/PARQUET-1365",
            "/jira/browse/PARQUET-922",
            "/jira/browse/PARQUET-1415",
            "/jira/browse/PARQUET-1435",
            "/jira/browse/PARQUET-1207",
            "/jira/browse/PARQUET-1414",
            "/jira/browse/PARQUET-1739",
            "https://github.com/apache/parquet-format/pull/81",
            "https://github.com/apache/parquet-mr/pull/527"
        ]
    },
    "PARQUET-1202": {
        "Key": "PARQUET-1202",
        "Summary": "Add differentiation of nested records with the same name",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7.0,                                            1.8.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Benoit Lacelle",
        "Created": "29/Jan/18 09:12",
        "Updated": "29/Jan/18 09:13",
        "Resolved": null,
        "Description": "Hello,\nWhile reading back a Parquet file produced with Spark, it appears the schema produced by Parquet-Avro is not valid.\nI consider the simple following piece of code:\n\n\r\n\r\nParquetReader<GenericRecord> reader =\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AvroParquetReader.<GenericRecord>builder(new org.apache.hadoop.fs.Path(path.toUri())).build();\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 System.out.println(reader.read().getSchema());\r\n\r\n\n\nI get a stack lile:\n\n\r\n\r\nException in thread \"main\" +org.apache.avro.SchemaParseException+: Can't redefine: value\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$Names.put(+Schema.java:1128+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$NamedSchema.writeNameRef(+Schema.java:562+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$RecordSchema.toJson(+Schema.java:690+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$UnionSchema.toJson(+Schema.java:882+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$MapSchema.toJson(+Schema.java:833+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$UnionSchema.toJson(+Schema.java:882+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$RecordSchema.fieldsToJson(+Schema.java:716+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema$RecordSchema.toJson(+Schema.java:701+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema.toString(+Schema.java:324+)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.avro.Schema.toString(+Schema.java:314+)\r\n\r\n\n\n\u00a0\nThe issue seems the same as the one reported in:\nhttps://www.bountysource.com/issues/22823013-spark-avro-fails-to-save-df-with-nested-records-having-the-same-name\n\u00a0\nIt have been fixed in Spark-avro within:\nhttps://github.com/databricks/spark-avro/pull/73\nIn our case, the parquet schema looks like:\n\n\r\n\r\nmessage spark_schema {\r\n\t optional group calculatedobjectinfomap (MAP) {\r\n\t\t repeated group key_value {\r\n\t\t\t required binary key (UTF8);\r\n\t\t\t optional group value {\r\n\t\t\t\t optional int64 calcobjid;\r\n\t\t\t\t optional int64 calcobjparentid;\r\n\t\t\t\t optional binary portfolioname (UTF8);\r\n\t\t\t\t optional binary portfolioscheme (UTF8);\r\n\t\t\t\t optional binary calcobjtype (UTF8);\r\n\t\t\t\t optional binary calcobjmnemonic (UTF8);\r\n\t\t\t\t optional binary calcobinstrumentype (UTF8);\r\n\t\t\t\t optional int64 calcobjectqty;\r\n\t\t\t\t optional binary calcobjboid (UTF8);\r\n\t\t\t\t optional binary analyticalfoldermnemonic (UTF8);\r\n\t\t\t\t optional binary calculatedidentifier (UTF8);\r\n\t\t\t\t optional binary calcobjlevel (UTF8);\r\n\t\t\t\t optional binary calcobjboidscheme (UTF8);\r\n\t\t\t }\r\n\t\t}\r\n\t}\r\n\toptional group riskfactorinfomap (MAP) {\r\n\t\t repeated group key_value {\r\n\t\t\t required binary key (UTF8);\r\n\t\t\t optional group value {\r\n\t\t\t optional binary riskfactorname (UTF8);\r\n\t\t\t optional binary riskfactortype (UTF8);\r\n\t\t\t optional binary riskfactorrole (UTF8);\r\n\t\t\t }\r\n\t\t }\r\n\t }\r\n}\r\n\r\n\n\nWe indeed have 2 Map field with a value fields named 'value'. The name 'value' is defaulted in org.apache.spark.sql.types.MapType. \nThe fix seems not trivial given current parquet-avro code then I doubt I will be able to craft a valid PR without directions.\nThanks,",
        "Issue Links": []
    },
    "PARQUET-1203": {
        "Key": "PARQUET-1203",
        "Summary": "Corrupted parquet file from Spark",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Dong Jiang",
        "Created": "05/Feb/18 17:12",
        "Updated": "04/Dec/21 05:05",
        "Resolved": "05/Feb/18 17:39",
        "Description": "Hi,\u00a0\nWe are running on Spark 2.2.1, generating parquet files on S3, like the following\u00a0\npseudo code\u00a0\ndf.write.parquet(...)\u00a0\nWe have recently noticed parquet file corruptions, when reading the parquet\u00a0\nin Spark or Presto. I downloaded the corrupted file from S3 and got following errors in Spark as the following:\u00a0\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read\u00a0\nvalue at 40870 in block 0 in file\u00a0\nfile:/Users/djiang/part-00122-80f4886a-75ce-42fa-b78f-4af35426f434.c000.snappy.parquet\u00a0\nCaused by: org.apache.parquet.io.ParquetDecodingException: could not read\u00a0\npage Page [bytes.size=1048594, valueCount=43663, uncompressedSize=1048594]\u00a0\nin col [incoming_aliases_array, list, element, key_value, value] BINARY\u00a0\nIt appears only one column in one of the rows in the file is corrupt, the\u00a0\nfile has 111041 rows.\u00a0\nMy questions are\u00a0\n1) How can I identify the corrupted row?\u00a0\n2) What could cause the corruption? Spark issue or Parquet issue?\u00a0\nAny help is greatly appreciated.\u00a0\nThanks,\u00a0\nDong",
        "Issue Links": []
    },
    "PARQUET-1204": {
        "Key": "PARQUET-1204",
        "Summary": "[C++] Less verbose logging from thirdparty toolchain",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "07/Feb/18 16:05",
        "Updated": "19/Nov/18 19:30",
        "Resolved": "19/Nov/18 19:30",
        "Description": "Following work in ARROW-2095, ARROW-2096, elsewhere",
        "Issue Links": [
            "/jira/browse/ARROW-2096"
        ]
    },
    "PARQUET-1205": {
        "Key": "PARQUET-1205",
        "Summary": "Fix msvc static build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "08/Feb/18 01:49",
        "Updated": "10/Feb/18 18:40",
        "Resolved": "10/Feb/18 18:40",
        "Description": "Build is failed with the following options:\n-DPARQUET_BUILD_SHARED=OFF\n -DPARQUET_BOOST_USE_SHARED=OFF\n -DPARQUET_ARROW_LINKAGE=static\n -DPARQUET_BUILD_EXECUTABLES=ON\n -DPARQUET_BUILD_TESTS=ON",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/435"
        ]
    },
    "PARQUET-1206": {
        "Key": "PARQUET-1206",
        "Summary": "Parquet properties are ignored at table creation time",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Istvan Szukacs",
        "Created": "08/Feb/18 20:43",
        "Updated": "08/Feb/18 21:12",
        "Resolved": "08/Feb/18 21:08",
        "Description": "Create table ignores table properties.\n\u00a0\nCREATE EXTERNAL TABLE\u00a0db.test\u00a0(\u00a0 month INT\n)\nSTORED AS PARQUET\nLOCATION 'hdfs://nn/user/user/test'\nTBLPROPERTIES (\n\u00a0 'STATS_GENERATED_VIA_STATS_TASK'='true','PARQUET.COMPRESSION'='GZIP','PARQUET.VERSION'='PARQUET_2_0');\n\u00a0\nAfter inserting into this table with the following:\nINSERT INTO TABLE db.test\nSELECT * FROM db.source;\nThe created Parquet files are SNAPPY compressed according to\u00a0parquet-tools meta. Is there a difference which query engine is used? Impala or Hive? Should this be resulting GZIP compressed files or Parquet ignores TBLPROPERTIES?",
        "Issue Links": []
    },
    "PARQUET-1207": {
        "Key": "PARQUET-1207",
        "Summary": "Write index page in parquet file",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "legend",
        "Created": "11/Feb/18 02:02",
        "Updated": "12/Feb/18 09:26",
        "Resolved": "12/Feb/18 09:26",
        "Description": "PARQUET-922 has been resolved, parquet-format 2.4.0 supported index page. Once\u00a0PARQUET-1201\u00a0has been resolved,\u00a0we need to\u00a0 write index page in parquet file.",
        "Issue Links": [
            "/jira/browse/PARQUET-922",
            "/jira/browse/PARQUET-1201"
        ]
    },
    "PARQUET-1208": {
        "Key": "PARQUET-1208",
        "Summary": "Occasional endless loop in unit test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "12/Feb/18 12:21",
        "Updated": "19/Feb/18 17:40",
        "Resolved": "19/Feb/18 17:39",
        "Description": "An excerpt from TestParquetMetadataConverter.randomTestFilterMetaData:\n\n\r\n    for (int j = 0; j < 100; j++) {\r\n      // ...\r\n      int splitSize = random.nextInt(10000);\r\n      // ...\r\n      verifyAllFilters(melead tadata(rgs), splitSize);\r\n      // ...\r\n    }\r\n\n\nAn excerpt from verifyAllFilters(FileMetaData md, long splitWidth):\n\n\r\n    for (long start = 0; start < fileSize(md); start += splitWidth) {\r\n\n\nWhen random.nextInt(10000); returns 0, this will result in an infinite loop.",
        "Issue Links": [
            "/jira/browse/PARQUET-1189"
        ]
    },
    "PARQUET-1209": {
        "Key": "PARQUET-1209",
        "Summary": "locally defined symbol ... imported in function ..",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "12/Feb/18 21:31",
        "Updated": "12/Mar/18 19:19",
        "Resolved": "12/Mar/18 19:18",
        "Description": "Got the following linker warning LNK4217:\nlocally defined symbol ??1Status@arrow@@QEAA@XZ (public: __cdecl arrow::Status::~Status(void)) imported in function \"private: void __cdecl parquet::TypedRowGroupStatistics<struct parquet::DataType<6> >::Copy(struct parquet::ByteArray const &,struct parquet::ByteArray *,class arrow::PoolBuffer *)\" (?Copy@?$TypedRowGroupStatistics@U?$DataType@$05@parquet@@@parquet@@AEAAXAEBUByteArray@2@PEAU32@PEAVPoolBuffer@arrow@@@Z)\nnot sure, is it parquet or arrow issue.\nhttps://docs.microsoft.com/en-us/cpp/error-messages/tool-errors/linker-tools-warning-lnk4217",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/446"
        ]
    },
    "PARQUET-1210": {
        "Key": "PARQUET-1210",
        "Summary": "[C++] Boost 1.66 compilation fails on Windows on linkage stage",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Max Risuhin",
        "Reporter": "Max Risuhin",
        "Created": "13/Feb/18 08:06",
        "Updated": "13/Feb/18 15:06",
        "Resolved": "13/Feb/18 15:06",
        "Description": "Boost's autolinking should be disable on compilation with MSVC, since it causes linkage with shared import libs, instead of expected static. Following error occurs:\n`LINK : fatal error LNK1104: cannot open file 'boost_filesystem.lib'`",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/437"
        ]
    },
    "PARQUET-1211": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column indexes: read/write API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Feb/18 16:06",
        "Updated": "01/Apr/23 13:04",
        "Resolved": "28/May/18 09:32",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1212",
            "/jira/browse/PARQUET-1213",
            "/jira/browse/PARQUET-1214",
            "https://github.com/apache/parquet-mr/pull/456",
            "https://github.com/apache/parquet-mr/pull/456"
        ]
    },
    "PARQUET-1212": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column indexes: Show indexes in tools",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Feb/18 16:08",
        "Updated": "28/May/18 09:32",
        "Resolved": "28/May/18 09:32",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1211",
            "https://github.com/apache/parquet-mr/pull/479"
        ]
    },
    "PARQUET-1213": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column indexes: Limit index size",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Feb/18 16:09",
        "Updated": "28/May/18 11:23",
        "Resolved": "28/May/18 11:23",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1211",
            "https://github.com/apache/parquet-mr/pull/480"
        ]
    },
    "PARQUET-1214": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column indexes: Truncate min/max values",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Feb/18 16:09",
        "Updated": "31/Jul/18 13:23",
        "Resolved": "31/Jul/18 13:23",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1211",
            "https://github.com/apache/parquet-mr/pull/481"
        ]
    },
    "PARQUET-1215": {
        "Key": "PARQUET-1215",
        "Summary": "Add accessor for footer after a file is closed",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "14/Feb/18 17:07",
        "Updated": "15/Feb/18 17:08",
        "Resolved": "15/Feb/18 17:08",
        "Description": "I'm storing metrics along with Parquet files in Iceberg and need to get the metrics from the Parquet footer just after writing a file. Parquet should be able to return the footer after closing a file so the caller doesn't have to open the file just after writing.",
        "Issue Links": [
            "/jira/browse/PARQUET-1189",
            "https://github.com/apache/parquet-mr/pull/457"
        ]
    },
    "PARQUET-1216": {
        "Key": "PARQUET-1216",
        "Summary": "Parquet protobuf: List wrapper should be optional",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Benoit Hanotte",
        "Created": "14/Feb/18 18:18",
        "Updated": "14/Feb/18 18:18",
        "Resolved": null,
        "Description": "Following\u00a0https://github.com/apache/parquet-mr/pull/411\u00a0, parquet-protobuf\u00a0will conform to the specs[1] regarding the definition of lists and will\u00a0adopt the 3-level structure:\n\n\r\n*required* group my_list (LIST) {\r\n  repeated group list {\r\n    optional binary element (UTF8);\r\n  }\r\n}\n\nThe first level has the repetition level required, however this won't allow making the distinction between a null list and an empty one.\nThis ticket is for changing the root level repetition to optional\u00a0as following:\n\n\r\n*optional* group my_list (LIST) {\r\n  repeated group list {\r\n    optional binary element (UTF8);\r\n  }\r\n}\r\n\n\nThis will allow differentiating between null and empty lists.\n[1]\u00a0https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists",
        "Issue Links": []
    },
    "PARQUET-1217": {
        "Key": "PARQUET-1217",
        "Summary": "Incorrect handling of missing values in Statistics",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.0,                                            1.6.0,                                            1.7.0,                                            1.8.0,                                            1.9.0,                                            1.10.0",
        "Fix Version/s": "1.10.0,                                            1.8.3",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "15/Feb/18 14:59",
        "Updated": "24/Apr/18 13:22",
        "Resolved": "27/Feb/18 13:21",
        "Description": "As per the parquet-format specs the min/max values in statistics are optional. Therefore, it is possible to have numNulls in Statistics while we don't have min/max values. In StatisticsFilter we rely on the method StatisticsFilter.isAllNulls(ColumnChunkMetaData) to handle the case of null min/max values which is not correct due to the described scenario. \n We shall check Statistics.hasNonNullValue() any time before using the actual min/max values.\nIn addition we don't check if the null_count\u00a0is set or not when reading from the parquet file. We simply use the value which is 0 in case of unset. In the parquet-mr side the Statistics object uses the value 0 to sign that the num_nulls is unset. It is incorrect if we are searching for null values and we falsely drop a column chunk thinking there are no null values but the field in the statistics was simply unset.",
        "Issue Links": [
            "/jira/browse/PARQUET-1277",
            "/jira/browse/SPARK-23852",
            "https://github.com/apache/parquet-mr/pull/465",
            "https://github.com/apache/parquet-mr/pull/458"
        ]
    },
    "PARQUET-1218": {
        "Key": "PARQUET-1218",
        "Summary": "[C++] More informative error message on too short pages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Feb/18 20:37",
        "Updated": "21/Feb/18 02:41",
        "Resolved": "21/Feb/18 02:41",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/438"
        ]
    },
    "PARQUET-1219": {
        "Key": "PARQUET-1219",
        "Summary": "[C++]\u00a0Update release-candidate script links to gitbox",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Feb/18 20:50",
        "Updated": "20/Feb/18 17:34",
        "Resolved": "20/Feb/18 17:34",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/439"
        ]
    },
    "PARQUET-1220": {
        "Key": "PARQUET-1220",
        "Summary": "[C++]\u00a0Don't build Thrift examples and tutorials in the ExternalProject",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "None",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Feb/18 21:11",
        "Updated": "20/Feb/18 17:33",
        "Resolved": "20/Feb/18 17:33",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/440"
        ]
    },
    "PARQUET-1221": {
        "Key": "PARQUET-1221",
        "Summary": "[C++] Extend release README",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "18/Feb/18 21:14",
        "Updated": "20/Feb/18 17:33",
        "Resolved": "20/Feb/18 17:33",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/441"
        ]
    },
    "PARQUET-1222": {
        "Key": "PARQUET-1222",
        "Summary": "Specify a well-defined sorting order for float and double types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Micah Kornfield",
        "Reporter": "Zoltan Ivanfi",
        "Created": "19/Feb/18 14:12",
        "Updated": "07/Dec/22 17:54",
        "Resolved": "07/Dec/22 14:10",
        "Description": "Currently parquet-format specifies the sort order for floating point numbers as follows:\n\n\r\n   *   FLOAT - signed comparison of the represented value\r\n   *   DOUBLE - signed comparison of the represented value\r\n\n\nThe problem is that the comparison of floating point numbers is only a partial ordering with strange behaviour in specific corner cases. For example, according to IEEE 754, -0 is neither less nor more than +0 and comparing NaN to anything always returns false. This ordering is not suitable for statistics. Additionally, the Java implementation already uses a different (total) ordering that handles these cases correctly but differently than the C++ implementations, which leads to interoperability problems.\nTypeDefinedOrder for doubles and floats should be deprecated and a new TotalFloatingPointOrder should be introduced. The default for writing doubles and floats would be the new TotalFloatingPointOrder. This ordering should be effective and easy to implement in all programming languages.",
        "Issue Links": [
            "/jira/browse/IMPALA-7304",
            "/jira/browse/IMPALA-6539",
            "/jira/browse/PARQUET-1223",
            "/jira/browse/PARQUET-1224",
            "/jira/browse/ARROW-12264",
            "/jira/browse/PARQUET-1246",
            "/jira/browse/PARQUET-1251"
        ]
    },
    "PARQUET-1223": {
        "Key": "PARQUET-1223",
        "Summary": "[parquet-mr] Implement specification-compliant floating point comparison",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "19/Feb/18 14:17",
        "Updated": "14/Mar/18 13:43",
        "Resolved": null,
        "Description": "Currently, the floating point comparison order defined by parquet-format is ambigous and different implementations use different orderings. Once the specification is made unambigous (PARQUET-1222), implementations should be updated to conform to it.",
        "Issue Links": [
            "/jira/browse/PARQUET-1222"
        ]
    },
    "PARQUET-1224": {
        "Key": "PARQUET-1224",
        "Summary": "[C++] Implement specification-compliant floating point comparison",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "19/Feb/18 14:19",
        "Updated": "02/May/19 19:44",
        "Resolved": null,
        "Description": "Currently, the floating point comparison order defined by parquet-format is ambigous and different implementations use different orderings. Once the specification is made unambigous (PARQUET-1222), implementations should be updated to conform to it.",
        "Issue Links": [
            "/jira/browse/PARQUET-1222"
        ]
    },
    "PARQUET-1225": {
        "Key": "PARQUET-1225",
        "Summary": "NaN values may lead to incorrect filtering under certain circumstances",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Zoltan Ivanfi",
        "Created": "19/Feb/18 14:37",
        "Updated": "26/Feb/18 08:08",
        "Resolved": "24/Feb/18 18:23",
        "Description": "This JIRA describes a generic problem with floating point comparisons that most probably affects parquet-cpp. It is known to affect Impala and by taking a quick look at the parquet-cpp code it seems to affect parquet-cpp as well, but it has not yet been confirmed in practice.\nFor comparing float and double values for min/max stats, parquet-cpp uses the C++ less-than operator (<) that returns false for comparisons involving a NaN. This means that while garthering statistics, if a NaN is the smallest value encountered so far (which happens to be the case after reading the first value if that value is NaN), no other value can ever replace it, since < will always be false. On the other hand, if NaN is not the first value, it won't affect the min value. So the min value depends on the order of elements.\nIf looking for specific values while reading back the data, the NaN value may lead to row groups being incorrectly discarded in spite of having matching rows. For details, please see the Impala bug IMPALA-6527.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/444"
        ]
    },
    "PARQUET-1226": {
        "Key": "PARQUET-1226",
        "Summary": "[C++] Fix new build warnings with clang 5.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "19/Feb/18 21:55",
        "Updated": "20/Feb/18 17:32",
        "Resolved": "20/Feb/18 17:32",
        "Description": "Follow-on work since Apache Arrow has migrated to clang 5.0 in ARROW-2117",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/442"
        ]
    },
    "PARQUET-1227": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Thrift crypto metadata structures",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0,                                            format-2.7.0",
        "Component/s": "parquet-cpp,                                            parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Feb/18 09:11",
        "Updated": "12/Feb/20 07:59",
        "Resolved": "31/Jul/18 03:29",
        "Description": "New Thrift structures for Parquet modular encryption",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/94",
            "https://github.com/apache/parquet-cpp/pull/463"
        ]
    },
    "PARQUET-1228": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "parquet-format-structures encryption",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Delivered",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch,                                            1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Feb/18 09:34",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "12/Feb/20 13:29",
        "Description": "Write/read the new Thrift structures (crypto metada)\nAdd encryption/decryption of Parquet\u00a0metadata Thrift structures",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/613"
        ]
    },
    "PARQUET-1229": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "parquet-mr code changes for encryption support",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch,                                            1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Feb/18 09:41",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "13/Jul/20 05:45",
        "Description": "Addition of encryption/decryption support to the existing Parquet classes and APIs",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/776"
        ]
    },
    "PARQUET-1230": {
        "Key": "PARQUET-1230",
        "Summary": "CLI tools for encrypted files",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Feb/18 09:42",
        "Updated": "04/May/21 08:28",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1231": {
        "Key": "PARQUET-1231",
        "Summary": "Not able to load the LocalFileSystem class",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Persistent NGP",
        "Created": "20/Feb/18 13:49",
        "Updated": "08/Mar/19 21:03",
        "Resolved": "08/Mar/19 21:03",
        "Description": "When we are running the code for converting parquet file to csv locally on eclipse, it runs successfully and convert the parque file to csv but when we are running in our UI environment then it is failing saying\u00a0\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.LocalFileSystem not found.\n\u00a0\nPlease help us on it",
        "Issue Links": []
    },
    "PARQUET-1232": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Document the modular encryption in parquet-format",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Feb/18 14:52",
        "Updated": "12/Feb/20 07:59",
        "Resolved": "18/Oct/18 09:36",
        "Description": "Create Encryption.md from the design googledoc.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/101",
            "https://github.com/apache/parquet-format/pull/110"
        ]
    },
    "PARQUET-1233": {
        "Key": "PARQUET-1233",
        "Summary": "[CPP ]Enable option to switch between stl classes and boost classes for thrift header",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.4.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "20/Feb/18 14:55",
        "Updated": "21/Feb/18 18:15",
        "Resolved": "21/Feb/18 18:15",
        "Description": "Thrift 0.11.0 introduced breaking changes by defaulting to stl classes. This causes an issue with older thrift versions. The scope of this Jira is to enable an option to choose between stl and boost in parquet thrift header.\nhttps://thrift.apache.org/lib/cpp",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/443"
        ]
    },
    "PARQUET-1234": {
        "Key": "PARQUET-1234",
        "Summary": "Release Parquet format 2.5.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.5.0",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "21/Feb/18 12:42",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "18/Apr/18 12:16",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1242",
            "/jira/browse/PARQUET-1251"
        ]
    },
    "PARQUET-1235": {
        "Key": "PARQUET-1235",
        "Summary": "Parquet-tools cat mangles strings created by other clients",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael McCarthy",
        "Created": "21/Feb/18 22:40",
        "Updated": "21/Feb/18 22:40",
        "Resolved": null,
        "Description": "I have some parquet files that are created by Java MR process (which I do not own). I am able to read these fields successfully in pig and Spark, but for some reason the String fields are being mangled when I view the files with parquet-tools (cat).\nHere are the details on the\u00a0file metadata\u00a0using today's build of parquet-tools:\n\nhadoop jar parquet-tools-1.9.1-SNAPSHOT.jar meta <hdfs>/parquet-r-00000\r\n\n\nOutput:\n\nfile:          hdfs://<path>/parquet-r-00000\r\ncreator:       parquet-mr version 1.8.1 (build 4aba4dae7bb0d4edbcf7923ae1339f28fd3f7fcf)\r\n\r\nfile schema:   MY_DATA\r\n--------------------------------------------------------------------------------\r\nmyfield:       OPTIONAL BINARY R:0 D:1\r\n\r\nrow group 1:   RC:37343 TS:32397576 OFFSET:4\r\n--------------------------------------------------------------------------------\r\nmyfield:       BINARY SNAPPY DO:0 FPO:4 SZ:273374/556406/2.04 VC:37343 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[no stats for this column]\r\n\n\n\u00a0Has anyone seen this before?",
        "Issue Links": []
    },
    "PARQUET-1236": {
        "Key": "PARQUET-1236",
        "Summary": "Upgrade org.slf4j:slf4j-api:1.7.2 to 1.7.12",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.5.0",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "parquet-format",
        "Assignee": "PandaMonkey",
        "Reporter": "PandaMonkey",
        "Created": "23/Feb/18 14:54",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "21/Mar/18 15:27",
        "Description": "Hi, I found two versions of library\u00a0org.slf4j:slf4j-api in your project. It would be nice to keep the version consistency.\nTheir introduced path is:\n\norg.apache.parquet:parquet-format:2.4.1-SNAPSHOT::null->org.slf4j:slf4j-api:1.7.2::compile\norg.apache.parquet:parquet-format:2.4.1-SNAPSHOT::null->org.apache.thrift:libthrift:0.9.3::compile->org.slf4j:slf4j-api:1.7.12::compile\n\n\u00a0Thanks!\n\u00a0\nRegards,\n\u00a0 \u00a0 Panda",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/85"
        ]
    },
    "PARQUET-1237": {
        "Key": "PARQUET-1237",
        "Summary": "Reading big texts cause OutOfMemmory Error. How to read text partialy?",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Andrei Iatsuk",
        "Created": "26/Feb/18 10:58",
        "Updated": "26/Feb/18 10:58",
        "Resolved": null,
        "Description": "I have dataset with big strings (every record about 15 mb) in parquet.\nWhen I try to open all parquet parts I get OutOfMemory exception.\nHow can I get only headers (first 100 symbols) for each string record without reading all record?\n\u00a0\n\u00a0 Schema avroProj = SchemaBuilder.builder()\n\u00a0\u00a0\u00a0 .record(\"proj\").fields()\n\u00a0\u00a0\u00a0 .name(\"idx\").type().nullable().longType().noDefault()\n\u00a0\u00a0\u00a0 .name(\"text\").type().nullable().bytesType().noDefault()\n\u00a0\u00a0\u00a0 .endRecord();\n\u00a0 Configuration conf = new Configuration();\n\u00a0 AvroReadSupport.setRequestedProjection(conf, avroProj);\n\u00a0 ParquetReader<GenericRecord> parquetReader = AvroParquetReader\n\u00a0\u00a0\u00a0 .<GenericRecord>builder(new Path(filePath))\n\u00a0\u00a0\u00a0 .withConf(conf)\n\u00a0\u00a0\u00a0 .build();\n\u00a0 GenericRecord record = parquetReader.read(); // record already have full text in RAM\n\u00a0 Long idx = (Long) record.get(\"idx\");\n\u00a0 ByteBuffer rawText = (ByteBuffer) record.get(\"text\");\n\u00a0 String header = new String(rawText.array()).substring(0, 200);",
        "Issue Links": []
    },
    "PARQUET-1238": {
        "Key": "PARQUET-1238",
        "Summary": "Invalid links found in parquet site document page",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chuanyin Xu",
        "Created": "27/Feb/18 09:22",
        "Updated": "06/Jun/18 08:44",
        "Resolved": "06/Jun/18 08:43",
        "Description": "Links to pictures in document page are invalid, such as Section \u2018File Format\u2019 and \u2018Metadata\u2019\n\u00a0\nLinks to external documents in document page are invalid, such as Section 'Motivation', 'Logical Types' and 'Data Pages'",
        "Issue Links": [
            "/jira/browse/PARQUET-1244"
        ]
    },
    "PARQUET-1239": {
        "Key": "ARROW-3762",
        "Summary": "[C++] Parquet arrow::Table reads error when overflowing capacity of BinaryArray",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.15.0",
        "Component/s": "C++,                                            Python",
        "Assignee": "Ben Kietzman",
        "Reporter": "Left Screen",
        "Created": "01/Mar/18 20:07",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "09/Sep/19 21:13",
        "Description": "When reading a parquet file with binary data > 2 GiB, we get an ArrowIOError due to it not creating chunked arrays. Reading each row group individually and then concatenating the tables works, however.\n\n\u00a0\n\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\n\r\nx = pa.array(list('1' * 2**30))\r\n\r\ndemo = 'demo.parquet'\r\n\r\n\r\ndef scenario():\r\n    t = pa.Table.from_arrays([x], ['x'])\r\n    writer = pq.ParquetWriter(demo, t.schema)\r\n    for i in range(2):\r\n        writer.write_table(t)\r\n    writer.close()\r\n\r\n    pf = pq.ParquetFile(demo)\r\n\r\n    # pyarrow.lib.ArrowIOError: Arrow error: Invalid: BinaryArray cannot contain more than 2147483646 bytes, have 2147483647\r\n    t2 = pf.read()\r\n\r\n    # Works, but note, there are 32 row groups, not 2 as suggested by:\r\n    # https://arrow.apache.org/docs/python/parquet.html#finer-grained-reading-and-writing\r\n    tables = [pf.read_row_group(i) for i in range(pf.num_row_groups)]\r\n    t3 = pa.concat_tables(tables)\r\n\r\nscenario()",
        "Issue Links": [
            "/jira/browse/ARROW-2654",
            "/jira/browse/ARROW-3139",
            "/jira/browse/ARROW-5030",
            "/jira/browse/ARROW-2532",
            "/jira/browse/ARROW-2227",
            "https://github.com/apache/arrow/issues/1677",
            "https://github.com/apache/arrow/pull/3171",
            "https://github.com/apache/arrow/pull/4695",
            "https://github.com/apache/arrow/pull/5312"
        ]
    },
    "PARQUET-1240": {
        "Key": "PARQUET-1240",
        "Summary": "Proto-Parquet cannot write a schema with an empty group",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andr\u00e9 Pinto",
        "Created": "05/Mar/18 11:32",
        "Updated": "05/Mar/18 11:32",
        "Resolved": null,
        "Description": "This is valid protobuf:\n\u00a0\n\n\r\nmessage Something {\r\n\u00a0 \u00a0 EmptyMessage outerEmptyMessage = 1;\r\n}\r\n\r\nmessage EmptyMessage {\r\n}\r\n\n\nHowever when we try to convert this to Parquet we get:\n\n\r\norg.apache.parquet.schema.InvalidSchemaException: Cannot write a schema with an empty group: optional group outerEmptyMessage = 1 {\r\n}\n\nIs this a limitation of the Parquet format, or just a bug?\nIf the former, can we break earlier when generating the schema out of Protobuf, and not when trying to write the content to the already generated schema?",
        "Issue Links": []
    },
    "PARQUET-1241": {
        "Key": "PARQUET-1241",
        "Summary": "[C++] Use LZ4 frame format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Do",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Lawrence Chan",
        "Created": "08/Mar/18 22:08",
        "Updated": "24/Mar/21 15:22",
        "Resolved": "24/Mar/21 15:22",
        "Description": "The parquet-format spec doesn't currently specify whether\u00a0lz4-compressed data\u00a0should be framed or not. We should choose one and make it explicit in the spec, as they are not inter-operable. After some discussions with others [1], we think it would be beneficial to use the framed format, which adds a\u00a0small\u00a0header in exchange for more self-contained decompression as well as a richer feature set (checksums, parallel decompression, etc).\nThe\u00a0current arrow implementation compresses using the lz4 block format, and this would need to be updated when we add the spec clarification.\nIf backwards compatibility is a concern, I would suggest adding an additional LZ4_FRAMED compression type, but that may be more noise than anything.\n[1]\u00a0https://github.com/dask/fastparquet/issues/314",
        "Issue Links": [
            "/jira/browse/PARQUET-1996",
            "/jira/browse/PARQUET-1998",
            "/jira/browse/PARQUET-1878",
            "/jira/browse/PARQUET-1118"
        ]
    },
    "PARQUET-1242": {
        "Key": "PARQUET-1242",
        "Summary": "parquet.thrift refers to wrong releases for the new compressions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "09/Mar/18 10:59",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "23/Mar/18 13:56",
        "Description": "parquet.thrift contains the following:\n/**\n\u00a0* Supported compression algorithms.\n\u00a0*\n\u00a0* Codecs added in 2.3.2 can be read by readers based on 2.3.2 and later.\n\u00a0* Codec support may vary between readers based on the format version and\n\u00a0* libraries available at runtime. Gzip, Snappy, and LZ4 codecs are\n\u00a0* widely available, while Zstd and Brotli require additional libraries.\n\u00a0*/\nenum CompressionCodec {\n\u00a0\u00a0UNCOMPRESSED = 0;\n\u00a0\u00a0SNAPPY = 1;\n\u00a0\u00a0GZIP = 2;\n\u00a0\u00a0LZO = 3;\n\u00a0\u00a0BROTLI = 4; // Added in 2.3.2\n\u00a0\u00a0LZ4 = 5;    // Added in 2.3.2\n\u00a0\u00a0ZSTD = 6;   // Added in 2.3.2\n}\nIn reality, there was no 2.3.2 release. These compression codecs were added in version 2.4.",
        "Issue Links": [
            "/jira/browse/PARQUET-1234",
            "https://github.com/apache/parquet-format/pull/87"
        ]
    },
    "PARQUET-1243": {
        "Key": "PARQUET-1243",
        "Summary": "[C++] Improve quality of error message for zero-length files, otherwise corrupted files",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "12/Mar/18 04:24",
        "Updated": "24/May/19 18:07",
        "Resolved": "24/May/19 18:07",
        "Description": "Currently, the error looks like, even for zero-size files:\n\n\r\n    if (file_size < FOOTER_SIZE) {\r\n      throw ParquetException(\"Corrupted file, smaller than file footer\");\r\n    }",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4240"
        ]
    },
    "PARQUET-1244": {
        "Key": "PARQUET-1244",
        "Summary": "Documentation link to logical types broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Antoine Pitrou",
        "Created": "12/Mar/18 16:17",
        "Updated": "06/Jun/18 08:40",
        "Resolved": "30/May/18 11:04",
        "Description": "The link to LogicalTypes.md here is broken:\nhttps://parquet.apache.org/documentation/latest/",
        "Issue Links": [
            "/jira/browse/PARQUET-1238"
        ]
    },
    "PARQUET-1245": {
        "Key": "PARQUET-1245",
        "Summary": "[C++] Segfault when writing Arrow table with duplicate columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "None",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Alexey Strokach",
        "Created": "07/Jan/18 03:34",
        "Updated": "22/Mar/18 23:54",
        "Resolved": "22/Mar/18 23:53",
        "Description": "I accidentally created a large number of Parquet files with two _index_level_0_ columns (through a Spark SQL query).\nPyArrow can read these files into tables, but it segfaults when converting the resulting tables to Pandas DataFrames or when saving the tables to Parquet files.\n\n\r\n# Duplicate columns cause segmentation faults\r\ntable = pq.read_table('/path/to/duplicate_column_file.parquet')\r\ntable.to_pandas()  # Segmentation fault\r\npq.write_table(table, '/some/output.parquet') # Segmentation fault\r\n\n\nIf I remove the duplicate column using table.remove_column(...) everything works without segfaults.\n\n\r\n# After removing duplicate columns, everything works fine\r\ntable = pq.read_table('/path/to/duplicate_column_file.parquet')\r\ntable.remove_column(34)\r\ntable.to_pandas()  # OK\r\npq.write_table(table, '/some/output.parquet')  # OK\r\n\n\nFor more concrete examples, see `test_segfault_1.py` and `test_segfault_2.py` here: https://gitlab.com/ostrokach/pyarrow_duplicate_column_errors.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/447"
        ]
    },
    "PARQUET-1246": {
        "Key": "PARQUET-1246",
        "Summary": "Ignore float/double statistics in case of NaN",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.10.0,                                            1.8.3",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Mar/18 13:02",
        "Updated": "13/May/20 17:14",
        "Resolved": "19/Mar/18 13:43",
        "Description": "The sorting order of the floating point values are not properly specified, therefore NaN values can cause skipping valid values when filtering. See PARQUET-1222 for more info.\nThis issue is for ignoring statistics for float/double if it contains NaN to prevent data loss at the read path when filtering.",
        "Issue Links": [
            "/jira/browse/PARQUET-1189",
            "/jira/browse/PARQUET-1277",
            "/jira/browse/PARQUET-1222",
            "/jira/browse/ORC-629",
            "https://github.com/apache/parquet-mr/pull/461",
            "https://github.com/apache/parquet-mr/pull/468"
        ]
    },
    "PARQUET-1247": {
        "Key": "PARQUET-1247",
        "Summary": "org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shrutika modi",
        "Created": "14/Mar/18 15:12",
        "Updated": "24/Jun/18 00:30",
        "Resolved": null,
        "Description": "java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n at org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:44)\n at org.apache.spark.sql.execution.vectorized.ColumnVector.getUTF8String(ColumnVector.java:625)\n at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:243)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188)\n at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n at org.apache.spark.scheduler.Task.run(Task.scala:99)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)",
        "Issue Links": []
    },
    "PARQUET-1248": {
        "Key": "PARQUET-1248",
        "Summary": "java.lang.UnsupportedOperationException: Unimplemented type: StringType",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shrutika modi",
        "Created": "14/Mar/18 15:18",
        "Updated": "06/Jun/18 03:19",
        "Resolved": null,
        "Description": "java.lang.UnsupportedOperationException: Unimplemented type: StringType\n at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readIntBatch(VectorizedColumnReader.java:356)\n at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:183)\n at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:230)\n at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)\n at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:133)\n at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:166)\n at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:102)\n at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:243)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188)\n at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341)\n at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193)\n ... 8 more",
        "Issue Links": []
    },
    "PARQUET-1249": {
        "Key": "PARQUET-1249",
        "Summary": "Clarify encoding schemes for boolean types",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "17/Mar/18 18:36",
        "Updated": "23/Mar/18 19:40",
        "Resolved": null,
        "Description": "In the Parquet format specification, under the section for Plain encoding, boolean is encoded using the deprecated bit-packed encoding. However, the section for bit-packed encoding specifies that it is only used for repetition/definition levels. This seems contradictory. \nThe section for RLE/bit-packed hybrid encoding says \"Boolean values in data pages, as an alternative to PLAIN encoding\" - perhaps we should be specific and indicate this is only used for data page V2?\nAlso, implementation-wise, I saw parquet-cpp still encode boolean as plain 1-bit value while parquet-mr uses bit-packed encoding as described in the specification. Perhaps consolidation should be done for this.",
        "Issue Links": []
    },
    "PARQUET-1250": {
        "Key": "PARQUET-1250",
        "Summary": "RLE decoding should treat 0 length runs as error",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Csaba Ringhofer",
        "Created": "19/Mar/18 18:12",
        "Updated": "19/Mar/18 18:12",
        "Resolved": null,
        "Description": "RunLengthBitPackingHybridDecoder accepts run headers that encode 0 length repeated runs, and treats them as if they were 2^32 length run, so effectively every value returned for that data page will be the same. (see https://github.com/apache/parquet-mr/blob/0a86429939075984edce5e3b8195dfb7f9e3ab6b/parquet-column/src/main/java/org/apache/parquet/column/values/rle/RunLengthBitPackingHybridDecoder.java#L66 )\nThrowing an exception if count is 0 would give a proper error message for some corrupt files, and would make it clear that these are not legal values.",
        "Issue Links": []
    },
    "PARQUET-1251": {
        "Key": "PARQUET-1251",
        "Summary": "Clarify ambiguous min/max stats for FLOAT/DOUBLE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.4.0",
        "Fix Version/s": "format-2.5.0",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "20/Mar/18 14:06",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "26/Mar/18 13:04",
        "Description": "Describe the handling of the ambigous min/max statistics for FLOAT/DOUBLE types in case of TypeDefinedOrder. (See PARQUET-1222 for details.)\n\nWhen looking for NaN values, min and max should be ignored.\nIf the min is a NaN, it should be ignored.\nIf the max is a NaN, it should be ignored.\nIf the min is +0, the row group may contain -0 values as well.\nIf the max is -0, the row group may contain +0 values as well.",
        "Issue Links": [
            "/jira/browse/PARQUET-1234",
            "/jira/browse/PARQUET-1222",
            "https://github.com/apache/parquet-format/pull/88"
        ]
    },
    "PARQUET-1252": {
        "Key": "PARQUET-1252",
        "Summary": "[C++] Pass BOOST_ROOT and Boost_NAMESPACE on to Thrift EP",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "22/Mar/18 17:03",
        "Updated": "02/May/19 19:57",
        "Resolved": "02/May/19 19:57",
        "Description": "Currently we build the thrift_ep with the Boost version it finds by itself. In the case where parquet-cpp is built with a very specific Boost version, we also need to build it using this version. This needs passing along of BOOST_ROOT\u00a0and Boost_NAMESPACE\u00a0to the Thrift ExternalProject.",
        "Issue Links": [
            "/jira/browse/PARQUET-1262"
        ]
    },
    "PARQUET-1253": {
        "Key": "PARQUET-1253",
        "Summary": "Support for new logical type representation",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "26/Mar/18 12:47",
        "Updated": "21/Nov/18 12:46",
        "Resolved": "24/May/18 11:48",
        "Description": "Latest parquet-format introduced a new\u00a0representation for logical types. As of now this is not yet supported in parquet-mr, thus there's no way to use parametrized UTC normalized timestamp data types. When reading and writing Parquet files, besides 'converted_type' parquet-mr should use the new 'logicalType' field in SchemaElement to tell the current logical type annotation. To maintain backward compatibility, the semantic of converted_type shouldn't change.",
        "Issue Links": [
            "/jira/browse/PARQUET-1293",
            "/jira/browse/PARQUET-1284",
            "/jira/browse/PARQUET-905",
            "/jira/browse/PARQUET-1284",
            "https://github.com/apache/parquet-mr/pull/463"
        ]
    },
    "PARQUET-1254": {
        "Key": "PARQUET-1254",
        "Summary": "Unable to read deeply nested records from Parquet file with Avro interface.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Bob smith",
        "Created": "26/Mar/18 17:12",
        "Updated": "14/Jun/21 14:21",
        "Resolved": null,
        "Description": "I am attempting to read Parquet data, whose schema contains a record nested in a wrapper record, which is also nested in an array. E.g:\n\n\r\n{\r\n  \"type\": \"record\",\r\n  \"name\": \"record\",\r\n  \"fields\": [\r\n    {\r\n      \"name\": \"elements\",\r\n      \"type\": {\r\n        \"type\": \"array\",\r\n        \"items\": {\r\n          \"type\": \"record\",\r\n          \"name\": \"elementWrapper\",\r\n          \"fields\": [\r\n            {\r\n              \"name\": \"array_element\",\r\n              \"type\": {\r\n                \"type\": \"record\",\r\n                \"name\": \"element\",\r\n                \"namespace\": \"test\",\r\n                \"fields\": [\r\n                  {\r\n                    \"name\": \"someField\",\r\n                    \"type\": \"int\"\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n\n\nWhen reading a parquet file with the above schema using the ParquetFileReader, I can see the file has the following schema, which appears to be correct:\n\n\r\nmessage record {\r\n  required group elements (LIST) {\r\n    repeated group array {\r\n      required group array_element {\r\n        required int32 someField;\r\n      }\r\n    }\r\n  }\r\n}\r\n\n\nHowever, when attempting to read records from this file with the Avro interface (see below), I get a InvalidRecordException.\n\n\r\nfinal ParquetReader<GenericRecord> parquetReader = AvroParquetReader.<GenericRecord>builder(path).build();\r\nfinal GenericRecord read = parquetReader.read();\r\n\n\nStepping through the code, it looks like when the record is converted to Avro, the field \"someField\" isn't in scope. Only fields at the top level of the schema are in scope.\nIs it expected that Avro Parquet does not support this schema? Is this a bug in the AvroRecordConverter?\nThanks, Iain\nStacktrace:\n\n\r\norg.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'someField' not found\r\n\r\n\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:220)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:125)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:274)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:227)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.access$100(AvroRecordConverter.java:73)\r\n\tat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter$ElementConverter.<init>(AvroRecordConverter.java:531)\r\n\tat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter.<init>(AvroRecordConverter.java:481)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:284)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:136)\r\n\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:90)\r\n\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\r\n\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:132)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:175)\r\n\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:149)\r\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:125)\r\n\n\nBelow is the full code that creates a Parquet file with this schema, and then fails to read it:\n\n\r\n    @Test\r\n    @SneakyThrows\r\n    public void canReadWithNestedArray() {\r\n        final Path path = new Path(\"test-resources/\" + UUID.randomUUID());\r\n\r\n        // Construct a record that defines the final nested value we can't read\r\n        final Schema element = Schema.createRecord(\"element\", null, \"test\", false);\r\n        element.setFields(Arrays.asList(new Schema.Field(\"someField\", Schema.create(Schema.Type.INT), null, null)));\r\n\r\n        // Create a wrapper for above nested record\r\n        final Schema elementWrapper = Schema.createRecord(\"elementWrapper\", null, null, false);\r\n        elementWrapper.setFields(Arrays.asList(new Schema.Field(\"array_element\", element, null, null)));\r\n\r\n        // Create top level field that contains array of wrapped records\r\n        final Schema.Field topLevelArrayOfWrappers = new Schema.Field(\"elements\", Schema.createArray(elementWrapper), null, null);\r\n\r\n        final Schema topLevelElement = Schema.createRecord(\"record\", null, null, false);\r\n        topLevelElement.setFields(Arrays.asList(topLevelArrayOfWrappers));\r\n        final GenericRecord genericRecord = new GenericData.Record(topLevelElement);\r\n\r\n        // Create element\r\n        final GenericData.Record recordValue = new GenericData.Record(element);\r\n        recordValue.put(\"someField\", 5);\r\n\r\n        // Create element of array, wrapper containing above element\r\n        final GenericData.Record wrapperValue = new GenericData.Record(elementWrapper);\r\n        wrapperValue.put(\"array_element\", recordValue);\r\n\r\n        genericRecord.put(topLevelArrayOfWrappers.name(), Arrays.asList(wrapperValue));\r\n        \r\n        AvroParquetWriter.Builder<GenericRecord> fileWriterBuilder = AvroParquetWriter.<GenericRecord>builder(path).withSchema(topLevelElement);\r\n        final ParquetWriter<GenericRecord> fileWriter = fileWriterBuilder.build();\r\n\r\n        fileWriter.write(genericRecord);\r\n        fileWriter.close();\r\n\r\n        final ParquetFileReader parquetFileReader = ParquetFileReader.open(new Configuration(), path);\r\n        final FileMetaData fileMetaData = parquetFileReader.getFileMetaData();\r\n        System.out.println(fileMetaData.getSchema().toString());\r\n\r\n        final ParquetReader<GenericRecord> parquetReader = AvroParquetReader.<GenericRecord>builder(path).build();\r\n        final GenericRecord read = parquetReader.read();\r\n    }",
        "Issue Links": []
    },
    "PARQUET-1255": {
        "Key": "PARQUET-1255",
        "Summary": "[C++] Exceptions thrown in some tests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "27/Mar/18 13:08",
        "Updated": "29/Mar/18 13:26",
        "Resolved": "28/Mar/18 16:20",
        "Description": "Some tests (not all) throw a basic_string exception. Example:\n\n\r\n$ ./debug/reader-test \r\nRunning main() from gtest_main.cc\r\n[==========] Running 11 tests from 4 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 7 tests from TestAllTypesPlain\r\n[ RUN      ] TestAllTypesPlain.NoopConstructDestruct\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.NoopConstructDestruct (0 ms)\r\n[ RUN      ] TestAllTypesPlain.TestBatchRead\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.TestBatchRead (0 ms)\r\n[ RUN      ] TestAllTypesPlain.TestFlatScannerInt32\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.TestFlatScannerInt32 (0 ms)\r\n[ RUN      ] TestAllTypesPlain.TestSetScannerBatchSize\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.TestSetScannerBatchSize (0 ms)\r\n[ RUN      ] TestAllTypesPlain.DebugPrintWorks\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.DebugPrintWorks (0 ms)\r\n[ RUN      ] TestAllTypesPlain.ColumnSelection\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.ColumnSelection (0 ms)\r\n[ RUN      ] TestAllTypesPlain.ColumnSelectionOutOfRange\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestAllTypesPlain.ColumnSelectionOutOfRange (0 ms)\r\n[----------] 7 tests from TestAllTypesPlain (0 ms total)\r\n\r\n[----------] 2 tests from TestLocalFile\r\n[ RUN      ] TestLocalFile.FileClosedOnDestruction\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestLocalFile.FileClosedOnDestruction (0 ms)\r\n[ RUN      ] TestLocalFile.OpenWithMetadata\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in SetUp().\r\n[  FAILED  ] TestLocalFile.OpenWithMetadata (0 ms)\r\n[----------] 2 tests from TestLocalFile (0 ms total)\r\n\r\n[----------] 1 test from TestFileReaderAdHoc\r\n[ RUN      ] TestFileReaderAdHoc.NationDictTruncatedDataPage\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in the test body.\r\n[  FAILED  ] TestFileReaderAdHoc.NationDictTruncatedDataPage (1 ms)\r\n[----------] 1 test from TestFileReaderAdHoc (1 ms total)\r\n\r\n[----------] 1 test from TestJSONWithLocalFile\r\n[ RUN      ] TestJSONWithLocalFile.JSONOutput\r\nunknown file: Failure\r\nC++ exception with description \"basic_string::_S_construct null not valid\" thrown in the test body.\r\n[  FAILED  ] TestJSONWithLocalFile.JSONOutput (0 ms)\r\n[----------] 1 test from TestJSONWithLocalFile (0 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 11 tests from 4 test cases ran. (1 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 11 tests, listed below:\r\n[  FAILED  ] TestAllTypesPlain.NoopConstructDestruct\r\n[  FAILED  ] TestAllTypesPlain.TestBatchRead\r\n[  FAILED  ] TestAllTypesPlain.TestFlatScannerInt32\r\n[  FAILED  ] TestAllTypesPlain.TestSetScannerBatchSize\r\n[  FAILED  ] TestAllTypesPlain.DebugPrintWorks\r\n[  FAILED  ] TestAllTypesPlain.ColumnSelection\r\n[  FAILED  ] TestAllTypesPlain.ColumnSelectionOutOfRange\r\n[  FAILED  ] TestLocalFile.FileClosedOnDestruction\r\n[  FAILED  ] TestLocalFile.OpenWithMetadata\r\n[  FAILED  ] TestFileReaderAdHoc.NationDictTruncatedDataPage\r\n[  FAILED  ] TestJSONWithLocalFile.JSONOutput\r\n\r\n11 FAILED TESTS",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/448"
        ]
    },
    "PARQUET-1256": {
        "Key": "PARQUET-1256",
        "Summary": "[C++] Add --print-key-value-metadata option to parquet_reader tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Jacek Pliszka",
        "Reporter": "Jacek Pliszka",
        "Created": "27/Mar/18 18:52",
        "Updated": "17/Aug/18 21:54",
        "Resolved": "17/Aug/18 21:53",
        "Description": "Added --print-key-value-metadata option to parquet_reader tool\nhttps://github.com/apache/parquet-cpp/pull/450",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/450"
        ]
    },
    "PARQUET-1257": {
        "Key": "ARROW-3771",
        "Summary": "[C++] GetRecordBatchReader in parquet/arrow/reader.h should be able to specify chunksize",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "C++",
        "Assignee": null,
        "Reporter": "YE",
        "Created": "28/Mar/18 09:46",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "02/Jul/19 09:53",
        "Description": "see https://github.com/apache/parquet-cpp/pull/445\u00a0comments",
        "Issue Links": [
            "/jira/browse/ARROW-1012"
        ]
    },
    "PARQUET-1258": {
        "Key": "PARQUET-1258",
        "Summary": "Update scm developer connection to github",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0,                                            format-2.5.0",
        "Fix Version/s": "1.10.0,                                            format-2.5.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "28/Mar/18 13:16",
        "Updated": "18/Apr/18 12:22",
        "Resolved": "30/Mar/18 21:44",
        "Description": "After moving to gitbox the old apache repo (https://git-wip-us.apache.org/repos/asf/parquet-format.git) is not working anymore. The pom.xml shall be updated accordingly.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/90",
            "https://github.com/apache/parquet-mr/pull/462"
        ]
    },
    "PARQUET-1259": {
        "Key": "PARQUET-1259",
        "Summary": "Parquet-protobuf support both protobuf 2 and protobuf 3",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Workaround",
        "Affects Version/s": "1.10.0,                                            1.9.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Qinghui Xu",
        "Created": "28/Mar/18 14:28",
        "Updated": "04/Apr/18 16:51",
        "Resolved": "04/Apr/18 16:51",
        "Description": "With the merge of pull request: https://github.com/apache/parquet-mr/pull/407, now it is protobuf 3 used in parquet-protobuf, and this implies that it cannot work in an environment where people are using protobuf 2 in their own dependencies because there is some new API / breaking change in protobuf 3. People have to face a dependency version conflict with next parquet-protobuf release (e.g. 1.9.1 or 1.10.0).\nWhat if we support both protobuf 2 and protobuf 3 by providing parquet-protobuf and parquet-protobuf2?",
        "Issue Links": []
    },
    "PARQUET-1260": {
        "Key": "PARQUET-1260",
        "Summary": "Add Zoltan Ivanfi's code signing key to the KEYS file",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "28/Mar/18 14:35",
        "Updated": "18/May/18 13:02",
        "Resolved": "18/May/18 13:02",
        "Description": "To make a release, I would need to have my gpg key added to the KEYS file. I can add it to the repos in a commit, but I don't know how to update https://dist.apache.org/repos/dist/dev/parquet/KEYS",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/91"
        ]
    },
    "PARQUET-1261": {
        "Key": "PARQUET-1261",
        "Summary": "Parquet-format interns strings when reading filemetadata",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robert Kruszewski",
        "Reporter": "Robert Kruszewski",
        "Created": "29/Mar/18 12:08",
        "Updated": "23/Nov/18 10:50",
        "Resolved": "23/Nov/18 10:50",
        "Description": "Parquet-format when deserializing metadata will intern strings. References I could find suggested that it had been done to reduce memory pressure early on. Java (and jvm in particular) went a long way since then and interning is generally discouraged, see https://shipilev.net/jvm-anatomy-park/10-string-intern/ for a good explanation. What is more since java 8 there's string deduplication implemented at GC level per http://openjdk.java.net/jeps/192. During our usage and testing we found the interning to cause significant gc pressure for long running applications due to bigger GC root set.\nThis issue proposes removing interning given it's questionable whether it should be used in modern jvms.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/92"
        ]
    },
    "PARQUET-1262": {
        "Key": "PARQUET-1262",
        "Summary": "[C++] Use the same BOOST_ROOT and Boost_NAMESPACE for Thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "29/Mar/18 15:33",
        "Updated": "23/Apr/18 14:54",
        "Resolved": "23/Apr/18 14:46",
        "Description": "When building Thrift using the ExternalProject facility, we do not pass on the variables for a custom Boost variant. Thus if the user uses a differently flavoured/located Boost, Thrift does not pick it up. As a cause of this, we explicitly build Thrift during the Arrow OS X Wheel build.",
        "Issue Links": [
            "/jira/browse/PARQUET-1252",
            "https://github.com/apache/parquet-cpp/pull/460"
        ]
    },
    "PARQUET-1263": {
        "Key": "PARQUET-1263",
        "Summary": "ParquetReader's builder should use Configuration from the InputFile",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.0",
        "Component/s": "None",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Mar/18 22:03",
        "Updated": "30/Mar/18 22:34",
        "Resolved": "30/Mar/18 22:34",
        "Description": "ParquetReader can be built using an InputFile, which may be a HadoopInputFile and have a Configuration. If it is, ParquetHadoopOptions should be be based on that configuration instance.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/464",
            "https://github.com/apache/parquet-mr/pull/464"
        ]
    },
    "PARQUET-1264": {
        "Key": "PARQUET-1264",
        "Summary": "Update Javadoc for Java 1.8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.10.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "30/Mar/18 22:42",
        "Updated": "06/Apr/18 00:46",
        "Resolved": "06/Apr/18 00:46",
        "Description": "After moving the build to Java 1.8, the release procedure no longer works because Javadoc generation fails.",
        "Issue Links": [
            "/jira/browse/PARQUET-1189"
        ]
    },
    "PARQUET-1265": {
        "Key": "PARQUET-1265",
        "Summary": "Segfault on static ApplicationVersion initialization",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Lawrence Chan",
        "Created": "04/Apr/18 22:14",
        "Updated": "12/Apr/18 21:12",
        "Resolved": "12/Apr/18 21:12",
        "Description": "I'm seeing a segfault when I link/run with a shared libparquet.so with statically linked boost. Given the backtrace, it seems that this is due to the static ApplicationVersion constants, likely due to some static initialization order issue. The problem goes away if I turn those static vars into static funcs returning function-local statics.\nBacktrace:\n\n\r\n#0  0x00007ffff753cf8b in std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&) () from /lib64/libstdc++.so.6\r\n#1  0x00007ffff7aeae9c in boost::re_detail_106600::cpp_regex_traits_char_layer<char>::init() () from debug/libparquet.so.1\r\n#2  0x00007ffff7adcc2b in boost::object_cache<boost::re_detail_106600::cpp_regex_traits_base<char>, boost::re_detail_106600::cpp_regex_traits_implementation<char> >::do_get(boost::re_detail_106600::cpp_regex_traits_base<char> const&, unsigned long) () from debug/libparquet.so.1\r\n#3  0x00007ffff7ae9023 in boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::do_assign(char const*, char const*, unsigned int) () from debug/libparquet.so.1\r\n#4  0x00007ffff7a5ed98 in boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::assign (this=0x7fffffff5580, p1=0x7ffff7af66d8 \"(.*?)\\\\s*(?:(version\\\\s*(?:([^(]*?)\\\\s*(?:\\\\(\\\\s*build\\\\s*([^)]*?)\\\\s*\\\\))?)?)?)\", p2=0x7ffff7af6720 \"\", f=0) at /tmp/boost-1.66.0/include/boost/regex/v4/basic_regex.hpp:381\r\n#5  0x00007ffff7a5b653 in boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::assign (this=0x7fffffff5580, p=0x7ffff7af66d8 \"(.*?)\\\\s*(?:(version\\\\s*(?:([^(]*?)\\\\s*(?:\\\\(\\\\s*build\\\\s*([^)]*?)\\\\s*\\\\))?)?)?)\", f=0) at /tmp/boost-1.66.0/include/boost/regex/v4/basic_regex.hpp:366\r\n#6  0x00007ffff7a57049 in boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::basic_regex (this=0x7fffffff5580, p=0x7ffff7af66d8 \"(.*?)\\\\s*(?:(version\\\\s*(?:([^(]*?)\\\\s*(?:\\\\(\\\\s*build\\\\s*([^)]*?)\\\\s*\\\\))?)?)?)\", f=0) at /tmp/boost-1.66.0/include/boost/regex/v4/basic_regex.hpp:335\r\n#7  0x00007ffff7a4fa1f in parquet::ApplicationVersion::ApplicationVersion (this=0x7ffff7ddbfc0 <parquet::ApplicationVersion::PARQUET_251_FIXED_VERSION>, created_by=\"parquet-mr version 1.8.0\") at /tmp/parquet-cpp-apache-parquet-cpp-1.4.0/src/parquet/metadata.cc:477\r\n#8  0x00007ffff7a516c5 in __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535) at /tmp/parquet-cpp-apache-parquet-cpp-1.4.0/src/parquet/metadata.cc:58\r\n#9  0x00007ffff7a5179e in _GLOBAL__sub_I_metadata.cc(void) () at /tmp/parquet-cpp-apache-parquet-cpp-1.4.0/src/parquet/metadata.cc:913\r\n#10 0x00007ffff7dec1e3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2\r\n#11 0x00007ffff7dde21a in _dl_start_user () from /lib64/ld-linux-x86-64.so.2\r\n#12 0x0000000000000001 in ?? ()\r\n#13 0x00007fffffff5ff5 in ?? ()\r\n#14 0x0000000000000000 in ?? ()\r\n\n\nVersions:\n\ngcc-4.8.5\nboost-1.66.0\nparquet-cpp-1.4.0",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/452"
        ]
    },
    "PARQUET-1266": {
        "Key": "PARQUET-1266",
        "Summary": "LogicalTypes union in parquet-format doesn't include UUID",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "05/Apr/18 07:42",
        "Updated": "14/Jan/21 11:51",
        "Resolved": "26/Sep/18 14:33",
        "Description": "parquet-format new logical type representation doesn't include UUID type",
        "Issue Links": [
            "/jira/browse/DRILL-7825",
            "/jira/browse/DRILL-7829",
            "https://github.com/apache/parquet-format/pull/93"
        ]
    },
    "PARQUET-1267": {
        "Key": "PARQUET-1267",
        "Summary": "replace \"unsafe\" std::equal by std::memcmp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "06/Apr/18 18:10",
        "Updated": "17/Apr/18 17:03",
        "Resolved": "17/Apr/18 17:03",
        "Description": "std::equal(first1, last1, first2, size) is considered unsafe by msvc, replace it by std::memcmp",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/451"
        ]
    },
    "PARQUET-1268": {
        "Key": "PARQUET-1268",
        "Summary": "[C++] Conversion of Arrow null list columns fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "12/Apr/18 15:05",
        "Updated": "17/Apr/18 17:08",
        "Resolved": "17/Apr/18 17:08",
        "Description": "See ARROW-2450",
        "Issue Links": [
            "/jira/browse/ARROW-2450",
            "https://github.com/apache/parquet-cpp/pull/454"
        ]
    },
    "PARQUET-1269": {
        "Key": "PARQUET-1269",
        "Summary": "[C++] Scanning fails with list columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "13/Apr/18 10:33",
        "Updated": "23/Apr/18 14:50",
        "Resolved": "23/Apr/18 14:50",
        "Description": ">>> list_arr = pa.array([[1, 2], [3, 4, 5]])\r\n>>> int_arr = pa.array([10, 11])\r\n>>> table = pa.Table.from_arrays([int_arr, list_arr], ['ints', 'lists'])\r\n>>> bio = io.BytesIO()\r\n>>> pq.write_table(table, bio)\r\n>>> bio.seek(0)\r\n0\r\n>>> reader = pq.ParquetReader()\r\n>>> reader.open(bio)\r\n>>> reader.scan_contents()\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-23-58e977f6d60b>\", line 1, in <module>\r\n    reader.scan_contents()\r\n  File \"_parquet.pyx\", line 753, in pyarrow._parquet.ParquetReader.scan_contents\r\n  File \"error.pxi\", line 79, in pyarrow.lib.check_status\r\nArrowIOError: Parquet error: Total rows among columns do not match\r\n\n\nScanFileContents() claims it returns the \"number of semantic rows\" but apparently it actually counts the number of physical elements?",
        "Issue Links": []
    },
    "PARQUET-1270": {
        "Key": "PARQUET-1270",
        "Summary": "[C++] Executable tools do not get installed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "13/Apr/18 10:51",
        "Updated": "17/Apr/18 17:08",
        "Resolved": "17/Apr/18 17:08",
        "Description": "I have the following build script:\n\n\r\nmkdir -p build-debug\r\npushd build-debug\r\n\r\ncmake -DCMAKE_BUILD_TYPE=debug \\\r\n      -DCMAKE_INSTALL_PREFIX=$PARQUET_HOME \\\r\n      -DPARQUET_BUILD_BENCHMARKS=off \\\r\n      -DPARQUET_BUILD_EXECUTABLES=on \\\r\n      -DPARQUET_BUILD_TESTS=on \\\r\n      ..\r\n\r\nmake -j16\r\nmake install\r\npopd\r\n\n\nparquet_reader does get built:\n\n\r\n$ find -name parquet_reader\r\n./build-debug/debug/parquet_reader\r\n\n\nbut it isn't installed:\n\n\r\n$ find $PARQUET_HOME -name parquet_reader\r\n$",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/455"
        ]
    },
    "PARQUET-1271": {
        "Key": "PARQUET-1271",
        "Summary": "[C++] \"parquet_reader\" should be \"parquet-reader\"",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Antoine Pitrou",
        "Created": "13/Apr/18 11:14",
        "Updated": "19/Nov/18 22:26",
        "Resolved": "19/Nov/18 22:26",
        "Description": "Out of \"parquet-dump-schema\", \"parquet_reader\" and \"parquet-scan\", \"parquet_reader\" gratuitously follows a different naming convention.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2994"
        ]
    },
    "PARQUET-1272": {
        "Key": "PARQUET-1272",
        "Summary": "[C++]\u00a0ScanFileContents reports wrong row count for nested columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "17/Apr/18 16:56",
        "Updated": "18/Apr/18 11:04",
        "Resolved": "18/Apr/18 11:04",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/457"
        ]
    },
    "PARQUET-1273": {
        "Key": "PARQUET-1273",
        "Summary": "[Python] Error writing to partitioned Parquet dataset",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Joshua Storck",
        "Reporter": "Robert Dailey",
        "Created": "19/Dec/17 17:22",
        "Updated": "18/Apr/18 08:10",
        "Resolved": "18/Apr/18 08:10",
        "Description": "I receive the following error after upgrading to pyarrow 0.8.0 when writing to a dataset:\n\nArrowIOError: Column 3 had 187374 while previous column had 10000\n\nThe command was:\nwrite_table_values = \n{'row_group_size': 10000}\npq.write_to_dataset(pa.Table.from_pandas(df, preserve_index=True), '/logs/parsed/test', partition_cols=['Product', 'year', 'month', 'day', 'hour'], **write_table_values)\nI've also tried write_table_values = \n{'chunk_size': 10000}\n and received the same error.\nThis same command works in version 0.7.1.  I am trying to troubleshoot the problem but wanted to submit a ticket.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/453"
        ]
    },
    "PARQUET-1274": {
        "Key": "PARQUET-1274",
        "Summary": "[Python] SegFault in pyarrow.parquet.write_table with specific options",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Joshua Storck",
        "Reporter": "Cl\u00e9ment Bouscasse",
        "Created": "02/Feb/18 09:34",
        "Updated": "18/Apr/18 10:56",
        "Resolved": "18/Apr/18 10:56",
        "Description": "I originally filed an issue in the pandas project but we've tracked it down to arrow itself, when called via pandas in specific circumstances:\nhttps://github.com/pandas-dev/pandas/issues/19493\nbasically using\n\n\r\n df.to_parquet('filename.parquet', flavor='spark')\n\ngives a seg fault if `df` contains a datetime column.\nUnder the covers,\u00a0 pandas translates this to the following call:\n\n\r\npq.write_table(table, 'output.parquet', flavor='spark', compression='snappy', coerce_timestamps='ms')\r\n\n\nwhich gives me an instant crash.\nThere is a repro on the github ticket.",
        "Issue Links": [
            "/jira/browse/ARROW-2020",
            "https://github.com/apache/parquet-cpp/pull/456"
        ]
    },
    "PARQUET-1275": {
        "Key": "PARQUET-1275",
        "Summary": "Travis fails on branch 1.8.x",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.3",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "19/Apr/18 11:38",
        "Updated": "20/Apr/18 06:32",
        "Resolved": "20/Apr/18 06:32",
        "Description": "Travis error message:\nThe command \"wget http://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz\" failed and exited with 8 during .\nThe site googlecode.com is not working anymore. Based on master, we shall use https://github.com/google/protobuf/archive/v2.5.0.tar.gz instead.",
        "Issue Links": [
            "/jira/browse/PARQUET-1277",
            "https://github.com/apache/parquet-mr/pull/466"
        ]
    },
    "PARQUET-1276": {
        "Key": "PARQUET-1276",
        "Summary": "[C++] Reduce the amount of memory used for writing null decimal values",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "19/Apr/18 14:54",
        "Updated": "20/Aug/18 16:07",
        "Resolved": "20/Aug/18 16:07",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/459",
            "https://github.com/apache/parquet-cpp/pull/493"
        ]
    },
    "PARQUET-1277": {
        "Key": "PARQUET-1277",
        "Summary": "Release Parquet-mr 1.8.3",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8.3",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "20/Apr/18 06:16",
        "Updated": "24/May/18 11:49",
        "Resolved": "24/May/18 11:49",
        "Description": "Please link needed issues as blockers.",
        "Issue Links": [
            "/jira/browse/PARQUET-1217",
            "/jira/browse/PARQUET-1246",
            "/jira/browse/PARQUET-1275"
        ]
    },
    "PARQUET-1278": {
        "Key": "PARQUET-1278",
        "Summary": "[Java] parquet-arrow is broken due to different JDK version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andy Grove",
        "Created": "23/Apr/18 17:40",
        "Updated": "11/Jan/19 14:33",
        "Resolved": "11/Jan/19 14:33",
        "Description": "Parquet was recently update to use Arrow 0.8.0 to resolve this JIRA: https://issues.apache.org/jira/browse/PARQUET-1128\u00a0but unfortunately the issue is actually not resolved because Arrow 0.8.0 uses JDK 7 and Parquet uses JDK 8.\nI have filed a Jira against Arrow to upgrade to JDK 8: https://issues.apache.org/jira/browse/ARROW-2498\nOnce this is done, we will need to update parquet-arrow to use the new version of arrow.",
        "Issue Links": [
            "/jira/browse/PARQUET-1390"
        ]
    },
    "PARQUET-1279": {
        "Key": "PARQUET-1279",
        "Summary": "Use ASSERT_NO_FATAIL_FAILURE in C++ unit tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Joshua Storck",
        "Reporter": "Joshua Storck",
        "Created": "23/Apr/18 15:15",
        "Updated": "23/Apr/18 20:52",
        "Resolved": "23/Apr/18 20:51",
        "Description": "A number of unit tests have helper functions that use gtest/arrow ASSERT_ macros. Those ASSERT_ macros simply return out of the current context and do not throw exceptions or abort. Since these helper functions return void, the unit test simply continues when the assertions are triggered. This can lead to additional failures, such as segfaults because the test is executing code that it did not expect to. By adding the gtest ASSERT_NO_FATAIL_FAILURE to the calls of those helper functions in the outermost scope of the unit test, the test will correctly terminate.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/458"
        ]
    },
    "PARQUET-1280": {
        "Key": "PARQUET-1280",
        "Summary": "[parquet-protobuf] Use maven protoc plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Qinghui Xu",
        "Created": "24/Apr/18 11:33",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "14/Jan/19 10:00",
        "Description": "Currently the build of parquet-protobuf requires protoc to be installed in your environment. By using maven protoc plugin, we can have a build independent of the environment (no need to install protoc), and more easy to change the version of protoc.",
        "Issue Links": [
            "/jira/browse/PARQUET-1492",
            "https://github.com/apache/parquet-mr/pull/506"
        ]
    },
    "PARQUET-1281": {
        "Key": "PARQUET-1281",
        "Summary": "Jackson dependency",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0,                                            1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Qinghui Xu",
        "Created": "24/Apr/18 11:41",
        "Updated": "18/Apr/19 16:23",
        "Resolved": "18/Apr/19 16:22",
        "Description": "Currently we shaded jackson in parquet-jackson module (org.codehaus.jackon --> shaded.parquet.org.codehaus.jackson), but in fact we do not use the shaded jackson in parquet-hadoop code. Is that a mistake? (see https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ParquetMetadata.java#L26)",
        "Issue Links": []
    },
    "PARQUET-1282": {
        "Key": "PARQUET-1282",
        "Summary": "\"missing LastBuildJobID\" error when building new cube segment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "liyang",
        "Created": "25/Apr/18 07:26",
        "Updated": "25/Apr/18 07:28",
        "Resolved": "25/Apr/18 07:28",
        "Description": "An unstable exception. Likely to happen when there are multiple concurrent builds.\n2018-04-18 20:11:16,856 ERROR [pool-33-thread-11] threadpool.DefaultScheduler : ExecuteException job:cc08da19-f53e-4344-a6c5-05e764834924\n{{ org.apache.kylin.job.exception.ExecuteException: org.apache.kylin.job.exception.ExecuteException: java.lang.IllegalStateException: For cube CUBE[name=cube2], segment cube2[2018041423000_2018041423001] missing LastBuildJobID}}\n{{ at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:140)}}\n{{ at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:307)}}\n{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}\n{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}\n{{ at java.lang.Thread.run(Thread.java:748)}}\n{{ Caused by: org.apache.kylin.job.exception.ExecuteException: java.lang.IllegalStateException: For cube CUBE[name=cube2], segment cube2[2018041423000_2018041423001] missing LastBuildJobID}}\n{{ at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:140)}}\n{{ at org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:67)}}\n{{ at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:129)}}\n{{ ... 4 more}}\n{{ Caused by: java.lang.IllegalStateException: For cube CUBE[name=cube2], segment cube2[2018041423000_2018041423001] missing LastBuildJobID}}\n{{ at org.apache.kylin.cube.CubeManager$SegmentAssist.promoteNewlyBuiltSegments(CubeManager.java:810)}}\n{{ at org.apache.kylin.cube.CubeManager.promoteNewlyBuiltSegments(CubeManager.java:535)}}\n{{ at org.apache.kylin.engine.mr.steps.UpdateCubeInfoAfterBuildStep.doWork(UpdateCubeInfoAfterBuildStep.java:78)}}\n{{ at io.kyligence.kap.engine.mr.steps.KapUpdateCubeInfoAfterBuildStep.doWork(SourceFile:47)}}\n{{ at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:129)}}\n{{ ... 6 more}}",
        "Issue Links": []
    },
    "PARQUET-1283": {
        "Key": "PARQUET-1283",
        "Summary": "[C++] FormatStatValue appends trailing space to string and int96",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Julius Neuffer",
        "Reporter": "Julius Neuffer",
        "Created": "25/Apr/18 12:11",
        "Updated": "01/May/18 10:52",
        "Resolved": "01/May/18 10:51",
        "Description": "The function FormatStatValue appends a trailing space to string and int96 statistics.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/461"
        ]
    },
    "PARQUET-1284": {
        "Key": "PARQUET-1284",
        "Summary": "Incorporate new logical type parameters into schema language",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "27/Apr/18 10:13",
        "Updated": "28/May/18 10:18",
        "Resolved": "28/May/18 10:17",
        "Description": "The new logical types API\u00a0introduces new type parameters for time logical type (isAdjustedToUTC, unit), int type (bitWidth, isSigned) and timestamp type\u00a0(isAdjustedToUTC, unit).\nThe schema language doesn't support these type parameters yet.",
        "Issue Links": [
            "/jira/browse/PARQUET-1253",
            "/jira/browse/PARQUET-1253"
        ]
    },
    "PARQUET-1285": {
        "Key": "PARQUET-1285",
        "Summary": "[Java] SchemaConverter should not convert from TimeUnit.SECOND AND TimeUnit.NANOSECOND of Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "29/Apr/18 09:17",
        "Updated": "02/Aug/18 13:04",
        "Resolved": "07/May/18 08:12",
        "Description": "Arrow's 'Time' definition is below:\n\n\r\n{ \"name\" : \"time\", \"unit\" : \"SECOND|MILLISECOND|MICROSECOND|NANOSECOND\", \"bitWidth\": /* integer: 32 or 64 */ }\n\nhttp://arrow.apache.org/docs/metadata.html\n\u00a0\nBut Parquet only supports 'TIME_MILLIS' and 'TIME_MICROS'.\nhttps://github.com/Apache/parquet-format/blob/master/LogicalTypes.md\nTherefore SchemaConverter should not convert from TimeUnit.SECOND AND TimeUnit.NANOSECOND of Arrow to Parquet.",
        "Issue Links": [
            "/jira/browse/PARQUET-1297",
            "https://github.com/apache/parquet-mr/pull/469"
        ]
    },
    "PARQUET-1286": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Crypto package in parquet-mr",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Delivered",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch,                                            1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/May/18 05:39",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "12/Feb/20 13:31",
        "Description": "The implementation of Parquet encryptors and decryptors.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/614"
        ]
    },
    "PARQUET-1287": {
        "Key": "PARQUET-1287",
        "Summary": "ColumnIndexes and encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/May/18 06:59",
        "Updated": "06/Apr/20 06:21",
        "Resolved": "06/Apr/20 06:21",
        "Description": "Implement encryption and decryption of column indexes, with column-specific keys.",
        "Issue Links": []
    },
    "PARQUET-1288": {
        "Key": "PARQUET-1288",
        "Summary": "Multi-key encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/May/18 08:26",
        "Updated": "28/Jun/18 05:53",
        "Resolved": "28/Jun/18 05:53",
        "Description": "Depends on Parquet modular encryption (phase 1) and ColumnIndex mechanism.\nNeeds additional protection (or dropping) of footer statistics.\nDesign document will be published for a review.",
        "Issue Links": []
    },
    "PARQUET-1289": {
        "Key": "PARQUET-1289",
        "Summary": "Spec for Updateable Parquet",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "David Lee",
        "Created": "01/May/18 14:55",
        "Updated": "07/Nov/18 15:45",
        "Resolved": "07/Nov/18 15:45",
        "Description": "Parquet today is a read only columnar format, but can we also make it updateable using the methods in Apache Arrow for row filtering?\nHere's how it would work:\nA. Add an insert timestamp for every single record in a parquet file.\nB. Add a list of modifiable row offsets to the parquet file's footer for records in the parquet file which have been logically deleted. We should also include the delete timestamp for every offset as well in order to reproduce snapshot of what data looked like at any point in time.\nC. If a parquet record is ever update. The updated record would be a new record in a different parquet file and the old record in the parquet file would be logically deleted by adding its row offset to its parquet file's footer. We would need a service that does this.\nD. When reading parquet files. Logically deleted rows would be excluded.\nE. Alternatively when reading parquet files with a snapshot time any rows in the parquet files with an insert timestamp > snapshot time would be excluded and any rows which have been logically flagged for deletion would be included if delete timestamp < snapshot time.\nThis way we do not have to reorganize the columnar data in existing parquet files. We just have to modify the metadata footer.",
        "Issue Links": []
    },
    "PARQUET-1290": {
        "Key": "PARQUET-1290",
        "Summary": "Clarify maximum run lengths for RLE encoding",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Tim Armstrong",
        "Reporter": "Tim Armstrong",
        "Created": "01/May/18 18:34",
        "Updated": "26/Sep/18 14:33",
        "Resolved": "07/May/18 17:04",
        "Description": "The Parquet spec isn't clear about what the upper bound on run lengths in the RLE encoding is - https://github.com/apache/parquet-format/blob/master/Encodings.md#run-length-encoding--bit-packing-hybrid-rle--3 .\nIt sounds like in practice that the major implementations don't support run lengths > (2^31 - 1) - see https://lists.apache.org/thread.html/6731a94a98b790ad24a9a5bb4e1bf9bb799d729e948e046efb40014f@%3Cdev.parquet.apache.org%3E\nI propose that we limit bit-pack-count and number of times repeated to being <= 2^31.\nIt seems unlikely that there are parquet files in existence with larger run lengths, given that it requires huge numbers of values per page and major implementations can't write or read such files without overflowing integers. Maybe it would be possible if all the columns in a file were extremely compressible, but it seems like in practice most implementations will hit page or file size limits before producing a very-large run.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/96"
        ]
    },
    "PARQUET-1291": {
        "Key": "PARQUET-1291",
        "Summary": "[C++] libparquet.so should be ABI-versioned",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Antoine Pitrou",
        "Created": "02/May/18 11:47",
        "Updated": "12/Nov/18 18:04",
        "Resolved": "12/Nov/18 18:04",
        "Description": "Currently, SO file versioning follows source versioning (e.g. libparquet.so.1.4.1). Instead, it should probably have ABI versioning as done in ARROW-2522: the SO major number must be incremented each time the ABI changes.",
        "Issue Links": []
    },
    "PARQUET-1292": {
        "Key": "PARQUET-1292",
        "Summary": "Add constructors to ProtoParquetWriter to write specs compliant Parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kunal Chawla",
        "Reporter": "Kunal Chawla",
        "Created": "03/May/18 15:56",
        "Updated": "06/Nov/18 17:31",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/473"
        ]
    },
    "PARQUET-1293": {
        "Key": "PARQUET-1293",
        "Summary": "Build failure when using Java 8 lambda expressions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.2",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "09/May/18 09:34",
        "Updated": "10/May/18 11:31",
        "Resolved": "10/May/18 11:31",
        "Description": "I tried to use Java 8 lambda expressions, but when I tried mvn clean install, I ran into compilation error:\n\n\r\n\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.1:shade (default) on project parquet-column: Error creating shaded jar: 10969 -> [Help 1]\r\n\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.1:shade (default) on project parquet-column: Error creating shaded jar: 10969\r\n\r\n at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)\r\n\r\n at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\r\n\r\n at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\r\n\r\n at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)\r\n\r\n at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)\r\n\r\n at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)\r\n\r\n at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)\r\n\r\n at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)\r\n\r\n at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)\r\n\r\n at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)\r\n\r\n at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)\r\n\r\n at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)\r\n\r\n at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)\r\n\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n\r\n at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\r\n\r\n at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\r\n\r\n at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\r\n\r\n at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\r\n\r\nCaused by: org.apache.maven.plugin.MojoExecutionException: Error creating shaded jar: 10969\r\n\r\n at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:528)\r\n\r\n at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)\r\n\r\n at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)\r\n\r\n ... 20 more\r\n\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 10969\r\n\r\n at org.objectweb.asm.ClassReader.<init>(Unknown Source)\r\n\r\n at org.objectweb.asm.ClassReader.<init>(Unknown Source)\r\n\r\n at org.objectweb.asm.ClassReader.<init>(Unknown Source)\r\n\r\n at org.vafer.jdependency.Clazzpath.addClazzpathUnit(Clazzpath.java:94)\r\n\r\n at org.apache.maven.plugins.shade.filter.MinijarFilter.<init>(MinijarFilter.java:77)\r\n\r\n at org.apache.maven.plugins.shade.mojo.ShadeMojo.getFilters(ShadeMojo.java:767)\r\n\r\n at org.apache.maven.plugins.shade.mojo.ShadeMojo.execute(ShadeMojo.java:445)\r\n\r\n ... 22 more\r\n\r\n\n\n\u00a0\nSince Parquet is already on Java 8, one should be able to use all Java 8 features. Upgrading\u00a0maven-shade-plugin version and\u00a0enforcer-rule dependency of\u00a0\nmaven-enforcer-plugin would solve the problem.",
        "Issue Links": [
            "/jira/browse/PARQUET-1253",
            "https://github.com/apache/parquet-mr/pull/474"
        ]
    },
    "PARQUET-1294": {
        "Key": "PARQUET-1294",
        "Summary": "Update release scripts for the new Apache policy",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "09/May/18 10:28",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "24/May/18 09:22",
        "Description": "The Apache policy about the checksums is changed recently so it is required to update to related release scripts. See the policy detailed here: http://www.apache.org/dev/release-distribution#sigs-and-sums",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/97",
            "https://github.com/apache/parquet-mr/pull/475"
        ]
    },
    "PARQUET-1295": {
        "Key": "PARQUET-1295",
        "Summary": "Parquet libraries do not follow proper semantic versioning",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Vlad Rozov",
        "Created": "09/May/18 15:53",
        "Updated": "22/May/18 12:21",
        "Resolved": null,
        "Description": "There are changes between 1.8.0 and 1.10.0 that break API compatibility. A minor version change is supposed to be backward compatible with 1.9.0 and 1.8.0.",
        "Issue Links": [
            "/jira/browse/PARQUET-1304",
            "/jira/browse/PARQUET-1305"
        ]
    },
    "PARQUET-1296": {
        "Key": "PARQUET-1296",
        "Summary": "Travis kills build after 10 minutes, because \"no output was received\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.2",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "11/May/18 15:31",
        "Updated": "02/Aug/18 12:52",
        "Resolved": "28/May/18 09:42",
        "Description": "mvn install --batch-mode -DskipTests=true -Dmaven.javadoc.skip=true -Dsource.skip=true > mvn_install.log || (cat mvn_install.log && false) could take more than 10 minutes, and Travis kills the build in this case, since no output is produced (it is redirected to the log file)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/476"
        ]
    },
    "PARQUET-1297": {
        "Key": "PARQUET-1297",
        "Summary": "[Java] SchemaConverter should not convert from Timestamp(TimeUnit.SECOND) and Timestamp(TimeUnit.NANOSECOND) of Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "12/May/18 12:40",
        "Updated": "13/May/18 17:31",
        "Resolved": "13/May/18 17:31",
        "Description": "Arrow's 'Timestamp' definition is below:\n\n\r\n{\r\n  \"name\" : \"timestamp\",\r\n  \"unit\" : \"SECOND|MILLISECOND|MICROSECOND|NANOSECOND\"\r\n}\r\n\n\nhttp://arrow.apache.org/docs/metadata.html\nBut Parquet only supports 'TIMESTAMP_MILLIS' and 'TIMESTAMP_MICROS'.\nhttps://github.com/Apache/parquet-format/blob/master/LogicalTypes.md\nTherefore SchemaConverter should not convert from Timestamp(TimeUnit.SECOND) and Timestamp(TimeUnit.NANOSECOND) of Arrow to Parquet.\nRelated:\nhttps://issues.apache.org/jira/browse/PARQUET-1285",
        "Issue Links": [
            "/jira/browse/PARQUET-1285",
            "https://github.com/apache/parquet-mr/pull/477"
        ]
    },
    "PARQUET-1298": {
        "Key": "PARQUET-1298",
        "Summary": "Project download page does not conform to Apache requirements",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "14/May/18 10:47",
        "Updated": "14/May/18 10:47",
        "Resolved": null,
        "Description": "Our last two release announcements were rejected on the following grounds:\nSorry, but the download page does not contain the required links to KEYS, sigs and hashes.\nPlease see:\nhttp://www.apache.org/dev/release-distribution#download-links\nand the previous section\nThere is a project download page at:\nhttp://parquet.apache.org/downloads/\nbut it is rather out of date and does not have the required links either.\nNote that the ASF releases open source, so the download page must include that.",
        "Issue Links": []
    },
    "PARQUET-1299": {
        "Key": "PARQUET-1299",
        "Summary": "[C++] Upgrade Brotli to latest version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Phillip Cloud",
        "Reporter": "Phillip Cloud",
        "Created": "14/May/18 14:02",
        "Updated": "12/Nov/18 22:04",
        "Resolved": "12/Nov/18 22:04",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1300": {
        "Key": "PARQUET-1300",
        "Summary": "[C++] Parquet modular encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Gidon Gershinsky",
        "Created": "15/May/18 06:49",
        "Updated": "07/Sep/20 07:00",
        "Resolved": "06/Mar/20 06:40",
        "Description": "CPP version of a mechanism for modular encryption and decryption of Parquet files. Allows to keep the data fully encrypted in the storage, while enabling a client to extract a required subset (footer, column(s), pages) and to authenticate / decrypt the extracted data.",
        "Issue Links": [
            "/jira/browse/PARQUET-1906",
            "/jira/browse/PARQUET-1178",
            "https://docs.google.com/document/d/1T89G7xR0zHFV1f2pjTO28jtfVm8qoNVGEJQ70Rsk-bY/edit?usp=sharing",
            "https://github.com/apache/parquet-cpp/pull/475",
            "https://github.com/apache/arrow/pull/2555",
            "https://github.com/apache/arrow/pull/4826"
        ]
    },
    "PARQUET-1301": {
        "Key": "PARQUET-1300 [C++] Parquet modular encryption",
        "Summary": "[C++] Crypto package in parquet-cpp",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "15/May/18 06:52",
        "Updated": "04/Aug/18 22:44",
        "Resolved": "04/Aug/18 22:44",
        "Description": "The C++ implementation of basic AES-GCM encryption and decryption",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/464"
        ]
    },
    "PARQUET-1302": {
        "Key": "PARQUET-1302",
        "Summary": "ParquetProperties should not use static ValuesWriterFactory",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Vlad Rozov",
        "Created": "17/May/18 14:44",
        "Updated": "17/May/18 14:46",
        "Resolved": null,
        "Description": "ParquetProperties\u00a0by default is initialized with static ValuesWriterFactory, but during ParquetProperties construction ValuesWriterFactory is initialized with property. It means that if I construct two ParquetProperties with different properties (for example allocators), both will reference a static ValuesWriterFactory that would be initialized with the latest ParquetProperties. The same problem applies to DefaultValuesWriterFactory.",
        "Issue Links": [
            "/jira/browse/PARQUET-601"
        ]
    },
    "PARQUET-1303": {
        "Key": "PARQUET-1303",
        "Summary": "Avro reflect @Stringable field write error if field not instanceof CharSequence",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0,                                            1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro",
        "Assignee": "Zack Behringer",
        "Reporter": "Zack Behringer",
        "Created": "17/May/18 17:48",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "25/Jul/19 13:15",
        "Description": "Annotate a field in a pojo with org.apache.avro.reflect.Stringable and the schema will consider it to be a String field. AvroWriteSupport.fromAvroString assumes the field is either a Utf8 or CharSequence and does not attempt to use the field class' toString method if it is not.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/482"
        ]
    },
    "PARQUET-1304": {
        "Key": "PARQUET-1304",
        "Summary": "Release 1.10 contains breaking changes for Hive",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Zoltan Ivanfi",
        "Created": "18/May/18 13:13",
        "Updated": "31/May/18 14:39",
        "Resolved": "31/May/18 14:39",
        "Description": "Hive uses the initFromPage(int valueCount, ByteBuffer page, int offset) method that got removed in PARQUET-787. As a result, Hive does not compile with Paruqet 1.10.",
        "Issue Links": [
            "/jira/browse/PARQUET-1295",
            "https://github.com/apache/parquet-mr/pull/485"
        ]
    },
    "PARQUET-1305": {
        "Key": "PARQUET-1305",
        "Summary": "Backward incompatible change introduced in 1.8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "18/May/18 15:00",
        "Updated": "07/Nov/18 10:19",
        "Resolved": "07/Nov/18 10:19",
        "Description": "ThriftSchemaConverter#convert(StructType struct) introduced a backward incompatible logic in Parquet 1.8, If StructType's deprecated constructor is used to create StructTypes, and this convert method is used, then\u00a0the client receives the following exception:\n\n\r\norg.apache.parquet.ShouldNeverHappenException: Encountered UNKNOWN StructOrUnionType\r\n\r\n\tat org.apache.parquet.thrift.ThriftSchemaConvertVisitor.isUnion(ThriftSchemaConvertVisitor.java:342)\r\n\tat org.apache.parquet.thrift.ThriftSchemaConvertVisitor.visit(ThriftSchemaConvertVisitor.java:218)\r\n\tat org.apache.parquet.thrift.ThriftSchemaConvertVisitor.visit(ThriftSchemaConvertVisitor.java:74)\r\n\tat org.apache.parquet.thrift.struct.ThriftType$StructType.accept(ThriftType.java:269)\r\n\tat org.apache.parquet.thrift.ThriftSchemaConvertVisitor.convert(ThriftSchemaConvertVisitor.java:93)\r\n\tat org.apache.parquet.thrift.ThriftSchemaConverter.convert(ThriftSchemaConverter.java:75)\r\n\n\nTo illustrate the issue, here's a test case, which passes on older versions, but fails on master:\n\n\r\n  @Test\r\n  public void testIncompatibleThriftConverterChange() {\r\n    ThriftSchemaConverter converter = new ThriftSchemaConverter();\r\n\r\n    ThriftType.StructType structType = new ThriftType.StructType(\r\n      asList(\r\n        new ThriftField(\"a\", (short)1, REQUIRED, new ThriftType.StringType()),\r\n        new ThriftField(\"b\", (short) 2, REQUIRED, new ThriftType.StringType())\r\n      )\r\n    );\r\n    converter.convert(structType);\r\n  }",
        "Issue Links": [
            "/jira/browse/PARQUET-1295",
            "https://github.com/apache/parquet-mr/pull/483"
        ]
    },
    "PARQUET-1306": {
        "Key": "PARQUET-1306",
        "Summary": "[C++] Improve code reuse and reduce redundancy between Arrow and Parquet C++ build systems",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "21/May/18 23:04",
        "Updated": "12/Nov/18 22:03",
        "Resolved": "12/Nov/18 22:03",
        "Description": "I would like to see if it's possible to modularize the build system in Apache Arrow sufficiently to use it as a git submodule dependency of parquet-cpp. Most of the build system would therefore live in Apache Arrow (which is already a hard build dependency for Parquet).\nIn an ideal world, the Parquet codebase would live in a common monorepo with the Arrow codebase, but this might hopefully be the next best thing",
        "Issue Links": []
    },
    "PARQUET-1307": {
        "Key": "PARQUET-1307",
        "Summary": "[C++] memory-test fails with latest Arrow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "22/May/18 15:32",
        "Updated": "26/May/18 14:34",
        "Resolved": "26/May/18 14:34",
        "Description": "$ ./build-debug/debug/memory-test \r\nRunning main() from gtest_main.cc\r\n[==========] Running 7 tests from 3 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from ChunkedAllocatorTest\r\n[ RUN      ] ChunkedAllocatorTest.Basic\r\n[       OK ] ChunkedAllocatorTest.Basic (0 ms)\r\n[ RUN      ] ChunkedAllocatorTest.Keep\r\n[       OK ] ChunkedAllocatorTest.Keep (0 ms)\r\n[ RUN      ] ChunkedAllocatorTest.ReturnPartial\r\n[       OK ] ChunkedAllocatorTest.ReturnPartial (0 ms)\r\n[ RUN      ] ChunkedAllocatorTest.MemoryOverhead\r\n[       OK ] ChunkedAllocatorTest.MemoryOverhead (1 ms)\r\n[ RUN      ] ChunkedAllocatorTest.FragmentationOverhead\r\n[       OK ] ChunkedAllocatorTest.FragmentationOverhead (1 ms)\r\n[----------] 5 tests from ChunkedAllocatorTest (2 ms total)\r\n\r\n[----------] 1 test from TestBufferedInputStream\r\n[ RUN      ] TestBufferedInputStream.Basics\r\n[       OK ] TestBufferedInputStream.Basics (0 ms)\r\n[----------] 1 test from TestBufferedInputStream (0 ms total)\r\n\r\n[----------] 1 test from TestArrowInputFile\r\n[ RUN      ] TestArrowInputFile.Basics\r\n/home/antoine/parquet-cpp/src/parquet/util/memory-test.cc:328: Failure\r\n      Expected: 4\r\nTo be equal to: source->Tell()\r\n      Which is: 0\r\n[  FAILED  ] TestArrowInputFile.Basics (0 ms)\r\n[----------] 1 test from TestArrowInputFile (0 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 7 tests from 3 test cases ran. (2 ms total)\r\n[  PASSED  ] 6 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] TestArrowInputFile.Basics\r\n\r\n 1 FAILED TEST",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/466"
        ]
    },
    "PARQUET-1308": {
        "Key": "PARQUET-1308",
        "Summary": "[C++] parquet::arrow should use thread pool, not ParallelFor",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "22/May/18 16:35",
        "Updated": "17/Aug/18 06:26",
        "Resolved": "17/Aug/18 06:26",
        "Description": "Arrow now has a global thread pool, parquet::arrow should use that instead of ParallelFor.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/467"
        ]
    },
    "PARQUET-1309": {
        "Key": "PARQUET-1309",
        "Summary": "Parquet Java uses incorrect stats and dictionary filter properties",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Ryan Blue",
        "Created": "24/May/18 18:23",
        "Updated": "25/Jan/19 20:26",
        "Resolved": "05/Jun/18 09:16",
        "Description": "In SPARK-24251, we found that the changes to use HadoopReadOptions accidentally switched the properties that enable stats and dictionary filters. Both are enabled by default so it is unlikely that anyone will need to turn them off and there is an easy work-around, but we should fix the properties for 1.10.1. This doesn't affect the 1.8.x or 1.9.x releases (Spark 2.3.x is on 1.8.x).",
        "Issue Links": [
            "/jira/browse/PARQUET-1512",
            "/jira/browse/PARQUET-1318",
            "/jira/browse/SPARK-26677",
            "https://github.com/apache/parquet-mr/pull/490",
            "https://github.com/apache/parquet-mr/pull/490"
        ]
    },
    "PARQUET-1310": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column indexes: Filtering",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "28/May/18 09:31",
        "Updated": "17/Aug/18 15:07",
        "Resolved": "17/Aug/18 15:07",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1364",
            "https://github.com/apache/parquet-mr/pull/509",
            "https://github.com/apache/parquet-mr/pull/510"
        ]
    },
    "PARQUET-1311": {
        "Key": "PARQUET-1311",
        "Summary": "Update README.md",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "28/May/18 13:50",
        "Updated": "02/Aug/18 12:54",
        "Resolved": "12/Jun/18 12:11",
        "Description": "parquet-mr documentation is not up to date:\n\npoints to broken URLs\ntells to install old Thrift version (while it uses newer)\ncurrent version is 1.8.1, but 1.10.0 is already released",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/487"
        ]
    },
    "PARQUET-1312": {
        "Key": "PARQUET-1312",
        "Summary": "Improve logical types documentation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "28/May/18 14:33",
        "Updated": "25/Jun/18 06:28",
        "Resolved": "25/Jun/18 06:28",
        "Description": "Logical types documentation should be updated with the new type parameters introduced with the new logical types API (see details in PARQUET-1253 and PARQUET-906)",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/98"
        ]
    },
    "PARQUET-1313": {
        "Key": "PARQUET-1313",
        "Summary": "[C++] Compilation failure with VS2017",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "29/May/18 08:01",
        "Updated": "31/May/18 10:41",
        "Resolved": "31/May/18 10:41",
        "Description": "I get hit by the following issue:\nhttps://github.com/google/googletest/issues/1111\nNot sure why I don't get the same problem with Arrow C++.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/468"
        ]
    },
    "PARQUET-1314": {
        "Key": "PARQUET-1314",
        "Summary": "[C++] arrow-reader-writer-test::TestInt96ParquetIO fails on Windows (VS2017)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "29/May/18 08:19",
        "Updated": "29/May/18 08:19",
        "Resolved": null,
        "Description": "[----------] 1 test from TestInt96ParquetIO\r\n[ RUN      ] TestInt96ParquetIO.ReadIntoTimestamp\r\n..\\src\\parquet\\arrow\\arrow-reader-writer-test.cc(344): error: Failed\r\nGot:\r\n[128100145738543]\r\nExpected:\r\n[145738543]\r\n..\\src\\parquet\\arrow\\arrow-reader-writer-test.cc(893): error: Expected: this->Re\r\nadAndCheckSingleColumnFile(*values) doesn't generate new fatal failures in the c\r\nurrent thread.\r\n  Actual: it does.\r\n[  FAILED  ] TestInt96ParquetIO.ReadIntoTimestamp (0 ms)\r\n[----------] 1 test from TestInt96ParquetIO (15 ms total)",
        "Issue Links": []
    },
    "PARQUET-1315": {
        "Key": "PARQUET-1315",
        "Summary": "[C++] ColumnChunkMetaData.has_dictionary_page() should return bool, not int64_t",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Antoine Pitrou",
        "Created": "29/May/18 14:32",
        "Updated": "31/May/18 10:38",
        "Resolved": "31/May/18 10:38",
        "Description": "It's semantically a boolean.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/469"
        ]
    },
    "PARQUET-1316": {
        "Key": "PARQUET-1316",
        "Summary": "Exception :\"org.apache.parquet.schema.IncompatibleSchemaModificationException: cannot merge original type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ashish",
        "Created": "31/May/18 07:12",
        "Updated": "04/Jun/18 05:13",
        "Resolved": "04/Jun/18 05:13",
        "Description": "I am merging large number(~5000) of tiny parq files into single bigger parq file using below command.\n\u00a0\njava -jar <path to parquet-tools-1.9.1-SNAPSHOT.jar> merge <path to tiny parq files> <output file name>.\nI am getting exception as \"org.apache.parquet.schema.IncompatibleSchemaModificationException: cannot merge original type null into UTF8\"\n\u00a0\nwhile searching for this exception I found that it is being generated from class 'GroupType.java' which is located in \"parquet-mr-master/parquet-column/src/main/java/org/apache/parquet/schema\" directory.\nI wanted to modify the code in this file and wanted to rebuild parq_tools using mvn clean package -Plocal command.\nBut the issue is mvn clean package -Plocal command is always downloading latest version of \"parquet-column-1.9.1-SNAPSHOT.jar\" in dir \"/home/ubuntu/.m2/repository/org/apache/parquet/parquet-column/1.9.1-SNAPSHOT\".\nplease suggest where shall I copy the compiled version of\u00a0parquet-column-1.9.1-SNAPSHOT.jar , so that rebuliding of \"parquet-tools-1.9.1-SNAPSHOT.jar\" can take changes from my parquet-coumn-1.91.1-snapshot.jar file.",
        "Issue Links": []
    },
    "PARQUET-1317": {
        "Key": "PARQUET-1317",
        "Summary": "ParquetMetadataConverter throw NPE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "04/Jun/18 06:46",
        "Updated": "04/Jun/18 16:58",
        "Resolved": "04/Jun/18 16:58",
        "Description": "How to reproduce:\n\n\r\n$ bin/spark-shell \r\nscala> spark.range(10).selectExpr(\"cast(id as string) as id\").coalesce(1).write.parquet(\"/tmp/parquet-1317\")\r\nscala> \r\n\r\njava -jar ./parquet-tools/target/parquet-tools-1.10.1-SNAPSHOT.jar head --debug file:///tmp/parquet-1317/part-00000-6cfafbdd-fdeb-4861-8499-8583852ba437-c000.snappy.parquet\r\n\n\n\njava.io.IOException: Could not read footer: java.lang.NullPointerException\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:271)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(ParquetFileReader.java:202)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooters(ParquetFileReader.java:354)\r\n\r\nat org.apache.parquet.tools.command.RowCountCommand.execute(RowCountCommand.java:88)\r\n\r\nat org.apache.parquet.tools.Main.main(Main.java:223)\r\n\r\nCaused by: java.lang.NullPointerException\r\n\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.getOriginalType(ParquetMetadataConverter.java:828)\r\n\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.buildChildren(ParquetMetadataConverter.java:1173)\r\n\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetSchema(ParquetMetadataConverter.java:1124)\r\n\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:1058)\r\n\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:1052)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:532)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:505)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:499)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:261)\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:257)\r\n\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\r\nat java.lang.Thread.run(Thread.java:748)\r\n\r\njava.io.IOException: Could not read footer: java.lang.NullPointerException",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/489",
            "https://github.com/apache/parquet-mr/pull/491"
        ]
    },
    "PARQUET-1318": {
        "Key": "PARQUET-1318",
        "Summary": "Wrong configuration is passed from Hadoop config to Parquet options",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "04/Jun/18 14:11",
        "Updated": "05/Jun/18 09:15",
        "Resolved": "05/Jun/18 09:15",
        "Description": "Hadoop configuration for dictionary and stats filtering is swapped: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/HadoopReadOptions.java#L83",
        "Issue Links": [
            "/jira/browse/PARQUET-1309",
            "https://github.com/apache/parquet-mr/pull/490"
        ]
    },
    "PARQUET-1319": {
        "Key": "PARQUET-1319",
        "Summary": "[C++] Pass BISON_EXECUTABLE to Thrift EP for MacOS",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Tham",
        "Reporter": "Tham",
        "Created": "05/Jun/18 10:13",
        "Updated": "14/Jun/18 13:42",
        "Resolved": "14/Jun/18 13:41",
        "Description": "I was building parquet-cpp on my Mac mini (High Sierra 10.13.4). I've got\u00a0an error:\n\n\r\n/Users/sdkteam/jenkins/workspace/Dev-Mac/src/ThirdParty/parquet-cpp/build/thrift_ep-prefix/src/thrift_ep/compiler/cpp/src/thrift/thrifty.yy:1.1-5: invalid directive: `%code'\r\n/Users/sdkteam/jenkins/workspace/Dev-Mac/src/ThirdParty/parquet-cpp/build/thrift_ep-prefix/src/thrift_ep/compiler/cpp/src/thrift/thrifty.yy:1.7-14: syntax error, unexpected identifier\r\nmake[5]: *** [compiler/cpp/thrift/thrifty.cc] Error 1\r\nmake[4]: *** [compiler/cpp/CMakeFiles/parse.dir/all] Error 2\r\nmake[3]: *** [all] Error 2\r\n\n\nI googled and they said that I need to upgrade bison to 3.0. I did and did every way I can to config $PATH and successfully change: on terminal \"bison --version\"\u00a0printed 3.0.5\nSomehow cmake still recognize old bison in /usr/bin/bison.\nI tried to add BISON_EXECUTABLE=/usr/local/Cellar/bison/3.0.5/bin/bison into THRIFT_CMAKE_ARGS in /cmake_modules/ThirdpartyToolchain.cmake\u00a0 and it works.\nI think it should be helpful if I can pass BISON_EXECUTABLE from parquet-cpp to thrift ep. Can you please help that or suggest me another way?",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/470"
        ]
    },
    "PARQUET-1320": {
        "Key": "PARQUET-1320",
        "Summary": "Fast clean unused direct memory when decompress",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "zhoukang",
        "Created": "08/Jun/18 11:40",
        "Updated": "25/Feb/19 12:26",
        "Resolved": "25/Feb/19 12:26",
        "Description": "When use\u00a0NonBlockedDecompressorStream\u00a0which call:\nSnappyDecompressor.setInput\n\n\r\npublic synchronized void setInput(byte[] buffer, int off, int len) {\r\n SnappyUtil\r\n\r\npublic synchronized void setInput(byte[] buffer, int off, int len) {\r\n SnappyUtil.validateBuffer(buffer, off, len);\r\n\r\n if (inputBuffer.capacity() - inputBuffer.position() < len) {\r\n ByteBuffer newBuffer = ByteBuffer.allocateDirect(inputBuffer.position() + len);\r\n inputBuffer.rewind();\r\n newBuffer.put(inputBuffer);\r\n inputBuffer = newBuffer; \r\n } else {\r\n inputBuffer.limit(inputBuffer.position() + len);\r\n }\r\n inputBuffer.put(buffer, off, len);\r\n}\r\n\r\n.validateBuffer(buffer, off, len);\r\n\r\n if (inputBuffer.capacity() - inputBuffer.position() < len) {\r\n ByteBuffer newBuffer = ByteBuffer.allocateDirect(inputBuffer.position() + len);\r\n inputBuffer.rewind();\r\n newBuffer.put(inputBuffer);\r\n inputBuffer = newBuffer; \r\n } else {\r\n inputBuffer.limit(inputBuffer.position() + len);\r\n }\r\n inputBuffer.put(buffer, off, len);\r\n}\r\n\r\n\n\nIf we do not get any full gc for old gen.we may failed by off-heap memory leak",
        "Issue Links": [
            "/jira/browse/PARQUET-1485",
            "https://github.com/apache/parquet-mr/pull/492"
        ]
    },
    "PARQUET-1321": {
        "Key": "PARQUET-1321",
        "Summary": "LogicalTypeAnnotation.LogicalTypeAnnotationVisitor#visit methods should have a return value",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "08/Jun/18 14:41",
        "Updated": "02/Aug/18 16:44",
        "Resolved": "12/Jun/18 09:48",
        "Description": "LogicalTypeAnnotationVisitor inside\u00a0LogicalTypeAnnotation is intended to be used by clients who would like to execute custom code which depends on the type of the logical type annotation. However, it is difficult to give back any result from the visitor, since\u00a0both LogicalTypeAnnotation#accept,\u00a0and LogicalTypeAnnotationVisitor has void return type.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/493"
        ]
    },
    "PARQUET-1322": {
        "Key": "PARQUET-1322",
        "Summary": "Statistics is not available for DECIMAL types",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0,                                            1.10.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Vlad Rozov",
        "Reporter": "Vlad Rozov",
        "Created": "10/Jun/18 17:19",
        "Updated": "05/Nov/18 21:54",
        "Resolved": null,
        "Description": "According to parquet format specification columns annotated as DECIMAL should use SIGNED comparator and statistics should be available. The sort order returned by org.apache.parquet.format.converter.ParquetMetadataConverter for DECIMAL is SortOrder.UNKNOWN which contradicts the specification and makes statistics for DECIMAL types unavailable.",
        "Issue Links": [
            "/jira/browse/PARQUET-686",
            "https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal",
            "https://github.com/apache/parquet-mr/pull/494"
        ]
    },
    "PARQUET-1323": {
        "Key": "PARQUET-1323",
        "Summary": "[C++] Fix compiler warnings with clang-6.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "11/Jun/18 20:46",
        "Updated": "24/Jul/18 01:14",
        "Resolved": "24/Jul/18 01:14",
        "Description": "Observed when building today\n\n\r\n[1/23] Running thrift compiler on parquet.thrift\r\n[WARNING:/home/wesm/code/parquet-cpp/src/parquet/parquet.thrift:295] The \"byte\" type is a compatibility alias for \"i8\". Use \"i8\" to emphasize the signedness of this type.\r\n\r\n[15/23] Building CXX object CMakeFiles/parquet_objlib.dir/src/parquet/schema.cc.o\r\nIn file included from ../src/parquet/schema.cc:18:\r\n../src/parquet/schema.h:376:27: warning: private field 'schema_descr_' is not used [-Wunused-private-field]\r\n  const SchemaDescriptor* schema_descr_;\r\n                          ^\r\n1 warning generated.\r\n[20/23] Building CXX object CMakeFiles/parquet_objlib.dir/src/parquet/metadata.cc.o\r\n../src/parquet/metadata.cc:84:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::FIXED_LEN_BYTE_ARRAY:\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n../src/parquet/metadata.cc:82:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::BYTE_ARRAY:\r\n         ^~~~~~~~~~~~~~~~\r\n../src/parquet/metadata.cc:80:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::FLOAT:\r\n         ^~~~~~~~~~~\r\n../src/parquet/metadata.cc:78:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::DOUBLE:\r\n         ^~~~~~~~~~~~\r\n../src/parquet/metadata.cc:76:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::INT96:\r\n         ^~~~~~~~~~~\r\n../src/parquet/metadata.cc:74:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::INT64:\r\n         ^~~~~~~~~~~\r\n../src/parquet/metadata.cc:72:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::INT32:\r\n         ^~~~~~~~~~~\r\n../src/parquet/metadata.cc:70:10: warning: comparison of two values with different enumeration types in switch statement ('Type::type' and 'parquet::Type::type') [-Wenum-compare-switch]\r\n    case Type::BOOLEAN:\r\n         ^~~~~~~~~~~~~\r\n8 warnings generated.\r\n[23/23] Creating library symlink debug/libparquet.so.1 debug/libparquet.so",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/478"
        ]
    },
    "PARQUET-1324": {
        "Key": "ARROW-3772",
        "Summary": "[C++] Read Parquet dictionary encoded ColumnChunks directly into an Arrow DictionaryArray",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.15.0",
        "Component/s": "C++",
        "Assignee": "Wes McKinney",
        "Reporter": "Stav Nir",
        "Created": "13/Jun/18 14:08",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "26/Jul/19 23:54",
        "Description": "Dictionary data is very common in parquet, in the current implementation parquet-cpp decodes\u00a0dictionary encoded data always before creating a\u00a0plain arrow array. This process is wasteful since\u00a0we could use arrow's DictionaryArray directly and achieve several benefits:\n\nSmaller memory footprint - both in the decoding process and in the resulting arrow table - especially when the dict values are large\nBetter decoding performance - mostly as a result of the first bullet - less memory fetches and less allocations.\n\nI think those\u00a0benefits\u00a0could\u00a0achieve\u00a0significant improvements in runtime.\nMy\u00a0direction for the implementation is to read the indices (through the DictionaryDecoder, after the RLE decoding) and values separately into 2 arrays and create a DictionaryArray\u00a0using them.\nThere are some questions\u00a0to discuss:\n\nShould this be the default behavior for dictionary encoded data\nShould it be controlled with a parameter in the API\nWhat should be the policy in case some of the chunks are dictionary encoded and some are not.\n\nI started implementing this but would like to\u00a0hear your opinions.",
        "Issue Links": [
            "/jira/browse/ARROW-3325",
            "/jira/browse/ARROW-3652",
            "/jira/browse/ARROW-5993",
            "/jira/browse/ARROW-5984",
            "/jira/browse/ARROW-3325",
            "/jira/browse/ARROW-6049",
            "/jira/browse/ARROW-3246",
            "/jira/browse/ARROW-6140",
            "https://github.com/apache/arrow/pull/4949"
        ]
    },
    "PARQUET-1325": {
        "Key": "PARQUET-1325",
        "Summary": "High-level flexible and fine-grained column level access control through encryption with pluggable key access",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "13/Jun/18 21:52",
        "Updated": "06/Nov/18 17:34",
        "Resolved": "20/Aug/18 18:00",
        "Description": "This JIRA is an extension to Parquet Modular Encryption Jira(PARQUET-1178) that will provide the basic building blocks and APIs for the encryption support. On top of PARQUET-1178, this feature will create a high-level layer that enables fine-grained and flexible column level access control, with pluggable key access module, without a need to use the low level encryption APIs. Also this feature will enable seamless integration with existing clients.\nA detailed design doc will follow soon.",
        "Issue Links": [
            "/jira/browse/PARQUET-1396"
        ]
    },
    "PARQUET-1326": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] Cross compatibility support with parquet-mr",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "15/Jun/18 14:37",
        "Updated": "31/Jul/19 16:51",
        "Resolved": null,
        "Description": "we need to ensure cross compatibility between parquet-mr and parquet-cpp",
        "Issue Links": []
    },
    "PARQUET-1327": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] Bloom filter read/write implementation",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "15/Jun/18 14:44",
        "Updated": "25/Sep/22 14:24",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1328": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[java]Bloom filter read/write implementation",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "15/Jun/18 14:45",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "26/Feb/20 16:00",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/521",
            "https://github.com/apache/parquet-mr/pull/587"
        ]
    },
    "PARQUET-1329": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] Integrate Bloom filter into row group filter logic",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "15/Jun/18 14:46",
        "Updated": "19/Aug/18 07:30",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1330": {
        "Key": "PARQUET-1330",
        "Summary": "Avro RequestedProjection incompatible with Hive written data",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Zejun Li",
        "Created": "20/Jun/18 10:50",
        "Updated": "20/Jun/18 10:54",
        "Resolved": null,
        "Description": "I got a Parquet file written by Hive with this schema:\n\n\r\nfile schema: hive_schema\r\n--------------------------------------------------------------------------------\r\ntaxi_id: OPTIONAL BINARY O:UTF8 R:0 D:1\r\ndate: OPTIONAL BINARY O:UTF8 R:0 D:1\r\nstart_time: OPTIONAL INT64 R:0 D:1\r\nend_time: OPTIONAL INT64 R:0 D:1\r\nmin_lat_wgs: OPTIONAL DOUBLE R:0 D:1\r\nmin_lng_wgs: OPTIONAL DOUBLE R:0 D:1\r\nmax_lat_wgs: OPTIONAL DOUBLE R:0 D:1\r\nmax_lng_wgs: OPTIONAL DOUBLE R:0 D:1\r\nfirst_lat_wgs: OPTIONAL DOUBLE R:0 D:1\r\nfirst_lng_wgs: OPTIONAL DOUBLE R:0 D:1\r\nlast_lat_wgs: OPTIONAL DOUBLE R:0 D:1\r\nlast_lng_wgs: OPTIONAL DOUBLE R:0 D:1\r\ngps_log: OPTIONAL F:1\r\n.bag: REPEATED F:1\r\n..array_element: OPTIONAL F:6\r\n...timestamp: OPTIONAL INT64 R:1 D:4\r\n...lat_wgs: OPTIONAL DOUBLE R:1 D:4\r\n...lng_wgs: OPTIONAL DOUBLE R:1 D:4\r\n...item: OPTIONAL INT32 R:1 D:4\r\n...direction: OPTIONAL INT32 R:1 D:4\r\n...vflag: OPTIONAL INT32 R:1 D:4\r\n\n\nI want to use parquet-avro to read it, and use `AvroReadSupport.setRequestedProjection` to select a subset of field.\n\n\r\n{\r\n  \"type\": \"record\",\r\n  \"name\": \"test\",\r\n  \"fields\": [\r\n    {\r\n      \"name\": \"taxi_id\",\r\n      \"type\": [\"null\", \"string\"]\r\n    },\r\n    {\r\n      \"name\": \"gps_log\",\r\n      \"type\": [{\r\n        \"type\": \"array\",\r\n        \"items\": [\"null\", {\r\n          \"name\": \"point\",\r\n          \"type\": \"record\",\r\n          \"fields\": [\r\n            {\r\n              \"name\": \"lat_wgs\",\r\n              \"type\": [\"null\", \"double\"]\r\n            },\r\n            {\r\n              \"name\": \"lng_wgs\",\r\n              \"type\": [\"null\", \"double\"]\r\n            }\r\n          ]\r\n        }]\r\n      }],\r\n      \"default\": \"null\"\r\n    }\r\n  ]\r\n}\n\nI try to read data with code:\u00a0\n\n\r\nConfiguration conf = new Configuration(); AvroReadSupport.setRequestedProjection(conf, schema); conf.setBoolean(AvroWriteSupport.WRITE_OLD_LIST_STRUCTURE, false); conf.setBoolean(AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS, false); AvroParquetReader<GenericRecord> reader = new AvroParquetReader<>(conf, path);\r\n\n\nAnd I got errors:\n\n\r\nException in thread \"main\" parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: \r\nrequired group gps_log (LIST) {\r\n  repeated group list {\r\n    optional group element {\r\n      optional double lat_wgs;\r\n      optional double lng_wgs;\r\n    }\r\n  }\r\n} != optional group gps_log (LIST) {\r\n  repeated group bag {\r\n    optional group array_element {\r\n      optional int64 timestamp;\r\n      optional double lat_wgs;\r\n      optional double lng_wgs;\r\n      optional int32 item;\r\n      optional int32 direction;\r\n      optional int32 vflag;\r\n    }\r\n  }\r\n}\r\n\n\nThis error doesn't caused by the\u00a0nullability of `gps_log`. If I mark it nullable in Avro schema, I'll always get a null value.\n\u00a0\nI try to add some code in\u00a0`AvroSchemaConverter.convertField`:\n\n\r\ndiff --git a/AvroSchemaConverter.java b/AvroSchemaConverterNew.java\r\nindex 0b8076b..48b56dd 100644\r\n--- a/AvroSchemaConverter.java\r\n+++ b/AvroSchemaConverterNew.java\r\n@@ -50,12 +50,17 @@ public class AvroSchemaConverter {\r\n\u00a0\u00a0\u00a0\u00a0 \"parquet.avro.add-list-element-records\";\r\nprivate static final boolean ADD_LIST_ELEMENT_RECORDS_DEFAULT = true;\r\n+ public static final String READ_HIVE_WRITE_FILE =\r\n+\u00a0\u00a0\u00a0 \"parquet.avro.read-hive-write-file\";\r\n+ private static final boolean READ_HIVE_WRITE_FILE_DEFAULT = false;\r\n+\r\nprivate final boolean assumeRepeatedIsListElement;\r\nprivate final boolean writeOldListStructure;\r\npublic AvroSchemaConverter() {\r\n\u00a0\u00a0\u00a0\u00a0 this.assumeRepeatedIsListElement = ADD_LIST_ELEMENT_RECORDS_DEFAULT;\r\n\u00a0\u00a0\u00a0\u00a0 this.writeOldListStructure = WRITE_OLD_LIST_STRUCTURE_DEFAULT;\r\n+\u00a0\u00a0\u00a0this.isReadHiveWriteFileDefault = READ_HIVE_WRITE_FILE_DEFAULT;\r\n}\r\npublic AvroSchemaConverter(Configuration conf) {\r\n@@ -63,6 +68,9 @@ public class AvroSchemaConverter {\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ADD_LIST_ELEMENT_RECORDS, ADD_LIST_ELEMENT_RECORDS_DEFAULT);\r\n\u00a0\u00a0\u00a0\u00a0 this.writeOldListStructure = conf.getBoolean(\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 WRITE_OLD_LIST_STRUCTURE, WRITE_OLD_LIST_STRUCTURE_DEFAULT);\r\n+\u00a0\u00a0\u00a0this.isReadHiveWriteFileDefault = conf.getBoolean(\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0READ_HIVE_WRITE_FILE, READ_HIVE_WRITE_FILE_DEFAULT\r\n+\u00a0\u00a0\u00a0);\r\n}\r\n/**\r\n@@ -137,7 +145,14 @@ public class AvroSchemaConverter {\r\n\u00a0\u00a0\u00a0\u00a0 if (writeOldListStructure) {\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return ConversionPatterns.listType(repetition, fieldName,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 convertField(\"array\", schema.getElementType(), REPEATED));\r\n-\u00a0\u00a0\u00a0 } else {\r\n+\u00a0\u00a0\u00a0 } else if (isReadHiveWriteFileDefault) {\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type elementType = convertField(\"array_element\", schema.getElementType());\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return new GroupType(\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0repetition,\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fieldName,\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0LIST,\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0new GroupType(Type.Repetition.REPEATED, \"bag\", elementType));\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0} else {\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return ConversionPatterns.listOfElements(repetition, fieldName,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 convertField(AvroWriteSupport.LIST_ELEMENT_NAME, schema.getElementType()));\r\n\u00a0\u00a0\u00a0\u00a0 }\r\n\n\nIt can read data with this file.\n\u00a0\nSo is this a\u00a0compatibility problem in parquet-avro, or just I missed some configuration?",
        "Issue Links": []
    },
    "PARQUET-1331": {
        "Key": "PARQUET-1331",
        "Summary": "Use new logical types API in parquet-mr",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "20/Jun/18 12:02",
        "Updated": "15/Oct/18 10:29",
        "Resolved": "15/Oct/18 10:29",
        "Description": "PARQUET-1253 introduced the a new API for logical types making OriginalTypes deprecated. Parquet-mr makes several decision based on OriginalTypes, this logic should be replaced to new logical type API",
        "Issue Links": [
            "/jira/browse/PARQUET-1410"
        ]
    },
    "PARQUET-1332": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] Add bloom filter utility class",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "22/Jun/18 02:22",
        "Updated": "07/Sep/22 08:29",
        "Resolved": "15/Aug/18 12:57",
        "Description": "This is subtask of PARQUET-41, this JIRA is used to track adding bloom filter class to paruqet-mr and parquet-cpp.",
        "Issue Links": [
            "/jira/browse/PARQUET-41",
            "https://github.com/apache/parquet-cpp/pull/432"
        ]
    },
    "PARQUET-1333": {
        "Key": "PARQUET-1333",
        "Summary": "[C++] Reading of files with dictionary size 0 fails on Windows with bad_alloc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Philipp Hoch",
        "Created": "22/Jun/18 06:44",
        "Updated": "28/Jun/18 15:27",
        "Resolved": "28/Jun/18 15:27",
        "Description": "Account for total_size being 0, having no dictionary entries to allocate for.\nThe call with size 0 ends up in arrows memory_pool,\u00a0https://github.com/apache/arrow/blob/884474ca5ca1b8da55c0b23eb7cb784c2cd9bdb4/cpp/src/arrow/memory_pool.cc#L50, and the according allocation fails. See according documentation,\u00a0https://docs.microsoft.com/en-us/cpp/c-runtime-library/reference/aligned-malloc. Only happens on Windows environment, as posix_memalign seems to handle 0 inputs in unix environments.",
        "Issue Links": []
    },
    "PARQUET-1334": {
        "Key": "PARQUET-1334",
        "Summary": "[C++] memory_map parameter seems missleading in parquet file opener",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Philipp Hoch",
        "Created": "22/Jun/18 06:47",
        "Updated": "28/Jun/18 15:23",
        "Resolved": "28/Jun/18 15:23",
        "Description": "If memory_map parameter is true, normal file operation is executed, while in negative case, the according memory mapped file operation happens. Seems either be used via inverted logic or being bug.",
        "Issue Links": []
    },
    "PARQUET-1335": {
        "Key": "PARQUET-1335",
        "Summary": "Logical type names in parquet-mr are not consistent with parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "22/Jun/18 11:49",
        "Updated": "02/Aug/18 13:01",
        "Resolved": "25/Jun/18 06:25",
        "Description": "UTF8 logical type should be called STRING, INT should be called INTEGER.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/496",
            "https://github.com/apache/parquet-mr/pull/503"
        ]
    },
    "PARQUET-1336": {
        "Key": "PARQUET-1336",
        "Summary": "PrimitiveComparator should implements Serializable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "24/Jun/18 01:10",
        "Updated": "02/Aug/18 16:44",
        "Resolved": "26/Jun/18 07:40",
        "Description": "[info] Cause: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.parquet.schema.PrimitiveComparator$8\r\n[info] at org.apache.parquet.hadoop.ParquetInputFormat.setFilterPredicate(ParquetInputFormat.java:211)\r\n[info] at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:399)\r\n[info] at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:349)\r\n[info] at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:128)\r\n[info] at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\r\n[info] at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\r\n[info] at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n[info] at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n[info] at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n[info] at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n[info] at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n[info] at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1791)\r\n[info] at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\r\n[info] at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\r\n[info] at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)\r\n[info] at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2071)\r\n[info] at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n[info] at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n[info] at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:367)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/497"
        ]
    },
    "PARQUET-1337": {
        "Key": "PARQUET-1337",
        "Summary": "Current block alignment logic may lead to several row groups per block",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "25/Jun/18 06:47",
        "Updated": "26/Sep/19 11:28",
        "Resolved": null,
        "Description": "When the size of buffered data gets near the desired row group size, Parquet flushes the data to a row group. However, at this point the data for the last page is not yet encoded nor compressed, thereby the row group may end up being significantly smaller than it was intended.\nIf the row group ends up being so small that it is farther away from the next disk block boundary than the maximum padding, Parquet will try to create a new group in the same disk block, this time targeting the remaning space. This may also be flushed prematurely, leading to the creation of an even smaller row group, which may lead to an even smaller one... This gets repeated until we get sufficiently close to the block boundary so that padding can be finally applied. The resulting superflous row groups can lead to bad read performance.\nAn example of the structure of a Parquet file suffering from this problem can be seen below. For easier interpretation, the row groups are visually grouped by disk blocks:\n\nrow group 1:  RC:18774 TS:22182960 OFFSET:       4\r\nrow group 2:  RC: 2896 TS: 3428160 OFFSET: 6574564\r\nrow group 3:  RC: 1964 TS: 2322560 OFFSET: 7679844\r\nrow group 4:  RC: 1074 TS: 1268880 OFFSET: 8732964\r\n\n\n\nrow group 5:  RC:18808 TS:22228560 OFFSET:10000000\r\nrow group 6:  RC: 2872 TS: 3389520 OFFSET:16612640\r\nrow group 7:  RC: 1930 TS: 2284960 OFFSET:17716800\r\nrow group 8:  RC: 1040 TS: 1233520 OFFSET:18768240\r\n\n\n\nrow group 9:  RC:18852 TS:22275520 OFFSET:20000000\r\nrow group 10: RC: 2831 TS: 3345680 OFFSET:26656320\r\nrow group 11: RC: 1893 TS: 2244640 OFFSET:27757200\r\nrow group 12: RC: 1008 TS: 1195520 OFFSET:28806560\r\n\n\n\nrow group 13: RC:18841 TS:22263360 OFFSET:30000000\r\nrow group 14: RC: 2835 TS: 3350480 OFFSET:36652000\r\nrow group 15: RC: 1900 TS: 2249040 OFFSET:37753600\r\nrow group 16: RC: 1016 TS: 1198640 OFFSET:38803600\r\n\n\n\nrow group 17: RC: 1466 TS: 1740320 OFFSET:40000000\r\n\n\nIn this example, both the disk block size and the row group size was set to 10000000. The data would fit in 5 row groups of this size, but instead, each of the disk blocks (except the last) is split into 4 row groups of progressively decreasing size.",
        "Issue Links": [
            "https://docs.google.com/document/d/1FJAVwzszZGkxZa8FtKtSbgBKm7qkS4cXuNW8hl4YKwU/edit#",
            "https://github.com/apache/parquet-mr/pull/523"
        ]
    },
    "PARQUET-1338": {
        "Key": "PARQUET-1338",
        "Summary": "PrimitiveType.equals throw NPE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "25/Jun/18 11:57",
        "Updated": "12/Nov/18 14:40",
        "Resolved": "30/Jun/18 02:05",
        "Description": "Error message:\n\njava.lang.NullPointerException\r\n\tat org.apache.parquet.schema.PrimitiveType.equals(PrimitiveType.java:614)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/498"
        ]
    },
    "PARQUET-1339": {
        "Key": "PARQUET-1339",
        "Summary": "H2SeekableInputStream won't close correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "guozhen",
        "Created": "27/Jun/18 12:39",
        "Updated": "27/Jun/18 12:41",
        "Resolved": "27/Jun/18 12:41",
        "Description": "InputStream's close method is an\u00a0empty function, and\u00a0\u00a0H2SeekableInputStream didn't override close method.",
        "Issue Links": []
    },
    "PARQUET-1340": {
        "Key": "PARQUET-1340",
        "Summary": "[C++] Fix Travis Ci valgrind errors related to std::random_device",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "27/Jun/18 13:18",
        "Updated": "28/Jun/18 15:22",
        "Resolved": "28/Jun/18 15:22",
        "Description": "https://travis-ci.org/apache/parquet-cpp/jobs/395095122\n==12164== Conditional jump or move depends on uninitialised value(s)\n==12164== at 0x510FFD8: std::random_device::_M_init(std::string const&) (cow-string-inst.cc:56)\n==12164== by 0x73EE6E: std::random_device::random_device(std::string const&) (random.h:1590)\n==12164== by 0x727C0C: parquet::test::flip_coins(int, double) (test-common.h:104)\n==12164== by 0x729421: void parquet::test::InitValues<bool>(int, std::vector<bool, std::allocator<bool> >&, std::vector<unsigned char, std::allocator<unsigned char> >&) (test-specialization.h:40)\n==12164== by 0x72E09D: int parquet::test::MakePages<parquet::DataType<(parquet::Type::type)0> >(parquet::ColumnDescriptor const*, int, int, std::vector<short, std::allocator<short> >&, std::vector<short, std::allocator<short> >&, std::vector<parquet::DataType<(parquet::Type::type)0>::c_type, std::allocator<parquet::DataType<(parquet::Type::type)0>::c_type> >&, std::vector<unsigned char, std::allocator<unsigned char> >&, std::vector<std::shared_ptr<parquet::Page>, std::allocator<std::shared_ptr<parquet::Page> > >&, parquet::Encoding::type) (test-util.h:429)\n==12164== by 0x75761D: parquet::test::TestFlatScanner<parquet::DataType<(parquet::Type::type)0> >::Execute(int, int, int, parquet::ColumnDescriptor const*, parquet::Encoding::type) (column_scanner-test.cc:96)\n==12164== by 0x74F955: parquet::test::TestFlatScanner<parquet::DataType<(parquet::Type::type)0> >::ExecuteAll(int, int, int, int, parquet::Encoding::type) (column_scanner-test.cc:125)",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/473"
        ]
    },
    "PARQUET-1341": {
        "Key": "PARQUET-1341",
        "Summary": "Null count is suppressed when columns have no min or max and use unsigned sort order",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "28/Jun/18 23:19",
        "Updated": "06/Nov/18 17:33",
        "Resolved": "02/Aug/18 13:00",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/499"
        ]
    },
    "PARQUET-1342": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Add bloom filter utility class",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "29/Jun/18 08:32",
        "Updated": "27/Feb/20 09:24",
        "Resolved": "26/Feb/20 15:58",
        "Description": "This Jira is used to track bloom filter utility class in parquet-mr.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/521",
            "https://github.com/apache/parquet-mr/pull/425"
        ]
    },
    "PARQUET-1343": {
        "Key": "PARQUET-1343",
        "Summary": "Unable to read a parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alok",
        "Created": "29/Jun/18 13:10",
        "Updated": "29/Jun/18 13:12",
        "Resolved": null,
        "Description": "I am unable to read a parquet file that was made\u00a0after converting a csv to a parquet file using\u00a0pyarrow. Following is the error\u00a0\nArrowIOError Traceback (most recent call last) <timed exec> in <module>() ~/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py in read_table(source, columns, nthreads, metadata, use_pandas_metadata)  937 return fs.read_parquet(source, columns=columns, metadata=metadata)  938 --> 939 pf = ParquetFile(source, metadata=metadata)  940 return pf.read(columns=columns, nthreads=nthreads,  941 use_pandas_metadata=use_pandas_metadata) ~/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py in _init_(self, source, metadata, common_metadata)  62 self.reader = ParquetReader()  63 source = _ensure_file(source) ---> 64 self.reader.open(source, metadata=metadata)  65 self.common_metadata = common_metadata  66 self._nested_paths_by_prefix = self._build_nested_paths() _parquet.pyx in pyarrow._parquet.ParquetReader.open() error.pxi in pyarrow.lib.check_status() ArrowIOError: Invalid parquet file. Corrupt footer.\n\u00a0\nI was able to read and write parquet file earlier (about a few days ago) and how its stopped working",
        "Issue Links": []
    },
    "PARQUET-1344": {
        "Key": "PARQUET-1344",
        "Summary": "Type builders don't honor new logical types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "29/Jun/18 16:49",
        "Updated": "02/Aug/18 13:00",
        "Resolved": "04/Jul/18 11:59",
        "Description": "Type builder still use OriginalType constructors even for new logical types. Therefore one can't build a timestamp type with UTC adjustment = false with the type builder, because they default utc adjustment to false (via OriginalType conversion)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/500"
        ]
    },
    "PARQUET-1345": {
        "Key": "PARQUET-1345",
        "Summary": "[C++] It is possible to overflow a TMemoryBuffer when serializing the file metadata",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "09/Jul/18 20:47",
        "Updated": "05/Oct/20 16:07",
        "Resolved": null,
        "Description": "I'm not sure if this is fixable, but see issue reported to Arrow:\nhttps://github.com/apache/arrow/issues/2077",
        "Issue Links": []
    },
    "PARQUET-1346": {
        "Key": "PARQUET-1346",
        "Summary": "[C++] Protect against null values data in empty Arrow array",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "10/Jul/18 15:49",
        "Updated": "12/Jul/18 06:40",
        "Resolved": "12/Jul/18 06:40",
        "Description": "See ARROW-2744.",
        "Issue Links": [
            "/jira/browse/ARROW-2744",
            "https://github.com/apache/parquet-cpp/pull/474"
        ]
    },
    "PARQUET-1347": {
        "Key": "PARQUET-1347",
        "Summary": "[parquet-tools] dump command shows binary values differenty than the cat or head commands",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "12/Jul/18 12:58",
        "Updated": "12/Jul/18 13:25",
        "Resolved": null,
        "Description": "parquet-tools dump shows binary values as strings, if they are valid UTF-8 sequences.\nparquet-tools cat and parquet-tools head show binary values base64-encoded, regardless of whether they are valid UTF-8 sequences or not.\n(If the type is annotated as UTF-8, the values are shown as strings by all of these commands.)",
        "Issue Links": []
    },
    "PARQUET-1348": {
        "Key": "PARQUET-1348",
        "Summary": "[C++] Allow Arrow FileWriter To Write FileMetaData",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Robbie Gruener",
        "Reporter": "Robbie Gruener",
        "Created": "12/Jul/18 15:04",
        "Updated": "28/Jul/18 15:22",
        "Resolved": "28/Jul/18 15:21",
        "Description": "The arrow FileWriter open method only takes in a schema (which does not include row group information) and not the full FileMetaData. This does not allow the summary _metadata file to be created, and will need to be changed to\u00a0write\u00a0the full file metadata object.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/481"
        ]
    },
    "PARQUET-1349": {
        "Key": "PARQUET-1349",
        "Summary": "[C++]\u00a0PARQUET_RPATH_ORIGIN is not picked by the build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "14/Jul/18 17:28",
        "Updated": "14/Jul/18 19:57",
        "Resolved": "14/Jul/18 19:57",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/476"
        ]
    },
    "PARQUET-1350": {
        "Key": "PARQUET-1350",
        "Summary": "[C++] Use abstract ResizableBuffer instead of concrete PoolBuffer",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "17/Jul/18 15:42",
        "Updated": "23/Jul/18 18:29",
        "Resolved": "23/Jul/18 18:29",
        "Description": "PoolBuffer is an implementation detail in Arrow and we're aiming to make it private.",
        "Issue Links": [
            "/jira/browse/ARROW-2837",
            "https://github.com/apache/parquet-cpp/pull/477"
        ]
    },
    "PARQUET-1351": {
        "Key": "PARQUET-1351",
        "Summary": "Travis builds fail for parquet-format",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "18/Jul/18 15:27",
        "Updated": "13/Nov/19 12:25",
        "Resolved": "24/Jul/18 16:05",
        "Description": "parquet-format Travis builds are failing with (see for example this build):\n\n\r\n[ERROR] Plugin org.apache.maven.plugins:maven-remote-resources-plugin:1.5 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-remote-resources-plugin:jar:1.5: Could not transfer artifact org.apache.maven.plugins:maven-remote-resources-plugin:pom:1.5 from/to central (https://repo.maven.apache.org/maven2): Received fatal alert: protocol_version -> [Help 1]\r\n\n\nAfter some investigation, I suspect that the problem is that parquet-format is built against JDK 1.7.0_80 (see in the logs of the failing job above), the default version of TLS for this JDK seems to be TLSv1 (see here), and Maven central dropped support TLS 1.1 and TLS 1.0 (see here and here).",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/100"
        ]
    },
    "PARQUET-1352": {
        "Key": "PARQUET-1352",
        "Summary": "[CPP] Trying to write an arrow table with structs to a parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Dragan Markovic",
        "Created": "18/Jul/18 21:16",
        "Updated": "02/Jun/20 22:42",
        "Resolved": null,
        "Description": "Relevant issue:https://github.com/apache/arrow/issues/2287\n\u00a0\nI'm creating a struct with the following schema in arrow: https://pastebin.com/Cc8nreBP\n\u00a0\nWhen I try to convert that table to a .parquet file, the file gets created with a valid schema (the one I posted above) and then throws this exception:\u00a0\"lemented: Level generation for Struct not supported yet\".\n\u00a0\nHere's the code: https://ideone.com/DJkKUF\n\u00a0\nIs there any way to write arrow table of structs to a .parquet file in cpp?",
        "Issue Links": []
    },
    "PARQUET-1353": {
        "Key": "PARQUET-1353",
        "Summary": "The random data generator used for tests repeats the same value over and over again",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "23/Jul/18 15:44",
        "Updated": "02/Dec/19 11:32",
        "Resolved": null,
        "Description": "The RandomValues class returns references to its internal buffer as random values. This buffer gets a random value every time a new random value is requested, but since earlier values reference the same internal buffer, they get changed to the same value as well. So even if successive calls return different values each time, the actual list of these values will always consist of a single value repeated multiple times. For example:\n\n\n\nn-th call\nreturned value\naccumulated list expected\naccumulated list actual\n\n\n1\n6C\n6C\n6C\n\n\n2\n8F\n6C 8F\n8F 8F\n\n\n3\n52\n6C 8F 52\n52 52 52\n\n\n4\nB8\n6C 8F 52 B8\nB8 B8 B8 B8",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/504"
        ]
    },
    "PARQUET-1354": {
        "Key": "PARQUET-1354",
        "Summary": "[C++] Fix deprecated Arrow builder API usages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "24/Jul/18 02:05",
        "Updated": "09/Oct/18 16:02",
        "Resolved": "09/Oct/18 16:02",
        "Description": "I see warnings like the following:\n\n\r\n[64/65] Building CXX object src/parquet/arrow/CMakeF...reader-writer-test.dir/arrow-reader-writer-test.cc.o\r\nIn file included from ../src/parquet/arrow/test-util.h:23:0,\r\n                 from ../src/parquet/arrow/arrow-reader-writer-test.cc:37:\r\n../src/parquet/arrow/test-util.h: In function 'void parquet::arrow::ExpectArrayT(void*, arrow::Array*) [with ArrowType = arrow::BooleanType]':\r\n../src/parquet/arrow/test-util.h:467:82: warning: 'arrow::Status arrow::BooleanBuilder::Append(const uint8_t*, int64_t, const uint8_t*)' is deprecated (declared at /opt/conda/envs/pyarrow-dev/include/arrow/builder.h:711): Use AppendValues instead [-Wdeprecated-declarations]\r\n   EXPECT_OK(builder.Append(reinterpret_cast<uint8_t*>(expected), result->length()));\r\n                                                                                  ^\r\nIn file included from /opt/conda/envs/pyarrow-dev/include/arrow/compute/context.h:24:0,\r\n                 from /opt/conda/envs/pyarrow-dev/include/arrow/compute/api.h:21,\r\n                 from ../src/parquet/arrow/arrow-reader-writer-test.cc:26:\r\n../src/parquet/arrow/test-util.h: In instantiation of 'typename std::enable_if<std::is_same<ArrowType, parquet::arrow::DecimalWithPrecisionAndScale<precision> >::value, arrow::Status>::type parquet::arrow::NullableArray(size_t, size_t, uint32_t, std::shared_ptr<arrow::Array>*) [with ArrowType = parquet::arrow::DecimalWithPrecisionAndScale<38>; int precision = 38; typename std::enable_if<std::is_same<ArrowType, parquet::arrow::DecimalWithPrecisionAndScale<precision> >::value, arrow::Status>::type = arrow::Status; size_t = long unsigned int; uint32_t = unsigned int]':\r\n../src/parquet/arrow/arrow-reader-writer-test.cc:845:3:   required from 'void parquet::arrow::TestParquetIO_SingleColumnTableOptionalChunkedWrite_Test<gtest_TypeParam_>::TestBody() [with gtest_TypeParam_ = parquet::arrow::DecimalWithPrecisionAndScale<38>]'\r\n/opt/conda/envs/pyarrow-dev/include/arrow/builder.h:1042:20:   required from here\r\n../src/parquet/arrow/test-util.h:331:73: warning: 'arrow::Status arrow::FixedSizeBinaryBuilder::Append(const uint8_t*, int64_t, const uint8_t*)' is deprecated (declared at /opt/conda/envs/pyarrow-dev/include/arrow/builder.h:1017): Use AppendValues instead [-Wdeprecated-declarations]\r\n   RETURN_NOT_OK(builder.Append(out_buf->data(), size, valid_bytes.data()));\r\n                                                                         ^\r\n../src/parquet/arrow/test-util.h: In instantiation of 'typename std::enable_if<std::is_same<ArrowType, parquet::arrow::DecimalWithPrecisionAndScale<precision> >::value, arrow::Status>::type parquet::arrow::NonNullArray(size_t, std::shared_ptr<arrow::Array>*) [with ArrowType = parquet::arrow::DecimalWithPrecisionAndScale<38>; int precision = 38; typename std::enable_if<std::is_same<ArrowType, parquet::arrow::DecimalWithPrecisionAndScale<precision> >::value, arrow::Status>::type = arrow::Status; size_t = long unsigned int]':\r\n../src/parquet/arrow/arrow-reader-writer-test.cc:791:3:   required from 'void parquet::arrow::TestParquetIO_SingleColumnTableRequiredChunkedWriteArrowIO_Test<gtest_TypeParam_>::TestBody() [with gtest_TypeParam_ = parquet::arrow::DecimalWithPrecisionAndScale<38>]'\r\n/opt/conda/envs/pyarrow-dev/include/arrow/builder.h:1042:20:   required from here\r\n../src/parquet/arrow/test-util.h:170:53: warning: 'arrow::Status arrow::FixedSizeBinaryBuilder::Append(const uint8_t*, int64_t, const uint8_t*)' is deprecated (declared at /opt/conda/envs/pyarrow-dev/include/arrow/builder.h:1017): Use AppendValues instead [-Wdeprecated-declarations]\r\n   RETURN_NOT_OK(builder.Append(out_buf->data(), size));",
        "Issue Links": []
    },
    "PARQUET-1355": {
        "Key": "PARQUET-1355",
        "Summary": "Improvement Binary write performance",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "24/Jul/18 02:42",
        "Updated": "28/Oct/22 00:18",
        "Resolved": "28/Oct/22 00:18",
        "Description": "Benchmark code:\n\n\r\ntest(\"Parquet write benchmark\") {\r\n  val count = 100 * 1024 * 1024\r\n  val numIters = 5\r\n  withTempPath { path =>\r\n    val benchmark = new Benchmark(s\"Parquet write benchmark ${spark.sparkContext.version}\", 5)\r\n\r\n    Seq(\"long\", \"string\", \"decimal(18, 0)\", \"decimal(38, 18)\").foreach { dt =>\r\n      benchmark.addCase(s\"$dt type\", numIters = numIters) { iter =>\r\n        spark.range(count).selectExpr(s\"cast(id as $dt) as id\")\r\n          .write.mode(\"overwrite\").parquet(path.getAbsolutePath)\r\n      }\r\n    }\r\n    benchmark.run()\r\n  }\r\n}\r\n\n\nResult:\n\n-- Spark 2.3.3-SNAPSHOT with Parquet 1.8.3\r\n\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_151-b12 on Mac OS X 10.12.6\r\nIntel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz\r\n\r\nParquet write benchmark 2.3.3-SNAPSHOT:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------\r\nlong type                                   10963 / 11344          0.0  2192675973.8       1.0X\r\nstring type                                 28423 / 29437          0.0  5684553922.2       0.4X\r\ndecimal(18, 0) type                         11558 / 11696          0.0  2311587203.6       0.9X\r\ndecimal(38, 18) type                        43858 / 44432          0.0  8771537663.4       0.2X\r\n\r\n\r\n-- Spark 2.4.0-SNAPSHOT with Parquet 1.10.0\r\n\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_151-b12 on Mac OS X 10.12.6\r\nIntel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz\r\n\r\nParquet write benchmark 2.4.0-SNAPSHOT:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------\r\nlong type                                   11633 / 12070          0.0  2326572295.8       1.0X\r\nstring type                                 31374 / 32178          0.0  6274760187.4       0.4X\r\ndecimal(18, 0) type                         13019 / 13294          0.0  2603841925.4       0.9X\r\ndecimal(38, 18) type                        50719 / 50983          0.0 10143775007.6       0.2X\r\n\n\nThe mainly\u00a0affects the performance is toByteBuffer.\n If don't use the toByteBuffer when compare binary, the result is:\n\n-- Spark 2.4.0-SNAPSHOT with Parquet 1.10.0\r\n\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_151-b12 on Mac OS X 10.12.6\r\nIntel(R) Core(TM) i7-7820HQ CPU @ 2.90GHz\r\n\r\nParquet write benchmark 2.4.0-SNAPSHOT:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------\r\nlong type                                   11171 / 11508          0.0  2234189382.0       1.0X\r\nstring type                                 30072 / 30290          0.0  6014346455.4       0.4X\r\ndecimal(18, 0) type                         12150 / 12239          0.0  2430052708.8       0.9X\r\ndecimal(38, 18) type                        44974 / 45423          0.0  8994773738.8       0.2X",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/505"
        ]
    },
    "PARQUET-1356": {
        "Key": "PARQUET-1356",
        "Summary": "Error when closing writer - Statistics comparator mismatched",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Not A Bug",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Andres",
        "Created": "25/Jul/18 17:15",
        "Updated": "01/Aug/18 09:45",
        "Resolved": "01/Aug/18 09:45",
        "Description": "Hi all\nAfter getting some error in my custom implementation, I was trying to run a test case copied from\u00a0here and I surprisingly got the same.\n\n\r\nval schema = new Schema.Parser().parse(\"{\\n \\\"type\\\": \\\"record\\\",\\n \\\"name\\\": \\\"myrecord\\\",\\n \\\"fields\\\": [ {\\n \\\"name\\\": \\\"myarray\\\",\\n \\\"type\\\": {\\n \\\"type\\\": \\\"array\\\",\\n \\\"items\\\": \\\"int\\\"\\n }\\n } ]\\n}\")\r\nval tmp = File.createTempFile(getClass().getSimpleName(), \".tmp\");\r\ntmp.deleteOnExit();\r\ntmp.delete();\r\nval file = new Path(tmp.getPath());\r\nval testConf = new Configuration();\r\nval writer = AvroParquetWriter\r\n .builder[GenericRecord](file)\r\n .withSchema(schema)\r\n .withConf(testConf)\r\n .build();\r\n\r\n// Write a record with an empty array.\r\nval emptyArray = new util.ArrayList[Integer]();\r\nval record = new GenericRecordBuilder(schema)\r\n .set(\"myarray\", emptyArray).build();\r\nwriter.write(record);\r\nwriter.close();\r\n\r\nval reader = new AvroParquetReader[GenericRecord](testConf, file);\r\nval nextRecord = reader.read()\n\nThe project is scala + sbt with dependencies as follow\n\u00a0\n\n\r\nlazy val parquetVersion = \"1.10.0\"\r\nlazy val parquet = \"org.apache.parquet\" % \"parquet\" % Version.parquetVersion\r\nlazy val parquetAvro = \"org.apache.parquet\" % \"parquet-avro\" % Version.parquetVersion\r\n\n\n\u00a0\nAnd this is the stack trace:\n\u00a0\n\n\r\nStatistics comparator mismatched: SIGNED_INT32_COMPARATOR vs. SIGNED_INT32_COMPARATOR (39 milliseconds)\r\n[info] org.apache.parquet.column.statistics.StatisticsClassException: Statistics comparator mismatched: SIGNED_INT32_COMPARATOR vs. SIGNED_INT32_COMPARATOR\r\n[info] at org.apache.parquet.column.statistics.StatisticsClassException.create(StatisticsClassException.java:42)\r\n[info] at org.apache.parquet.column.statistics.Statistics.mergeStatistics(Statistics.java:327)\r\n[info] at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:119)\r\n[info] at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)\r\n[info] at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235)\r\n[info] at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122)\r\n[info] at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:169)\r\n[info] at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)\r\n[info] at org.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:301)\r\n[info] at uk.co.mypackage.di.nrt.HdfsPipelineSpec.$anonfun$new$4(HdfsPipelineSpec.scala:132)\r\n\u00a0\r\n\n\nAs you can see, this is confusing. The error is itself strange because the mismatch doesn't happen at all. Would really appreciate help with this issue.\nThanks",
        "Issue Links": []
    },
    "PARQUET-1357": {
        "Key": "PARQUET-1357",
        "Summary": "[C++]\u00a0FormatStatValue truncates binary statistics on zero character",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "26/Jul/18 08:02",
        "Updated": "01/Aug/18 13:35",
        "Resolved": "01/Aug/18 13:35",
        "Description": "As FormatStatValue is currently called with a C-style string, we cannot pass the actual binary content with its length. Instead change the interface to std::string.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/479"
        ]
    },
    "PARQUET-1358": {
        "Key": "PARQUET-1358",
        "Summary": "[C++] index_page_offset should be unset as it is not supported.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "26/Jul/18 11:47",
        "Updated": "26/Jul/18 21:12",
        "Resolved": "26/Jul/18 21:12",
        "Description": "We currently set to 0 while this is an optional attribute and should not be set at all.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/480"
        ]
    },
    "PARQUET-1359": {
        "Key": "PARQUET-1359",
        "Summary": "Out of Memory when reading large parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ryan Sachs",
        "Created": "26/Jul/18 15:10",
        "Updated": "27/Jul/18 06:42",
        "Resolved": null,
        "Description": "Hi,\nWe are successfully reading parquet files block by block, and are running into a JVM out of memory issue in a certain edge case. Consider the following scenario:\nParquet file has one column and one block and is 10 GB\nOur JVM is 5 GB\nIs there any way to read such a file? Below is our implementation/stack trace\n\n\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\nat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:778)\r\nat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)\r\n\r\ntry {\r\n  ParquetMetadata readFooter = ParquetFileReader.readFooter(hfsConfig, path,\r\n                               ParquetMetadataConverter.NO_FILTER);\r\n  MessageType schema = readFooter.getFileMetaData().getSchema();\r\n  long a = readFooter.getBlocks().stream().\r\n    reduce(0L, (left, right) -> left > \r\n      right.getTotalByteSize() ? left : right.getTotalByteSize(), \r\n    (leftl, rightl) -> leftl > rightl ? leftl : rightl);\r\n\r\n  for (BlockMetaData block : readFooter.getBlocks()) {\r\n    try {\r\n      fileReader = new ParquetFileReader(hfsConfig, \r\n                   readFooter.getFileMetaData(), path, Collections\r\n      .singletonList(block), schema.getColumns());\r\n      PageReadStore pages;\r\n\r\n    while (null != (pages = fileReader.readNextRowGroup())) {\r\n      //exception gets thrown here on blocks larger than jvm memory\r\n      final long rows = pages.getRowCount();\r\n      final MessageColumnIO columnIO = new \r\n                            ColumnIOFactory().getColumnIO(schema);\r\n      final RecordReader<Group> recordReader = \r\n            columnIO.getRecordReader(pages, new GroupRecordConverter(schema));\r\n\r\n      for (int i = 0; i < rows; i++) {\r\n        final Group group = recordReader.read();\r\n        int fieldCount = group.getType().getFieldCount();\r\n\r\n        for (int field = 0; field < fieldCount; field++) {\r\n          int valueCount = group.getFieldRepetitionCount(field);\r\n          Type fieldType = group.getType().getType(field);\r\n          String fieldName = fieldType.getName();\r\n\r\n          for (int index = 0; index < valueCount; index++) {\r\n            // Process data \r\n          }\r\n        }\r\n      }\r\n    }\r\n  } catch (IOException e) {\r\n    ...\r\n  } finally {\r\n    ...\r\n  }\r\n}",
        "Issue Links": []
    },
    "PARQUET-1360": {
        "Key": "PARQUET-1360",
        "Summary": "[C++] Minor API + style changes follow up to PARQUET-1348",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "28/Jul/18 16:35",
        "Updated": "29/Jul/18 23:57",
        "Resolved": "29/Jul/18 23:57",
        "Description": "see comments in https://github.com/apache/parquet-cpp/pull/481",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/482"
        ]
    },
    "PARQUET-1361": {
        "Key": "PARQUET-1361",
        "Summary": "[C++] 1.4.1 library allows creation of parquet file w/NULL values for INT types",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Ken Terada",
        "Created": "29/Jul/18 04:08",
        "Updated": "04/Jan/22 08:54",
        "Resolved": null,
        "Description": "The parquet-cpp v1.4.1 library allows generation of parquet files with NULL values for INT type columns which causes unexpected parsing errors in downstream systems ingesting those files.\ne.g.,\nError parsing the parquet file: UNKNOWN can not be applied to a primitive type\nReproduction Steps\nOS: CentOS 7.5.1804\nPython: 3.4.8\nPrerequisites:\n\nInstall the following packages: Numpy: 1.14.5, Pandas: 0.22.0, PyArrow: 0.9.0\n\nStep 1\nGenerate the parquet file.\nsample_w_null.csv\n\n\r\ncol1,col2,col3,col4,col5\r\n1,2,,4,5\r\n\n\nparquet-1361-repro-1.py\n\n\r\n#!/usr/bin/python\r\n\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pandas as pd\r\n\r\ninput_file = 'sample_w_null.csv'\r\noutput_file = 'int_unknown.parquet'\r\np_schema = {'col1': np.int32,\r\n        'col2': np.int32,\r\n        'col3': np.unicode_,\r\n        'col4': np.int32,\r\n        'col5': np.int32}\r\n\r\ndf = pd.read_csv(input_file, dtype=p_schema)\r\ntable = pa.Table.from_pandas(df)\r\npq.write_table(table, output_file)\r\n\n\nStep 2\nInspect the metadata of the generated file.\nparquet-1361-repro-2.py\n\n\r\n#!/usr/bin/python\r\n\r\nimport pyarrow.parquet as pq\r\n\r\nfor filename in ['int_unknown.parquet']:\r\n        pq_file = pq.ParquetFile(filename)\r\n        print(pq_file.metadata)\r\n        print(pq_file.schema)\r\n        print(pq_file.num_row_groups)\r\n        print(pq.read_table(filename, columns=['col1','col2','col3','col4','col5']).to_pandas())\r\n\n\nResults\n\n\r\n<pyarrow._parquet.FileMetaData object at 0x7f53e8621100>\r\n  created_by: parquet-cpp version 1.4.1-SNAPSHOT\r\n  num_columns: 6\r\n  num_rows: 1\r\n  num_row_groups: 1\r\n  format_version: 1.0\r\n  serialized_size: 1434\r\n<pyarrow._parquet.ParquetSchema object at 0x7f53e85bd170>\r\ncol1: INT32\r\ncol2: INT32\r\ncol3: INT32 UNKNOWN\r\ncol4: INT32\r\ncol5: INT32\r\n__index_level_0__: INT64\r\n\r\n1\r\n   col1  col2  col3  col4  col5\r\n0     1     2  None     4     5",
        "Issue Links": []
    },
    "PARQUET-1362": {
        "Key": "ARROW-3770",
        "Summary": "[C++] Validate or add option to validate arrow::Table schema in parquet::arrow::FileWriter::WriteTable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "C++",
        "Assignee": "Ben Kietzman",
        "Reporter": "Wes McKinney",
        "Created": "29/Jul/18 23:35",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "06/Mar/19 01:30",
        "Description": "Failing to validate will cause a segfault when the passed table does not match the schema used to instantiate the writer. See ARROW-2926",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3816"
        ]
    },
    "PARQUET-1363": {
        "Key": "PARQUET-1363",
        "Summary": "Add IP address logical type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tristan Stevens",
        "Created": "30/Jul/18 15:56",
        "Updated": "30/Jul/18 16:02",
        "Resolved": null,
        "Description": "IP addresses can be much more optimally represented as a 64 bit integer, meaning that it's much more efficient for storage and allowing consumers to do equality or subnet (range) comparisons using long-integer arithmetic.",
        "Issue Links": []
    },
    "PARQUET-1364": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Column Indexes: Invalid row indexes for pages starting with nulls",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "31/Jul/18 13:37",
        "Updated": "13/Nov/19 11:32",
        "Resolved": "03/Aug/18 05:39",
        "Description": "The current implementation for writing managing row indexes for the pages is not reliable. There is a logic MessageColumnIO which caches null values and flush them just before opening a new group. This logic might cause starting pages with these cached nulls which are not correctly counted in the written rows so the rowIndexes are incorrect. It does not cause any issues if all the pages are read continuously put it is a huge problem for column index based filtering.\nThe implementation described above is really complicated and would not like to redesign because of the mentioned issue. It is easier to simply count the 0 repetition levels as record boundaries at the column writer level.",
        "Issue Links": [
            "/jira/browse/PARQUET-1310",
            "https://github.com/apache/parquet-mr/pull/507"
        ]
    },
    "PARQUET-1365": {
        "Key": "PARQUET-1365",
        "Summary": "Don't write page level statistics",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "31/Jul/18 13:43",
        "Updated": "19/Nov/18 12:19",
        "Resolved": "19/Nov/18 12:19",
        "Description": "Page level statistics are never used in production and after adding column indexes they are completely useless. Fortunately, statistics are optional in both the v1 and v2 pages therefore, we can safely stop writing them.",
        "Issue Links": [
            "/jira/browse/PARQUET-1201",
            "https://github.com/apache/parquet-mr/pull/549"
        ]
    },
    "PARQUET-1366": {
        "Key": "PARQUET-1366",
        "Summary": "[C++] Streamline use of Arrow bit-util.h",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "01/Aug/18 14:07",
        "Updated": "01/Aug/18 19:14",
        "Resolved": "01/Aug/18 19:14",
        "Description": "Required for ARROW-2950: stop using certain bit-util APIs that will be removed.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/483"
        ]
    },
    "PARQUET-1367": {
        "Key": "PARQUET-1367",
        "Summary": "upgrade libraries to work around security issues",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Matt Darwin",
        "Created": "02/Aug/18 16:40",
        "Updated": "06/Aug/18 13:19",
        "Resolved": null,
        "Description": "There are a number of libraries which need updating.\u00a0 Among other reasons, there are several security issues\u00a0filed in CVE for Hadoop and guava",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/508"
        ]
    },
    "PARQUET-1368": {
        "Key": "PARQUET-1368",
        "Summary": "ParquetFileReader should close its input stream for the failure in constructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Hyukjin Kwon",
        "Reporter": "Hyukjin Kwon",
        "Created": "03/Aug/18 07:13",
        "Updated": "12/Dec/22 17:54",
        "Resolved": "02/Oct/18 16:07",
        "Description": "I was trying to replace deprecated usage readFooter to ParquetFileReader.open according to the node:\n\n\r\n\r\n[warn] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:368: method readFooter in object ParquetFileReader is deprecated: see corresponding Javadoc for more information.\r\n[warn]         ParquetFileReader.readFooter(sharedConf, filePath, SKIP_ROW_GROUPS).getFileMetaData\r\n[warn]                           ^\r\n\r\n[warn] /home/jenkins/workspace/spark-master-test-maven-hadoop-2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala:545: method readFooter in object ParquetFileReader is deprecated: see corresponding Javadoc for more information.\r\n[warn]             ParquetFileReader.readFooter(\r\n[warn]                               ^\r\n\n\nThen, I realised some test suites reports resource leak:\n\n\r\njava.lang.Throwable\r\n\tat org.apache.spark.DebugFilesystem$.addOpenStream(DebugFilesystem.scala:36)\r\n\tat org.apache.spark.DebugFilesystem.open(DebugFilesystem.scala:70)\r\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\r\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:687)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:595)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.createParquetReader(ParquetUtils.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.readFooter(ParquetUtils.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:544)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:539)\r\n\tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.internal(Tasks.scala:159)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:443)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:149)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n\tat scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\n\n\nThe root cause seems to be, the test case intentionally tries to read malformed Parquet file and see if the error can be handled correctly.\nIn that case, the error is thrown in it's constructor:\n\n\r\njava.lang.RuntimeException: file:/private/var/folders/71/484zt4z10ks1vydt03bhp6hr0000gp/T/spark-c102dafc-b3f7-4c7e-90ee-33d8ecbcd225/second/_SUCCESS is not a Parquet file (too small length: 0)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:514)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:689)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:595)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.createParquetReader(ParquetUtils.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.readFooter(ParquetUtils.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:544)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:539)\r\n\tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.internal(Tasks.scala:159)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:443)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:149)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\n\n\nSo, in this case, \n\n\r\n  public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\r\n    this.converter = new ParquetMetadataConverter(options);\r\n    this.file = file;\r\n    this.f = file.newStream();\r\n    this.options = options;\r\n    this.footer = readFooter(file, options, f, converter);\r\n    this.fileMetaData = footer.getFileMetaData();\r\n    this.blocks = filterRowGroups(footer.getBlocks());\r\n    for (ColumnDescriptor col : footer.getFileMetaData().getSchema().getColumns()) {\r\n      paths.put(ColumnPath.get(col.getPath()), col);\r\n    }\r\n  }\r\n\n\nthe open stream this.f = file.newStream() looks unable to be closed.\nTherefore, looks the test case reports the resource leak.\nIn case of the old deprecated readFooter it's done as below:\n\n\r\n  @Deprecated\r\n  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\r\n    ParquetReadOptions options;\r\n    if (file instanceof HadoopInputFile) {\r\n      options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration())\r\n          .withMetadataFilter(filter).build();\r\n    } else {\r\n      options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\r\n    }\r\n\r\n    try (SeekableInputStream in = file.newStream()) {\r\n      return readFooter(file, options, in);\r\n    }\r\n  }\r\n\n\nSo, looks we are fine with this deprecated method.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/510"
        ]
    },
    "PARQUET-1369": {
        "Key": "PARQUET-1369",
        "Summary": "[Python] Unavailable Parquet column statistics from Spark-generated file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.4.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Robbie Gruener",
        "Reporter": "Robbie Gruener",
        "Created": "06/Jul/18 16:50",
        "Updated": "30/Sep/18 21:53",
        "Resolved": "30/Sep/18 21:53",
        "Description": "I have a dataset generated by spark which shows it has statistics for the string column when using the java parquet-mr code (shown by using `parquet-tools meta`) however reading from pyarrow shows that the statistics for that column are not set.  I should not the column only has a single value, though it still seems like a problem that pyarrow can't recognize it (it can recognize statistics set for the long and double types).\nSee https://github.com/apache/arrow/files/2161147/metadata.zip for file example.\nPyarrow Code To Check Statistics:\n\n\r\nfrom pyarrow import parquet as pq\r\n\r\nmeta = pq.read_metadata('/tmp/metadata.parquet')\r\n# No Statistics For String Column, prints false and statistics object is None\r\nprint(meta.row_group(0).column(1).is_stats_set)\r\n\n\nExample parquet-meta output:\n\n\r\nfile schema: spark_schema \r\n--------------------------------------------------------------------------------\r\nint:         REQUIRED INT64 R:0 D:0\r\nstring:      OPTIONAL BINARY O:UTF8 R:0 D:1\r\nfloat:       REQUIRED DOUBLE R:0 D:0\r\n\r\nrow group 1: RC:8333 TS:76031 OFFSET:4 \r\n--------------------------------------------------------------------------------\r\nint:          INT64 SNAPPY DO:0 FPO:4 SZ:7793/8181/1.05 VC:8333 ENC:PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 100, num_nulls: 0]\r\nstring:       BINARY SNAPPY DO:0 FPO:7797 SZ:1146/1139/0.99 VC:8333 ENC:PLAIN_DICTIONARY,BIT_PACKED,RLE ST:[min: hello, max: hello, num_nulls: 4192]\r\nfloat:        DOUBLE SNAPPY DO:0 FPO:8943 SZ:66720/66711/1.00 VC:8333 ENC:PLAIN,BIT_PACKED ST:[min: 0.0057611096964338415, max: 99.99811053829232, num_nulls: 0]\r\n\n\nI realize the column only has a single value though it still seems like pyarrow should be able to read the statistics set. I made this here and not a JIRA since I wanted to be sure this is actually an issue and there wasnt a ticket already made there (I couldnt find one but I wanted to be sure). Either way I would like to understand why this is",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/491",
            "https://github.com/apache/arrow/pull/2655"
        ]
    },
    "PARQUET-1370": {
        "Key": "PARQUET-1370",
        "Summary": "[C++] Read consecutive column chunks in a single scan",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Robbie Gruener",
        "Created": "03/Aug/18 15:35",
        "Updated": "15/Aug/19 16:55",
        "Resolved": "15/Aug/19 16:54",
        "Description": "Currently parquet-cpp\u00a0calls\u00a0for a filesystem scan with every single data page see https://github.com/apache/parquet-cpp/blob/a0d1669cf67b055cd7b724dea04886a0ded53c8f/src/parquet/column_reader.cc#L181\nFor remote filesystems this can be very inefficient when reading many small columns. The java implementation\u00a0already does this and will read consecutive column chunks (and the resulting pages) in a single scan see https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L786\n\u00a0\nThis might be a bit difficult to do, as it would require changing a lot of the code structure but it\u00a0would certainly be valuable for workloads concerned with optimal read performance.",
        "Issue Links": []
    },
    "PARQUET-1371": {
        "Key": "PARQUET-1371",
        "Summary": "Time/Timestamp UTC normalization parameter doesn't work",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "06/Aug/18 16:36",
        "Updated": "06/Sep/18 07:55",
        "Resolved": "06/Sep/18 07:55",
        "Description": "After creating a Parquet file with non-UTC normalized logical type, when reading back with the API, the result show it is UTC normalized. Looks like the read path incorrectly reads\u00a0the actual logical type (with new API).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/511"
        ]
    },
    "PARQUET-1372": {
        "Key": "PARQUET-1372",
        "Summary": "[C++] Add an API to allow writing RowGroups based on their size rather than num_rows",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Anatoli Shein",
        "Created": "07/Aug/18 13:50",
        "Updated": "25/Aug/18 11:33",
        "Resolved": "25/Aug/18 11:33",
        "Description": "The current API allows writing RowGroups with specified numbers of rows, however does not allow writing RowGroups with specified size. In order to write RowGroups of specified size we need to write rows in chunks while checking the total_bytes_written after each chunk is written. This is currently impossible because the call to NextColumn() closes the current column writer.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/484"
        ]
    },
    "PARQUET-1373": {
        "Key": "PARQUET-1373",
        "Summary": "Encryption key management tools",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "08/Aug/18 07:35",
        "Updated": "24/Sep/20 06:14",
        "Resolved": "13/Jul/20 05:46",
        "Description": "Parquet Modular Encryption (PARQUET-1178)\u00a0provides\u00a0an API that accepts keys, arbitrary key metadata and key retrieval callbacks - which allows to implement basically any key management policy on top of it. This Jira will add tools that implement a set of best practice elements for key management. This is not an end-to-end key management, but rather a set of components that might simplify design and development of an end-to-end solution.\nThis tool set is one of many possible. There is no goal to create a single or \u201cstandard\u201d toolkit for Parquet encryption keys. Parquet has a Crypto Factory interface (PARQUET-1817) that allows to plug in different implementations of encryption key management.",
        "Issue Links": [
            "/jira/browse/PARQUET-1178",
            "/jira/browse/PARQUET-1568",
            "/jira/browse/PARQUET-1854",
            "https://docs.google.com/document/d/1bEu903840yb95k9q2X-BlsYKuXoygE4VnMDl9xz_zhk/edit?usp=sharing"
        ]
    },
    "PARQUET-1374": {
        "Key": "PARQUET-1374",
        "Summary": "[C++] Segfault on writing zero columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Wes McKinney",
        "Reporter": "Philip Felton",
        "Created": "09/Aug/18 09:36",
        "Updated": "14/Feb/19 17:03",
        "Resolved": "14/Feb/19 17:03",
        "Description": "Here's a gist which reproduces it: https://gist.github.com/philjdf/594ab431f135a040586aff08c7fb7666\n\nThe problem starts with the call to ParquetFileWriter::Close().\nAs a result of that call, FileMetaDataBuilder::FileMetaDataBuilderImpl::Finish() gets called, which relies on metadata_ being non-null. At the end of that call Finish, it std::moves metadata_ somewhere else, setting it to null. So obviously it assumes it only gets called once.\nLater on still inside Close(), FlatSchemaConverter::Convert() gets called, which throws an exception because we have no columns.\nIn handling this exception, we leave the try block, which destructs our ParquetFileWriter. This calls Close() again. This calls Finish() again, which now has a null metadata_ and segfaults.\n\nSo file_writer.cc FileSerializer::Close is presumably wrong, it should set is_open_ to false at the start rather than the end of the if block.\nIt's better to get an exception rather than a segfault, but ideally I'd like to write/read Parquet files with zero rows and/or zero columns. It means one less edge case for client code.",
        "Issue Links": [
            "/jira/browse/PARQUET-1508",
            "https://github.com/apache/arrow/pull/3164"
        ]
    },
    "PARQUET-1375": {
        "Key": "PARQUET-1375",
        "Summary": "Upgrade to supported version of Jackson",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Matt Darwin",
        "Created": "10/Aug/18 15:01",
        "Updated": "14/Apr/20 15:53",
        "Resolved": "10/Jun/19 18:51",
        "Description": "we are currently using a version of Jackson which is 5 years old. \u00a0We should upgrade to the latest version on the 2.x branch.",
        "Issue Links": [
            "/jira/browse/SPARK-30466",
            "https://github.com/apache/parquet-mr/pull/616"
        ]
    },
    "PARQUET-1376": {
        "Key": "PARQUET-1376",
        "Summary": "Data obfuscation layer for encryption",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "10/Aug/18 20:49",
        "Updated": "04/Aug/20 08:19",
        "Resolved": null,
        "Description": "Data obfuscation in sensitive columns - for users without access to column encryption keys.\n\nImplement on top of basic Parquet encryption\u00a0\nBuilt-in support for multiple masking mechanisms, with different trade-off between data utility, leakage, and size/throughput overhead\nProvide interface for plug-in custom masking mechanism\nEnable storing multiple masked versions of the same column in a file\nProvide readers with explicit list of column\u2019s masked versions in a file\nEnable readers to select a masked version of a column\nStretch: Implement tools for analysis of file data privacy properties and information leakage\nStretch: Leverage privacy analysis tools for tuning file data anonymity\nOptional: Support aggregated obfuscation",
        "Issue Links": [
            "/jira/browse/PARQUET-1178",
            "https://docs.google.com/document/d/1LMs74uhqvMNJacBySPnWq6tM8qIpgcIZz444c7vfibM/edit?usp=sharing"
        ]
    },
    "PARQUET-1377": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] replace shared_ptr to unique_ptr in Bloom filter buffer allocation",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "10/Aug/18 23:22",
        "Updated": "10/Aug/18 23:22",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1378": {
        "Key": "PARQUET-1378",
        "Summary": "[c++] Allow RowGroups with zero rows to be written",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "13/Aug/18 14:00",
        "Updated": "15/Aug/18 13:04",
        "Resolved": "15/Aug/18 13:04",
        "Description": "Currently, the reader-writer.cc example fails when zero rows are written.",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/485"
        ]
    },
    "PARQUET-1379": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Incorrect check for ASCENDING/DESCENDING at column index write path",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "15/Aug/18 09:04",
        "Updated": "18/Aug/18 09:39",
        "Resolved": "18/Aug/18 09:39",
        "Description": "On the branch column-index the check of ASCENDING/DESCENDING orders at column index write path does not check if min[i] <= max[i].",
        "Issue Links": []
    },
    "PARQUET-1380": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[C++] move Bloom filter test binary to parquet-testing repo",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Junjie Chen",
        "Created": "15/Aug/18 14:55",
        "Updated": "12/Nov/18 22:02",
        "Resolved": "12/Nov/18 22:02",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1381": {
        "Key": "PARQUET-1381",
        "Summary": "Add merge blocks command to parquet-tools",
        "Type": "New Feature",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ekaterina Galieva",
        "Reporter": "Ekaterina Galieva",
        "Created": "15/Aug/18 23:40",
        "Updated": "28/Jun/23 11:08",
        "Resolved": null,
        "Description": "Current implementation of merge command in parquet-tools doesn't merge row groups, just places one after the other. Add API and command option to be able to merge small blocks into larger ones up to specified size limit.\nImplementation details:\nBlocks are not reordered not to break possible initial predicate pushdown optimizations.\nBlocks are not divided to fit upper bound perfectly. \nThis is an intentional performance optimization. \nThis gives an opportunity to form new blocks by coping full content of smaller blocks by column, not by row.\nExamples:\n\nInput files with blocks sizes:\n\n\r\n[128 | 35], [128 | 40], [120]\n\nExpected output file blocks sizes:\n{{merge }}\n\n\r\n[128 | 35 | 128 | 40 | 120]\r\n\n\nmerge -b\n\n\r\n[128 | 35 | 128 | 40 | 120]\r\n\n\n{{merge -b -l 256 }}\n\n\r\n[163 | 168 | 120]\r\n\n\n\n\nInput files with blocks sizes:\n\n\r\n[128 | 35], [40], [120], [6]\u00a0\n\nExpected output file blocks sizes:\nmerge\n\n\r\n[128 | 35 | 40 | 120 | 6]\u00a0\r\n\n\nmerge -b\n\n\r\n[128 | 75 | 126]\u00a0\r\n\n\nmerge -b -l 256\n\n\r\n[203 | 126]",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/512",
            "https://github.com/apache/parquet-mr/pull/512",
            "https://github.com/apache/parquet-mr/pull/621",
            "https://github.com/apache/parquet-mr/pull/775"
        ]
    },
    "PARQUET-1382": {
        "Key": "PARQUET-1382",
        "Summary": "[C++] Prepare for arrow::test namespace removal",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "16/Aug/18 07:51",
        "Updated": "08/Nov/21 17:46",
        "Resolved": "17/Aug/18 14:10",
        "Description": "ARROW-3059 will remove the arrow::test namespace, make sure the parquet-cpp codebase doesn't break.",
        "Issue Links": [
            "/jira/browse/PARQUET-1385",
            "/jira/browse/ARROW-3059",
            "https://github.com/apache/parquet-cpp/pull/487"
        ]
    },
    "PARQUET-1383": {
        "Key": "PARQUET-1383",
        "Summary": "Parquet tools should indicate UTC parameter for time/timestamp types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "16/Aug/18 12:35",
        "Updated": "17/Oct/18 15:05",
        "Resolved": "17/Oct/18 15:05",
        "Description": "Parquet-tools should indicate if a time/timestamp is UTC adjusted or timezone agnostic, the values written by the tools should take UTC normalized parameters into account. Right now, every time and timestamp value is adjusted to UTC when printed via parquet-tools",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/513"
        ]
    },
    "PARQUET-1384": {
        "Key": "PARQUET-1384",
        "Summary": "[C++] Clang compiler warnings in bloom_filter-test.cc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Junjie Chen",
        "Reporter": "Wes McKinney",
        "Created": "16/Aug/18 23:03",
        "Updated": "17/Aug/18 16:56",
        "Resolved": "17/Aug/18 16:56",
        "Description": "[69/95] Building CXX object src/parquet/CMakeFiles/bloom_filter-test.dir/bloom_filter-test.cc.o\r\n../src/parquet/bloom_filter-test.cc:75:36: warning: moving a temporary object prevents copy elision [-Wpessimizing-move]\r\n  BlockSplitBloomFilter de_bloom = std::move(BlockSplitBloomFilter::Deserialize(&source));\r\n                                   ^\r\n../src/parquet/bloom_filter-test.cc:75:36: note: remove std::move call here\r\n  BlockSplitBloomFilter de_bloom = std::move(BlockSplitBloomFilter::Deserialize(&source));\r\n                                   ^~~~~~~~~~                                           ~\r\n../src/parquet/bloom_filter-test.cc:168:7: warning: moving a temporary object prevents copy elision [-Wpessimizing-move]\r\n      std::move(BlockSplitBloomFilter::Deserialize(&source));\r\n      ^\r\n../src/parquet/bloom_filter-test.cc:168:7: note: remove std::move call here\r\n      std::move(BlockSplitBloomFilter::Deserialize(&source));\r\n      ^~~~~~~~~~                                           ~\r\n../src/parquet/bloom_filter-test.cc:164:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n  handle->Read(size, &buffer);\r\n  ^~~~~~~~~~~~ ~~~~~~~~~~~~~\r\n../src/parquet/bloom_filter-test.cc:192:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n  handle->Seek(0);\r\n  ^~~~~~~~~~~~ ~\r\n../src/parquet/bloom_filter-test.cc:193:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n  handle->GetSize(&size);\r\n  ^~~~~~~~~~~~~~~ ~~~~~\r\n../src/parquet/bloom_filter-test.cc:195:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]\r\n  handle->Read(size, &buffer2);",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/488",
            "https://github.com/apache/parquet-cpp/pull/490"
        ]
    },
    "PARQUET-1385": {
        "Key": "PARQUET-1385",
        "Summary": "[C++] bloom_filter-test is very slow under valgrind",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "17/Aug/18 06:07",
        "Updated": "08/Nov/21 17:46",
        "Resolved": "08/Nov/21 17:46",
        "Description": "This test takes ~5 minutes to run under valgrind in Travis CI\n\n\r\n1: [==========] Running 6 tests from 6 test cases.\r\n1: [----------] Global test environment set-up.\r\n1: [----------] 1 test from Murmur3Test\r\n1: [ RUN      ] Murmur3Test.TestBloomFilter\r\n1: [       OK ] Murmur3Test.TestBloomFilter (19 ms)\r\n1: [----------] 1 test from Murmur3Test (34 ms total)\r\n1: \r\n1: [----------] 1 test from ConstructorTest\r\n1: [ RUN      ] ConstructorTest.TestBloomFilter\r\n1: [       OK ] ConstructorTest.TestBloomFilter (101 ms)\r\n1: [----------] 1 test from ConstructorTest (101 ms total)\r\n1: \r\n1: [----------] 1 test from BasicTest\r\n1: [ RUN      ] BasicTest.TestBloomFilter\r\n1: [       OK ] BasicTest.TestBloomFilter (49 ms)\r\n1: [----------] 1 test from BasicTest (49 ms total)\r\n1: \r\n1: [----------] 1 test from FPPTest\r\n1: [ RUN      ] FPPTest.TestBloomFilter\r\n1: [       OK ] FPPTest.TestBloomFilter (308731 ms)\r\n1: [----------] 1 test from FPPTest (308741 ms total)\r\n1: \r\n1: [----------] 1 test from CompatibilityTest\r\n1: [ RUN      ] CompatibilityTest.TestBloomFilter\r\n1: [       OK ] CompatibilityTest.TestBloomFilter (62 ms)\r\n1: [----------] 1 test from CompatibilityTest (62 ms total)\r\n1: \r\n1: [----------] 1 test from OptimalValueTest\r\n1: [ RUN      ] OptimalValueTest.TestBloomFilter\r\n1: [       OK ] OptimalValueTest.TestBloomFilter (27 ms)\r\n1: [----------] 1 test from OptimalValueTest (27 ms total)\r\n1: \r\n1: [----------] Global test environment tear-down\r\n1: [==========] 6 tests from 6 test cases ran. (309081 ms total)\r\n1: [  PASSED  ] 6 tests.\r\n\n\nEither we should change the FPPTest parameters to be faster, or we should not run that test when using valrind",
        "Issue Links": [
            "/jira/browse/PARQUET-1382",
            "https://github.com/apache/parquet-cpp/pull/489"
        ]
    },
    "PARQUET-1386": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Fix issues of NaN and +-0.0 in case of float/double column indexes",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "17/Aug/18 07:25",
        "Updated": "23/Aug/18 07:44",
        "Resolved": "23/Aug/18 07:44",
        "Description": "Workaround the float/double column indexes just like we did for statistics in PARQUET-1246.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/515"
        ]
    },
    "PARQUET-1387": {
        "Key": "PARQUET-1387",
        "Summary": "Nanosecond precision time and timestamp - parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "17/Aug/18 11:34",
        "Updated": "26/Sep/18 14:35",
        "Resolved": "28/Aug/18 13:15",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/102"
        ]
    },
    "PARQUET-1388": {
        "Key": "PARQUET-1388",
        "Summary": "Nanosecond precision time and timestamp - parquet-mr",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "17/Aug/18 11:37",
        "Updated": "21/Nov/18 12:46",
        "Resolved": "04/Oct/18 13:40",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1410",
            "https://github.com/apache/parquet-mr/pull/519"
        ]
    },
    "PARQUET-1389": {
        "Key": "PARQUET-1201 Column indexes",
        "Summary": "Improve value skipping at page synchronization",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "17/Aug/18 13:58",
        "Updated": "11/Sep/18 11:59",
        "Resolved": "11/Sep/18 11:59",
        "Description": "Currently, value skipping is done one-by-one for page synchronization. There are encodings (e.g. plain) where several values can be skipped at once.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/514"
        ]
    },
    "PARQUET-1390": {
        "Key": "PARQUET-1390",
        "Summary": "[Java] Upgrade to Arrow 0.10.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Andy Grove",
        "Reporter": "Andy Grove",
        "Created": "18/Aug/18 14:25",
        "Updated": "11/Jan/19 14:33",
        "Resolved": "19/Aug/18 09:13",
        "Description": "Parquet is using Arrow 0.8.0 but version 0.10.0 was recently released. There are numerous bug fixes and improvements, including building with JDK 8.",
        "Issue Links": [
            "/jira/browse/PARQUET-1278",
            "https://github.com/apache/parquet-mr/pull/516"
        ]
    },
    "PARQUET-1391": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "[java] Integrate Bloom filter logic",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "19/Aug/18 07:32",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "26/Feb/20 15:58",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/619"
        ]
    },
    "PARQUET-1392": {
        "Key": "PARQUET-1392",
        "Summary": "[C++] Supply row group indices to parquet::arrow::FileReader::ReadTable",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "19/Aug/18 13:03",
        "Updated": "23/Aug/18 13:18",
        "Resolved": "23/Aug/18 13:18",
        "Description": "By looking at the Parquet statistics, a user can already determine with its own logic which RowGroups are interesting for him. Currently we only provide functions to read the whole file or individual RowGroups. By supplying parquet::arrow with the RowGroups at once, it can better optimize its memory allocations as well as make better use of the underlying thread pool.",
        "Issue Links": [
            "/jira/browse/PARQUET-1158",
            "https://github.com/apache/parquet-cpp/pull/492"
        ]
    },
    "PARQUET-1393": {
        "Key": "ARROW-3774",
        "Summary": "[C++] Change parquet::arrow::FileReader::ReadRowGroups to read into contiguous arrays",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "C++",
        "Assignee": "Wes McKinney",
        "Reporter": "Uwe Korn",
        "Created": "19/Aug/18 14:27",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "13/Dec/18 03:18",
        "Description": "Instead of creating a chunk per RowGroup, we should read at least for primitive type into a single, pre-allocated Array. This needs some new functionality in the Record reader classes and thus should be done after https://github.com/apache/parquet-cpp/pull/462 is merged.",
        "Issue Links": [
            "/jira/browse/PARQUET-1158"
        ]
    },
    "PARQUET-1394": {
        "Key": "PARQUET-1394",
        "Summary": "[C++] Linking errors with boost_regex",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "20/Aug/18 09:15",
        "Updated": "12/Nov/18 18:00",
        "Resolved": "12/Nov/18 18:00",
        "Description": "I'm now getting linking errors with boost_regex:\n\n\r\n../../debug/libparquet.a(metadata.cc.o): In function `bool boost::regex_match<__gnu_cxx::__normal_iterator<char const*, std::string>, std::allocator<boost::sub_match<__gnu_cxx::__normal_iterator<char const*, std::string> > >, char, boost::regex_traits<char, boost::cpp_regex_traits<char> > >(__gnu_cxx::__normal_iterator<char const*, std::string>, __gnu_cxx::__normal_iterator<char const*, std::string>, boost::match_results<__gnu_cxx::__normal_iterator<char const*, std::string>, std::allocator<boost::sub_match<__gnu_cxx::__normal_iterator<char const*, std::string> > > >&, boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > > const&, boost::regex_constants::_match_flags)':\r\n/home/antoine/miniconda3/envs/pyarrow/include/boost/regex/v4/regex_match.hpp:50: undefined reference to `boost::re_detail_106700::perl_matcher<__gnu_cxx::__normal_iterator<char const*, std::string>, std::allocator<boost::sub_match<__gnu_cxx::__normal_iterator<char const*, std::string> > >, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::match()'\r\n../../debug/libparquet.a(metadata.cc.o): In function `perl_matcher':\r\n/home/antoine/miniconda3/envs/pyarrow/include/boost/regex/v4/perl_matcher.hpp:386: undefined reference to `boost::re_detail_106700::perl_matcher<__gnu_cxx::__normal_iterator<char const*, std::string>, std::allocator<boost::sub_match<__gnu_cxx::__normal_iterator<char const*, std::string> > >, boost::regex_traits<char, boost::cpp_regex_traits<char> > >::construct_init(boost::basic_regex<char, boost::regex_traits<char, boost::cpp_regex_traits<char> > > const&, boost::regex_constants::_match_flags)'\r\n\n\nCompiler is clang-6.0. There are no system boost headers. I'm using boost-cpp 1.67.0 from Anaconda, and -D_GLIBCXX_USE_CXX11_ABI=0 is in the compile flags.",
        "Issue Links": []
    },
    "PARQUET-1395": {
        "Key": "PARQUET-1395",
        "Summary": "[C++] Tests fail due to not finding libboost_system.so",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Antoine Pitrou",
        "Created": "20/Aug/18 09:57",
        "Updated": "12/Nov/18 22:00",
        "Resolved": "12/Nov/18 22:00",
        "Description": "When building:\n\n\r\n-- Boost version: 1.67.0\r\n-- Found the following Boost libraries:\r\n--   regex\r\n-- Boost include dir: /home/antoine/miniconda3/envs/pyarrow/include\r\n-- Boost libraries: /home/antoine/miniconda3/envs/pyarrow/lib/libboost_regex.so\r\n\n\nThen:\n\n\r\n$ ./build-debug/debug/memory-test \r\n./build-debug/debug/memory-test: error while loading shared libraries: \r\nlibboost_system.so.1.67.0: cannot open shared object file: No such file or directory\r\n\n\n\n\r\n$ ldd ./build-debug/debug/memory-test \r\n\tlinux-vdso.so.1 (0x00007fffcbfed000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f64e2f07000)\r\n\tlibarrow.so.11 => /home/antoine/miniconda3/envs/pyarrow/lib/libarrow.so.11 (0x00007f64e28ad000)\r\n\tlibboost_regex.so.1.67.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_regex.so.1.67.0 (0x00007f64e25a9000)\r\n\tlibstdc++.so.6 => /home/antoine/miniconda3/envs/pyarrow/lib/libstdc++.so.6 (0x00007f64e226a000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f64e1ecc000)\r\n\tlibgcc_s.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libgcc_s.so.1 (0x00007f64e1cb9000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f64e18c8000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f64e3415000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f64e16c4000)\r\n\tlibboost_system.so.1.67.0 => not found\r\n\tlibboost_filesystem.so.1.67.0 => not found\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f64e14bc000)\r\n\tlibicudata.so.58 => /home/antoine/miniconda3/envs/pyarrow/lib/./libicudata.so.58 (0x00007f64df9bc000)\r\n\tlibicui18n.so.58 => /home/antoine/miniconda3/envs/pyarrow/lib/./libicui18n.so.58 (0x00007f64df547000)\r\n\tlibicuuc.so.58 => /home/antoine/miniconda3/envs/pyarrow/lib/./libicuuc.so.58 (0x00007f64df199000)\r\n\n\nIt looks like our cmake build script doesn't link explicitly with the conda env's libboost_system.so.",
        "Issue Links": []
    },
    "PARQUET-1396": {
        "Key": "PARQUET-1396",
        "Summary": "EncryptionPropertiesFactory and DecryptionPropertiesFactory",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0,                                            1.10.1",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "20/Aug/18 17:15",
        "Updated": "04/Dec/21 04:04",
        "Resolved": "27/Jan/21 14:36",
        "Description": "This JIRA is an extension to Parquet Modular Encryption Jira(PARQUET-1178) that will provide the basic building blocks and APIs for the encryption support.\u00a0\nThis JIRA provides a\u00a0crypto data interface for\u00a0schema activation of Parquet encryption and serves as a high-level layer on top of\u00a0PARQUET-1178\u00a0to make the\u00a0adoption of Parquet-1178 easier, with pluggable key access module, without a need to use the low-level encryption APIs. Also, this feature will enable seamless integration with existing clients.\nNo change to specifications (Parquet-format), no new Parquet APIs, and no changes in existing Parquet APIs. All current applications, tests, etc, will work.\nFrom developer perspective, they can just implement the interface into a plugin which can be attached any Parquet application like Hive/Spark etc. This decouples the complexity of dealing with KMS and schema from Parquet applications. In large organization, they may have hundreds or even thousands of Parquet applications and pipelines. The decoupling would make Parquet encryption easier to be adopted.\u00a0\u00a0\nFrom end user(for example data owner) perspective, if they think a column is sensitive, they can just set that column\u2019s schema as sensitive and then the Parquet application just encrypt that column automatically. This makes end user easy to manage the encryptions of their columns.",
        "Issue Links": [
            "/jira/browse/PARQUET-1325",
            "/jira/browse/SPARK-25858",
            "/jira/browse/PARQUET-1397",
            "/jira/browse/PARQUET-1568",
            "https://github.com/apache/parquet-mr/pull/764",
            "https://docs.google.com/document/d/17GTQAezl1ZC1pMNHjYU_bPVxMU6DIPjtXOiLclXUlyA"
        ]
    },
    "PARQUET-1397": {
        "Key": "PARQUET-1397",
        "Summary": "Sample of usage Parquet-1396 and Parquet-1178 for column level encryption with pluggable key access",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "20/Aug/18 17:53",
        "Updated": "07/Nov/19 08:43",
        "Resolved": null,
        "Description": "This Jira provides a sample to use Parquet-1396 and Parquet-1178 column level encryption with pluggable key access. The Spark SQL application shows how to configure\u00a0Parquet-1396 to encrypt and columns. The project CryptoMetadataRetriever shows how to implement the interface defined in\u00a0Parquet-1396 as an example.\u00a0\nThe project to be uploaded soon.",
        "Issue Links": [
            "/jira/browse/PARQUET-1396",
            "/jira/browse/SPARK-25858",
            "/jira/browse/PARQUET-1178"
        ]
    },
    "PARQUET-1398": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Separate iv_prefix for GCM and CTR modes",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-encryption-feature-branch",
        "Component/s": "parquet-cpp,                                            parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "21/Aug/18 12:18",
        "Updated": "27/Sep/18 13:33",
        "Resolved": "18/Sep/18 04:04",
        "Description": "There is an\u00a0ambiguity in what the iv_prefix applies to - GCM or CTR or both.\u00a0This parameter will be\u00a0moved it to the Algorithms structures (from the FileCryptoMetaData structure).",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/103",
            "https://github.com/apache/parquet-cpp/pull/496",
            "https://github.com/apache/arrow/pull/2574",
            "https://github.com/apache/parquet-format/pull/103"
        ]
    },
    "PARQUET-1399": {
        "Key": "PARQUET-1399",
        "Summary": "Move parquet-mr related code from parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Aug/18 08:08",
        "Updated": "02/Oct/18 15:53",
        "Resolved": "24/Sep/18 12:16",
        "Description": "There are java classes in the parquet-format repo that shall be in the parquet-mr repo instead: java classes and test classes\nThe idea is to create a separate module in parquet-mr and depend on it instead of depending on parquet-format. Only this separate module would depend on parquet-format directly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1400",
            "https://github.com/apache/parquet-mr/pull/517"
        ]
    },
    "PARQUET-1400": {
        "Key": "PARQUET-1400",
        "Summary": "Deprecate parquet-mr related code in parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Aug/18 08:16",
        "Updated": "24/Sep/18 12:20",
        "Resolved": "24/Sep/18 12:20",
        "Description": "There are java classes in the\u00a0parquet-format\u00a0repo that shall be in the\u00a0parquet-mr\u00a0repo instead:\u00a0java classes\u00a0and\u00a0test classes\nThese classes shall be deprecated by mentioning they will be moved to the\u00a0parquet-mr\u00a0repo.",
        "Issue Links": [
            "/jira/browse/PARQUET-1399",
            "https://github.com/apache/parquet-format/pull/105"
        ]
    },
    "PARQUET-1401": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "RowGroup offset and total compressed size fields",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch",
        "Component/s": "parquet-cpp,                                            parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "22/Aug/18 14:50",
        "Updated": "07/Apr/20 05:27",
        "Resolved": "18/Sep/18 04:05",
        "Description": "Spark uses filterFileMetaData* methods in ParquetMetadataConverter class, that\u00a0 calculate the offset and total compressed size of a RowGroup data.\nThe offset calculation is done by extracting the ColumnMetaData of the first column, and using its offset fields.\nThe total compressed size calculation is done by running a loop over all column chunks in the RowGroup, and summing up the size values from each chunk's ColumnMetaData .\nIf one or more columns are hidden (encrypted with a key unavailable to the reader), these calculations can't be performed, because the column metadata is protected.\u00a0\n\u00a0\nBut: these calculations don't really need the individual column values. The results pertain to the whole RowGroup, not specific columns.\u00a0\nTherefore, we\u00a0will define two new optional fields in the RowGroup Thrift structure:\n\u00a0\noptional i64 file_offset\noptional i64 total_compressed_size\n\u00a0\nand calculate/set them upon file writing. Then, Spark will be able to query a file with hidden columns (of course, only if the query itself doesn't need the hidden columns - works with a masked version of them, or reads columns with available keys).\n\u00a0\nThese values can be set only for encrypted files (or for all files, to skip the loop upon reading). I've tested this, works fine in Spark writers and readers.\n\u00a0\nI've also checked other references to ColumnMetaData fields in parquet-mr. There are none - therefore, its the only change we need in parquet.thrift to handle hidden columns.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/104",
            "https://github.com/apache/parquet-cpp/pull/497",
            "https://github.com/apache/arrow/pull/2573",
            "https://github.com/apache/arrow/pull/6807",
            "https://github.com/apache/parquet-format/pull/104"
        ]
    },
    "PARQUET-1402": {
        "Key": "PARQUET-1402",
        "Summary": "[C++] incorrect calculation column start offset for files created by parquet-mr 1.8.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "22/Aug/18 23:44",
        "Updated": "21/May/19 16:54",
        "Resolved": "21/May/19 16:54",
        "Description": "parquet-mr (at least version 1.8.1-fast-201712141648170019-ab0622b)\nwrites to ColumnChunk's metadata dictionary_page_offset == 0 when it is (supposed?) equal to data_page_offset.\ncalculation of col_start in\u00a0std::unique_ptr<PageReader> GetColumnPageReader(int i)\nworks incorrectly in this case.",
        "Issue Links": [
            "/jira/browse/ARROW-5322",
            "https://github.com/apache/parquet-cpp/pull/494",
            "https://github.com/apache/arrow/pull/2667",
            "https://github.com/apache/arrow/pull/4359"
        ]
    },
    "PARQUET-1403": {
        "Key": "ARROW-7242",
        "Summary": "[C++] Coerce Arrow half-precision float to float32",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "C++,                                            Python",
        "Assignee": null,
        "Reporter": "Naor Volkovich",
        "Created": "23/Aug/18 11:36",
        "Updated": "11/Jan/23 07:52",
        "Resolved": null,
        "Description": "When trying to save a Pandas DF using \"to_parquet\" when that DF has a column with a dtype of float16, I get the error: \"pyarrow.lib.ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: halffloat\"",
        "Issue Links": []
    },
    "PARQUET-1404": {
        "Key": "PARQUET-1404",
        "Summary": "[C++] Add index pages to the format to support efficient page skipping to parquet-cpp",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Gang Wu",
        "Reporter": "Renato Javier Marroqu\u00edn Mogrovejo",
        "Created": "27/Aug/18 08:42",
        "Updated": "01/Dec/22 16:27",
        "Resolved": null,
        "Description": "The scope of the Jira is to take advantage of indexes added as part of PARQUET-922\nIt is easier to implement this if we create sub-tasks",
        "Issue Links": [
            "/jira/browse/ARROW-10158",
            "https://github.com/apache/arrow/pull/6807"
        ]
    },
    "PARQUET-1405": {
        "Key": "PARQUET-1405",
        "Summary": "[C++] 'Couldn't deserialize thrift' error when reading large binary column",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Jeremy Heffner",
        "Created": "28/Aug/18 20:49",
        "Updated": "23/Aug/19 16:01",
        "Resolved": "03/May/19 02:20",
        "Description": "We've run into issues reading Parquet files that contain long binary columns (utf8 strings).\u00a0 In particular, we were generating WKT representations of polygons that contained ~34 million characters when we ran into the issue.\u00a0\nThe attached example generates a dataframe with one record and one column containing a random string with 10^7 characters.\nPandas (using the default pyarrow engine) successfully writes the file, but fails upon reading the file:\n\n\r\n---------------------------------------------------------------------------\r\nArrowIOError Traceback (most recent call last)\r\n<ipython-input-25-25d21204cbad> in <module>()\r\n----> 1 df_read_in = pd.read_parquet('test.parquet')\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, **kwargs)\r\n286 \r\n287 impl = get_engine(engine)\r\n--> 288 return impl.read(path, columns=columns, **kwargs)\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pandas/io/parquet.py in read(self, path, columns, **kwargs)\r\n129 kwargs['use_pandas_metadata'] = True\r\n130 result = self.api.parquet.read_table(path, columns=columns,\r\n--> 131 **kwargs).to_pandas()\r\n132 if should_close:\r\n133 try:\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/parquet.py in read_table(source, columns, nthreads, metadata, use_pandas_metadata)\r\n1044 fs = _get_fs_from_path(source)\r\n1045 return fs.read_parquet(source, columns=columns, metadata=metadata,\r\n-> 1046 use_pandas_metadata=use_pandas_metadata)\r\n1047 \r\n1048 pf = ParquetFile(source, metadata=metadata)\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/filesystem.py in read_parquet(self, path, columns, metadata, schema, nthreads, use_pandas_metadata)\r\n175 filesystem=self)\r\n176 return dataset.read(columns=columns, nthreads=nthreads,\r\n--> 177 use_pandas_metadata=use_pandas_metadata)\r\n178 \r\n179 def open(self, path, mode='rb'):\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/parquet.py in read(self, columns, nthreads, use_pandas_metadata)\r\n896 partitions=self.partitions,\r\n897 open_file_func=open_file,\r\n--> 898 use_pandas_metadata=use_pandas_metadata)\r\n899 tables.append(table)\r\n900 \r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/parquet.py in read(self, columns, nthreads, partitions, open_file_func, file, use_pandas_metadata)\r\n459 table = reader.read_row_group(self.row_group, **options)\r\n460 else:\r\n--> 461 table = reader.read(**options)\r\n462 \r\n463 if len(self.partition_keys) > 0:\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/parquet.py in read(self, columns, nthreads, use_pandas_metadata)\r\n150 columns, use_pandas_metadata=use_pandas_metadata)\r\n151 return self.reader.read_all(column_indices=column_indices,\r\n--> 152 nthreads=nthreads)\r\n153 \r\n154 def scan_contents(self, columns=None, batch_size=65536):\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.ParquetReader.read_all()\r\n\r\n~/anaconda3/envs/uda/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowIOError: Couldn't deserialize thrift: No more data to read.\r\nDeserializing page header failed.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4230",
            "https://github.com/apache/arrow/pull/4242"
        ]
    },
    "PARQUET-1406": {
        "Key": "PARQUET-1406",
        "Summary": "unit test fails on some cases",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.5.0",
        "Fix Version/s": "1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Peter Pan",
        "Created": "29/Aug/18 18:14",
        "Updated": "11/Nov/18 22:15",
        "Resolved": "11/Nov/18 22:15",
        "Description": "Hi, i'm trying to integrate parquet-cpp 1.5.0rc0 with arrow 0.10.0 and thrift 0.11.0 but experiencing unit test failure as follows:\n\n\r\nTest project /mnt/tmp/parquet-cpp-apache-parquet-cpp-1.5.0-rc0/build\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 1: bloom_filter-test\r\n\r\n1/18 Test\u00a0 #1: bloom_filter-test ................***Failed \u00a0 18.69 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 2: column_reader-test\r\n\r\n2/18 Test\u00a0 #2: column_reader-test ............... \u00a0 Passed\u00a0 \u00a0 0.07 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 3: column_scanner-test\r\n\r\n3/18 Test\u00a0 #3: column_scanner-test .............. \u00a0 Passed\u00a0 \u00a0 0.08 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 4: column_writer-test\r\n\r\n4/18 Test\u00a0 #4: column_writer-test ...............***Failed\u00a0 \u00a0 4.14 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 5: file-deserialize-test\r\n\r\n5/18 Test\u00a0 #5: file-deserialize-test ............***Failed\u00a0 \u00a0 0.04 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 6: file-serialize-test\r\n\r\n6/18 Test\u00a0 #6: file-serialize-test ..............***Failed\u00a0 \u00a0 0.11 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 7: properties-test\r\n\r\n7/18 Test\u00a0 #7: properties-test .................. \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 8: statistics-test\r\n\r\n8/18 Test\u00a0 #8: statistics-test .................. \u00a0 Passed\u00a0 \u00a0 0.10 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start\u00a0 9: encoding-test\r\n\r\n9/18 Test\u00a0 #9: encoding-test .................... \u00a0 Passed\u00a0 \u00a0 0.07 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 10: metadata-test\r\n\r\n10/18 Test #10: metadata-test .................... \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 11: public-api-test\r\n\r\n11/18 Test #11: public-api-test .................. \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 12: types-test\r\n\r\n12/18 Test #12: types-test ....................... \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 13: reader-test\r\n\r\n13/18 Test #13: reader-test ......................***Failed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 14: schema-test\r\n\r\n14/18 Test #14: schema-test ...................... \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 15: arrow-schema-test\r\n\r\n15/18 Test #15: arrow-schema-test ................ \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 16: arrow-reader-writer-test\r\n\r\n16/18 Test #16: arrow-reader-writer-test .........***Failed\u00a0 \u00a0 7.63 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 17: comparison-test\r\n\r\n17/18 Test #17: comparison-test .................. \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\u00a0 \u00a0 \u00a0 Start 18: memory-test\r\n\r\n18/18 Test #18: memory-test ...................... \u00a0 Passed\u00a0 \u00a0 0.03 sec\r\n\r\n\r\n\r\n67% tests passed, 6 tests failed out of 18\r\n\r\n\r\n\r\n\n\nAny idea to fix these failures?\nBTW, i needed to manually\u00a0replace references to `AllocateEmptyBitmap` with `GetEmptyBitmap` to make this rc build successfully before i could do this unit test. Can it be fixed as well?",
        "Issue Links": []
    },
    "PARQUET-1407": {
        "Key": "PARQUET-1407",
        "Summary": "Data loss on duplicate values with AvroParquetWriter/Reader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0,                                            1.10.0,                                            1.8.3",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Scott Carey",
        "Created": "29/Aug/18 23:32",
        "Updated": "19/Nov/18 22:16",
        "Resolved": "19/Nov/18 22:16",
        "Description": "public class Blah {\r\n\r\n  private static Path parquetFile = new Path(\"oops\");\r\n  private static Schema schema = SchemaBuilder.record(\"spark_schema\")\r\n      .fields().optionalBytes(\"value\").endRecord();\r\n\r\n  private static GenericData.Record recordFor(String value) {\r\n    return new GenericRecordBuilder(schema)\r\n        .set(\"value\", value.getBytes()).build();\r\n  }\r\n\r\n  public static void main(String ... args) throws IOException {\r\n    try (ParquetWriter<GenericData.Record> writer = AvroParquetWriter\r\n          .<GenericData.Record>builder(parquetFile)\r\n          .withSchema(schema)\r\n          .build()) {\r\n      writer.write(recordFor(\"one\"));\r\n      writer.write(recordFor(\"two\"));\r\n      writer.write(recordFor(\"three\"));\r\n      writer.write(recordFor(\"three\"));\r\n      writer.write(recordFor(\"two\"));\r\n      writer.write(recordFor(\"one\"));\r\n      writer.write(recordFor(\"zero\"));\r\n    }\r\n\r\n    try (ParquetReader<GenericRecord> reader = AvroParquetReader\r\n        .<GenericRecord>builder(parquetFile)\r\n        .withConf(new Configuration()).build()) {\r\n      GenericRecord rec;\r\n      int i = 0;\r\n      while ((rec = reader.read()) != null) {\r\n        ByteBuffer buf = (ByteBuffer) rec.get(\"value\");\r\n        byte[] bytes = new byte[buf.remaining()];\r\n        buf.get(bytes);\r\n        System.out.println(\"rec \" + i++ + \": \" + new String(bytes));\r\n      }\r\n    }\r\n  }\r\n}\r\n\n\nExpected output:\n\nrec 0: one\r\nrec 1: two\r\nrec 2: three\r\nrec 3: three\r\nrec 4: two\r\nrec 5: one\r\nrec 6: zero\n\nActual:\n\nrec 0: one\r\nrec 1: two\r\nrec 2: three\r\nrec 3: \r\nrec 4: \r\nrec 5: \r\nrec 6: zero\n\n\u00a0\nThis was found when we started getting empty byte[] values back in spark unexpectedly.\u00a0 (Spark 2.3.1 and Parquet 1.8.3).\u00a0 \u00a0I have not tried to reproduce with parquet 1.9.0, but its a bad enough bug that I would like a 1.8.4 release that I can drop-in replace 1.8.3 without any binary compatibility issues.\n\u00a0Duplicate byte[] values are lost.\n\u00a0\nA few clues:\u00a0\nIf I do not call ByteBuffer.get, the size of ByteBuffer.remaining does not go to zero.\u00a0 I suspect a ByteBuffer is being recycled, but the call to ByteBuffer.get mutates it.\u00a0 I wonder if an appropriately placed ByteBuffer.duplicate() would fix it.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/551",
            "https://github.com/apache/parquet-mr/pull/552"
        ]
    },
    "PARQUET-1408": {
        "Key": "PARQUET-1408",
        "Summary": "parquet-tools SimpleRecord does not display empty fields",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nicholas Rushton",
        "Created": "30/Aug/18 21:18",
        "Updated": "07/Nov/19 08:36",
        "Resolved": null,
        "Description": "When using parquet-tools on a parquet file with null records the null columns are omitted from the output.\n\u00a0\nExample:\n\n\r\nscala> case class Foo(a: Int, b: String)\r\ndefined class Foo\r\n\r\nscala> org.apache.spark.sql.SparkSession.builder.getOrCreate.createDataset((0 to 1000).map(x => Foo(1,null))).write.parquet(\"/tmp/foobar/\")\n\nActual:\n\n\r\n\u2601\u00a0 parquet-tools [master] \u26a1\u00a0 java -jar target/parquet-tools-1.10.1-SNAPSHOT.jar cat -j /tmp/foobar/part-00000-436a4d37-d82a-4771-8e7e-e4d428464675-c000.snappy.parquet | head -n5\r\n{\"a\":1}\r\n{\"a\":1}\r\n{\"a\":1}\r\n{\"a\":1}\r\n{\"a\":1}\n\nExpected:\n\n\r\n\u2601\u00a0 parquet-tools [master] \u26a1\u00a0 java -jar target/parquet-tools-1.10.1-SNAPSHOT.jar cat -j /tmp/foobar/part-00000-436a4d37-d82a-4771-8e7e-e4d428464675-c000.snappy.parquet | head -n5\r\n{\"a\":1,\"b\":null}\r\n{\"a\":1,\"b\":null}\r\n{\"a\":1,\"b\":null}\r\n{\"a\":1,\"b\":null}\r\n{\"a\":1,\"b\":null}",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/518"
        ]
    },
    "PARQUET-1409": {
        "Key": "PARQUET-1409",
        "Summary": "Can write but read parquet file with nested arrays",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "cesar matos",
        "Created": "31/Aug/18 07:54",
        "Updated": "26/Nov/18 10:11",
        "Resolved": null,
        "Description": "I am trying to read a parquet file in scala using the Avro interface (1.10.). The file was also generated using the same interface.\nThe data that I am writing looks like this:\n\u00a0\n\n\r\ncase class Inner(b: Array[Int])\r\ncase class Outer(a: Array[Inner])\r\n\r\nval data = Outer(\r\nArray(\r\n  Inner(Array(1, 2)),\r\n  Inner(Array(3, 4))\r\n )\r\n)\r\n\n\n\u00a0\nUsing parquet-tools to read read the file looks like this:\n\u00a0\n\n\r\n$ parquet-tools cat /tmp/test.parquet \r\na: \r\n.array: \r\n..b: \r\n...array = 1 \r\n...array = 2 \r\n.array: \r\n..b: \r\n...array = 3 \r\n...array = 4\r\n\n\n\u00a0\nBut while trying to read the file I get the following exception:\n\u00a0\n\u00a0\n\n\r\n\u00a0\r\nException in thread \"main\" org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'array' not found\r\nat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\r\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\r\nat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:279)\r\nat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:232)\r\nat org.apache.parquet.avro.AvroRecordConverter.access$100(AvroRecordConverter.java:78)\r\nat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter$ElementConverter.<init>(AvroRecordConverter.java:536)\r\nat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter.<init>(AvroRecordConverter.java:486)\r\nat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:289)\r\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:141)\r\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\r\nat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\r\nat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\r\nat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\r\nat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\r\nat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\r\nat raw.runtime.writer.parquet.avro.Lixo$.main(Lixo.scala:78)\r\nat raw.runtime.writer.parquet.avro.Lixo.main(Lixo.scala)\r\n\r\n\u00a0\r\n\n\n\u00a0\nThis is the code used to generate this file:\n\n\r\nval filename = \"/tmp/test.parquet\"\r\nval path = Paths.get(filename).toFile\r\nval conf = new Configuration()\r\n\r\nval schema: Schema = {\r\n\r\n  val inner = Schema.createRecord(\"inner\", \"some doc\", \"outer\", false,\r\n    List(new Schema.Field(\"b\", Schema.createArray(Schema.create(Schema.Type.INT)), \"\", null: Object)).asJava\r\n  )\r\n\r\n  Schema.createRecord(\"outer\", \"\", \"\", false,\r\n    List(new Schema.Field(\"a\", Schema.createArray(inner), \"\", null: Object)).asJava\r\n  )\r\n}\r\n\r\nval os = new FileOutputStream(path)\r\n\r\nval outputFile = new RawParquetOutputFile(os)\r\nval parquetWriter: ParquetWriter[GenericRecord] = AvroParquetWriter.builder[GenericRecord](outputFile)\r\n  .withConf(conf)\r\n  .withSchema(schema)\r\n  .build()\r\n\r\nval data = Outer(\r\n  Array(\r\n    Inner(Array(1, 2)),\r\n    Inner(Array(3, 4))\r\n  )\r\n)\r\n\r\nval record = new GenericData.Record(schema)\r\nval fieldA = schema.getField(\"a\").schema()\r\nval recorData = {\r\n  val fieldAType = fieldA.getElementType()\r\n  data.a.map { x =>\r\n    val innerRecord = new GenericData.Record(fieldAType)\r\n    innerRecord.put(\"b\", x.b)\r\n    innerRecord\r\n  }\r\n}\r\n\r\nrecord.put(\"a\", recorData)\r\nparquetWriter.write(record)\r\nparquetWriter.close()\r\nos.close()\r\n\n\n\u00a0Also if I pass the configuration option\u00a0\n\n\r\nparquet.avro.add-list-element-records = false\r\n\r\n\n\nI get a different exception:\norg.apache.avro.SchemaParseException: Can't redefine: list\n\u00a0\nAm I doing something wrong?",
        "Issue Links": [
            "/jira/browse/PARQUET-1441"
        ]
    },
    "PARQUET-1410": {
        "Key": "PARQUET-1410",
        "Summary": "Refactor modules to use the new logical type API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "31/Aug/18 12:00",
        "Updated": "08/Nov/22 15:34",
        "Resolved": "21/Sep/18 15:13",
        "Description": "Refactor parquet-mr modules to use the new logical type API for internal decisions (e.g.: replace the OriginalType-based switch cases to a more flexible solution, for example in type builder when checking if the proper annotation is present on physical type)",
        "Issue Links": [
            "/jira/browse/PARQUET-1331",
            "/jira/browse/PARQUET-1388",
            "https://github.com/apache/parquet-mr/pull/520"
        ]
    },
    "PARQUET-1411": {
        "Key": "PARQUET-1411",
        "Summary": "[C++] Upgrade to use LogicalType annotations instead of ConvertedType",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "TP Boudreau",
        "Reporter": "Wes McKinney",
        "Created": "31/Aug/18 22:27",
        "Updated": "29/May/19 05:50",
        "Resolved": "27/May/19 20:03",
        "Description": "Keeping in line with format evolution\nsee also PARQUET-1410",
        "Issue Links": [
            "/jira/browse/ARROW-3729",
            "/jira/browse/ARROW-1957",
            "https://github.com/apache/arrow/pull/4185"
        ]
    },
    "PARQUET-1412": {
        "Key": "PARQUET-1412",
        "Summary": "[C++] Incorporate CLI utilities and benchmarks into Arrow codebase",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "04/Sep/18 17:18",
        "Updated": "04/Jan/22 09:01",
        "Resolved": "04/Jan/22 09:01",
        "Description": "Follow-up work to ARROW-3075",
        "Issue Links": [
            "/jira/browse/ARROW-3075"
        ]
    },
    "PARQUET-1413": {
        "Key": "PARQUET-1413",
        "Summary": "[C++] Remove virtual calls from parquet::Comparator hot paths on writing file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Sep/18 20:45",
        "Updated": "01/May/19 22:41",
        "Resolved": "01/May/19 22:41",
        "Description": "Having virtual calls on the innermost loop of writing to a file would be best avoided. While we need to provide for users to write custom comparators (I think?), an implementation of a single-value comparison should be inlined into an array-oriented update step so that the per-element looping does not contain a virtual call",
        "Issue Links": []
    },
    "PARQUET-1414": {
        "Key": "PARQUET-1414",
        "Summary": "Limit page size based on maximum row count",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "07/Sep/18 11:56",
        "Updated": "22/Dec/20 22:16",
        "Resolved": "07/Nov/18 08:45",
        "Description": "For column index based filtering it is important to have enough pages for a column. In case of a perfectly matching encoding for the suitable data it can happen that all of the values can be encoded in one page (e.g. a column of an ascending counter).\nWith this improvement we would be able to limit the pages by the maximum number of rows to be written in it so we would have enough pages for every column.\nBased on the benchmarks listed here 20k seems to be a good choice for the default value.",
        "Issue Links": [
            "/jira/browse/PARQUET-1201",
            "/jira/browse/IMPALA-8449",
            "/jira/browse/IMPALA-10405",
            "https://github.com/apache/parquet-mr/pull/531",
            "https://github.com/apache/parquet-mr/pull/537",
            "https://docs.google.com/spreadsheets/d/1hfQPy8NkGbgGugnHWvIHSzZ-3Q5M7f3Dtf_oD9ACFRg"
        ]
    },
    "PARQUET-1415": {
        "Key": "PARQUET-1415",
        "Summary": "Improve logic when to write column indexes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "07/Sep/18 12:09",
        "Updated": "07/Nov/18 11:51",
        "Resolved": null,
        "Description": "Currently, we always write column indexes. In case of the data is ordered (ASCENDING or DESCENDING) the filtering would highly benefit from column indexes. While, if the data is UNORDERED it is not obvious if ordering based on column indexes would make sense. For example if the data is\u00a0random then the min/max values of the different pages might be close to each other so in most cases filtering based on these values would not drop any of the pages. In the other hand UNORDERED values does not mean that the values are random. It can happen that the values are clustered or semi-ordered. We shall discover these cases somehow before writing the column indexes and write only if the min/max values for the pages do not overlap too much.\nAnother simple case if we have only one page. In this case writing column indexes is useless.",
        "Issue Links": [
            "/jira/browse/PARQUET-1201",
            "/jira/browse/PARQUET-1435"
        ]
    },
    "PARQUET-1416": {
        "Key": "PARQUET-1416",
        "Summary": "[C++] Deprecate parquet/api/* in favor of simpler public API \"parquet/api.h\"",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "10/Sep/18 13:16",
        "Updated": "22/Aug/22 11:33",
        "Resolved": null,
        "Description": "The public API for this project is simple enough that I don't think we need anything more complex than a single public API header file",
        "Issue Links": []
    },
    "PARQUET-1417": {
        "Key": "PARQUET-1417",
        "Summary": "BINARY_AS_SIGNED_INTEGER_COMPARATOR fails with IOBE for the same arrays with the different length",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Vova Vysotskyi",
        "Reporter": "Vova Vysotskyi",
        "Created": "14/Sep/18 16:44",
        "Updated": "21/Sep/18 15:12",
        "Resolved": "20/Sep/18 09:36",
        "Description": "BINARY_AS_SIGNED_INTEGER_COMPARATOR fails when the same byte arrays but with the different number leading zeros are compared:\n\n\r\n    BINARY_AS_SIGNED_INTEGER_COMPARATOR.compare(\r\n        Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, 0, -108 })),\r\n        Binary.fromConstantByteBuffer(ByteBuffer.wrap(new byte[] { 0, -108 })));\r\n\n\nError is:\n\njava.lang.IndexOutOfBoundsException\r\n\tat java.nio.Buffer.checkIndex(Buffer.java:540)\r\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\r\n\tat org.apache.parquet.schema.PrimitiveComparator$9.compare(PrimitiveComparator.java:280)\r\n\tat org.apache.parquet.schema.PrimitiveComparator$9.compare(PrimitiveComparator.java:262)\r\n\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:186)\r\n\tat org.apache.parquet.schema.PrimitiveComparator$BinaryComparator.compareNotNulls(PrimitiveComparator.java:183)\r\n\tat org.apache.parquet.schema.PrimitiveComparator.compare(PrimitiveComparator.java:63)\r\n\n\nThe problem is that BINARY_AS_SIGNED_INTEGER_COMPARATOR.compare(ByteBuffer b1, ByteBuffer b2) method passes the length of the first ByteBuffer, but it should pass the less length\u00a0since padding was calculated and passed for the\u00a0ByteBuffer with greater\u00a0length to the compare(int length, ByteBuffer b1, int p1, ByteBuffer b2, int p2) method.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/522"
        ]
    },
    "PARQUET-1418": {
        "Key": "PARQUET-1418",
        "Summary": "Run integration tests in Travis",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "20/Sep/18 09:22",
        "Updated": "21/Sep/18 15:11",
        "Resolved": "21/Sep/18 15:10",
        "Description": "Currently Travis only runs the unit tests. It should run the integration tests as well.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/524"
        ]
    },
    "PARQUET-1419": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Enable old readers to access unencrypted columns in files with plaintext footer",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp,                                            parquet-format,                                            parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Sep/18 13:17",
        "Updated": "08/Jul/19 13:26",
        "Resolved": "01/May/19 12:55",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/106",
            "https://github.com/apache/parquet-format/pull/109",
            "https://github.com/apache/parquet-format/pull/109"
        ]
    },
    "PARQUET-1420": {
        "Key": "PARQUET-1420",
        "Summary": "[C++] Thrift-generated symbols not exported in DLL",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "20/Sep/18 13:34",
        "Updated": "09/Jan/20 19:50",
        "Resolved": null,
        "Description": "Thirft-generated symbols don't have any PARQUET_EXPORT-like annotation, so they are not reachable from the Parquet DLL. In turn this makes it impossible to link Parquet unit tests with the Parquet DLL (instead of the Parquet static lib). I'm not sure it can impact other applications.\nExample linking error:\n\n\r\ncolumn_writer-test.cc.obj : error LNK2019: unresolved external symbol \"public: v\r\nirtual unsigned int __cdecl parquet::format::Statistics::read(class apache::thri\r\nft::protocol::TProtocol *)\" (?read@Statistics@format@parquet@@UEAAIPEAVTProtocol\r\n@protocol@thrift@apache@@@Z) referenced in function \"[thunk]:public: virtual uns\r\nigned int __cdecl parquet::format::Statistics::read`vtordisp{4294967292,0}' (cla\r\nss apache::thrift::protocol::TProtocol *)\" (?read@Statistics@format@parquet@@$4P\r\nPPPPPPM@A@EAAIPEAVTProtocol@protocol@thrift@apache@@@Z)",
        "Issue Links": [
            "/jira/browse/ARROW-3873"
        ]
    },
    "PARQUET-1421": {
        "Key": "PARQUET-1421",
        "Summary": "InternalParquetRecordWriter logs debug messages at the INFO level",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "20/Sep/18 14:17",
        "Updated": "21/Sep/18 15:10",
        "Resolved": "21/Sep/18 15:10",
        "Description": "The superflous log messages clutter the output and may make Travis build due to too long output.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/526"
        ]
    },
    "PARQUET-1422": {
        "Key": "PARQUET-1422",
        "Summary": "[C++] Use Arrow IO interfaces natively rather than current parquet:: wrappers",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "21/Sep/18 15:32",
        "Updated": "08/Aug/19 14:03",
        "Resolved": "31/May/19 15:13",
        "Description": "We are beginning to do some work on asynchronous IO in Arrow and it would be great to be able to leverage this in the Parquet core internals. \nI am proposing to remove the Parquet-specific virtual file interfaces in\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/util/memory.h#L221\nand instead rely directly on the Arrow ones in arrow::io. In addition to reducing the amount of code we have to maintain, we will also be able to improve performance of Parquet by utilizing common utilities for managing asynchronous / background IO\ncc mdeepak xhochy",
        "Issue Links": [
            "/jira/browse/ARROW-3166",
            "https://github.com/apache/arrow/pull/4404"
        ]
    },
    "PARQUET-1423": {
        "Key": "ARROW-3769",
        "Summary": "[C++] Support reading non-dictionary encoded binary Parquet columns directly as DictionaryArray",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "C++",
        "Assignee": "Hatem Helal",
        "Reporter": "Wes McKinney",
        "Created": "25/Sep/18 11:25",
        "Updated": "11/Jan/23 07:29",
        "Resolved": "18/Mar/19 00:13",
        "Description": "If the goal is to hash this data anyway into a categorical-type array, then it would be better to offer the option to \"push down\" the hashing into the Parquet read hot path rather than first fully materializing a dense vector of ByteArray values, which could use a lot of memory after decompression",
        "Issue Links": [
            "/jira/browse/ARROW-3325",
            "/jira/browse/PARQUET-1508",
            "https://github.com/apache/arrow/pull/3721"
        ]
    },
    "PARQUET-1424": {
        "Key": "PARQUET-1424",
        "Summary": "Release parquet-format 2.6.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "25/Sep/18 16:33",
        "Updated": "02/Oct/18 15:55",
        "Resolved": "02/Oct/18 15:55",
        "Description": "Release parquet-format 2.6.0\nThe release requires reverting of the merged PRs related to columnar encryption, since there's no signed spec yet. Those PRs should be developed on a feature branch instead and merge to master once the spec is signed and the format changes are ready to get released.",
        "Issue Links": []
    },
    "PARQUET-1425": {
        "Key": "PARQUET-1425",
        "Summary": "[Format] Fix Thrift compiler warning",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "26/Sep/18 14:39",
        "Updated": "31/Mar/21 15:50",
        "Resolved": "31/Mar/21 15:50",
        "Description": "I see this warning frequently\n\n\r\n[1/127] Running thrift compiler on parquet.thrift\r\n[WARNING:/home/wesm/code/arrow/cpp/src/parquet/parquet.thrift:295] The \"byte\" type is a compatibility alias for \"i8\". Use \"i8\" to emphasize the signedness of this type.",
        "Issue Links": [
            "/jira/browse/PARQUET-1708"
        ]
    },
    "PARQUET-1426": {
        "Key": "PARQUET-1426",
        "Summary": "[C++] parquet-dump-schema has poor usability",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Wes McKinney",
        "Created": "26/Sep/18 14:43",
        "Updated": "08/Oct/18 14:07",
        "Resolved": "08/Oct/18 14:07",
        "Description": "$ ./debug/parquet-dump-schema\r\nterminate called after throwing an instance of 'std::logic_error'\r\n  what():  basic_string::_S_construct null not valid\r\nAborted (core dumped)\r\n\r\n$ ./debug/parquet-dump-schema --help\r\nParquet error: Arrow error: IOError: ../src/arrow/io/file.cc:508 code: result->memory_map_->Open(path, mode)\r\n../src/arrow/io/file.cc:380 code: file_->OpenReadable(path)\r\n../src/arrow/io/file.cc:99 code: internal::FileOpenReadable(file_name_, &fd_)\r\nFailed to open local file: --help , error: No such file or directory",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2715"
        ]
    },
    "PARQUET-1427": {
        "Key": "PARQUET-1427",
        "Summary": "[C++] Move example executables and CLI tools to Apache Arrow repo",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "26/Sep/18 14:44",
        "Updated": "27/Sep/18 12:20",
        "Resolved": "27/Sep/18 12:20",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2632"
        ]
    },
    "PARQUET-1428": {
        "Key": "PARQUET-1428",
        "Summary": "Move columnar encryption into its feature branch",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "27/Sep/18 09:33",
        "Updated": "02/Oct/18 15:55",
        "Resolved": "02/Oct/18 15:55",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/107"
        ]
    },
    "PARQUET-1429": {
        "Key": "PARQUET-1429",
        "Summary": "Turn off DocLint on parquet-format",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.6.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "27/Sep/18 14:02",
        "Updated": "27/Sep/18 14:24",
        "Resolved": "27/Sep/18 14:24",
        "Description": "DocLint is introduced in Java 8, and since the generated code in parquet-format has several issues found by DocLint, attach-javadocs\u00a0goal will fail.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/108"
        ]
    },
    "PARQUET-1430": {
        "Key": "PARQUET-1430",
        "Summary": "[C++] Add tests for C++ tools",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "27/Sep/18 14:12",
        "Updated": "22/Aug/22 11:36",
        "Resolved": null,
        "Description": "We currently do not have any tests for the tools.",
        "Issue Links": []
    },
    "PARQUET-1431": {
        "Key": "PARQUET-1431",
        "Summary": "[C++] Automaticaly set thrift to use boost for thrift versions before 0.11",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "27/Sep/18 15:50",
        "Updated": "29/Sep/18 11:12",
        "Resolved": "29/Sep/18 11:12",
        "Description": "PARQUET_THRIFT_USE_BOOST is a cmake option. But instead parquet should automatically set the definition PARQUET_THRIFT_USE_BOOST based on the thrift version.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2645"
        ]
    },
    "PARQUET-1432": {
        "Key": "PARQUET-1432",
        "Summary": "ACID support",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Yuming Wang",
        "Created": "01/Oct/18 08:02",
        "Updated": "01/Oct/18 16:38",
        "Resolved": null,
        "Description": "https://orc.apache.org/docs/acid.html",
        "Issue Links": []
    },
    "PARQUET-1433": {
        "Key": "PARQUET-1433",
        "Summary": "Parquet-format doesn't compile with Thrift 0.10.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "01/Oct/18 09:33",
        "Updated": "15/Oct/18 10:27",
        "Resolved": "15/Oct/18 10:27",
        "Description": "Compilation of parquet-format\u00a0fails with Thrift 0.10.0:\n[ERROR] thrift failed error: [FAILURE:generation:1] Error: unknown\noption java:hashcode",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/111"
        ]
    },
    "PARQUET-1434": {
        "Key": "PARQUET-1434",
        "Summary": "Release parquet-mr 1.11.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "03/Oct/18 09:39",
        "Updated": "09/Dec/19 11:20",
        "Resolved": "09/Dec/19 11:20",
        "Description": null,
        "Issue Links": [
            "/jira/browse/SPARK-26346",
            "/jira/browse/SPARK-26797",
            "/jira/browse/HIVE-21050",
            "/jira/browse/HIVE-21355",
            "/jira/browse/HIVE-21359",
            "/jira/browse/PARQUET-1488",
            "/jira/browse/PARQUET-1531",
            "/jira/browse/PARQUET-1650",
            "/jira/browse/PARQUET-1570"
        ]
    },
    "PARQUET-1435": {
        "Key": "PARQUET-1435",
        "Summary": "Benchmark filtering column-indexes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "03/Oct/18 09:48",
        "Updated": "12/Nov/18 10:17",
        "Resolved": "12/Nov/18 10:17",
        "Description": "Benchmark the improvements or drawbacks of filtering with and without using column-indexes. We shall also benchmark the overhead of reading and using the column-indexes in case of it is not useful (e.g. completely randomized data).",
        "Issue Links": [
            "/jira/browse/PARQUET-1201",
            "/jira/browse/PARQUET-1415",
            "https://github.com/apache/parquet-mr/pull/536"
        ]
    },
    "PARQUET-1436": {
        "Key": "PARQUET-1436",
        "Summary": "TimestampMicrosStringifier shows wrong microseconds for timestamps before 1970",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "03/Oct/18 13:56",
        "Updated": "15/Oct/18 08:53",
        "Resolved": "15/Oct/18 08:53",
        "Description": "testTimestampMicrosStringifier takes the timestamp 1848-03-15T09:23:59.765 and subtracts 1 microseconds from it. The result (both expected and actual) is 1848-03-15T09:23:59.765001, but it should be 1848-03-15T09:23:59.764999 instead.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/529"
        ]
    },
    "PARQUET-1437": {
        "Key": "PARQUET-1437",
        "Summary": "Misleading comment in parquet.thrift",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "08/Oct/18 15:01",
        "Updated": "07/Nov/18 10:42",
        "Resolved": "07/Nov/18 10:42",
        "Description": "The documentation for list<ColumnOrder> column_orders states that \"Each sort order corresponds to one column, determined by its position in the list, matching the position of the column in the schema.\"\nHowever, in reality, while the order of elements in these two lists (schema and sort order) are the same, only leaf nodes are represented in the list of sort orders, so the positions do not match.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/115"
        ]
    },
    "PARQUET-1438": {
        "Key": "PARQUET-1438",
        "Summary": "[C++] corrupted files produced on 32-bit architecture (i686)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitry Kalinkin",
        "Created": "09/Oct/18 03:25",
        "Updated": "17/Feb/19 08:09",
        "Resolved": "17/Feb/19 08:09",
        "Description": "I'm using C++ API to convert some data to parquet files. I've noticed a regression when upgrading from arrow-cpp 0.10.0 + parquet-cpp 1.5.0 to arrow-cpp 0.11.0. The issue is that I can write parquet files without an error, but when I try to read those using pyarrow I get a segfault:\n\n#0  0x00007fffd17c7f0f in int arrow::util::RleDecoder::GetBatchWithDictSpaced<float>(float const*, float*, int, int, unsigned char const*, long) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#1  0x00007fffd17c8025 in parquet::DictionaryDecoder<parquet::DataType<(parquet::Type::type)4> >::DecodeSpaced(float*, int, int, unsigned char const*, long) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#2  0x00007fffd17bcf0f in parquet::internal::TypedRecordReader<parquet::DataType<(parquet::Type::type)4> >::ReadRecordData(long) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#3  0x00007fffd17bfbea in parquet::internal::TypedRecordReader<parquet::DataType<(parquet::Type::type)4> >::ReadRecords(long) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#4  0x00007fffd179d2f7 in parquet::arrow::PrimitiveImpl::NextBatch(long, std::shared_ptr<arrow::Array>*) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#5  0x00007fffd1797162 in parquet::arrow::ColumnReader::NextBatch(long, std::shared_ptr<arrow::Array>*) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#6  0x00007fffd179a6e5 in parquet::arrow::FileReader::Impl::ReadSchemaField(int, std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Array>*) ()\r\n   from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n#7  0x00007fffd179aaad in parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}::operator()(int) const () from /nix/store/k6sy2ncjnkn5wnb2dq9m5f0qh446kjhg-arrow-cpp-0.11.0/lib/libparquet.so.11\r\n\n\nI have not been able to dig to the bottom of the issue, but it seems like the problem reproduces only when I run 32 bit binaries. After I learned that, I found that 32 bit and 64 bit codes produce very different different parquet files for the same data. The sizes of the structures are clearly different if I look at their hexdumps. I'm attaching those example files. Reading \"32.parquet\" (produced using i686 binaries) will cause a segfault on macOS and linux, \"64.parquet\" will read just fine.",
        "Issue Links": []
    },
    "PARQUET-1439": {
        "Key": "PARQUET-1439",
        "Summary": "[C++] Parquet build fails when PARQUET_ARROW_LINKAGE is static",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "09/Oct/18 14:00",
        "Updated": "10/Nov/18 20:37",
        "Resolved": "10/Nov/18 20:37",
        "Description": "The error is as follows\n\nCMake Error at cmake_modules/BuildUtils.cmake:145 (add_dependencies):\r\n\u00a0 The dependency target \"/usr/lib/x86_64-linux-gnu/libpthread.so\" of target\r\n\u00a0 \"parquet_objlib\" does not exist.\r\nCall Stack (most recent call first):\r\n\u00a0 src/parquet/CMakeLists.txt:183 (ADD_ARROW_LIB",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2735"
        ]
    },
    "PARQUET-1440": {
        "Key": "PARQUET-1440",
        "Summary": "Parquet-tools: Decimal values stored in an int32 or int64 in the parquet file aren't displayed with their proper scale",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Gardner",
        "Reporter": "Ryan Gardner",
        "Created": "09/Oct/18 14:02",
        "Updated": "10/Oct/18 16:00",
        "Resolved": "10/Oct/18 16:00",
        "Description": "When\u00a0working with the parquet-tools, I noticed that decimal values that were stored with int32 or int64 were not being displayed properly.\nI opened up a pull request to fix this:\nhttps://github.com/apache/parquet-mr/pull/530#issuecomment-428137066",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/530"
        ]
    },
    "PARQUET-1441": {
        "Key": "PARQUET-1441",
        "Summary": "SchemaParseException: Can't redefine: list in AvroIndexedRecordConverter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Michael Heuer",
        "Created": "10/Oct/18 02:16",
        "Updated": "24/Oct/19 08:28",
        "Resolved": "24/Oct/19 08:28",
        "Description": "The following unit test added to TestAvroSchemaConverter fails\n\n\r\n@Test\r\npublic void testConvertedSchemaToStringCantRedefineList() throws Exception {\r\n  String parquet = \"message spark_schema {\\n\" +\r\n      \"  optional group annotation {\\n\" +\r\n      \"    optional group transcriptEffects (LIST) {\\n\" +\r\n      \"      repeated group list {\\n\" +\r\n      \"        optional group element {\\n\" +\r\n      \"          optional group effects (LIST) {\\n\" +\r\n      \"            repeated group list {\\n\" +\r\n      \"              optional binary element (UTF8);\\n\" +\r\n      \"            }\\n\" +\r\n      \"          }\\n\" +\r\n      \"        }\\n\" +\r\n      \"      }\\n\" +\r\n      \"    }\\n\" +\r\n      \"  }\\n\" +\r\n      \"}\\n\";\r\n\r\n  Configuration conf = new Configuration(false);\r\n  AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);\r\n  Schema schema = avroSchemaConverter.convert(MessageTypeParser.parseMessageType(parquet));\r\n  schema.toString();\r\n}\r\n\n\nwhile this one succeeds\n\n\r\n@Test\r\npublic void testConvertedSchemaToStringCantRedefineList() throws Exception {\r\n  String parquet = \"message spark_schema {\\n\" +\r\n      \"  optional group annotation {\\n\" +\r\n      \"    optional group transcriptEffects (LIST) {\\n\" +\r\n      \"      repeated group list {\\n\" +\r\n      \"        optional group element {\\n\" +\r\n      \"          optional group effects (LIST) {\\n\" +\r\n      \"            repeated group list {\\n\" +\r\n      \"              optional binary element (UTF8);\\n\" +\r\n      \"            }\\n\" +\r\n      \"          }\\n\" +\r\n      \"        }\\n\" +\r\n      \"      }\\n\" +\r\n      \"    }\\n\" +\r\n      \"  }\\n\" +\r\n      \"}\\n\";\r\n \r\n  Configuration conf = new Configuration(false);\r\n  conf.setBoolean(\"parquet.avro.add-list-element-records\", false);\r\n  AvroSchemaConverter avroSchemaConverter = new AvroSchemaConverter(conf);\r\n  Schema schema = avroSchemaConverter.convert(MessageTypeParser.parseMessageType(parquet));\r\n  schema.toString();\r\n}\r\n\n\nI don't see a way to influence the code path in AvroIndexedRecordConverter to respect this configuration, resulting in the following stack trace downstream\n\n  Cause: org.apache.avro.SchemaParseException: Can't redefine: list\r\n  at org.apache.avro.Schema$Names.put(Schema.java:1128)\r\n  at org.apache.avro.Schema$NamedSchema.writeNameRef(Schema.java:562)\r\n  at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:690)\r\n  at org.apache.avro.Schema$ArraySchema.toJson(Schema.java:805)\r\n  at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:882)\r\n  at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:716)\r\n  at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:701)\r\n  at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:882)\r\n  at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:716)\r\n  at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:701)\r\n  at org.apache.avro.Schema.toString(Schema.java:324)\r\n  at org.apache.avro.SchemaCompatibility.checkReaderWriterCompatibility(SchemaCompatibility.java:68)\r\n  at org.apache.parquet.avro.AvroRecordConverter.isElementType(AvroRecordConverter.java:866)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter$AvroArrayConverter.<init>(AvroIndexedRecordConverter.java:333)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter.newConverter(AvroIndexedRecordConverter.java:172)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter.<init>(AvroIndexedRecordConverter.java:94)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter.newConverter(AvroIndexedRecordConverter.java:168)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter.<init>(AvroIndexedRecordConverter.java:94)\r\n  at org.apache.parquet.avro.AvroIndexedRecordConverter.<init>(AvroIndexedRecordConverter.java:66)\r\n  at org.apache.parquet.avro.AvroCompatRecordMaterializer.<init>(AvroCompatRecordMaterializer.java:34)\r\n  at org.apache.parquet.avro.AvroReadSupport.newCompatMaterializer(AvroReadSupport.java:144)\r\n  at org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:136)\r\n  at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:204)\r\n  at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:182)\r\n  at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)\r\n...\r\n\n\nSee also downstream issues\nhttps://issues.apache.org/jira/browse/SPARK-25588\nhttps://github.com/bigdatagenomics/adam/issues/2058",
        "Issue Links": [
            "/jira/browse/PARQUET-1409",
            "https://github.com/apache/parquet-mr/pull/555",
            "https://github.com/apache/parquet-mr/pull/560"
        ]
    },
    "PARQUET-1442": {
        "Key": "PARQUET-1442",
        "Summary": "ParquetReadProtocol Prefer Array over LinkedList",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-thrift",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "17/Oct/18 02:35",
        "Updated": "07/Feb/19 13:55",
        "Resolved": "07/Feb/19 13:55",
        "Description": "Prefer ArrayDeque over LinkedList\nhttp://brianandstuff.com/2016/12/12/java-arraydeque-vs-linkedlist/",
        "Issue Links": [
            "/jira/browse/PARQUET-1444"
        ]
    },
    "PARQUET-1443": {
        "Key": "PARQUET-1443",
        "Summary": "ColumnChunkPageReader Clean Up",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "17/Oct/18 02:41",
        "Updated": "07/Feb/19 13:54",
        "Resolved": "07/Feb/19 13:54",
        "Description": "Remove unused imports\nRemove unused Logger\nUse an ArrayDeque instead of a LinkedList",
        "Issue Links": [
            "/jira/browse/PARQUET-1444"
        ]
    },
    "PARQUET-1444": {
        "Key": "PARQUET-1444",
        "Summary": "Prefer ArrayList over LinkedList",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-thrift",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "17/Oct/18 02:49",
        "Updated": "23/Oct/19 13:24",
        "Resolved": "23/Oct/19 13:24",
        "Description": "https://stackoverflow.com/questions/322715/when-to-use-linkedlist-over-arraylist-in-java",
        "Issue Links": [
            "/jira/browse/PARQUET-1442",
            "/jira/browse/PARQUET-1443",
            "https://github.com/apache/parquet-mr/pull/583"
        ]
    },
    "PARQUET-1445": {
        "Key": "PARQUET-1445",
        "Summary": "Remove Files.java",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "17/Oct/18 14:14",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "05/Sep/19 08:07",
        "Description": "TODO: Use java.nio.file.Files when Parquet is updated to Java 7\nhttps://github.com/apache/parquet-mr/blob/dc61e510126aaa1a95a46fe39bf1529f394147e9/parquet-common/src/main/java/org/apache/parquet/Files.java#L31",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/584"
        ]
    },
    "PARQUET-1446": {
        "Key": "PARQUET-1446",
        "Summary": "Review FilteringRecordMaterializer.java",
        "Type": "Improvement",
        "Status": "Patch Available",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "18/Oct/18 14:28",
        "Updated": "02/Dec/18 14:59",
        "Resolved": null,
        "Description": "Remove two methods and replace with Java 8 stream facility.",
        "Issue Links": []
    },
    "PARQUET-1447": {
        "Key": "PARQUET-1447",
        "Summary": "MapredParquetOutputFormat - Save Some Array Allocations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "David Mollitor",
        "Created": "18/Oct/18 14:39",
        "Updated": "08/Jan/19 17:46",
        "Resolved": "07/Jan/19 18:37",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/582"
        ]
    },
    "PARQUET-1448": {
        "Key": "PARQUET-1448",
        "Summary": "Review of ParquetFileReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Aaron Blake Niskode-Dossett",
        "Reporter": "David Mollitor",
        "Created": "18/Oct/18 16:06",
        "Updated": "22/Apr/21 21:52",
        "Resolved": "19/Apr/21 15:51",
        "Description": "Pre-Size ArrayList objects\nImprove code readability\nFix some check-style issues\nImprove use of Collections and Map API",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/586"
        ]
    },
    "PARQUET-1449": {
        "Key": "PARQUET-1449",
        "Summary": "[C++] Can't build with ARROW_BOOST_VENDORED=ON",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "24/Oct/18 09:35",
        "Updated": "25/Oct/18 00:50",
        "Resolved": "25/Oct/18 00:50",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2828"
        ]
    },
    "PARQUET-1450": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Detailed crypto specification",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "25/Oct/18 08:52",
        "Updated": "18/Jan/19 01:56",
        "Resolved": "18/Jan/19 01:56",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/114"
        ]
    },
    "PARQUET-1451": {
        "Key": "PARQUET-1451",
        "Summary": "Deprecate old logical types API",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "29/Oct/18 16:26",
        "Updated": "06/Nov/18 17:35",
        "Resolved": "29/Oct/18 16:38",
        "Description": "Now that the new logical types API is ready, we should deprecate the old one because new types will not support it (in fact, nano precision has already been added without support in the old API).",
        "Issue Links": []
    },
    "PARQUET-1452": {
        "Key": "PARQUET-1452",
        "Summary": "Deprecate old logical types API",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "29/Oct/18 16:30",
        "Updated": "07/Nov/18 11:31",
        "Resolved": "07/Nov/18 11:31",
        "Description": "Now that the new logical types API is ready, we should deprecate the old one because new types will not support it (in fact, nano precision has already been added without support in the old API).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/535"
        ]
    },
    "PARQUET-1453": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Support nested column Bloom filter",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "30/Oct/18 17:11",
        "Updated": "27/Feb/20 09:24",
        "Resolved": "26/Feb/20 16:00",
        "Description": "Bloom filters don't support nested columns yet.",
        "Issue Links": []
    },
    "PARQUET-1454": {
        "Key": "PARQUET-1454",
        "Summary": "ld-linux-x86-64.so.2 is missing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Stets Alexander",
        "Created": "31/Oct/18 07:36",
        "Updated": "07/Feb/19 12:13",
        "Resolved": "07/Feb/19 12:13",
        "Description": "parquet-avro uses dependensy org.xerial.snappy:snappy-java .\nsnappy-java need extract native lib. For this goal it uses ld-linux-x86-64.so.2.\nIf your OS doesn't contain ld-linux-x86-64.so.2 you catch exception like this\njava.lang.UnsatisfiedLinkError: /tmp/snappy-1.1.2-b0bbcae9-e398-4a99-ad6d-19c86734be76-libsnappyjava.so: Error loading shared library ld-linux-x86-64.so.2: No such file or directory (needed by /tmp/snappy-1.1.2-b0bbcae9-e398-4a99-ad6d-19c86734be76-libsnappyjava.so)\nBut documentation doesn't contain information about it.",
        "Issue Links": []
    },
    "PARQUET-1455": {
        "Key": "PARQUET-1455",
        "Summary": "[parquet-protobuf] Handle \"unknown\" enum values for parquet-protobuf",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Qinghui Xu",
        "Reporter": "Qinghui Xu",
        "Created": "05/Nov/18 15:24",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "25/Aug/20 09:33",
        "Description": "Background - \nIn protobuf enum is more like integers other than string, and is encoded as integer on the wire.\nIn Protobuf, each enum value is associated with a number (integer), and people can set enum field using number directly regardless whether the number is associated to an enum value or not. While enum filed is set with a number that does not match any enum value defined in the schema, by using protobuf reflection API (as parquet-protobuf does) to read the enum field we will get a label \"UNKNOWN_ENUM_<enumName><number>\" generated by protobuf reflection. Thus parquet-protobuf will write string \"UNKNOWN_ENUM<enumName>_<number>\" into the enum column whenever its protobuf schema does not recognize the number.\n\u00a0\nProblematics -\nThere are two cases of unknown enum while using parquet-protobuf:\n 1. Protobuf already contains unknown enum when we write it to parquet (sometimes people manipulate enum using numbers), so it will write a label \"UNKNOWN_ENUM_*\" as string in parquet. And when we read it back to protobuf, we found this \"true\" unknown value\n 2. Protobuf contains valid value when write to parquet, but the reader uses an outdated proto schema which misses some enum values. So the not-in-old-schema enum values are \"unknown\" to the reader.\nCurrent behavior of parquet-proto reader is to reject in both cases with some runtime exception. This does not make sense in case 1, the write part does respect protobuf enum behavior while the read part does not. And case 2 should be handled if protobuf user is interested in the number instead of label.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/561"
        ]
    },
    "PARQUET-1456": {
        "Key": "PARQUET-1456",
        "Summary": "Use page index, ParquetFileReader throw ArrayIndexOutOfBoundsException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "yiming.xu",
        "Created": "09/Nov/18 03:12",
        "Updated": "19/Nov/18 12:18",
        "Resolved": "19/Nov/18 12:18",
        "Description": "hi, We use page index to adaptive spark with master branch find a concurrent problem, class org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder#build cached ColumnIndexBuilder but not lock it,\n    PrimitiveTypeName typeName = type.getPrimitiveTypeName();\n    ColumnIndexBuilder builder = BUILDERS.get(typeName);\n    if (builder == null) \n{\r\n      builder = createNewBuilder(type, Integer.MAX_VALUE);\r\n      BUILDERS.put(typeName, builder);\r\n    }",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/548"
        ]
    },
    "PARQUET-1457": {
        "Key": "PARQUET-1457",
        "Summary": "[C++] Data set integrity tool",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "12/Nov/18 08:02",
        "Updated": "03/Jul/19 02:29",
        "Resolved": null,
        "Description": "Parquet encryption protects integrity of individual files. However, data sets (such as tables) are often written as a collection of files, say\n\"/path/to/dataset\"/part0.parquet.encrypted\n..\n\"/path/to/dataset\"/partN.parquet.encrypted\n\u00a0\nIn an untrusted storage, removal of one or more files will go unnoticed. Replacement of one file contents with another will go unnoticed, unless a user has provided unique AAD prefixes for each file.\n\u00a0\nThe data set integrity tool solves these problems. While it doesn't necessarily belong in Parquet functionality (that is focused on individual files ) - it will assist higher level frameworks that use Parquet, to cryptographically protect integrity of data sets comprised of multiple files.\nThe use of this tool is not obligatory, as frameworks can use other means to verify table (file collection) integrity.\n\u00a0\nThe tool works by creating a small file, that can be stored as say\n\"/path/to/dataset\"/.dataset.signature\n\u00a0\nthat contains the dataset unique name (URI) and the number of files. It can also contain an explicit list of file names (with or without full path). The file contents is either encrypted with AES-GCM\u00a0 (authenticated, encrypted) - or hashed and signed (authenticated, plaintext).\u00a0\n\u00a0\nOn the writer side, the tools creates AAD prefixes for every data file, and creates the signature file itself. The input is the dataset URI, N and the encryption/signature key; plus (optionally) the\u00a0list of file names (with or without full path).\n\u00a0\nOn the reader side, the tool parses and verifies the signature file, and provides the framework with the verified dataset name, number of files that must be accounted for, and the AAD prefix for each file; \u00a0plus (optionally) the\u00a0list of file names (with or without full path). The input is the expected dataset URI and\u00a0the encryption/signature key.",
        "Issue Links": [
            "/jira/browse/PARQUET-1178"
        ]
    },
    "PARQUET-1458": {
        "Key": "PARQUET-1458",
        "Summary": "[C++] parquet::CompressionToString not recognizing brotli compression",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Hatem Helal",
        "Created": "14/Nov/18 16:00",
        "Updated": "15/Nov/18 00:25",
        "Resolved": "15/Nov/18 00:25",
        "Description": "It looks like we just need to add a case to handle the brotli codec here",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/2963"
        ]
    },
    "PARQUET-1459": {
        "Key": "ARROW-3843",
        "Summary": "[Python] Writing Parquet file from empty table created with Table.from_pandas(..., preserve_index=False) fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "Python",
        "Assignee": "Wes McKinney",
        "Reporter": "Justin Lewis",
        "Created": "20/Nov/18 16:41",
        "Updated": "11/Jan/23 07:30",
        "Resolved": "22/Mar/19 14:41",
        "Description": "import pandas as pd\r\nimport pyarrow.parquet as pq\r\nimport pyarrow as pa\r\n\r\n\r\ndef test_write_empty_preserve_index():\r\n\r\n# passes\r\n\r\ndf = pd.DataFrame()\r\ntable = pa.Table.from_pandas(df, preserve_index=True)\r\npq.write_table(table, 'test1.parquet')\r\ntable2 = pq.read_table('test1.parquet')\r\ndf2 = table2.to_pandas()\r\npd.util.testing.assert_frame_equal(df, df2)\r\n\r\n\r\ndef test_write_empty_no_preserve_index():\r\ndf = pd.DataFrame()\r\ntable = pa.Table.from_pandas(df, preserve_index=False)\r\n\r\n# fails here\r\npq.write_table(table, 'test2.parquet')\r\n\r\ntable2 = pq.read_table('test2.parquet')\r\ndf2 = table2.to_pandas()\r\npd.util.testing.assert_frame_equal(df, df2)\n\n\u00a0\nFirst test passes.\u00a0 Second one fails with this:\n\u00a0\n\n\r\n___________________________________ test_write_empty_no_preserve_index ___________________________________\r\n\r\ndef test_write_empty_no_preserve_index():\r\ndf = pd.DataFrame()\r\ntable = pa.Table.from_pandas(df, preserve_index=False)\r\n\r\n# fails here\r\n> pq.write_table(table, 'test2.parquet')\r\n\r\ntest_empty.py:24: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:1125: in write_table\r\nwriter.write_table(table, row_group_size=row_group_size)\r\n../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:361: in __exit__\r\nself.close()\r\n../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:380: in close\r\nself.writer.close()\r\npyarrow/_parquet.pyx:916: in pyarrow._parquet.ParquetWriter.close\r\n???\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\n> ???\r\nE pyarrow.lib.ArrowIOError: Root node did not have children\r\n\r\npyarrow/error.pxi:83: ArrowIOError\r\n\n\n\u00a0\nI haven't had a chance to investigate but seems not desired behavior.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3985"
        ]
    },
    "PARQUET-1460": {
        "Key": "PARQUET-1460",
        "Summary": "Fix javadoc errors and include javadoc checking in Travis checks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Zoltan Ivanfi",
        "Created": "21/Nov/18 13:54",
        "Updated": "21/Nov/18 16:12",
        "Resolved": "21/Nov/18 16:12",
        "Description": "Javadoc generation fails with various errors, preventing us from running the release script.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/553",
            "https://github.com/apache/parquet-mr/pull/554"
        ]
    },
    "PARQUET-1461": {
        "Key": "PARQUET-1461",
        "Summary": "Third party code does not compile after parquet-mr minor version update",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Zoltan Ivanfi",
        "Created": "22/Nov/18 16:25",
        "Updated": "09/Jan/19 15:01",
        "Resolved": "09/Jan/19 15:01",
        "Description": "Third party code implemented public void initFromPage(int valueCount, ByteBuffer page, int offset), but new version has public abstract initFromPage(int valueCount, ByteBufferInputStream in) instead.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/556"
        ]
    },
    "PARQUET-1462": {
        "Key": "PARQUET-1462",
        "Summary": "Allow specifying new development version in prepare-release.sh",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0,                                            format-2.7.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "23/Nov/18 14:18",
        "Updated": "14/Feb/19 10:53",
        "Resolved": "04/Dec/18 13:31",
        "Description": "Currently prepare-release.sh only takes the release version as a parameter, the new development version is asked interactively for each individual pom.xml file, which makes answering them tedious.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/116",
            "https://github.com/apache/parquet-mr/pull/557"
        ]
    },
    "PARQUET-1463": {
        "Key": "PARQUET-1463",
        "Summary": "[C++] Utilize revamped common hashing machinery for dictionary encoding",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Wes McKinney",
        "Created": "25/Nov/18 00:33",
        "Updated": "30/Nov/18 00:37",
        "Resolved": "28/Nov/18 15:42",
        "Description": "pitrou has recently made some significant improvements to hashing / dictionary encoding machinery in Apache Arrow\nhttps://github.com/apache/arrow/commit/eaf8d32e5f292dca0aa5b5508041d5d39406224d\nparquet-cpp is using a custom hash table\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/encoding-internal.h#L456\nIt would be nice to utilize common hash table machinery if possible. We should of course make sure that such a change does not cause performance regressions (performance improved due to Antoine's patch, so perf may also get better on the Parquet write path)",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3036"
        ]
    },
    "PARQUET-1464": {
        "Key": "PARQUET-1464",
        "Summary": "Not able to do backwards compatible projection",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Mouli Mukherjee",
        "Created": "26/Nov/18 19:19",
        "Updated": "26/Nov/18 19:21",
        "Resolved": null,
        "Description": "Currently the following scenarios fail while using projections. This makes it hard to do backwards compatible schema changes like adding an optional field or making a required field optional.\n1. Adding an optional field to an existing thrift schema and making it part of the projection for reading data written using older thrift.\n2. Projecting a required field using optional.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/558"
        ]
    },
    "PARQUET-1465": {
        "Key": "PARQUET-1465",
        "Summary": "CLONE - Add a way to append encoded blocks in ParquetFileWriter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "1.9.0,                                            1.8.2",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Steven Paster",
        "Created": "29/Nov/18 09:26",
        "Updated": "29/Nov/18 18:36",
        "Resolved": "29/Nov/18 18:36",
        "Description": "Concatenating two files together currently requires reading the source files and rewriting the content from scratch. This ends up taking a lot of memory, even if the data is already encoded correctly and blocks just need to be appended and have their metadata updated. Merging two files should be fast and not take much memory.",
        "Issue Links": [
            "/jira/browse/PARQUET-382"
        ]
    },
    "PARQUET-1466": {
        "Key": "PARQUET-1466",
        "Summary": "Upgrade to the latest guava 27.0-jre",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "29/Nov/18 10:34",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "14/Jan/19 12:43",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/559"
        ]
    },
    "PARQUET-1467": {
        "Key": "PARQUET-1467",
        "Summary": "[C++] Remove ChunkedAllocator code, now unused",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "02/Dec/18 03:38",
        "Updated": "03/Dec/18 03:44",
        "Resolved": "03/Dec/18 03:44",
        "Description": "This code was initially lifted from the Impala codebase, where it's used for dictionary encoding. After PARQUET-1463, this is no longer used",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3069"
        ]
    },
    "PARQUET-1468": {
        "Key": "PARQUET-1468",
        "Summary": "[C++] Consolidate RecordReader, ColumnReader code paths",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Dec/18 23:22",
        "Updated": "19/Jul/19 22:08",
        "Resolved": "19/Jul/19 22:08",
        "Description": "The record-delimiting logic is central to reading nested data from columns. When RecordReader was created, I duplicated quite a bit of logic from column_reader.h, and never got around to refactoring further. We'll have to be careful about preserving backwards compatibility for flat column reads, though",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4906"
        ]
    },
    "PARQUET-1469": {
        "Key": "PARQUET-1469",
        "Summary": "[C++] DefinitionLevelsToBitmap can overwrite prior decoded data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/Dec/18 23:35",
        "Updated": "04/Dec/18 01:11",
        "Resolved": "04/Dec/18 01:11",
        "Description": "Bug exposed in ARROW-3930",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3080"
        ]
    },
    "PARQUET-1470": {
        "Key": "PARQUET-1470",
        "Summary": "Inputstream leakage in ParquetFileWriter.appendFile",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Arnaud Linz",
        "Created": "04/Dec/18 10:01",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "06/Feb/19 08:53",
        "Description": "Current implementation of\u00a0ParquetFileWriter.appendFile is:\n\u00a0\npublic void appendFile(InputFile file) throws IOException {\n\u00a0 \u00a0 ParquetFileReader.open(file).appendTo(this);\n{{ }}}\nthis method never closes the inputstream created when the file is opened in the ParquetFileReader constructor.\nThis leads for instance to TooManyFilesOpened exceptions when large merge are made with the parquet tools.\nsomething\u00a0 like\n{{ try (ParquetFileReader reader = ParquetFileReader.open(file)) {}}\n\u00a0 \u00a0 reader.appendTo(this);\n{{ }}}\nwould be cleaner.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/611"
        ]
    },
    "PARQUET-1471": {
        "Key": "PARQUET-1471",
        "Summary": "[C++] Out of bounds access in statistics UpdateSpaced when writing optional list with null list slots",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "06/Dec/18 04:49",
        "Updated": "06/Dec/18 20:26",
        "Resolved": "06/Dec/18 20:26",
        "Description": "Bug that arose in https://github.com/apache/arrow/pull/3074",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3114"
        ]
    },
    "PARQUET-1472": {
        "Key": "PARQUET-1472",
        "Summary": "Dictionary filter fails on FIXED_LEN_BYTE_ARRAY",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Dec/18 10:35",
        "Updated": "13/Dec/18 16:01",
        "Resolved": "13/Dec/18 16:01",
        "Description": "DictonaryFilter does not handle FIXED_LEN_BYTE_ARRAY. Moreover, DictionaryFilter.expandFilter(ColumnChunkMetaData) returns an empty map instead of null therefore the row-group might be dropped as the value seems to not being in the dictionary.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/562"
        ]
    },
    "PARQUET-1473": {
        "Key": "PARQUET-1473",
        "Summary": "[C++] Add helper function that converts ParquetVersion to human-friendly string",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Hatem Helal",
        "Reporter": "Hatem Helal",
        "Created": "10/Dec/18 16:52",
        "Updated": "13/Dec/18 14:56",
        "Resolved": "13/Dec/18 14:56",
        "Description": "I noticed this while working on ARROW-3564: the parquet-reader utility prints a line like:\nVersion: 0\nwhich corresponds to the PARQUET_1_0 enum value.\u00a0\u00a0I couldn't find anything that obviously did this in the parquet-cpp code base.\u00a0 It would be good if there were a function that could map from the ParquetVersion enum to a human-friendly string.\u00a0 e.g:\n\u00a0\nVersion: 1.0",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3148"
        ]
    },
    "PARQUET-1474": {
        "Key": "PARQUET-1474",
        "Summary": "Less verbose and lower level logging for missing column/offset indexes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "11/Dec/18 15:12",
        "Updated": "13/Dec/18 16:01",
        "Resolved": "13/Dec/18 16:01",
        "Description": "Currently, exception stacktrace is logged at warn level if an offset index is missing. Also a warn level log happens if a column index is missing which is required for column-index based filtering. Both cases are properly valid scenarios if the file is written by older libraries (where no column/offset indexes are written at all) or the sorting order is undefined for the related column type (e.g. INT96).\nThese logs shall be kept at INFO level and no stacktrace shall be provided.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/563"
        ]
    },
    "PARQUET-1475": {
        "Key": "PARQUET-1475",
        "Summary": "DirectCodecFactory's ParquetCompressionCodecException drops a passed in cause in one constructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Jacques Nadeau",
        "Reporter": "Jacques Nadeau",
        "Created": "12/Dec/18 03:31",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "14/Jan/19 14:36",
        "Description": "https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/DirectCodecFactory.java#L521\n\u00a0\nCause is not actually passed to super.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/564"
        ]
    },
    "PARQUET-1476": {
        "Key": "PARQUET-1476",
        "Summary": "Don't emit a warning message for files without new logical type",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "13/Dec/18 13:52",
        "Updated": "14/Feb/19 10:36",
        "Resolved": "14/Feb/19 10:36",
        "Description": "When the only the old logical type representation is present in the file, the converter emits a warning that the two types mismatch. This creates unwanted noise in the logs, metadata converter should only emit a warning if the new logical type is not null.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/577"
        ]
    },
    "PARQUET-1477": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Thrift crypto updates",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "14/Dec/18 08:17",
        "Updated": "25/Mar/19 22:07",
        "Resolved": "25/Mar/19 22:07",
        "Description": "New/changed AAD fields, footer signing key, encrypted column metadata field, renamed\u00a0metadata fields.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/124",
            "https://github.com/apache/arrow/pull/4022"
        ]
    },
    "PARQUET-1478": {
        "Key": "PARQUET-1478",
        "Summary": "Can't read spec compliant, 3-level lists via parquet-proto",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "14/Dec/18 10:20",
        "Updated": "07/Jan/19 16:22",
        "Resolved": "07/Jan/19 16:22",
        "Description": "I noticed that ProtoInputOutputFormatTest doesn't test the following case properly: when lists are written using the spec compliant 3-level structure. The test actually doesn't write 3-level list, because the passed configuration is not used at all, a new one is created each time. See attached PR.\nWhen I fixed this test, it turned out that it is failing: now it writes the correct 3-level structure, but looks like the read path is broken. Is it indeed a bug, or I'm doing something wrong?",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/578"
        ]
    },
    "PARQUET-1479": {
        "Key": "PARQUET-1479",
        "Summary": "[Java] Arrow read write support",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Masayuki Takahashi",
        "Created": "15/Dec/18 09:40",
        "Updated": "15/Dec/18 21:55",
        "Resolved": null,
        "Description": "Native reader/writer implementation on C++ allow user directly read parquet file into Arrow Buffer and Write to parquet file from arrow buffer. However, these features are not supported on Java.\nSupports read/write arrow on Java.",
        "Issue Links": []
    },
    "PARQUET-1480": {
        "Key": "PARQUET-1480",
        "Summary": "INT96 to avro not yet implemented error should mention deprecation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Tim Sell",
        "Reporter": "Tim Sell",
        "Created": "21/Dec/18 03:49",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "07/Feb/19 12:33",
        "Description": "Since PARQUET-323, INT96 is marked as deprecated.\nIn parquet-mr AvroSchemaConverter, the convertINT96 method\u00a0throws\u00a0an error saying:\u00a0\"INT96 not yet implemented.\"\nIs this likely to be implemented, since INT96 is deprecated? or can we remove the \"yet\" and return a note of that.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/579"
        ]
    },
    "PARQUET-1481": {
        "Key": "PARQUET-1481",
        "Summary": "[C++] SEGV when reading corrupt parquet file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-4.0.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Wes McKinney",
        "Reporter": "Hatem Helal",
        "Created": "21/Dec/18 13:56",
        "Updated": "24/Dec/18 22:50",
        "Resolved": "24/Dec/18 22:50",
        "Description": ">>> import pyarrow.parquet as pq\n>>> pq.read_table('corrupt.parquet')\nfish: 'python' terminated by signal SIGSEGV (Address boundary error)\n\u00a0\nStack report from macOS:\n\u00a0\n0 libsystem_kernel.dylib 0x00007fff51164cee __psynch_cvwait + 10\n1 libsystem_pthread.dylib 0x00007fff512a1662 _pthread_cond_wait + 732\n2 libc++.1.dylib 0x00007fff4f04acb0 std::_1::condition_variable::wait(std::1::unique_lock<std::_1::mutex>&) + 18\n3 libc++.1.dylib 0x00007fff4f04b728 std::_1::assoc_sub_state::sub_wait(std::1::unique_lock<std::_1::mutex>&) + 46\n4 libparquet.11.dylib 0x0000000115512d00 std::_1::_assoc_state<arrow::Status>::move() + 48\n5 libparquet.11.dylib 0x00000001154faa15 parquet::arrow::FileReader::Impl::ReadTable(std::_1::vector<int, std::1::allocator<int> > const&, std::_1::shared_ptr<arrow::Table>*) + 1093\n6 libparquet.11.dylib 0x00000001154fb6fe parquet::arrow::FileReader::Impl::ReadTable(std::__1::shared_ptr<arrow::Table>*) + 350\n7 libparquet.11.dylib 0x00000001154fce47 parquet::arrow::FileReader::ReadTable(std::__1::shared_ptr<arrow::Table>*) + 23\n8 _parquet.so 0x000000011598d97b __pyx_pw_7pyarrow_8_parquet_13ParquetReader_9read_all(_object*, _object*, _object*) + 1035",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3242"
        ]
    },
    "PARQUET-1482": {
        "Key": "PARQUET-1482",
        "Summary": "[C++] Unable to read data from parquet file generated with parquetjs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Rylan Dmello",
        "Reporter": "Hatem Helal",
        "Created": "21/Dec/18 16:37",
        "Updated": "17/May/19 11:15",
        "Resolved": "06/Mar/19 21:46",
        "Description": "See attached file, when I debug:\n% ./parquet-reader feed1kMicros.parquet\nI see that the scanner->HasNext() always returns false.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3312"
        ]
    },
    "PARQUET-1483": {
        "Key": "PARQUET-1483",
        "Summary": "Parquet benchmarks fail with multiple threads",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gidon Gershinsky",
        "Created": "23/Dec/18 10:11",
        "Updated": "23/Dec/18 10:13",
        "Resolved": null,
        "Description": "Temp file name doesn't have a per-thread (or a random) part, so the benchmarks fail with an \"existing file\" exception.",
        "Issue Links": []
    },
    "PARQUET-1484": {
        "Key": "PARQUET-1484",
        "Summary": "[C++] Improve memory usage of FileMetaDataBuilder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "27/Dec/18 23:52",
        "Updated": "31/Dec/18 16:50",
        "Resolved": "31/Dec/18 16:50",
        "Description": "Changes in the PR for ARROW-3324 (https://github.com/apache/arrow/pull/3261) allow further improving the memory usage by avoiding a copy of the row group metadata inside the Finish() implementation of the FileMetaDataBuilder class.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3277"
        ]
    },
    "PARQUET-1485": {
        "Key": "PARQUET-1485",
        "Summary": "Snappy Decompressor/Compressor may cause direct memory leak",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "liupengcheng",
        "Reporter": "liupengcheng",
        "Created": "02/Jan/19 06:33",
        "Updated": "20/Nov/19 22:56",
        "Resolved": "12/Feb/19 10:33",
        "Description": "In out production environment, we encountered a direct memory oom issues caused by the direct buffer not released in time.\nAfter carefully checked the code, it seems that the some methods of SnappyDecompressor/SnappyCompressor would not release the direct buffer manually. If too much direct memory allocated and no GC happens, this bug may result in direct memory oom.\nMoreover, if the `-XX:+DisableImplicitGC` jvm option is specified, the direct memory oom would happen easily for large datasets.\nSeems that the problem still exist in the latest code.",
        "Issue Links": [
            "/jira/browse/PARQUET-1320",
            "/jira/browse/PARQUET-1533",
            "https://github.com/apache/parquet-mr/pull/581"
        ]
    },
    "PARQUET-1486": {
        "Key": "PARQUET-1486",
        "Summary": "ParentValueContainer.getConversionContainer does not handle FIXED schema types properly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Daniel Steffy",
        "Created": "04/Jan/19 15:59",
        "Updated": "04/Jan/19 15:59",
        "Resolved": null,
        "Description": "ParentValueContainer.getConversionContainer attempts to convert the value to (GenericData.Fixed) instead of the more broad (GenericFixed) class. GenericData.Fixed implements GenericFixed, and GenericFixed is the avro class that should be used instead of the more specific parquet-avro GenericData.Fixed.",
        "Issue Links": []
    },
    "PARQUET-1487": {
        "Key": "PARQUET-1487",
        "Summary": "Do not write original type for timezone-agnostic timestamps",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0,                                            format-2.7.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "07/Jan/19 14:49",
        "Updated": "09/Jan/19 12:42",
        "Resolved": "09/Jan/19 12:42",
        "Description": "Historically, the TIMESTAMP_MILLIS and TIMESTAMP_MICROS original types used for the INT64 physical type were always UTC-normalized.\nThe new TIMESTAMP logical type allows both UTC-normalized and timezone-agnostic timestamps and writes the legacy original types for compatibility reasons. However, the latter should only be written for UTC-normalized timestamps, because legacy readers are not prepared to handle timezone-agnostic timestamps correctly and the original type would just be misleading.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/125",
            "https://github.com/apache/parquet-mr/pull/585"
        ]
    },
    "PARQUET-1488": {
        "Key": "PARQUET-1488",
        "Summary": "UserDefinedPredicate throw NPE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Yuming Wang",
        "Created": "09/Jan/19 02:51",
        "Updated": "17/Jul/19 15:26",
        "Resolved": "17/Jul/19 15:26",
        "Description": "It throws\u00a0NullPointerException\u00a0after upgrade parquet to 1.11.0 when using UserDefinedPredicate.\nThe \u00a0UserDefinedPredicate is:\n\n\r\nnew UserDefinedPredicate[Binary] with Serializable {                                  \r\n  private val strToBinary = Binary.fromReusedByteArray(v.getBytes)                    \r\n  private val size = strToBinary.length                                               \r\n                                                                                      \r\n  override def canDrop(statistics: Statistics[Binary]): Boolean = {                   \r\n    val comparator = PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR   \r\n    val max = statistics.getMax                                                       \r\n    val min = statistics.getMin                                                       \r\n    comparator.compare(max.slice(0, math.min(size, max.length)), strToBinary) < 0 ||  \r\n      comparator.compare(min.slice(0, math.min(size, min.length)), strToBinary) > 0   \r\n  }                                                                                   \r\n                                                                                      \r\n  override def inverseCanDrop(statistics: Statistics[Binary]): Boolean = {            \r\n    val comparator = PrimitiveComparator.UNSIGNED_LEXICOGRAPHICAL_BINARY_COMPARATOR   \r\n    val max = statistics.getMax                                                       \r\n    val min = statistics.getMin                                                       \r\n    comparator.compare(max.slice(0, math.min(size, max.length)), strToBinary) == 0 && \r\n      comparator.compare(min.slice(0, math.min(size, min.length)), strToBinary) == 0  \r\n  }                                                                                   \r\n                                                                                      \r\n  override def keep(value: Binary): Boolean = {                                       \r\n    UTF8String.fromBytes(value.getBytes).startsWith(                                  \r\n      UTF8String.fromBytes(strToBinary.getBytes))                                     \r\n  }                                                                                   \r\n}                                                                                     \r\n\n\nThe stack trace is:\n\njava.lang.NullPointerException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anon$1.keep(ParquetFilters.scala:573)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anon$1.keep(ParquetFilters.scala:552)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:152)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$UserDefined.accept(Operators.java:377)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:181)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$And.accept(Operators.java:309)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)",
        "Issue Links": [
            "/jira/browse/PARQUET-1434",
            "/jira/browse/PARQUET-1489",
            "https://github.com/apache/parquet-mr/pull/663"
        ]
    },
    "PARQUET-1489": {
        "Key": "PARQUET-1489",
        "Summary": "Insufficient documentation for UserDefinedPredicate.keep(T)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "09/Jan/19 10:32",
        "Updated": "09/Jan/19 12:41",
        "Resolved": "09/Jan/19 12:41",
        "Description": "The parquet-mr library invokes\u00a0UserDefinedPredicate.keep(T)\u00a0with\u00a0null values. (This is the only way parquet-mr can check if a null fulfills the UDP.) This fact is not highlighted in the documentation of the method which can easily lead to an unexpected NPE.",
        "Issue Links": [
            "/jira/browse/PARQUET-1488",
            "https://github.com/apache/parquet-mr/pull/588"
        ]
    },
    "PARQUET-1490": {
        "Key": "PARQUET-1490",
        "Summary": "Add branch-specific Travis steps",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "09/Jan/19 16:41",
        "Updated": "14/Feb/19 10:36",
        "Resolved": "14/Feb/19 10:36",
        "Description": "The script for the main branch has to make sure that POM files in the master branch do not refer to SNAPSHOT versions.\nThe possiblity of scripts for feature branches will allow building a SNAPSHOT version of parquet-format and depending on it in the POM files.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/590"
        ]
    },
    "PARQUET-1491": {
        "Key": "PARQUET-1491",
        "Summary": "Conditional debug logging in InternalParquetRecordReader to reduce GC",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Artavazd Balaian",
        "Created": "11/Jan/19 21:07",
        "Updated": "14/Jan/19 12:26",
        "Resolved": "14/Jan/19 12:26",
        "Description": "Currently there is no check for the log level in https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.javaL249\u00a0which causes a lot of memory allocation and performance degradation.\nLink to parquet file which was used: https://drive.google.com/open?id=1xCMZrUPWvlS4KOFO8m9EmtkvDy-SiRHq\nScreenshot of Java Mission Control comparison with fix and without (link to the JFR files https://drive.google.com/open?id=1blSeF-AyAhQyRYaqVsihyzy7pJCJt7U3):",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/591"
        ]
    },
    "PARQUET-1492": {
        "Key": "PARQUET-1492",
        "Summary": "Remove protobuf install in travis build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Qinghui Xu",
        "Reporter": "Qinghui Xu",
        "Created": "14/Jan/19 12:03",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "15/Jan/19 12:08",
        "Description": "Now that we use maven protoc plugin (PARQUET-1280), we do not need any more to build protoc ourselves. So we could remove protobuf build at the beginning of the travis build.",
        "Issue Links": [
            "/jira/browse/PARQUET-1280",
            "https://github.com/apache/parquet-mr/pull/592"
        ]
    },
    "PARQUET-1493": {
        "Key": "PARQUET-1493",
        "Summary": "maven protobuf plugin not work properly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Junjie Chen",
        "Created": "16/Jan/19 08:25",
        "Updated": "16/Mar/21 10:16",
        "Resolved": "16/Mar/21 10:16",
        "Description": "I checked out master branch and executed \"mvn clean install -DskipTests\",  it failed to build with following error: \n[ERROR] Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.6.0.1:run (generate-sources) on project parquet-protobuf: Error extracting protoc for version 3.5.1: Unsupported platform: protoc-3.5.1-linux-x86_64.exe -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :parquet-protobuf\nAfter I updated the plugin version as same as protobuf version, it pass the build.\n\n\r\n--- a/parquet-protobuf/pom.xml\r\n+++ b/parquet-protobuf/pom.xml\r\n@@ -159,7 +159,7 @@\r\n       <plugin>\r\n         <groupId>com.github.os72</groupId>\r\n         <artifactId>protoc-jar-maven-plugin</artifactId>\r\n-        <version>3.6.0.1</version>\r\n+        <version>3.5.1</version>\r\n         <executions>\r\n           <execution>\r\n             <id>generate-sources</id>\r\n\n\nDo we need to set this version to ${protobuf.version}?",
        "Issue Links": []
    },
    "PARQUET-1494": {
        "Key": "PARQUET-1494",
        "Summary": "[C++] Can't access parquet statistics on binary columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Ildar",
        "Reporter": "Ildar",
        "Created": "18/Jan/19 16:52",
        "Updated": "23/Jan/19 19:50",
        "Resolved": "23/Jan/19 19:49",
        "Description": "Hi,\nI'm trying to use per-column statistics (min/max values) to filter out row groups while reading parquet file. But I don't see statistics built for binary columns. I noticed that\u00a0ApplicationVersion::HasCorrectStatistics()\u00a0discards statistics that have sort order\u00a0UNSIGNED and haven't been created by parquet-cpp. As I understand there used to be some issues in\u00a0parquet-mr\u00a0before. But do they still persist?\nFor example, I have parquet file created with\u00a0parquet-mr\u00a0version 1.10, it seems to have correct min/max values for binary columns. And\u00a0parquet-cpp\u00a0works fine for me if I remove this code from\u00a0HasCorrectStatistics()\u00a0func:\n\u00a0\n\n\r\nif (SortOrder::SIGNED != sort_order && !max_equals_min) {\r\n\u00a0 \u00a0 return false;\r\n}",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3441"
        ]
    },
    "PARQUET-1495": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Perform encoding before bloom filters write out",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "21/Jan/19 09:35",
        "Updated": "21/Jan/19 09:35",
        "Resolved": null,
        "Description": "When entropy of bloom filter is low say nearly all 0 or all 1, we may want to consider to use encoding to save space.",
        "Issue Links": []
    },
    "PARQUET-1496": {
        "Key": "PARQUET-1496",
        "Summary": "[Java] Update Scala for JDK 11 compatibility",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Jan/19 17:30",
        "Updated": "25/Oct/19 21:44",
        "Resolved": "25/Oct/19 21:44",
        "Description": "When trying to build the parquet-mr code on OSX Mojave with OpenJDK 10 and 9, the build fails for me in parquet-scala with:\n\n\r\n[INFO] --- maven-scala-plugin:2.15.2:compile (default) @ parquet-scala_2.10 ---\r\n[INFO] Checking for multiple versions of scala\r\n[INFO] includes = [**/*.java,**/*.scala,]\r\n[INFO] excludes = []\r\n[INFO] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-scala/src/main/scala:-1: info: compiling\r\n[INFO] Compiling 1 source files to /Users/uwe/tmp/apache-parquet-1.11.0/parquet-scala/target/classes at 1547922718010\r\n[ERROR] error: error while loading package, Missing dependency 'object java.lang.Object in compiler mirror', required by /Users/uwe/.m2/repository/org/scala-lang/scala-library/2.10.6/scala-library-2.10.6.jar(scala/package.class)\r\n[ERROR] error: error while loading package, Missing dependency 'object java.lang.Object in compiler mirror', required by /Users/uwe/.m2/repository/org/scala-lang/scala-library/2.10.6/scala-library-2.10.6.jar(scala/runtime/package.class)\r\n[ERROR] error: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found.\r\n[ERROR] at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:16)\r\n[ERROR] at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:17)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:40)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:40)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:61)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getClassByName(Mirrors.scala:99)\r\n[INFO] at scala.reflect.internal.Mirrors$RootsBase.getRequiredClass(Mirrors.scala:102)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass$lzycompute(Definitions.scala:264)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.ObjectClass(Definitions.scala:264)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass$lzycompute(Definitions.scala:263)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.AnyRefClass(Definitions.scala:263)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.specialPolyClass(Definitions.scala:1120)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass$lzycompute(Definitions.scala:407)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.RepeatedParamClass(Definitions.scala:407)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses$lzycompute(Definitions.scala:1154)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.syntheticCoreClasses(Definitions.scala:1152)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode$lzycompute(Definitions.scala:1196)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.symbolsNotPresentInBytecode(Definitions.scala:1196)\r\n[INFO] at scala.reflect.internal.Definitions$DefinitionsClass.init(Definitions.scala:1261)\r\n[INFO] at scala.tools.nsc.Global$Run.<init>(Global.scala:1290)\r\n[INFO] at scala.tools.nsc.Driver.doCompile(Driver.scala:32)\r\n[INFO] at scala.tools.nsc.Main$.doCompile(Main.scala:79)\r\n[INFO] at scala.tools.nsc.Driver.process(Driver.scala:54)\r\n[INFO] at scala.tools.nsc.Driver.main(Driver.scala:67)\r\n[INFO] at scala.tools.nsc.Main.main(Main.scala)\r\n[INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n[INFO] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n[INFO] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n[INFO] at java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n[INFO] at org_scala_tools_maven_executions.MainHelper.runMain(MainHelper.java:161)\r\n[INFO] at org_scala_tools_maven_executions.MainWithArgsInFile.main(MainWithArgsInFile.java:26)\n\nThis is because the referenced JARs were made for a Scala version not supporting JDK 11, we need to update to 2.12.",
        "Issue Links": [
            "/jira/browse/PARQUET-1551",
            "/jira/browse/PARQUET-1499",
            "https://github.com/apache/parquet-mr/pull/605",
            "https://github.com/apache/parquet-mr/pull/693"
        ]
    },
    "PARQUET-1497": {
        "Key": "PARQUET-1497",
        "Summary": "[Java]\u00a0javax annotations dependency missing for Java 11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Jan/19 17:31",
        "Updated": "27/Mar/19 12:57",
        "Resolved": "27/Mar/19 12:57",
        "Description": "When trying to build with OpenJDK 11, I get errors due to the Generated annotation not being resolved:\n\n\r\n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ parquet-format-structures ---\r\n\r\n[INFO] Changes detected - recompiling the module!\r\n\r\n[INFO] Compiling 51 source files to /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/classes\r\n\r\n[INFO] -------------------------------------------------------------\r\n\r\n[WARNING] COMPILATION WARNING :\r\n\r\n[INFO] -------------------------------------------------------------\r\n\r\n[WARNING] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/src/main/java/org/apache/parquet/format/event/Consumers.java: /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/src/main/java/org/apache/parquet/format/event/Consumers.java uses or overrides a deprecated API.\r\n\r\n[WARNING] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/src/main/java/org/apache/parquet/format/event/Consumers.java: Recompile with -Xlint:deprecation for details.\r\n\r\n[INFO] 2 warnings\r\n\r\n[INFO] -------------------------------------------------------------\r\n\r\n[INFO] -------------------------------------------------------------\r\n\r\n[ERROR] COMPILATION ERROR :\r\n\r\n[INFO] -------------------------------------------------------------\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/NanoSeconds.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/NanoSeconds.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/StringType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/StringType.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DataPageHeaderV2.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DataPageHeaderV2.java:[43,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/Statistics.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/Statistics.java:[41,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/SortingColumn.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/SortingColumn.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TimestampType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TimestampType.java:[42,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TimeUnit.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MilliSeconds.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MilliSeconds.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MicroSeconds.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MicroSeconds.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DecimalType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DecimalType.java:[45,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/LogicalType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MapType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/MapType.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ListType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ListType.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/EnumType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/EnumType.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DateType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DateType.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TimeType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TimeType.java:[42,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/IntType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/IntType.java:[44,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/NullType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/NullType.java:[44,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/JsonType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/JsonType.java:[42,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/BsonType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/BsonType.java:[42,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/UUIDType.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/UUIDType.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/IndexPageHeader.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/IndexPageHeader.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageEncodingStats.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageEncodingStats.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnChunk.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnChunk.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnMetaData.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnMetaData.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/KeyValue.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/KeyValue.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageLocation.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageLocation.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DictionaryPageHeader.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DictionaryPageHeader.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageHeader.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/PageHeader.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DataPageHeader.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/DataPageHeader.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/FileMetaData.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/FileMetaData.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnIndex.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnIndex.java:[41,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/OffsetIndex.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/OffsetIndex.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/SchemaElement.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/SchemaElement.java:[43,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/RowGroup.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/RowGroup.java:[37,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/ColumnOrder.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TypeDefinedOrder.java:[32,24] package javax.annotation does not exist\r\n\r\n[ERROR] /Users/uwe/tmp/apache-parquet-1.11.0/parquet-format-structures/target/generated-sources/thrift/org/apache/parquet/format/TypeDefinedOrder.java:[40,2] cannot find symbol\r\n\r\n\u00a0 symbol: class Generated\r\n\r\n[INFO] 71 errors",
        "Issue Links": [
            "/jira/browse/PARQUET-1551",
            "/jira/browse/PARQUET-1499",
            "https://github.com/apache/parquet-mr/pull/604"
        ]
    },
    "PARQUET-1498": {
        "Key": "PARQUET-1498",
        "Summary": "[Java] Add instructions to install thrift via homebrew",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "21/Jan/19 17:35",
        "Updated": "24/Jan/19 10:35",
        "Resolved": "24/Jan/19 10:35",
        "Description": "Instead of manually building it, one can also install it via homebrew which much more comfortable. As this is not the latest thrift version, you need to explicitly include it in your PATH.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/595"
        ]
    },
    "PARQUET-1499": {
        "Key": "PARQUET-1499",
        "Summary": "[parquet-mr] Add Java 11 to Travis",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Jan/19 18:20",
        "Updated": "13/Nov/19 12:00",
        "Resolved": "13/Nov/19 12:00",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1496",
            "/jira/browse/PARQUET-1497",
            "/jira/browse/PARQUET-1590",
            "/jira/browse/PARQUET-1551",
            "https://github.com/apache/parquet-mr/pull/596"
        ]
    },
    "PARQUET-1500": {
        "Key": "PARQUET-1500",
        "Summary": "Remove the Closables",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Jan/19 19:23",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "25/Jan/19 08:09",
        "Description": "Remove the closable class and replace it with-resources.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/597"
        ]
    },
    "PARQUET-1501": {
        "Key": "PARQUET-1501",
        "Summary": "v1.8.x to be fixed with PARQUET-952 solution",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.8.1,                                            1.8.2,                                            1.8.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Vijayakumar N",
        "Created": "22/Jan/19 07:35",
        "Updated": "22/Jan/19 08:42",
        "Resolved": "22/Jan/19 08:42",
        "Description": "The following issue fixed in AVro parquet v1.11.0. PARQUET-952: Avro union with single type fails with 'is not a group'\n\u00a0\nBut we require this fix in v1.8.x series also.",
        "Issue Links": []
    },
    "PARQUET-1502": {
        "Key": "PARQUET-1502",
        "Summary": "Convert FIXED_LEN_BYTE_ARRAY to arrow type in logicalTypeAnnotation if it is not null",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Done",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Yongyan Wang",
        "Reporter": "Yongyan Wang",
        "Created": "22/Jan/19 17:48",
        "Updated": "14/Feb/19 11:05",
        "Resolved": "23/Jan/19 19:23",
        "Description": "Convert FIXED_LEN_BYTE_ARRAY to arrow type in logicalTypeAnnotation if it is not null in SchemaConverter",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/593",
            "https://github.com/apache/parquet-mr/pull/593"
        ]
    },
    "PARQUET-1503": {
        "Key": "PARQUET-1503",
        "Summary": "Remove Ints Utility Class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "23/Jan/19 04:25",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "25/Jan/19 08:23",
        "Description": "public static int checkedCast(long value) {\r\n    int valueI = (int) value;\r\n    if (valueI != value) {\r\n      throw new IllegalArgumentException(String.format(\"Overflow casting %d to an int\", value));\r\n    }\r\n    return valueI;\r\n  }\r\n\n\nLink Here\nNow that the Parquet-MR project uses Java 1.8, this method can now be replaced with Java Math class and toIntExact method.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/598"
        ]
    },
    "PARQUET-1504": {
        "Key": "PARQUET-1504",
        "Summary": "Add an option to convert Parquet Int96 to Arrow Timestamp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Yongyan Wang",
        "Reporter": "Yongyan Wang",
        "Created": "23/Jan/19 19:25",
        "Updated": "14/Feb/19 11:04",
        "Resolved": "27/Jan/19 20:26",
        "Description": "Add an option to convert Parquet Int96 to Arrow Timestamp for SchemaConverter",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/594"
        ]
    },
    "PARQUET-1505": {
        "Key": "PARQUET-1505",
        "Summary": "Use Java 7 NIO StandardCharsets",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Jan/19 01:19",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "07/Feb/19 12:27",
        "Description": "Starting with Java 1.7, all JVMs need to support UTF-8 encoding.\u00a0 Using NIO StandardCharsets removes need to catch\u00a0UnsupportedEncodingException",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/599"
        ]
    },
    "PARQUET-1506": {
        "Key": "PARQUET-1506",
        "Summary": "Migrate from maven-thrift-plugin to thrift-maven-plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "24/Jan/19 10:28",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "24/Jan/19 12:51",
        "Description": "Hi guys,\nI'd would like to migrate from maven-thrift-plugin to thrift-maven-plugin.\nThe current one that we use is rather old.\nmaven-thrift-plugin (Aug 13, 2013) https://mvnrepository.com/artifact/org.apache.thrift.tools/maven-thrift-plugin/0.1.11\nthrift-maven-plugin (Jan 18, 2017) https://mvnrepository.com/artifact/org.apache.thrift/thrift-maven-plugin/0.10.0\nThe maven-thrift-plugin is the old one which has been migrated to the ASF\nand continued as thrift-maven-plugin:\nhttps://issues.apache.org/jira/browse/THRIFT-4083\nI'm investigating bumping the thift version to make it Java 11 compatible. \nBut having a newer version of the maven plugin is a requirement:\nhttps://issues.apache.org/jira/browse/THRIFT-4083",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/600"
        ]
    },
    "PARQUET-1507": {
        "Key": "PARQUET-1507",
        "Summary": "Bump Apache Thrift to 0.12.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "24/Jan/19 19:25",
        "Updated": "11/Jun/19 12:15",
        "Resolved": "30/Jan/19 09:24",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1588",
            "https://github.com/apache/parquet-mr/pull/601"
        ]
    },
    "PARQUET-1508": {
        "Key": "PARQUET-1508",
        "Summary": "[C++] Enable reading from ByteArray and FixedLenByteArray decoders directly into arrow::BinaryBuilder or arrow::BinaryDictionaryBuilder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "25/Jan/19 06:25",
        "Updated": "14/Feb/19 17:03",
        "Resolved": "29/Jan/19 16:32",
        "Description": "Currently we are passing data through the parquet::ByteArray and parquet::FixedLenByteArray. I'm going to refactor the encoder/decoder classes to make them more extensible so we can avoid this intermediate layer",
        "Issue Links": [
            "/jira/browse/ARROW-3769",
            "/jira/browse/PARQUET-1374",
            "https://github.com/apache/arrow/pull/3492"
        ]
    },
    "PARQUET-1509": {
        "Key": "PARQUET-1509",
        "Summary": "Update Docs for Hive Deprecation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "25/Jan/19 14:16",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "27/Jan/19 21:43",
        "Description": "Update docs to state that Hive integration is now deprecated. PARQUET-1447",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/602"
        ]
    },
    "PARQUET-1510": {
        "Key": "PARQUET-1510",
        "Summary": "Dictionary filter skips null values when evaluating not-equals.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9.0,                                            1.10.0,                                            1.9.1",
        "Fix Version/s": "1.11.0,                                            1.10.1",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "25/Jan/19 16:43",
        "Updated": "28/Jan/19 17:07",
        "Resolved": "28/Jan/19 17:07",
        "Description": "This was discovered in Spark, see SPARK-26677. From the Spark PR:\n\n\r\n// Repeat the values to get dictionary encoding.\r\nSeq(Some(\"A\"), Some(\"A\"), None).toDF.repartition(1).write.mode(\"overwrite\").parquet(\"/tmp/foo\")\r\nspark.read.parquet(\"/tmp/foo\").where(\"NOT (value <=> 'A')\").show()\r\n+-----+\r\n|value|\r\n+-----+\r\n+-----+\r\n\n\n\n\r\n// Use plain encoding.\r\nSeq(Some(\"A\"), None).toDF.repartition(1).write.mode(\"overwrite\").parquet(\"/tmp/bar\")\r\nspark.read.parquet(\"/tmp/bar\").where(\"NOT (value <=> 'A')\").show()\r\n+-----+\r\n|value|\r\n+-----+\r\n| null|\r\n+-----+\r\n\n\nThis is a correctness issue.",
        "Issue Links": [
            "/jira/browse/PARQUET-1512",
            "/jira/browse/SPARK-26677",
            "https://github.com/apache/parquet-mr/pull/603"
        ]
    },
    "PARQUET-1511": {
        "Key": "PARQUET-1511",
        "Summary": "Pass the parameters in a configuration object to SchemaConverter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yongyan Wang",
        "Created": "25/Jan/19 17:37",
        "Updated": "25/Jan/19 17:37",
        "Resolved": null,
        "Description": "Pass the parameters in a configuration object to SchemaConverter for converting Parquet Int96 to Arrow Timestamp, and other options\u00a0that might be added later.",
        "Issue Links": []
    },
    "PARQUET-1512": {
        "Key": "PARQUET-1512",
        "Summary": "Release Parquet Java 1.10.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10.1",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "25/Jan/19 20:26",
        "Updated": "05/Feb/19 00:54",
        "Resolved": "05/Feb/19 00:54",
        "Description": "This is an umbrella issue to track the 1.10.1 release. Please link issues to include in the release as blockers.",
        "Issue Links": [
            "/jira/browse/PARQUET-1510",
            "/jira/browse/PARQUET-1309"
        ]
    },
    "PARQUET-1513": {
        "Key": "PARQUET-1513",
        "Summary": "HiddenFileFilter Streamline",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "27/Jan/19 18:54",
        "Updated": "14/Feb/19 11:03",
        "Resolved": "27/Jan/19 19:44",
        "Description": "public boolean accept(Path p) {\r\n    return !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\r\n  }\r\n\n\nThis can be streamlined a bit further.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/606"
        ]
    },
    "PARQUET-1514": {
        "Key": "PARQUET-1514",
        "Summary": "ParquetFileWriter Records Compressed Bytes instead of Uncompressed Bytes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "27/Jan/19 21:38",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "06/Feb/19 09:21",
        "Description": "As I understand it, the class BlockMetaData has a getTotalByteSize and a getCompressedSize.  However, in the case of ParquetFileWriter, there is one place where the compressed size is being written into totalByteSize instead of the uncompressed size.  This looks incorrect.\nParquetFileWriter\n\r\nprivate BlockMetaData currentBlock;\r\n...\r\npublic void appendRowGroup(SeekableInputStream from, BlockMetaData rowGroup,\r\n    boolean dropColumns) throws IOException {\r\n    ...\r\n    currentBlock.setTotalByteSize(blockCompressedSize);\r\n}",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/607"
        ]
    },
    "PARQUET-1515": {
        "Key": "PARQUET-1515",
        "Summary": "[C++] Disable LZ4 codec",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Deepak Majeti",
        "Created": "28/Jan/19 02:25",
        "Updated": "22/Aug/22 11:36",
        "Resolved": null,
        "Description": "As discussed in https://issues.apache.org/jira/browse/PARQUET-1241,\u00a0the parquet-cpp's\u00a0LZ4 codec is not compatible with Hadoop and parquet-mr. We must disable the codec until we resolve the compatibility issues.",
        "Issue Links": [
            "/jira/browse/PARQUET-1878"
        ]
    },
    "PARQUET-1516": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Store Bloom filters near to footer.",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "28/Jan/19 03:49",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "26/Feb/20 15:59",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/608"
        ]
    },
    "PARQUET-1517": {
        "Key": "PARQUET-1300 [C++] Parquet modular encryption",
        "Summary": "[C++] Update cpp crypto package to match signed-off specification",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "29/Jan/19 09:05",
        "Updated": "29/May/19 14:57",
        "Resolved": "29/May/19 14:57",
        "Description": "An initial version of crypto package is merged. This Jira updates the crypto code to\u00a0\n\nconform the signed off specification (wire protocol updates, signature tag creation, AAD support, etc)\nimprove performance by extending cipher lifecycle to file writing/reading - instead of creating cipher on each encrypt/decrypt operation",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3520"
        ]
    },
    "PARQUET-1518": {
        "Key": "PARQUET-1518",
        "Summary": "Bump Jackson2 version of parquet-cli",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cli",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "29/Jan/19 12:33",
        "Updated": "04/Feb/19 20:35",
        "Resolved": "04/Feb/19 20:35",
        "Description": "There are some vulnerabilities:\nhttps://ossindex.sonatype.org/vuln/1205a1ec-0837-406f-b081-623b9fb02992\nhttps://ossindex.sonatype.org/vuln/b85a00e3-7d9b-49cf-9b19-b73f8ee60275\nhttps://ossindex.sonatype.org/vuln/4f7e98ad-2212-45d3-ac21-089b3b082e6c\nhttps://ossindex.sonatype.org/vuln/ab9013f0-09a2-4f01-bce5-751dc7437494\nhttps://ossindex.sonatype.org/vuln/3f596fc0-9615-4b93-b30a-d4e0532e667f\nhttps://ossindex.sonatype.org/vuln/4f7e98ad-2212-45d3-ac21-089b3b082e6c",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/609"
        ]
    },
    "PARQUET-1519": {
        "Key": "PARQUET-1519",
        "Summary": "[C++] Remove use of \"extern template class\" from parquet/column_reader.h",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "29/Jan/19 21:13",
        "Updated": "30/Jan/19 15:45",
        "Resolved": "30/Jan/19 15:45",
        "Description": "These declarations are doing little to improve compile times, and they produce esoteric linking issues. Like parquet/encoding.h, I'll also refactor these classes into virtual interfaces and move the implementation into column_reader.cc",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3525"
        ]
    },
    "PARQUET-1520": {
        "Key": "PARQUET-1520",
        "Summary": "Update README to use correct build and version info",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "1.10.2",
        "Component/s": "None",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "31/Jan/19 03:43",
        "Updated": "31/Jan/19 19:59",
        "Resolved": "31/Jan/19 19:44",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/610"
        ]
    },
    "PARQUET-1521": {
        "Key": "PARQUET-1521",
        "Summary": "[C++] Do not use \"extern template class\" with parquet::ColumnWriter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Feb/19 14:59",
        "Updated": "06/Feb/19 02:28",
        "Resolved": "05/Feb/19 14:22",
        "Description": "As continued cleaning, similar to parquet::TypedColumnReader I will do similar refactoring for parquet::TypedColumnWriter, leaving the current public API for writing columns unchanged. This also makes symbol visibility less brittle",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3551"
        ]
    },
    "PARQUET-1522": {
        "Key": "PARQUET-1522",
        "Summary": "Bump Parquet version to 1.10.1 on website",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris Aniszczyk",
        "Reporter": "Fokko Driesprong",
        "Created": "04/Feb/19 20:57",
        "Updated": "04/Feb/19 20:57",
        "Resolved": null,
        "Description": "It still points to 1.9.0:\nhttps://svn.apache.org/repos/asf/parquet/site/source/downloads.html.md",
        "Issue Links": []
    },
    "PARQUET-1523": {
        "Key": "PARQUET-1523",
        "Summary": "[C++] Vectorize comparator interface",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "04/Feb/19 21:07",
        "Updated": "02/May/19 20:53",
        "Resolved": "02/May/19 20:16",
        "Description": "The parquet::Comparator interface yields scalar virtual calls on the innermost loop. In addition to removing the usage of PARQUET_TEMPLATE_EXPORT as with other recent patches, I propose to refactor to a vector-based comparison to update the minimum and maximum elements in a single virtual call\ncc mdeepak xhochy",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3752",
            "https://github.com/apache/arrow/pull/4233"
        ]
    },
    "PARQUET-1524": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1525": {
        "Key": "PARQUET-1525",
        "Summary": "[C++] remove dependency on getopt in parquet tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "01/Feb/19 22:30",
        "Updated": "05/Feb/19 15:54",
        "Resolved": "05/Feb/19 15:54",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3545"
        ]
    },
    "PARQUET-1526": {
        "Key": "PARQUET-1526",
        "Summary": "[C++] parquet cpp - improve examples",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Rajeshwar Agrawal",
        "Created": "06/Dec/18 08:41",
        "Updated": "08/Feb/19 04:33",
        "Resolved": null,
        "Description": "It would be a great to have examples of using parquet arrow high-level API for the following 2 cases\n\nStoring nested data types (storing nested data types is touted as major merit of parquet, so I think this case should be included as an example).\u00a0Ideally, an example of how to use arrow::StructArray nested with several primities types, list types and other nested types would cover every case of nested hierarchy of complex data representations\nBuffered or Batched writes to parquet file. Parquet is meant to be used for large amounts of data. The current examples stores all of the data as in arrow data structures, before writing to parquet file, which has a huge memory footprint, proportional to the amount of data being stored. An example of writing directly to row groups and columns, can nicely demonstrate how to store data with smaller memory footprint. The current example\u00a0creates a arrow::Table, which needs to be filled with arrow::Array(s) of entire data, size of which is bounded by the amount of RAM.\u00a0Ideally, an example which generates some data in several arrow::Array(s), and then stores (appends) them as a new Row Group (or Column Chunk) in a parquet::arrow::FileWriter, using NewRowGroup and WriteColumnChunk functions, thus demonstrating a lower memory footprint for writing a parquet file with huge amounts of data",
        "Issue Links": []
    },
    "PARQUET-1527": {
        "Key": "PARQUET-1527",
        "Summary": "[parquet-tools] cat command throw java.lang.ClassCastException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "11/Feb/19 07:59",
        "Updated": "14/Feb/19 10:35",
        "Resolved": "12/Feb/19 12:04",
        "Description": "I download sources from release/1.11.0 and built.\nhttps://github.com/apache/parquet-mr/releases/tag/apache-parquet-1.11.0 \nAs I ran cat command of parquet-tools, the below exception occurred. \n\n\r\njava -jar ./parquet-tools-1.11.0.jar cat D:\\development\\repository\\git\\parquet-column-index-benchmark\\src\\test\\res ources\\v11.parquet\r\njava.lang.ClassCastException: required int32 int32_field is not a group\r\n\n\nThe schema of v11.parquet is below:\n\n\r\njava -jar ./parquet-tools-1.11.0.jar schema D:\\development\\repository\\git\\parquet-column-index-benchmark\\src\\test\\ resources\\v11.parquet\r\nmessage test {\r\n  required int32 int32_field;\r\n  required int64 int64_field;\r\n  required float float_field;\r\n  required double double_field;\r\n  required binary binary_field;\r\n  required int64 timestamp_field (TIMESTAMP(MILLIS,true));\r\n}",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/612"
        ]
    },
    "PARQUET-1528": {
        "Key": "PARQUET-1528",
        "Summary": "Add JSON support to `parquet-tools head`",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Rapha\u00ebl Afanyan",
        "Reporter": "David Nies",
        "Created": "12/Feb/19 09:29",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Oct/20 10:39",
        "Description": "parquet-tools cat supports JSON output via the -j flag. It would be really handy if parquet-tools head would support this too.",
        "Issue Links": []
    },
    "PARQUET-1529": {
        "Key": "PARQUET-1529",
        "Summary": "Shade fastutil in all modules where used",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "12/Feb/19 15:46",
        "Updated": "30/Apr/20 19:55",
        "Resolved": "13/Feb/19 15:59",
        "Description": "The dependency fastutil is shaded in the module parquet-column while also used in parquet-avro and parquet-hadoop. As parquet-avro and parquet-hadoop refer to the original packages of fastutil they won't work if fastutil is not on the classpath. Let's shade fastutil in every module it is used.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/617"
        ]
    },
    "PARQUET-1530": {
        "Key": "PARQUET-1530",
        "Summary": "Remove Dependency on commons-codec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Feb/19 19:25",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "05/Sep/19 06:57",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1194",
            "https://github.com/apache/parquet-mr/pull/618"
        ]
    },
    "PARQUET-1531": {
        "Key": "PARQUET-1531",
        "Summary": "Page row count limit causes empty pages to be written from MessageColumnIO",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Matt Cheah",
        "Created": "15/Feb/19 02:02",
        "Updated": "27/Jul/20 10:36",
        "Resolved": "13/Mar/19 07:26",
        "Description": "This originally manifested as https://issues.apache.org/jira/browse/SPARK-26874\u00a0but we realized that this is fundamentally an issue in the way PARQUET-1414's solution interacts with MessageColumnIO, where Spark is one such user of that API.\nIn MessageColumnIO#endMessage(), we first examine if any fields are missing and fill in the values with null in MessageColumnIO#writeNullForMissingFieldsAtCurrentLevel. However, this method might not actually write any nulls to the underlying page. MessageColumnIO can buffer nulls in memory and flush them to the page store lazily.\nRegardless of whether or not nulls are flushed to the page store, in MessageColumnIO#endMessage we always call columns#endRecord() which will signal to the ColumnWriteStore that a record was written. At that point, the write store\u00a0increments the row count for the current page by 1, and then check if the page needs to be flushed due to hitting the page row count limit.\nThe problem is that with the above writing scheme, MessageColumnIO can cause empty pages to be written to Parquet files, and empty pages are not readable by Parquet readers. Suppose the page row count limit is N, and the MessageColumnIO receives N nulls for a column. The MessageColumnIO will buffer the nulls in memory, and doesn't necessarily flush the nulls to the writer yet. On the Nth call to endMessage(), however, the column store will think there are N values in memory and that the page has hit the row count limit, despite the fact that no rows have actually been written at all. But the underlying page writer will write an empty page regardless.\nTo illustrate the problem, one can try running this simple example inserted into Spark's ParquetIOSuite when Spark has been upgraded to use the master\u00a0branch of Parquet. Attach a debugger to MessageColumnIO#endMessage() and trace the logic accordingly - the column writer will push a page with 0 values:\n\n\r\ntest(\"PARQUET-1414 Problems\") {\r\n  // Manually adjust the maximum row count to reproduce the issue on small data\r\n  sparkContext.hadoopConfiguration.set(\"parquet.page.row.count.limit\", \"1\")\r\n  withTempPath { location =>\r\n    val path = new Path(location.getCanonicalPath + \"/parquet-data\")\r\n    val schema = StructType(\r\n      Array(StructField(\"timestamps1\", ArrayType(TimestampType))))\r\n    val rows = ListBuffer[Row]()\r\n    for (j <- 0 until 10) {\r\n      rows += Row(\r\n        null.asInstanceOf[Array[java.sql.Timestamp]])\r\n    }\r\n    val srcDf = spark.createDataFrame(\r\n      sparkContext.parallelize(rows, 3),\r\n      schema,\r\n      true)\r\n    srcDf.write.parquet(path.toString)\r\n    assert(spark.read.parquet(path.toString).collect.size > 0)\r\n  }\r\n}",
        "Issue Links": [
            "/jira/browse/PARQUET-1434",
            "/jira/browse/SPARK-26874",
            "https://github.com/apache/parquet-mr/pull/620"
        ]
    },
    "PARQUET-1532": {
        "Key": "PARQUET-1532",
        "Summary": "[C++] Can't build column reader test with MinGW",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "17/Feb/19 03:53",
        "Updated": "17/Feb/19 21:54",
        "Resolved": "17/Feb/19 20:57",
        "Description": "In file included from C:/msys64/mingw64/include/thrift/TApplicationException.h:23,\r\n                 from C:/Users/kou/work/arrow/arrow.kou/cpp/src/parquet/thrift.h:35,\r\n                 from C:/Users/kou/work/arrow/arrow.kou/cpp/src/parquet/column_reader.cc:36:\r\nC:/msys64/mingw64/include/thrift/Thrift.h:32:10: fatal error: netinet/in.h: No such file or directory\r\n #include <netinet/in.h>\r\n          ^~~~~~~~~~~~~~",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3676"
        ]
    },
    "PARQUET-1533": {
        "Key": "PARQUET-1533",
        "Summary": "TestSnappy() throws OOM exception with Parquet-1485 change",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Xinli Shang",
        "Created": "18/Feb/19 19:57",
        "Updated": "13/Nov/19 12:18",
        "Resolved": "25/Feb/19 12:26",
        "Description": "Parquet-1485 initialize the buffer size(inputBuffer and outputBuffer) from 0 to 128M in total. This cause the unit test\u00a0TestSnappy() failed with\u00a0OOM exception. This is on my Mac laptop.\u00a0\nTo solve the unit test failure, we can increase the size of -Xmx from 512m to 1024m like below. However, we need to evaluate whether or not the increase of the initial size of direct memory usage for inputBuffer and outputBuffer will cause real Parquet application OOM or not, if that application is not with big enough -Xmx size.\u00a0\n<groupId>org.apache.maven.plugins</groupId>\n <artifactId>maven-surefire-plugin</artifactId>\n ...\n <argLine>-Xmx1014m</argLine>\n...\nFor details of the exception, the commit page (https://github.com/apache/parquet-mr/commit/7dcdcdcf0eb5e91618c443d4a84973bf7883d79b) has the detail.",
        "Issue Links": [
            "/jira/browse/PARQUET-1485",
            "https://github.com/apache/parquet-mr/pull/622"
        ]
    },
    "PARQUET-1534": {
        "Key": "PARQUET-1534",
        "Summary": "[parquet-cli] Argument error: Illegal character in opaque part at index 2 on Windows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "20/Feb/19 12:11",
        "Updated": "24/Oct/19 12:36",
        "Resolved": "24/Oct/19 12:36",
        "Description": "Some commands of parquet-cli don't work on Windows.\n\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main schema C:\\Users\\masayuki\\Downloads\\test.parquet                                                               Argument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.parquet          \r\n                                    \r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main csv-schema C:\\Users\\masayuki\\Downloads\\test.csv --record-name Test                                              \r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.csv              \r\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main convert-csv C:\\Users\\masayuki\\Downlo ads\\test.csv -o C:\\Users\\masayuki\\Downloads\\test-csv.parquet\r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.csv\r\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main convert C:\\Users\\masayuki\\Downloads\\ userdata1.avro -o C:\\Users\\masayuki\\Downloads\\userdata1.parquet\r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\userdata1.avro\r\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main to-avro C:\\Users\\masayuki\\Downloads\\ test.parquet -o C:\\Users\\masayuki\\Downloads\\test.avro\r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.parquet\r\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main cat C:\\Users\\masayuki\\Downloads\\test.parquet\r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.parquet\r\n\r\n$ java -cp target/classes;target/dependency/* org.apache.parquet.cli.Main head -n 10 C:\\Users\\masayuki\\Downloa ds\\test.parquet\r\nArgument error: Illegal character in opaque part at index 2: C:\\Users\\masayuki\\Downloads\\test.parquet\r\n\n\nThe following commands work on Windows.\n\nmeta\npages\ncheck-stats\ncolumn-index",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/627"
        ]
    },
    "PARQUET-1535": {
        "Key": "PARQUET-1535",
        "Summary": "[parquet-cli] dictionary command throw NPE when specified column isn't dictionary encoding",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Masayuki Takahashi",
        "Created": "20/Feb/19 12:26",
        "Updated": "20/Feb/19 12:26",
        "Resolved": null,
        "Description": "'dictionary' command of parquet-cli throw NPE when specified column isn't dictionary encoding.\n\n\r\n$ java -cp 'target/classes:target/dependency/*' org.apache.parquet.cli.Main dictionary /work/parquet-mr/data/test.parquet -c binary_field\r\nUnknown error\r\njava.lang.NullPointerException\r\n        at org.apache.parquet.cli.commands.ShowDictionaryCommand.run(ShowDictionaryCommand.java:78)\r\n        at org.apache.parquet.cli.Main.run(Main.java:147)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.parquet.cli.Main.main(Main.java:177)\r\n\n\nThe schema of 'test.parquet' is following:\n\n\r\n$ java -cp 'target/classes:target/dependency/*' org.apache.parquet.cli.Main meta /work/parquet-mr/data/test.parquet\r\n\r\nFile path:  /work/parquet-mr/data/test.parquet\r\nCreated by: parquet-mr version 1.12.0-SNAPSHOT (build 1e62e2e2ca903d4109480bc87ceec1dc954b6c92)\r\nProperties:\r\n  writer.model.name: example\r\nSchema:\r\nmessage test {\r\n  required int32 int32_field;\r\n  required int64 int64_field;\r\n  required float float_field;\r\n  required double double_field;\r\n  required binary binary_field;\r\n  required int64 timestamp_field (TIMESTAMP(MILLIS,true));\r\n}\r\n\r\n\r\nRow group 0:  count: 395  15.87 B records  start: 4  total: 6.120 kB\r\n--------------------------------------------------------------------------------\r\n                 type      encodings count     avg size   nulls   min / max\r\nint32_field      INT32     _   D     395       0.20 B     0       \"32\" / \"426\"\r\nint64_field      INT64     _   D     395       0.20 B     0       \"64\" / \"458\"\r\nfloat_field      FLOAT     _   _     395       4.13 B     0       \"1.0\" / \"395.0\"\r\ndouble_field     DOUBLE    _   _     395       8.13 B     0       \"2.0\" / \"396.0\"\r\nbinary_field     BINARY    _   D     395       2.98 B     0       \"0x6162636465666768696A6B6...\" / \"0x6162636465666768696A6B6...\"\r\ntimestamp_field  INT64     _   D     395       0.23 B     0       \"2018-11-04T12:41:15.123+0000\" / \"2018-11-04T12:47:49.123+0000\"\r\n\r\nRow group 1:  count: 395  15.92 B records  start: 6271  total: 6.142 kB\r\n--------------------------------------------------------------------------------\r\n                 type      encodings count     avg size   nulls   min / max\r\nint32_field      INT32     _   D     395       0.20 B     0       \"427\" / \"821\"\r\nint64_field      INT64     _   D     395       0.20 B     0       \"459\" / \"853\"\r\nfloat_field      FLOAT     _   _     395       4.13 B     0       \"396.0\" / \"790.0\"\r\ndouble_field     DOUBLE    _   _     395       8.13 B     0       \"397.0\" / \"791.0\"\r\nbinary_field     BINARY    _   D     395       3.03 B     0       \"0x6162636465666768696A6B6...\" / \"0x6162636465666768696A6B6...\"\r\ntimestamp_field  INT64     _   D     395       0.23 B     0       \"2018-11-04T12:47:50.123+0000\" / \"2018-11-04T12:54:24.123+0000\"\r\n\r\nRow group 2:  count: 234  16.53 B records  start: 12560  total: 3.777 kB\r\n--------------------------------------------------------------------------------\r\n                 type      encodings count     avg size   nulls   min / max\r\nint32_field      INT32     _   D     234       0.17 B     0       \"822\" / \"1055\"\r\nint64_field      INT64     _   D     234       0.31 B     0       \"854\" / \"1087\"\r\nfloat_field      FLOAT     _   _     234       4.11 B     0       \"791.0\" / \"1024.0\"\r\ndouble_field     DOUBLE    _   _     234       8.21 B     0       \"792.0\" / \"1025.0\"\r\nbinary_field     BINARY    _   D     234       3.38 B     0       \"0x6162636465666768696A6B6...\" / \"0x6162636465666768696A6B6...\"\r\ntimestamp_field  INT64     _   D     234       0.35 B     0       \"2018-11-04T12:54:25.123+0000\" / \"2018-11-04T12:58:18.123+0000\"",
        "Issue Links": []
    },
    "PARQUET-1536": {
        "Key": "PARQUET-1536",
        "Summary": "[parquet-cli] Add simple tests for each command",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Masayuki Takahashi",
        "Reporter": "Masayuki Takahashi",
        "Created": "21/Feb/19 14:44",
        "Updated": "24/Oct/19 12:36",
        "Resolved": "24/Oct/19 12:36",
        "Description": "Currently, parquet-cli has no tests. At first, adding\u00a0simple tests for each command.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/625"
        ]
    },
    "PARQUET-1537": {
        "Key": "PARQUET-1537",
        "Summary": "[C++] The patch for PARQUET-1508 leads to infinite loop and infinite memory allocation when reading very sparse ByteArray columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Valery Meleshkin",
        "Created": "21/Feb/19 15:56",
        "Updated": "21/Mar/19 10:07",
        "Resolved": "21/Mar/19 10:07",
        "Description": "In this loop\nhttps://github.com/apache/arrow/commit/3d435e4f8d5fb7a54a4a9d285e1a42d60186d8dc#diff-47fe879cb9baad6c633c55f0a34a09c3R739\nThe branch of if dealing with null values does not increment variable 'i'. Therefore on chunks containing only NULLs once a thread enters the loop, it stays in that loop forever. I'm not entirely sure whether 'num_values' variable was meant to be the number of non-NULL values, yet the total number of values is passed here https://github.com/apache/arrow/blob/3d435e4f8d5fb7a54a4a9d285e1a42d60186d8dc/cpp/src/parquet/arrow/record_reader.cc#L528\n\u00a0\nOn my local machine adding `++i` to the NULL-handling branch seems to fix the problem. Unfortunately,\u00a0I'm not familiar with the codebase enough to be certain it's a proper fix.",
        "Issue Links": []
    },
    "PARQUET-1538": {
        "Key": "PARQUET-1373 Encryption key management tools",
        "Summary": "[C++] Encryption key management",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Tham",
        "Reporter": "Gidon Gershinsky",
        "Created": "24/Feb/19 07:36",
        "Updated": "04/Apr/21 11:41",
        "Resolved": "04/Apr/21 11:41",
        "Description": "C++ version of 1373",
        "Issue Links": []
    },
    "PARQUET-1539": {
        "Key": "PARQUET-1539",
        "Summary": "Clarify CRC checksum in page header",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Boudewijn Braams",
        "Reporter": "Boudewijn Braams",
        "Created": "24/Feb/19 20:49",
        "Updated": "13/Dec/22 14:44",
        "Resolved": "05/Mar/19 13:41",
        "Description": "Although a page-level CRC field is defined in the Thrift specification, currently neither parquet-cpp nor parquet-mr leverage it. Moreover, the comment in the Thrift specification reads \u201832bit crc for the data below\u2019, which is somewhat ambiguous to what exactly constitutes the \u2018data\u2019 that the checksum should be calculated on. To ensure backward- and cross-compatibility of Parquet readers/writes which do want to leverage the CRC checksums, the format should specify exactly how and on what data the checksum should be calculated.\nAlternatives\nThere are three main choices to be made here:\n\nWhich variant of CRC32 to use\nWhether to include the page header itself in the checksum calculation\nWhether to calculate the checksum on uncompressed or compressed data\n\nAlgorithm\nThe CRC field holds a 32-bit value. There are many different variants of the original CRC32 algorithm, each producing different values for the same input. For ease of implementation we propose to use the standard CRC32 algorithm.\nIncluding page header\nThe page header itself could be included in the checksum calculation using an approach similar to what TCP does, whereby the checksum field itself is zeroed out before calculating the checksum that will be inserted there. Evidently, including the page header is better in the sense that it increases the data covered by the checksum. However, from an implementation perspective, not including it is likely easier. Furthermore, given the relatively small size of the page header compared to the page itself, simply not including it will likely be good enough.\nCompressed vs uncompressed\nCompressed\n Pros\n\nInherently faster, less data to operate on\nPotentially better triaging when determining where a corruption may have been introduced, as checksum is calculated in a later stage\n\nCons\n\nWe have to trust both the encoding stage and the compression stage\n\nUncompressed\n Pros\n\nWe only have to trust the encoding stage\nPossibly able to detect more corruptions, as data is checksummed at earliest possible moment, checksum will be more sensitive to corruption introduced further down the line\n\nCons\n\nInherently slower, more data to operate on, always need to decompress first\nPotentially harder triaging, more stages in which corruption could have been introduced\n\nProposal\nThe checksum will be calculated using the standard CRC32 algorithm, whereby the checksum is to be calculated on the data only, not including the page header itself (simple implementation) and the checksum will be calculated on\u00a0compressed data\u00a0(inherently faster, likely better triaging).",
        "Issue Links": [
            "/jira/browse/PARQUET-2218",
            "https://github.com/apache/parquet-format/pull/126"
        ]
    },
    "PARQUET-1540": {
        "Key": "PARQUET-1540",
        "Summary": "[C++] Set shared library version for linux and mac builds",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Hatem Helal",
        "Created": "25/Feb/19 13:17",
        "Updated": "08/Jul/19 13:27",
        "Resolved": "06/Mar/19 10:20",
        "Description": "It looks like this was previously implemented when parquet-cpp was managed as a separate repo (PARQUET-935).\u00a0 It would be good to add this back now that parquet-cpp was incorporated into the arrow project.",
        "Issue Links": [
            "/jira/browse/ARROW-3185",
            "https://github.com/apache/arrow/pull/3743"
        ]
    },
    "PARQUET-1541": {
        "Key": "PARQUET-1541",
        "Summary": "ClassCastException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "the coder",
        "Created": "26/Feb/19 12:33",
        "Updated": "28/Feb/19 09:16",
        "Resolved": null,
        "Description": "Hi\nI'm writing protobuf to Parquet using ProtoParquetWriter, i'm also writing to local HDFS and write only\u00a0\u00a0one file.\nmy code running in a bit data environment.\nthe problem is that after about 30 min of run i'm starting to get exception like this:\n\u00a0\njava.lang.ClassCastException: org.apache.parquet.io.MessageColumnIO cannot be cast to org.apache.parquet.io.PrimitiveColumnIO\n\u00a0at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.getColumnWriter(MessageColumnIO.java:432)\n\u00a0at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addLong(MessageColumnIO.java:449)\n\u00a0at org.apache.parquet.proto.ProtoWriteSupport$LongWriter.writeRawValue(ProtoWriteSupport.java:294)\n\u00a0at org.apache.parquet.proto.ProtoWriteSupport$FieldWriter.writeField(ProtoWriteSupport.java:145)\n\u00a0at org.apache.parquet.proto.ProtoWriteSupport$MessageWriter.writeAllFields(ProtoWriteSupport.java:229)\n\u00a0at org.apache.parquet.proto.ProtoWriteSupport$MessageWriter.writeTopLevelMessage(ProtoWriteSupport.java:194)\n\u00a0\nI printed the message that cause to this error and I can't see any problem with it.\nany idea?",
        "Issue Links": []
    },
    "PARQUET-1542": {
        "Key": "PARQUET-1542",
        "Summary": "Merge multiple I/O to one time I/O when read footer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Wang, Gang",
        "Reporter": "Wang, Gang",
        "Created": "27/Feb/19 07:08",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "24/Sep/19 05:00",
        "Description": "Currently in method readFooter of class\u00a0ParquetFileReader, it will invoke method converter.readParquetMetadata. To decode file metadata,\u00a0\u00a0there\u00a0may be quite a number of times of I/O processing underlying, which may heavily slow down parquet read performance.\nA better way is to read all the footer bytes in a single I/O, since we have already got footer length before, and pass down all the bytes\u00a0to underlying.\n\u00a0\nAnd we have a test in our environment, this change may bring\u00a0 50 percent improvement for some cases.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/624"
        ]
    },
    "PARQUET-1543": {
        "Key": "PARQUET-1543",
        "Summary": "Execute the TIMESTAMP types roadmap",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "28/Feb/19 15:38",
        "Updated": "28/Feb/19 15:42",
        "Resolved": "28/Feb/19 15:42",
        "Description": "This is the top-level JIRA for tracking the addition and/or alteration of different TIMESTAMP types in order to eventually reach the desired state as specified in the design doc for TIMESTAMP types.",
        "Issue Links": [
            "https://cwiki.apache.org/confluence/display/Hive/Different+TIMESTAMP+types"
        ]
    },
    "PARQUET-1544": {
        "Key": "PARQUET-1544",
        "Summary": "Possible over-shading of modules",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Matt Cheah",
        "Created": "14/Mar/19 17:58",
        "Updated": "19/Mar/19 13:01",
        "Resolved": "19/Mar/19 13:01",
        "Description": "When upgrading Apache Iceberg to use what's effectively the master branch of Parquet, we hit this stack trace in one of the unit tests:\n\u00a0\njava.lang.NoSuchMethodError: org.apache.avro.Schema.setLogicalType(Lorg/apache/avro/LogicalType;)V at org.apache.avro.LogicalType.addToSchema(LogicalType.java:72) at com.netflix.iceberg.avro.TypeToSchema.<clinit>(TypeToSchema.java:42) at com.netflix.iceberg.avro.AvroSchemaUtil.convert(AvroSchemaUtil.java:69) at com.netflix.iceberg.avro.AvroSchemaUtil.convert(AvroSchemaUtil.java:65) at com.netflix.iceberg.PartitionData.getSchema(PartitionData.java:38) at com.netflix.iceberg.PartitionData.<init>(PartitionData.java:67) at com.netflix.iceberg.DataFiles.newPartitionData(DataFiles.java:41) at com.netflix.iceberg.DataFiles.access$000(DataFiles.java:36) at com.netflix.iceberg.DataFiles$Builder.<init>(DataFiles.java:225) at com.netflix.iceberg.DataFiles.builder(DataFiles.java:191) at com.netflix.iceberg.hive.HiveTablesTest.testConcurrentFastAppends(HiveTablesTest.java:108) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) at com.sun.proxy.$Proxy2.processTestClass(Unknown Source) at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) at java.lang.Thread.run(Thread.java:748)\nAt first it's not immediately obvious if this is a problem in Iceberg's dependency chain, or if it's a problem in how parquet-avro is packaged. But I'm filing here because it appears linked to PARQUET-1529, where we shaded FastUtil in various modules, including parquet-avro. A hypothesis is that applying the shading plugin caused references to Jackson 1 to be shaded as well, and that libraries that depend on both parquet-avro and Avro itself (as Iceberg does) are at risk of having mismatching implementations being loaded on the classpath.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/628"
        ]
    },
    "PARQUET-1545": {
        "Key": "PARQUET-1545",
        "Summary": "Logical Type for timezone-naive timestamps",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tim Swast`",
        "Created": "15/Mar/19 01:05",
        "Updated": "20/Mar/19 10:01",
        "Resolved": "20/Mar/19 10:01",
        "Description": "In\u00a0many systems there is a difference between a timezone-naive timestamp column (called DATETIME in BigQuery, 'logicalType': 'datetime' in Avro) and a timezone-aware timestamp (called TIMESTAMP in BigQuery and always stored in UTC). It seems from\u00a0this discussion\u00a0and the list of logical types\u00a0that\u00a0parquet only has the timezone-aware version, as all timestamps are stored according to UTC.",
        "Issue Links": []
    },
    "PARQUET-1546": {
        "Key": "PARQUET-1546",
        "Summary": "[C++] page level min / max written by parquet-cpp  is not recognized by parquet-tools",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "colin fang",
        "Created": "18/Mar/19 17:42",
        "Updated": "02/May/19 19:36",
        "Resolved": null,
        "Description": "test parquet is created by\n\n\r\nn = 1000000\r\nx = [1.0, 2.0, 3.0, 4.0, 5.0, 5.0, None] * n\r\ny = [u'\u00e9', u'\u00e9', u'\u00e9', u'\u00e9'] * n + [u'a', None, u'a'] * n\r\n\r\nz = np.random.rand(len(x)).tolist()\r\ndf = pd.DataFrame({'x': x, 'y': y, 'z': z})\r\ndf.to_parquet('test_arrow.parquet', use_dictionary=False, row_group_size= 1900100)\r\n\n\n\u00a0\noutput from parquet-tools\n\u00a0\n\n\r\n    y TV=1900100 RL=0 DL=1\r\n    ----------------------------------------------------------------------------\r\n    page 0:   DLE:RLE RLE:RLE VLE:PLAIN ST:[min: \u00e9, max: \u00e9, num_nulls: 0] SZ:1050632 VC:175104\r\n    page 1:   DLE:RLE RLE:RLE VLE:PLAIN ST:[num_nulls: 90072, min/max not defined] SZ:1083218 VC:294912\r\n    page 2:   DLE:RLE RLE:RLE VLE:PLAIN ST:[min: a, max: a, num_nulls: 105131] SZ:1091359 VC:315392\r\n    page 3:   DLE:RLE RLE:RLE VLE:PLAIN ST:[min: a, max: a, num_nulls: 105130] SZ:1091364 VC:315392\r\n\n\n\u00a0\n In the above \"min/max not defined\"\nThe parquet generated by `parquet-mr` has the correct page min\u00a0 max.\n\u00a0\nIt would be nice if it can show as \n\nST:[min: a, max: \u00e9, num_nulls: 90072]",
        "Issue Links": []
    },
    "PARQUET-1547": {
        "Key": "PARQUET-1547",
        "Summary": "[C++] Detect parquet-mr style dictionary_page",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "colin fang",
        "Created": "18/Mar/19 18:28",
        "Updated": "16/Aug/19 14:06",
        "Resolved": null,
        "Description": "parquet-mr incorrectly writes (dictionary_page_offset, first_data_page_offset) as (0, dictionary_page_offset)\nSo whenever parquet-cpp (pyarrow) reads the file, it sets `has_dictionary_page: False` and `dictionary_page_offset: None`\n\n\r\nrow group 0 \r\n--------------------------------------------------------------------------------\r\nx:  DOUBLE SNAPPY DO:0 FPO:4 SZ:1632/31635/19.38 VC:70000 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: 1.0, max: 5.0, num_nulls: 10000]\r\ny:  BINARY SNAPPY DO:0 FPO:1636 SZ:268/3885/14.50 VC:70000 ENC:RLE,BIT_PACKED,PLAIN_DICTIONARY ST:[min: a, max: \u00e9, num_nulls: 10000]\r\n\r\n    x TV=70000 RL=0 DL=1 DS: 5 DE:PLAIN_DICTIONARY\r\n    ----------------------------------------------------------------------------\r\n    page 0:                   DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY ST:[min: 1.0, max: 5.0, num_nulls: 10000] SZ:31514 VC:70000\r\n\r\n\n\n\n\r\n<pyarrow._parquet.ColumnChunkMetaData object at 0x7fd3effc1120>\r\n  file_offset: 4\r\n  file_path: \r\n  physical_type: DOUBLE\r\n  num_values: 70000\r\n  path_in_schema: x\r\n  is_stats_set: True\r\n  statistics:\r\n    <pyarrow._parquet.RowGroupStatistics object at 0x7fd3effc1cb0>\r\n      has_min_max: True\r\n      min: 1.0\r\n      max: 5.0\r\n      null_count: 10000\r\n      distinct_count: 0\r\n      num_values: 60000\r\n      physical_type: DOUBLE\r\n  compression: SNAPPY\r\n  encodings: ('PLAIN_DICTIONARY', 'RLE', 'BIT_PACKED')\r\n  has_dictionary_page: False\r\n  dictionary_page_offset: None\r\n  data_page_offset: 4\r\n  total_compressed_size: 1632\r\n  total_uncompressed_size: 31635\r\n\n\nIs parquet-cpp still able to use the dictionary in this case?\nIt would be nice if parquet-cpp can recognize the parquet-mr issue and set `has_dictionary_page` to True.\nhttps://stackoverflow.com/questions/55225108/why-is-dictionary-page-offset-0-for-plain-dictionary-encoding/",
        "Issue Links": []
    },
    "PARQUET-1548": {
        "Key": "PARQUET-1548",
        "Summary": "Meta data is lost when writing avro union types to parquet",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Michael O'Shea",
        "Reporter": "Michael O'Shea",
        "Created": "21/Mar/19 12:37",
        "Updated": "26/Mar/19 13:36",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1549": {
        "Key": "PARQUET-1549",
        "Summary": "Option for one block per file in MapReduce output",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gustavo Figueiredo",
        "Created": "25/Mar/19 02:35",
        "Updated": "25/Mar/19 13:02",
        "Resolved": null,
        "Description": "When we create PARQUET files using a MapReduce application with current ParquetOutputFormat implementation, we don't have any option to reliably limit the number of blocks (row groups) we want to generate per file.\nThe implemented configuration option 'parquet.block.size' (ParquetOutputFormat.BLOCK_SIZE) refers to the amount of data that goes into one block of data, but there are no guarantees that this will be the only block in a file. If one sets this configuration option to a very high value, it's likely there will be a single block per PARQUET file. However, this approach might lead to undesirably big files, so this would not be a good option in some scenarios.\nThis behaviour can't be achieved by the client's 'mapper' either. Although there are some helpfull classes in Hadoop API, such as 'MultipleOutputs', we don't have enough information available at 'mapper' code in order to have this kind of control, unless one uses unsafe 'hacks' to gather information from private fields.\nBy instance, suppose we have an ETL application that loads data from HBASE regions (might be one or more MAPs per region) and produces PARQUET files to be consumed in IMPALA tables (might be one or more PARQUET files per MAP task). To simplify, let's say there is no 'REDUCE' task in this application.\nFor concreteness, lets say one could use for such job 'org.apache.hadoop.hbase.mapreduce.TableInputFormat' as input and 'org.apache.parquet.hadoop.ParquetOutputFormat' as output. \nFollowing the guidelines for maximum query performance in Impala queries in HADOOP ecosystem, each PARQUET file should be approximately equal in size to a HDFS block and there should be only one single block of data (row group) in each of them (see https://impala.apache.org/docs/build/html/topics/impala_perf_cookbook.html#perf_cookbook__perf_cookbook_parquet_block_size).\nCurrently we are only able to do this by trial and error with different configuration options.\nIt would be nice to have a new boolean configuration option (lets call it 'parquet.split.file.per.block') related to the existing one 'parquet.block.size'. If it's set to false (default value), we would have the current behaviour. If it's to true, we would have one different PARQUET file being generated for each 'block' created, all coming from the same ParquetRecordWriter.\nIn doing so, we would only have to worry about tuning the 'parquet.block.size' parameter in order to generate PARQUET files with one single block per file whose size is closer to the configured HDFS block size.\nIn order to implement this new feature, we only need to change a few classes in 'org.apache.parquet.hadoop' package, namely:\n InternalParquetRecordWriter\n ParquetFileWriter\n ParquetOutputFormat\n ParquetRecordWriter\nBriefly, these are the changes needed:\n InternalParquetRecordWriter:\n The field 'ParquetFileWriter parquetFileWriter' should not be 'final' anymore, since we want to be able to change this throughout the task.\n The method 'checkBlockSizeReached' should call a new function 'startNewFile' just after a call to 'flushRowGroupToStore'.\n The new method 'startNewFile' should have all the logic for closing the current file and starting a new one at the same location with a proper filename.\n ParquetFileWriter\n The constructor argument 'OutputFile file' should be persisted as a new member field and made available by a new public method. This information is usefull for the 'startNewFile' implementation mentioned above.\n The field 'MessageType schema' should be available by a new public method. This information is also usefull for the 'startNewFile' implementation.\n ParquetOutputFormat\n The existing private method 'getMaxPaddingSize' should be made 'public' or at least 'package protected'. This information is usefull for the 'startNewFile' implementation mentioned above.\n The new configuration option 'parquet.split.file.per.block' should be specified here like the other ones. The new behaviour in 'InternalParquetRecordWriter' is conditioned on this configuration option.\n ParquetRecordWriter\n Just pass away the configuration option to the internal InternalParquetRecordWriter instance.",
        "Issue Links": []
    },
    "PARQUET-1550": {
        "Key": "PARQUET-1550",
        "Summary": "CleanUtil does not work in Java 11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Zoltan Ivanfi",
        "Created": "26/Mar/19 14:31",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "04/Jul/19 08:55",
        "Description": "I'm trying to run the tests with Java 11 using the mvn clean install command. After various dependency updates and some workarounds, the tests are green, but the output is littered with warnings about swallowed IllegalAccessExceptions caused by CleanUtil. One example of many indentical ones:\n\n\r\n2019-03-26 15:07:34 WARN CleanUtil - Clean failed for buffer DirectByteBuffer\r\njava.lang.IllegalAccessException: class org.apache.parquet.hadoop.codec.CleanUtil cannot access class jdk.internal.ref.Cleaner (in module java.base) because module java.base does not export jdk.internal.ref to unnamed module @413f69cc\r\n\tat java.base/jdk.internal.reflect.Reflection.newIllegalAccessException(Reflection.java:361)\r\n\tat java.base/java.lang.reflect.AccessibleObject.checkAccess(AccessibleObject.java:591)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:558)\r\n\tat org.apache.parquet.hadoop.codec.CleanUtil.clean(CleanUtil.java:64)\r\n\tat org.apache.parquet.hadoop.codec.SnappyDecompressor.setInput(SnappyDecompressor.java:109)\r\n\tat org.apache.parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:46)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:200)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:170)\r\n\tat org.apache.parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:279)\r\n\tat org.apache.parquet.hadoop.TestDirectCodecFactory.test(TestDirectCodecFactory.java:114)\r\n\tat org.apache.parquet.hadoop.TestDirectCodecFactory.compressionCodecs(TestDirectCodecFactory.java:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\r\n\tat org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)\r\n\tat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)\r\n\tat org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)\r\n\tat org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:107)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)",
        "Issue Links": [
            "/jira/browse/PARQUET-1551",
            "https://github.com/apache/parquet-mr/pull/654"
        ]
    },
    "PARQUET-1551": {
        "Key": "PARQUET-1551",
        "Summary": "Support Java 11 - top-level JIRA",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zoltan Ivanfi",
        "Created": "26/Mar/19 14:33",
        "Updated": "13/Nov/19 12:01",
        "Resolved": "13/Nov/19 12:01",
        "Description": "This JIRA groups all other JIRA-s related to Java 11.",
        "Issue Links": [
            "/jira/browse/PARQUET-1496",
            "/jira/browse/PARQUET-1497",
            "/jira/browse/PARQUET-1550",
            "/jira/browse/PARQUET-1554",
            "/jira/browse/PARQUET-1499",
            "/jira/browse/PARQUET-1605"
        ]
    },
    "PARQUET-1552": {
        "Key": "PARQUET-1552",
        "Summary": "upgrade protoc-jar-maven-plugin to 3.8.0",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "29/Mar/19 02:34",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "10/Jul/19 08:15",
        "Description": "Current protoc-jar-maven-plugin has a problem when building project after a proxy network. The latest release 3.8.0 version fix this issue.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/659"
        ]
    },
    "PARQUET-1553": {
        "Key": "PARQUET-1553",
        "Summary": "Support xxHash in Bloom filter",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "02/Apr/19 08:01",
        "Updated": "02/Apr/19 08:01",
        "Resolved": null,
        "Description": "xxHash is proved perform better than Murmur3Hash in case of small inputs.",
        "Issue Links": []
    },
    "PARQUET-1554": {
        "Key": "PARQUET-1554",
        "Summary": "Compilation error when upgrading Scrooge version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0,                                            1.12.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "02/Apr/19 14:00",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "03/Apr/19 10:23",
        "Description": "When upgrading Scrooge version to 19.1.0, the build fails with\n\n\r\n[510.1] failure: string matching regex `[A-Za-z_][A-Za-z0-9\\._]*' expected but `}' found\r\n\n\nThis is due to Javadoc style comment in IndexPageHeader struct. Changing the style of the comment would solve the failure.",
        "Issue Links": [
            "/jira/browse/PARQUET-1551",
            "https://github.com/apache/parquet-format/pull/127"
        ]
    },
    "PARQUET-1555": {
        "Key": "PARQUET-1555",
        "Summary": "Bump snappy-java to 1.1.7.3",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "03/Apr/19 08:53",
        "Updated": "24/Oct/19 12:34",
        "Resolved": "24/Oct/19 12:34",
        "Description": "Just to make sure that it compiles well against the latest 1.1.7.3 for Java9 compatibility.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/632"
        ]
    },
    "PARQUET-1556": {
        "Key": "PARQUET-1556",
        "Summary": "Problem with Maven repo specifications in POMs of dependencies in some development environments",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andy Grove",
        "Reporter": "Andy Grove",
        "Created": "03/Apr/19 15:16",
        "Updated": "13/Nov/19 11:58",
        "Resolved": "13/Nov/19 11:58",
        "Description": "Running mvn verify based on the instructions in the README results in this error\n\n\r\nCould not resolve dependencies for project org.apache.parquet:parquet-thrift:jar:1.11.0: Could not find artifact com.hadoop.gplcompression:hadoop-lzo:jar:0.4.16\n\nAs a workaround, the local ~/.m2/settings.xml file can be modified to include the twitter maven repo:\n\n\r\n<repository>\r\n  <id>twitter</id>\r\n  <name>twitter</name>\r\n  <url>http://maven.twttr.com</url>\r\n</repository>\r\n\n\nAfter adding this, mvn verify works. This should not be necessary though, since the artifact is a transitive dependency and the POM of the direct dependency (elephant-bird) contains the repo specification, which works in most environments.",
        "Issue Links": [
            "/jira/browse/PARQUET-1691",
            "https://github.com/apache/parquet-mr/pull/639"
        ]
    },
    "PARQUET-1557": {
        "Key": "PARQUET-1557",
        "Summary": "Replace deprecated Apache Avro methods",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "10/Apr/19 12:05",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "03/Jun/19 17:57",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/633",
            "https://github.com/apache/parquet-mr/pull/636"
        ]
    },
    "PARQUET-1558": {
        "Key": "PARQUET-1558",
        "Summary": "Use try-with-resource in Apache Avro tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "10/Apr/19 12:53",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "03/Jun/19 17:57",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/634"
        ]
    },
    "PARQUET-1559": {
        "Key": "PARQUET-1559",
        "Summary": "Add way to manually commit already written data to disk",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Victor",
        "Created": "10/Apr/19 14:21",
        "Updated": "06/Aug/20 09:40",
        "Resolved": "16/Apr/19 12:38",
        "Description": "I'm not exactly sure this is compliant with the way parquet works, but I have the following need:\n\nI'm using parquet-avro to write to a parquet file during a long running process\nI would like to be able from time to time to access the already written data\n\nSo I was expecting to be able to flush manually the file to ensure the data is on disk and then copy the file for preliminary analysis.\nIf it's contradictory to the way parquet works (for example there is something about metadata being at the footer of the file), what would then be the alternative?\nClosing the file and opening a new one to continue writing?\nCould this be supported directly by parquet-mr maybe? It would then write multiple files in that case.",
        "Issue Links": []
    },
    "PARQUET-1560": {
        "Key": "PARQUET-1560",
        "Summary": "[C++] Unable to read parquetjs-created file using low-level parquet-cpp API",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Rylan Dmello",
        "Reporter": "Rylan Dmello",
        "Created": "10/Apr/19 21:07",
        "Updated": "02/Jul/19 16:23",
        "Resolved": null,
        "Description": "Follow-up to PARQUET-1482: basic support for reading Parquet files with Data page V2 pages was added as a part of PARQUET-1482.\n\u00a0\nHowever, this was only added to the higher-level Arrow API, and not to the lower-level Parquet API. We could port this fix to the lower-level API so that more users can read Parquet files with Data page V2 pages.",
        "Issue Links": [
            "/jira/browse/PARQUET-458"
        ]
    },
    "PARQUET-1561": {
        "Key": "PARQUET-1561",
        "Summary": "Inconsistencies in the Parquet Delta Encoding specification",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Daniel Becker",
        "Reporter": "Daniel Becker",
        "Created": "11/Apr/19 16:58",
        "Updated": "09/Sep/19 15:36",
        "Resolved": "09/Sep/19 15:36",
        "Description": "There are several imprecise/inconsistent formulations in the specification of the Parquet Delta Encoding (https://github.com/apache/parquet-format/blob/master/Encodings.md).\n\nIn the beginning of the Delta Encoding section, it is written that\nWhen there are not enough values to encode a full block we pad with zeros (added to the frame of reference).\nFrom the parquet-mr implementation of Delta Encoding (https://github.com/apache/parquet-mr/blob/dc61e510126aaa1a95a46fe39bf1529f394147e9/parquet-column/src/main/java/org/apache/parquet/column/values/delta/DeltaBinaryPackingValuesWriterForInteger.java), it seems that when the number of elements does not fill a complete miniblock, we do use padding (otherwise the data would not always end on a byte boundary), but that short blocks are not padded, i.e. we do not add empty/unspecified miniblocks to the block and do not even set the bit width to zero for the remaining miniblocks (which is not very good in my opinion). The specification should be clearer on this point.\nIn the description of the header, it is written that\nthe block size is a multiple of 128 stored as VLQ int\nAccording to Wikipedia, VLQ is big-endian and the corresponding little-endian encoding us ULEB128 (https://en.wikipedia.org/wiki/Variable-length_quantity, https://en.wikipedia.org/wiki/LEB128). The parquet-mr implementation uses the little-endian format. The number encoding is called VLQ in the whole Delta Encoding specification, not just here.\nAs the implementaion is already in use, the best would be to update the specification to match the implementation.\nThe next line is:\nthe miniblock count per block is a diviser of the block size stored as VLQ int the number of values in the miniblock is a multiple of 32.\nThis should be stylistically improved. Also, divisor is spelled with an \u2018o\u2019. For example:\nthe miniblock count per block is a divisor of the block size such that their quotient, the number of values in a miniblock, is a multiple of 32; it is stored as a ULEB128 int\nIn the section describing the block:\nthe min delta is a VLQ int\nI think it should be more precise and say that the min delta is a zigzag VLQ/ULEB int as plain VLQ and ULEB are unsigned and the zigzag version is actually used in parquet-mr.\nLater in the same section:\nHaving multiple blocks allows us to escape values and restart from a new base value.\nThe reader may think that in each block, we have a new base value according to which we compute the delta of the next element, but it is not true. The base value is the very first value in the page, which is stored in the header. What the author meant is that we have a new min delta in each block that is the frame of reference for the deltas in that block (we subtract it from the deltas to make them non-negative), but in my opinion it is not clear from this sentence.\nIn the section describing the algorithm to encode the values (beginning with \u201cTo encode each delta block...\u201c), in step 2, it says this:\nEncode the first value as zigzag VLQ int\nThis is misleading as we do not store the first value of each block as a VLQ/ULEB int, only the very first value in the page is stored in such a way, in the header, not in each block. Generally I think the description of the algorithm could be more straightforward, I find it a little difficult to understand.\nIn the examples, the block sizes are not multiples of 128, but the specification requires that. Either the examples should be replaced with valid ones or it should be noted that this is to keep the examples shorter. Also, it would be useful to include examples with multiple blocks.\nIn the \u2018Characteristics\u2019 section, miniblock is written in two words, while in the rest of the specification, it is written as one.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/128",
            "https://github.com/apache/parquet-format/pull/129"
        ]
    },
    "PARQUET-1562": {
        "Key": "PARQUET-1562",
        "Summary": "ParquetWriter.Builder (and subclasses) instances are hard to reuse",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Victor",
        "Created": "12/Apr/19 11:56",
        "Updated": "12/Apr/19 11:56",
        "Resolved": null,
        "Description": "Hi,\nThe builders to create ParquetWriter and AvroParquetWriter are very useful, but unfortunately their instance are very hard to reuse when you want to build multiple instance of ParquetWriter for different files but similar settings.\nThis is a shame because that's one of the concern solved by the builder pattern: from one builder you can build multiple things with similar parameters.\nIt would be useful if either:\n\nit was possible to create a builder from a pre-existing builder (except for the file name)\nit was possible to change the file name specified in the builder (and not have it definable only via constructor of the builder).\n\nThis would allow people to pass a set of options in the form of a builder to an object responsible of generically instantiating a PaquetWriter for multiple files for example.",
        "Issue Links": []
    },
    "PARQUET-1563": {
        "Key": "PARQUET-1563",
        "Summary": "cannot read 'date' datatype which write by spark",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fan Mo",
        "Created": "12/Apr/19 17:55",
        "Updated": "17/Apr/19 18:27",
        "Resolved": "17/Apr/19 18:27",
        "Description": "I'm using spark 2.4.0 to write parquet file and try to use parquet-column-1.10.jar to read the data. All the primary datatypes are working however for the date datatype it gets some meanless number.\u00a0 For example, input date is '1970-04-26', output data is '115'. if I use Spark to read the data, it can get the correct date.\u00a0\nfollowing are my reader code:\nval reader = ParquetFileReader.open(HadoopInputFile.fromPath(new Path((\"testfile.snappy.parquet\")), new Configuration()))\nval schema = reader.getFooter.getFileMetaData.getSchema\nvar pages : PageReadStore = null\nwhile((pages = reader.readNextRowGroup()) != null) {\n val rows = pages.getRowCount\n val columnIO = new ColumnIOFactory().getColumnIO(schema)\n val recordReader = columnIO.getRecordReader(pages,new GroupRecordConverter(schema))\n (0L until rows).foreach\n{ _ : Long =>\r\n val simpleGroup = recordReader.read()\r\n println(simpleGroup)\r\n }\n}",
        "Issue Links": []
    },
    "PARQUET-1564": {
        "Key": "PARQUET-1564",
        "Summary": "Simplify the interface of the PartFileWriter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "17/Apr/19 12:38",
        "Updated": "17/Apr/19 12:39",
        "Resolved": "17/Apr/19 12:39",
        "Description": "The Path is not being used, so no sense in including it in the interface",
        "Issue Links": []
    },
    "PARQUET-1565": {
        "Key": "PARQUET-1565",
        "Summary": "[C++] SEGV in FromParquetSchema with corrupt file from PARQUET-1481",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-4.0.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Hatem Helal",
        "Created": "18/Apr/19 14:17",
        "Updated": "23/Apr/19 15:17",
        "Resolved": "23/Apr/19 15:17",
        "Description": "Calling parquet::arrow::FromParquetSchema when reading the corrupt file attached to PARQUET-1481 results in a SEGV.\u00a0\u00a0I'm not sure when this was introduced but I didn't observe this problem with our app that uses parquet-cpp v1.4.0.\u00a0\u00a0Our team caught this while integrating Arrow 0.12.1 into MATLAB.\u00a0\nTo reproduce this, add the following lines to parquet-reader.cc, build, and try to read the\u00a0corrupt file attached to PARQUET-1481.\n\n\r\n\u00a0 \u00a0 const auto parquet_schema = reader->metadata()->schema();\r\n\u00a0 \u00a0 std::shared_ptr<::arrow::Schema> arrow_schema;\r\n\u00a0 \u00a0 PARQUET_THROW_NOT_OK(parquet::arrow::FromParquetSchema(parquet_schema, &arrow_schema));",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4171"
        ]
    },
    "PARQUET-1566": {
        "Key": "PARQUET-1566",
        "Summary": "[C++] Indicate if null count, distinct count are present in column statistics",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Andrew Wieteska",
        "Reporter": "Wes McKinney",
        "Created": "01/May/19 22:37",
        "Updated": "30/Nov/20 11:11",
        "Resolved": "30/Nov/20 11:09",
        "Description": "One can determine if the min/max statistics are present with HasMinMax but the same is not possible with null_count or distinct_count",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/8774"
        ]
    },
    "PARQUET-1567": {
        "Key": "PARQUET-1567",
        "Summary": "[C++][Parquet] Bad initialization in statistics computation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Francois Saint-Jacques",
        "Created": "02/May/19 20:21",
        "Updated": "16/Aug/19 13:55",
        "Resolved": "16/Aug/19 13:55",
        "Description": "The following lines are undefined if the first element is null.\nhttps://github.com/apache/arrow/blob/250e97c70f497581bca412dfd2a654a1f9736064/cpp/src/parquet/statistics.cc#L159-L160",
        "Issue Links": []
    },
    "PARQUET-1568": {
        "Key": "PARQUET-1568",
        "Summary": "High level interface to Parquet encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "03/May/19 07:49",
        "Updated": "13/Jul/20 05:48",
        "Resolved": "13/Jul/20 05:48",
        "Description": "<to be updated> Per discussion at the community sync, we're working on a design of a high level interface to Parquet encryption. A draft doc will be published soon, that covers the current proposals from Xinli (cryptodata interface), Ryan (table properties) and Gidon (key tools, hadoop config) - and starts to\u00a0draft a unified/streamlined design based on them.",
        "Issue Links": [
            "/jira/browse/PARQUET-1178",
            "/jira/browse/PARQUET-1373",
            "/jira/browse/PARQUET-1396",
            "/jira/browse/PARQUET-1817",
            "/jira/browse/PARQUET-1854"
        ]
    },
    "PARQUET-1569": {
        "Key": "PARQUET-1569",
        "Summary": "[C++] Consolidate testing header files",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "03/May/19 21:45",
        "Updated": "06/May/19 16:54",
        "Resolved": "06/May/19 16:54",
        "Description": "Among\n\nparquet/test-util.h\nparquet/test-specialization.h\nparquet/util/test-common.h\n\nIt isn't clear what would would expect to find in each, nor is it obvious where to put new shared testing code. I will combine them all into parquet/test-util.h",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4252"
        ]
    },
    "PARQUET-1570": {
        "Key": "PARQUET-1570",
        "Summary": "Publish 1.11.0 to maven central",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Devin Smith",
        "Created": "07/May/19 21:25",
        "Updated": "18/Oct/19 07:50",
        "Resolved": "18/Oct/19 07:50",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1434"
        ]
    },
    "PARQUET-1571": {
        "Key": "PARQUET-1571",
        "Summary": "[C++] Can't read data from parquet file in C++ library",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "worker24h",
        "Created": "08/May/19 01:38",
        "Updated": "08/May/19 18:53",
        "Resolved": "08/May/19 15:51",
        "Description": "Specified the second param parquet::ReaderProperties When I used parquet::ParquetFileReader::Open, it can't work.\n The following code\uff1a\n\n\r\nparquet::ReaderProperties _properties;\r\n_properties = parquet::ReaderProperties(); \r\n_properties.enable_buffered_stream();  // used  buffer stream.  Don't set buffer-size\r\nparquet_reader = parquet::ParquetFileReader::Open(_parquet, _properties);\r\n...\r\nint32_t value;\r\nparquet::Int32Reader* int32_reader =\r\nstatic_cast<parquet::Int32Reader*>(column_reader.get());\r\nint32_reader->Skip(_current_line_of_group);// skip lines of processed.\r\nrows_read = int32_reader->ReadBatch(1, nullptr, nullptr, &value, &values_read);  \r\n\r\n\n\nThe interface\u00a0Skip\u00a0throw exception\uff1a\nCouldn't deserialize thrift: TProtocolException: Invalid data Deserializing page header failed.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4271",
            "https://github.com/apache/arrow/pull/4275"
        ]
    },
    "PARQUET-1572": {
        "Key": "PARQUET-1572",
        "Summary": "Clarify the definition of timestamp types",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "09/May/19 15:14",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "24/May/19 11:50",
        "Description": "The current definition only makes sense for the isUtcAdjusted=true case.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/130"
        ]
    },
    "PARQUET-1573": {
        "Key": "PARQUET-1573",
        "Summary": "Add a docker development image and use it in travis",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "14/May/19 15:58",
        "Updated": "14/May/19 16:06",
        "Resolved": null,
        "Description": "Having a reproducible local environment for development is worth for Parquet in particular because its dependency on thrift, the idea is to add a docker image for this and use it in travis.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/637"
        ]
    },
    "PARQUET-1574": {
        "Key": "PARQUET-1574",
        "Summary": "[C++] parquet-encoding-test failed with msvc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "14/May/19 23:18",
        "Updated": "08/Jul/19 13:26",
        "Resolved": "15/May/19 15:20",
        "Description": "std::vector.reserve should be replaced with resize",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4311"
        ]
    },
    "PARQUET-1575": {
        "Key": "PARQUET-1575",
        "Summary": "Parquet reader throws error \"Reading past RLE/BitPacking stream\" for parquet file with null values",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "shyam narayan singh",
        "Created": "15/May/19 08:46",
        "Updated": "16/May/19 05:55",
        "Resolved": null,
        "Description": "Recently moved from parquet 1.8.x to 1.12 recently.\nDataset has\u00a0> 20k null values to be written to a complex type. Earlier with 1.8.x, it would create single page but with 1.12 it creates 20 pages (parquet - 1414). Writing nulls to complex types has been optimised to be cached (null cache) that would be flushed on next non null encounter or explicit flush/close. With 1.8, it would have encountered explicit close and flush the null cache and write the page. But with 1.12, after encountering 20k values, the page is written prematurely.\n\u00a0\nBelow is the metadata dump in both cases.\n1.8 :\nindex._id TV=111396 RL=0 DL=2 ---------------------------------------------------------------------------- page 0: DLE:RLE RLE:BIT_PACKED VLE:PLAIN ST:[num_nulls: 111396, min/max not defined] SZ:8 VC:111396\n\u00a0\n1.12 :\nindex._index TV=111396 RL=0 DL=2 ---------------------------------------------------------------------------- page 0: DLE:RLE RLE:BIT_PACKED VLE:PLAIN ST:[no stats for this column] SZ:4 VC:0 ...... page 19: DLE:RLE RLE:BIT_PACKED VLE:PLAIN ST:[no stats for this column] SZ:8 VC:111396\nAll the pages in 1.12 except the last page have same metadata. Now the issue is when the parquet reader kicks in, it sees that the RLE is bit packed and reads 8 bytes which goes beyond the stream as the size is only 4 (Reading past RLE/BitPacking stream).",
        "Issue Links": []
    },
    "PARQUET-1576": {
        "Key": "PARQUET-1576",
        "Summary": "Upgrade to Avro 1.9.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro,                                            parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "15/May/19 17:36",
        "Updated": "24/Oct/19 12:42",
        "Resolved": "03/Jun/19 17:56",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1661",
            "https://github.com/apache/parquet-mr/pull/638",
            "https://github.com/apache/parquet-mr/pull/681"
        ]
    },
    "PARQUET-1577": {
        "Key": "PARQUET-1577",
        "Summary": "Remove duplicate license",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "15/May/19 19:01",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "03/Jun/19 17:56",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/640"
        ]
    },
    "PARQUET-1578": {
        "Key": "PARQUET-1578",
        "Summary": "Introduce Lambdas",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "15/May/19 19:05",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "07/Oct/19 07:36",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/641"
        ]
    },
    "PARQUET-1579": {
        "Key": "PARQUET-1579",
        "Summary": "Add Github PR template",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0,                                            format-2.7.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "15/May/19 19:45",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "14/Jun/19 08:29",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/135",
            "https://github.com/apache/parquet-mr/pull/642"
        ]
    },
    "PARQUET-1580": {
        "Key": "PARQUET-1580",
        "Summary": "Page-level CRC checksum verification for DataPageV1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Boudewijn Braams",
        "Reporter": "Boudewijn Braams",
        "Created": "17/May/19 09:03",
        "Updated": "22/Jan/21 06:39",
        "Resolved": "24/Jul/19 06:35",
        "Description": "Now that the specification with respect to page level checksums has been clarified (see https://jira.apache.org/jira/browse/PARQUET-1539), we can actually implement it. What needs to be done:\n\nImplement calculating and writing checksums on write path\nImplement checksum validation on read path\nMake writing out checksums and validation on reads optional (default off)\nImplement tests",
        "Issue Links": [
            "/jira/browse/PARQUET-1746",
            "https://github.com/apache/parquet-mr/pull/647"
        ]
    },
    "PARQUET-1581": {
        "Key": "PARQUET-1581",
        "Summary": "[C++] Fix undefined behavior in encoding.cc when num_dictionary_values is 0.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "17/May/19 18:21",
        "Updated": "08/Jul/19 13:26",
        "Resolved": "20/May/19 13:04",
        "Description": "There is code that potentially references a nullptr.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4336"
        ]
    },
    "PARQUET-1582": {
        "Key": "PARQUET-1582",
        "Summary": "[C++] Add ToString method ColumnDescriptor",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "17/May/19 19:12",
        "Updated": "20/May/19 21:41",
        "Resolved": "20/May/19 21:41",
        "Description": "This can help in certain debugging scenarios.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4338"
        ]
    },
    "PARQUET-1583": {
        "Key": "PARQUET-1583",
        "Summary": "[C++] Remove parquet::Vector class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/May/19 18:01",
        "Updated": "21/May/19 06:38",
        "Resolved": "21/May/19 06:38",
        "Description": "I'm not sure this code is needed anymore, added during the early days of the project in 2016",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4354"
        ]
    },
    "PARQUET-1584": {
        "Key": "PARQUET-1584",
        "Summary": "[C++] Crash tests with glog and MinGW build",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "20/May/19 21:41",
        "Updated": "01/Jan/23 06:03",
        "Resolved": "01/Jan/23 06:03",
        "Description": "When we enable glog with MinGW build, Parquet tests are only crashed with the following error:\n\n64/80 Test #65: parquet-column_reader-test .........***Failed    0.04 sec\r\nRunning main() from C:/repo/mingw-w64-gtest/src/googletest-release-1.8.1/googletest/src/gtest_main.cc\r\n[==========] Running 6 tests from 2 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 5 tests from TestPrimitiveReader\r\n[ RUN      ] TestPrimitiveReader.TestInt32FlatRequired\r\n[       OK ] TestPrimitiveReader.TestInt32FlatRequired (0 ms)\r\n[ RUN      ] TestPrimitiveReader.TestInt32FlatOptional\r\n[       OK ] TestPrimitiveReader.TestInt32FlatOptional (0 ms)\r\n[ RUN      ] TestPrimitiveReader.TestInt32FlatRepeated\r\n[       OK ] TestPrimitiveReader.TestInt32FlatRepeated (0 ms)\r\n[ RUN      ] TestPrimitiveReader.TestInt32FlatRequiredSkip\r\n[       OK ] TestPrimitiveReader.TestInt32FlatRequiredSkip (0 ms)\r\n[ RUN      ] TestPrimitiveReader.TestDictionaryEncodedPages\r\nlibunwind: _Unwind_Resume C:/repo/mingw-w64-clang/src/llvm-7.0.1.src/projects/libunwind/src/UnwindLevel1.c:391 - _Unwind_Resume() can't return\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\n\n\nhttps://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/24639044/job/munbig5qtkfxuqqc#L1553\nArrow tests aren't crashed.\nSee also: ARROW-5369",
        "Issue Links": []
    },
    "PARQUET-1585": {
        "Key": "PARQUET-1585",
        "Summary": "Update old external links in the code base",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0,                                            format-2.7.0",
        "Component/s": "None",
        "Assignee": "Zoltan Ivanfi",
        "Reporter": "Zoltan Ivanfi",
        "Created": "24/May/19 11:48",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "24/May/19 13:42",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/131",
            "https://github.com/apache/parquet-mr/pull/644"
        ]
    },
    "PARQUET-1586": {
        "Key": "PARQUET-1586",
        "Summary": "[C++] Add --dump options to parquet-reader tool to dump def/rep levels",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Renat Valiullin",
        "Reporter": "Renat Valiullin",
        "Created": "24/May/19 19:48",
        "Updated": "29/May/19 04:24",
        "Resolved": "26/May/19 12:21",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4385"
        ]
    },
    "PARQUET-1587": {
        "Key": "PARQUET-1587",
        "Summary": "Parquet Writer doesn't support to write list of Groups directly into hadoop",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Udit Joshi",
        "Created": "29/May/19 04:17",
        "Updated": "29/Jan/22 04:47",
        "Resolved": null,
        "Description": "Problem :-\u00a0\u00a0Parquet Writer doesn't support to write list of Groups directly into hadoop.\nCause of Problem :- Suitable classes are not available\n Constructors are not visible\nAll possible solution which i tried -\n\nParquetWriter\nParquetFileWriter\nExampleParquetWriter\nParquetRecordWriter\n\nGradle dependency\ncompile group: 'org.apache.parquet', name: 'parquet-hadoop', version: '1.10.1'\n\u00a0\nIs there any way to writer parquet file into Hadoop with GroupSupport?\n\u00a0\nProblem description\nI am trying to create the object of ParquetWriter class which accepts the argument (OutputFile, Mode, WriteSupport, CompressionCodecName, int, boolean, Configuration, int, ParquetProperties). But this constructor has default access modifier. I can't able to access it.\n\u00a0I also used ParquetFileWriter class but it doesn't show any group support.\n\u00a0\nParquetWriter\n\n\u00a0ParquetFileWriter\n\n\r\nParquetFileWriter writer = new ParquetFileWriter(HadoopOutputFile.fromPath(writePathFile, configuration), schema, Mode.CREATE, DEFAULT_BLOCK_SIZE, MAX_PADDING_SIZE_DEFAULT) ;\r\n writer.start();\n\n\u00a0\n\u00a0It creates\u00a0 to create parquet file in hadoop but can't facilitates to write\u00a0 List of Groups in parquet file\n\u00a0\nExampleParquetWriter\n\n\r\nParquetWriter<Group> writer = ExampleParquetWriter.builder(writePathFile).withConf(configuration).withType(getSchema()).build(); \r\n System.out.println(\"Number of groups to write:\" + groups.size());\r\n for (Group g : groups) {\r\n writer.write(g);\r\n }\u00a0\n\n\u00a0Doesn't support for writing\u00a0 parquet file in hadoop",
        "Issue Links": []
    },
    "PARQUET-1588": {
        "Key": "PARQUET-1588",
        "Summary": "Bump Apache Thrift to 0.12.0 in parquet-format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "03/Jun/19 17:26",
        "Updated": "12/Jun/19 09:20",
        "Resolved": "12/Jun/19 09:20",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1590",
            "/jira/browse/PARQUET-1507",
            "https://github.com/apache/parquet-format/pull/133"
        ]
    },
    "PARQUET-1589": {
        "Key": "PARQUET-1589",
        "Summary": "Bump Java to 8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "03/Jun/19 17:42",
        "Updated": "09/Sep/19 15:41",
        "Resolved": "09/Sep/19 15:41",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/134"
        ]
    },
    "PARQUET-1590": {
        "Key": "PARQUET-1590",
        "Summary": "[parquet-format] Add Java 11 to Travis",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "03/Jun/19 17:54",
        "Updated": "18/Jun/19 09:21",
        "Resolved": "18/Jun/19 09:21",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1588",
            "/jira/browse/PARQUET-1499",
            "/jira/browse/SPARK-24417",
            "https://github.com/apache/parquet-format/pull/136"
        ]
    },
    "PARQUET-1591": {
        "Key": "PARQUET-1591",
        "Summary": "Remove @author tags from the source",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "03/Jun/19 18:02",
        "Updated": "09/Sep/19 15:40",
        "Resolved": "09/Sep/19 15:40",
        "Description": "I greatly appreciate all the work and effort being done by the community, however, from the ASF perspective, there shouldn't be any author tags in the code.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/137"
        ]
    },
    "PARQUET-1592": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "update hash naming of bloom filter",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "11/Jun/19 12:11",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "31/Aug/19 01:51",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/138"
        ]
    },
    "PARQUET-1593": {
        "Key": "PARQUET-1593",
        "Summary": "Replace the example usage in parquet-cli's help message with an actually existent subcommand",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "11/Jun/19 15:22",
        "Updated": "10/Jan/20 12:53",
        "Resolved": "10/Jan/20 12:53",
        "Description": "The following description feels a bit weird since there's no command called \"create\" actually.\n\n\r\n  Examples:\r\n\r\n    # print information for create\r\n    parquet help create",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/646"
        ]
    },
    "PARQUET-1594": {
        "Key": "PARQUET-1594",
        "Summary": "Parquet File is not able to read from Spark and Hive",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Prashanth pampanna desai",
        "Created": "11/Jun/19 21:01",
        "Updated": "31/Mar/21 15:50",
        "Resolved": null,
        "Description": "Issue:\u00a0Caused by: java.io.IOException: Expected 35393 values in column chunk at maprfs:////path/date=20190605/caa63aa9-abfa-46e1-8221-10f6c669512d.parquet offset 4 but got 46402 values instead over 2 pages ending at file offset 341624\u00a0\nwe are getting Avro Serialized messages from kafka which are being consumed by Spring-kafka and converted into parquet gets persisted into MaprFS(hdfs) file system.\u00a0\ni have tried replicating the issue in local with same Avro file but i was able to read parquet successfully, I am not sure why the parquet being corrupted in HDFS .",
        "Issue Links": []
    },
    "PARQUET-1595": {
        "Key": "PARQUET-1595",
        "Summary": "Parquet proto writer de-nest Protobuf wrapper classes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ying Xu",
        "Created": "12/Jun/19 06:54",
        "Updated": "20/Mar/22 17:47",
        "Resolved": null,
        "Description": "Existing Parquet protobuf writer support preserves the structure of any Protobuf Message objects.\u00a0 This works well in most cases. However, when dealing with Protobuf wrapper messages,\u00a0users may prefer directly writing the de-nested value into the Parquet files, for ease of querying them directly (in query engine such as Hive/Presto).\u00a0\nProposal:\u00a0\n\nImplement a\u00a0control flag, e.g., enableDenestingWrappers, to control whether or not to denest Protobuf wrapper classes.\u00a0\nWhen this flag is set to true,\u00a0write the Protobuf wrapper classes as single primitive fields, based on the type of the wrapped value field.\n \u00a0\n\n\n\nProtobuf Type\nParquet Type\n\n\nBoolValue\nboolean\n\n\nBytesValue\nbinary\n\n\nDoubleValue\ndouble\n\n\nFloatValue\nfloat\n\n\nInt32Value\nint64 (32-bit, signed)\n\n\nInt64Value\nint64 (64-bit, signed)\n\n\nStringValue\nbinary (string)\n\n\nUInt32Value\nint64 (32-bit, unsigned)\n\n\nUInt64Value\nint64 (64-bit, unsigned)",
        "Issue Links": []
    },
    "PARQUET-1596": {
        "Key": "PARQUET-1596",
        "Summary": "PARQUET-1375 broke parquet-cli's to-avro command",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cli",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Kengo Seki",
        "Created": "12/Jun/19 15:19",
        "Updated": "07/Oct/19 09:25",
        "Resolved": "07/Oct/19 09:25",
        "Description": "Given the following JSON file:\n\n\r\n$ cat /tmp/sample.json \r\n{ \"id\": 1, \"name\": \"Alice\" }\r\n{ \"id\": 2, \"name\": \"Bob\" }\r\n{ \"id\": 3, \"name\": \"Carol\" }\r\n{ \"id\": 4, \"name\": \"Dave\" }\r\n\n\nusing to-avro on the master branch for converting this into avro fails with NPE:\n\n\r\n$ git branch -v\r\n* master 47398be7 PARQUET-1375: Upgrade to Jackson 2.9.9 (#616)\r\n$ mvn clean install -DskipTests\r\n\r\n(snip)\r\n\r\n[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ parquet-cli ---\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT.jar\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/pom.xml to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT.pom\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT-tests.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT-tests.jar\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT-runtime.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT-runtime.jar\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  14.769 s\r\n[INFO] Finished at: 2019-06-12T23:52:57+09:00\r\n[INFO] ------------------------------------------------------------------------\r\n$ mvn dependency:copy-dependencies\r\n\r\n(snip)\r\n\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main to-avro /tmp/sample.json -o /tmp/sample.avro\r\nUnknown error\r\njava.lang.RuntimeException: Failed on record 0\r\n\tat org.apache.parquet.cli.commands.ToAvroCommand.run(ToAvroCommand.java:120)\r\n\tat org.apache.parquet.cli.Main.run(Main.java:147)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.parquet.cli.Main.main(Main.java:177)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.avro.file.DataFileWriter.create(DataFileWriter.java:153)\r\n\tat org.apache.avro.file.DataFileWriter.create(DataFileWriter.java:145)\r\n\tat org.apache.parquet.cli.commands.ToAvroCommand.run(ToAvroCommand.java:112)\r\n\t... 3 more\r\n$ echo $?\r\n1\r\n\n\nBut with its previous revision, it succeeds:\n\n\r\n$ git checkout HEAD^\r\nHEAD is now at 9d6fb45e PARQUET-1576 Bump Apache Avro to 1.9.0 (#638)\r\n$ mvn clean install -DskipTests\r\n\r\n(snip)\r\n\r\n[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ parquet-cli ---\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT.jar\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/pom.xml to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT.pom\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT-tests.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT-tests.jar\r\n[INFO] Installing /home/sekikn/repo/parquet-mr/parquet-cli/target/parquet-cli-1.12.0-SNAPSHOT-runtime.jar to /home/sekikn/.m2/repository/org/apache/parquet/parquet-cli/1.12.0-SNAPSHOT/parquet-cli-1.12.0-SNAPSHOT-runtime.jar\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  15.822 s\r\n[INFO] Finished at: 2019-06-12T23:57:04+09:00\r\n[INFO] ------------------------------------------------------------------------\r\n$ mvn dependency:copy-dependencies\r\n\r\n(snip)\r\n\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main to-avro /tmp/sample.json -o /tmp/sample.avro\r\n$ echo $?\r\n0\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main head /tmp/sample.avro\r\n{\"id\": 1, \"name\": \"Alice\"}\r\n{\"id\": 2, \"name\": \"Bob\"}\r\n{\"id\": 3, \"name\": \"Carol\"}\r\n{\"id\": 4, \"name\": \"Dave\"}\r\n\n\nReverting the following code\nAvroJson.java\n\r\n   public static Iterator<JsonNode> parser(final InputStream stream) {\r\n     try(JsonParser parser = FACTORY.createParser(stream)) {\r\n\n\nto\n\n\r\n   public static Iterator<JsonNode> parser(final InputStream stream) {\r\n     try {\r\n      JsonParser parser = FACTORY.createParser(stream);\r\n\n\nseems to work.\ncc Fokko",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/648"
        ]
    },
    "PARQUET-1597": {
        "Key": "PARQUET-1597",
        "Summary": "Fix parquet-cli's wrong or missing usage examples",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "13/Jun/19 03:53",
        "Updated": "18/Sep/19 04:47",
        "Resolved": null,
        "Description": "1. The following parquet-cli's to-avro usage examples fail due to the lack of -o options.\n   In addition, \"sample.parquet\" in the second example should be \"sample.avro\".\n\n\r\n  Examples:\r\n\r\n    # Create an Avro file from a Parquet file\r\n    parquet to-avro sample.parquet sample.avro\r\n\r\n    # Create an Avro file in HDFS from a local JSON file\r\n    parquet to-avro path/to/sample.json hdfs:/user/me/sample.parquet\r\n\r\n    # Create an Avro file from data in S3\r\n    parquet to-avro s3:/data/path/sample.parquet sample.avro\r\n\n\n2. The above is the same for convert-csv.\n\n\r\n  Examples:\r\n\r\n    # Create a Parquet file from a CSV file\r\n    parquet convert-csv sample.csv sample.parquet --schema schema.avsc\r\n\r\n    # Create a Parquet file in HDFS from local CSV\r\n    parquet convert-csv path/to/sample.csv hdfs:/user/me/sample.parquet --schema schema.avsc\r\n\r\n    # Create an Avro file from CSV data in S3\r\n    parquet convert-csv s3:/data/path/sample.csv sample.avro --format avro --schema s3:/schemas/schema.avsc\r\n\n\n3. The meta command has an \"Examples:\" heading but lacks its content.\n\n\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main help meta\r\n\r\nUsage: parquet [general options] meta <parquet path> [command options]\r\n\r\n  Description:\r\n\r\n    Print a Parquet file's metadata\r\n\r\n  Examples:",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/649"
        ]
    },
    "PARQUET-1598": {
        "Key": "PARQUET-1598",
        "Summary": "Make convert-csv work with the input filename which starts with a period or an numeric",
        "Type": "Improvement",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "13/Jun/19 03:56",
        "Updated": "18/Sep/19 04:48",
        "Resolved": null,
        "Description": "I ran parquet-cli's convert-csv with an input file which name starts with a numeric character without --schema option and got the following error:\n\n\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main convert-csv 0sample.csv -o sample.parquet\r\nUnknown error\r\nshaded.parquet.org.apache.avro.SchemaParseException: Illegal initial character: 0sample\r\n\tat shaded.parquet.org.apache.avro.Schema.validateName(Schema.java:1498)\r\n\tat shaded.parquet.org.apache.avro.Schema.access$200(Schema.java:86)\r\n\tat shaded.parquet.org.apache.avro.Schema$Name.<init>(Schema.java:645)\r\n\tat shaded.parquet.org.apache.avro.Schema.createRecord(Schema.java:182)\r\n\tat shaded.parquet.org.apache.avro.SchemaBuilder$RecordBuilder.fields(SchemaBuilder.java:1805)\r\n\tat org.apache.parquet.cli.csv.AvroCSV.inferSchemaInternal(AvroCSV.java:158)\r\n\tat org.apache.parquet.cli.csv.AvroCSV.inferNullableSchema(AvroCSV.java:78)\r\n\tat org.apache.parquet.cli.commands.ConvertCSVCommand.run(ConvertCSVCommand.java:160)\r\n\tat org.apache.parquet.cli.Main.run(Main.java:147)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.parquet.cli.Main.main(Main.java:177)\r\n\n\nThis is because that convert-csv uses the input file name as the name for the output schema, while Avro requires its schema name to match the regex pattern [A-Za-z_][A-Za-z0-9_]*.\nSo users have to change the input file name or use the --schema option explicitly, but it's not so obvious from the error message.\nIt'd be nice if the message were improved, or the schema name were automatically replaced with valid characters to avoid this problem.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/652"
        ]
    },
    "PARQUET-1599": {
        "Key": "PARQUET-1599",
        "Summary": "Fix to-avro to respect the overwrite option",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "13/Jun/19 03:58",
        "Updated": "11/Apr/20 07:01",
        "Resolved": "11/Apr/20 07:01",
        "Description": "parquet-cli's to-avro has --overwrite option, and it works as expected:\n\n\r\n$ ls -l output\r\ntotal 8\r\n-rw-r--r--  1 sekikn  staff  2010 Jun 13 12:37 sample.avro\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main to-avro sample.parquet -o output/sample.avro --overwrite\r\n$ ls -l output\r\ntotal 8\r\n-rw-r--r--  1 sekikn  staff  2010 Jun 13 12:38 sample.avro\r\n\n\nBut even without this flag, it overwrites the existing file with no warning.\n\n\r\n$ java -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main to-avro sample.parquet -o output/sample.avro\r\n$ ls -l output\r\ntotal 8\r\n-rw-r--r--  1 sekikn  staff  2010 Jun 13 12:39 sample.avro\r\n\n\nThis behaviour should be fixed for consistency with other subcommands.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/650"
        ]
    },
    "PARQUET-1600": {
        "Key": "PARQUET-1600",
        "Summary": "Fix shebang in parquet-benchmarks/run.sh",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "15/Jun/19 14:52",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "23/Jul/19 09:28",
        "Description": "The following shebang does not work as expected since it's not on the first line and there's a space between # and !.\nparquet-benchmarks/run.sh\n\r\n(snip)\r\n\r\n# !/usr/bin/env bash\r\n\n\nFor example, if users use tcsh, it fails as follows:\n\n\r\n> parquet-benchmarks/run.sh\r\nIllegal variable name.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/651"
        ]
    },
    "PARQUET-1601": {
        "Key": "PARQUET-1601",
        "Summary": "Add zstd support to parquet-cli to-avro",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "17/Jun/19 00:26",
        "Updated": "22/Sep/19 06:59",
        "Resolved": "22/Sep/19 06:59",
        "Description": "Avro supports zstd compression from v1.9.0. It'd be useful if to-avro could generate zstd-compressed Avro file directly.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/653"
        ]
    },
    "PARQUET-1602": {
        "Key": "PARQUET-1602",
        "Summary": "PageIndex not working as suggested ?",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Anthony Pessy",
        "Created": "18/Jun/19 10:18",
        "Updated": "18/Jun/19 14:26",
        "Resolved": "18/Jun/19 14:26",
        "Description": "I have a schema such as:\n\n\r\nschema message pages {\r\n\u00a0 required binary url (STRING);\r\n\u00a0 optional binary content (STRING);\r\n}\r\n\n\nWhere `url` is unique and ordered, the file is created in such a way\u00a0that I have ~600 pages of `content` for 1 page of `url`.\n\u00a0\nFrom\u00a0https://github.com/apache/parquet-format/blob/master/PageIndex.md\u00a0I saw:\n\nA single-row lookup in a rowgroup based on the sort column of that rowgroup will only read one data page per retrieved column.\n\nI was expecting `ParquetReader`\u00a0 to find the matching row thanks to the `FilterPredicate`\u00a0 on `url`, decoding only this column, then, using `offset index`, directly seek to the appropriate page for `content` and decode it.\nInstead, what I'm seeing, is that the reader fully reads & decode the ~600 pages of content (until it actually find the url).\nIs there something I misunderstood or some step to ensure to make the reader only consume the necessary pages?",
        "Issue Links": []
    },
    "PARQUET-1603": {
        "Key": "PARQUET-1603",
        "Summary": "[C++] rename parquet::LogicalType to parquet::ConvertedType",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Joris Van den Bossche",
        "Created": "18/Jun/19 16:04",
        "Updated": "22/Jun/19 20:00",
        "Resolved": "22/Jun/19 20:00",
        "Description": "From discussion on the mailing list (https://lists.apache.org/thread.html/5cbbd351b17aed50f40df286bd2f080cb6e5e9b23e5a5c79b7e6e041@%3Cdev.parquet.apache.org%3E), the idea is to rename parquet-cpp's current LogicalType to ConvertedType, and the new LogicalAnnotation to LogicalType",
        "Issue Links": [
            "/jira/browse/ARROW-3729",
            "https://github.com/apache/arrow/pull/4653"
        ]
    },
    "PARQUET-1604": {
        "Key": "PARQUET-1604",
        "Summary": "Bump fastutil from 7.0.13 to 8.2.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Jun/19 11:56",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "08/Jul/19 19:33",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/655"
        ]
    },
    "PARQUET-1605": {
        "Key": "PARQUET-1605",
        "Summary": "Bump maven-javadoc-plugin from 2.9 to 3.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Jun/19 12:02",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "24/Jul/19 15:12",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1551",
            "https://github.com/apache/parquet-mr/pull/656"
        ]
    },
    "PARQUET-1606": {
        "Key": "PARQUET-1606",
        "Summary": "Fix invalid tests scope",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Jun/19 20:07",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "23/Jul/19 09:30",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/657"
        ]
    },
    "PARQUET-1607": {
        "Key": "PARQUET-1607",
        "Summary": "Remove duplicate maven-enforcer-plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cascading",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Jun/19 20:09",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "09/Sep/19 15:47",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/658"
        ]
    },
    "PARQUET-1608": {
        "Key": "PARQUET-1608",
        "Summary": "Release Parquet format 2.7.0",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "20/Jun/19 07:26",
        "Updated": "13/Sep/19 04:24",
        "Resolved": null,
        "Description": "This is an umbrella issue for the 2.7.0 release. Please add any issues you'd like to get into that release as blockers.",
        "Issue Links": [
            "/jira/browse/PARQUET-1619",
            "/jira/browse/PARQUET-319",
            "https://github.com/apache/parquet-format/pull/151"
        ]
    },
    "PARQUET-1609": {
        "Key": "PARQUET-1609",
        "Summary": "support xxhash in bloom filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.6.0",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "25/Jun/19 14:28",
        "Updated": "10/Sep/19 01:22",
        "Resolved": "10/Sep/19 01:22",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/143"
        ]
    },
    "PARQUET-1610": {
        "Key": "PARQUET-1610",
        "Summary": "Small spelling issues",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.6.0",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Fokko Driesprong",
        "Created": "25/Jun/19 20:19",
        "Updated": "25/Jun/19 20:22",
        "Resolved": "25/Jun/19 20:20",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/82",
            "https://github.com/apache/parquet-format/pull/132"
        ]
    },
    "PARQUET-1611": {
        "Key": "PARQUET-1611",
        "Summary": "Add missing getFileBlockLocations overload to FilterFileSystem",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Phillips",
        "Created": "27/Jun/19 01:09",
        "Updated": "27/Jun/19 01:11",
        "Resolved": "27/Jun/19 01:11",
        "Description": "The getFileBlockLocations(Path, long, long) overload is missing.",
        "Issue Links": []
    },
    "PARQUET-1612": {
        "Key": "PARQUET-1373 Encryption key management tools",
        "Summary": "Double wrapped key manager",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "30/Jun/19 09:45",
        "Updated": "04/Apr/21 11:39",
        "Resolved": "04/Apr/21 11:39",
        "Description": "To minimize interaction with KMS, this manager will wrap the encryption keys\u00a0twice.\u00a0\u00a0Might be combined with key rotation for further optimization.",
        "Issue Links": []
    },
    "PARQUET-1613": {
        "Key": "PARQUET-1373 Encryption key management tools",
        "Summary": "Key rotation tool",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Maya Anderson",
        "Reporter": "Gidon Gershinsky",
        "Created": "30/Jun/19 09:47",
        "Updated": "04/Apr/21 11:39",
        "Resolved": "04/Apr/21 11:39",
        "Description": "Rotates the master key, for both single and double wrappers.\nFor the latter, enables support for a single KMS call per column, in readers of any data sets.",
        "Issue Links": []
    },
    "PARQUET-1614": {
        "Key": "PARQUET-1614",
        "Summary": "[C++] Reuse arrow::Buffer used as scratch space for decryption in Thrift deserialization hot path",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "30/Jun/19 19:19",
        "Updated": "22/Aug/22 11:35",
        "Resolved": null,
        "Description": "If it is possible to reuse memory on the decrypt-deserialize hot path that will improve performance",
        "Issue Links": []
    },
    "PARQUET-1615": {
        "Key": "PARQUET-1615",
        "Summary": "getRecordWriter shouldn't hardcode CREAT mode when new ParquetFileWriter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Lantao Jin",
        "Reporter": "Lantao Jin",
        "Created": "01/Jul/19 09:47",
        "Updated": "02/Aug/21 16:59",
        "Resolved": "09/Jul/19 08:28",
        "Description": "getRecordWriter shouldn't hardcode CREAT mode when new ParquetFileWriter.",
        "Issue Links": [
            "/jira/browse/SPARK-27194",
            "https://github.com/apache/parquet-mr/pull/660"
        ]
    },
    "PARQUET-1616": {
        "Key": "PARQUET-1616",
        "Summary": "Enable Maven batch mode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "04/Jul/19 11:47",
        "Updated": "24/Oct/19 12:24",
        "Resolved": "24/Oct/19 12:24",
        "Description": "To avoid excessive output in the Travis logs.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/661"
        ]
    },
    "PARQUET-1617": {
        "Key": "PARQUET-1617",
        "Summary": "Add more details to bloom filter spec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "05/Jul/19 08:21",
        "Updated": "10/Sep/19 01:22",
        "Resolved": "10/Sep/19 01:22",
        "Description": "The current spec doesn't contain deep detail of some reference, which may bring confusion.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/140"
        ]
    },
    "PARQUET-1618": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Update encryption spec for Bloom filter encryption",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "10/Jul/19 07:22",
        "Updated": "12/Feb/20 13:31",
        "Resolved": "12/Feb/20 13:31",
        "Description": "update Encryption.md with the new module types for Bloom filter encryption.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/141"
        ]
    },
    "PARQUET-1619": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Merge crypto spec and structures to format master",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "10/Jul/19 10:00",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "08/Aug/19 08:16",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1608",
            "https://github.com/apache/parquet-format/pull/142"
        ]
    },
    "PARQUET-1620": {
        "Key": "PARQUET-1620",
        "Summary": "Schema creation from another schema will not be possible - deprecated",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Werner Daehn",
        "Created": "10/Jul/19 19:59",
        "Updated": "10/Jul/19 19:59",
        "Resolved": null,
        "Description": "Imagine I have a current schema and want to create a projection schema from that. One option is the schema.Types.*Builder but the more direct version would be to clone the schema itself without children.\nList<org.apache.parquet.schema.Type> l = new ArrayList<>();\n{{ for (String c : childmappings.keySet()) {}}\n\u00a0 Mapping m = childmappings.get(c);\n\u00a0 l.add(m.getProjectionSchema());\n{{ }}}\n{{ GroupType gt = new GroupType(schema.getRepetition(), schema.getName(), schema.getOriginalType(), l);}}\n\u00a0\nThe last line, the new GroupType(..) constructor is deprecated. We should use the version with the LogicalTypeAnnotation instead. Fine. But how do you get the\u00a0LogicalTypeAnnotation\u00a0 from an existing schema?\nI feel you should not deprecate these methods and if, provide an extra method to create a Type column from a type column (column alone, without children. Else the projection would have all child columns).\n\u00a0\nDo you agree?",
        "Issue Links": []
    },
    "PARQUET-1621": {
        "Key": "PARQUET-1300 [C++] Parquet modular encryption",
        "Summary": "[C++] Add encrypted parquet files to apache parquet-testing repository",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Revital Sur",
        "Reporter": "Revital Sur",
        "Created": "11/Jul/19 11:18",
        "Updated": "29/Aug/19 05:40",
        "Resolved": "29/Aug/19 05:40",
        "Description": "Add encrypted parquet files to apache parquet-testing repository (https://github.com/apache/parquet-testing) for the purpose of testing.",
        "Issue Links": [
            "https://github.com/apache/parquet-testing/pull/7"
        ]
    },
    "PARQUET-1622": {
        "Key": "PARQUET-1622",
        "Summary": "Add BYTE_STREAM_SPLIT encoding",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0,                                            format-2.8.0",
        "Component/s": "parquet-cpp,                                            parquet-format,                                            parquet-mr,                                            parquet-thrift",
        "Assignee": "Martin Radev",
        "Reporter": "Martin Radev",
        "Created": "11/Jul/19 13:39",
        "Updated": "18/Jan/23 03:05",
        "Resolved": "12/Feb/20 11:38",
        "Description": "Apache Parquet does not have any encodings suitable for FP data and the available text compressors (zstd, gzip, etc) do not handle FP data very well.\nIt is possible to apply a simple data transformation named \"stream splitting\". Such could be \"byte stream splitting\" which creates K streams of length N where K is the number of bytes in the data type (4 for floats, 8 for doubles) and N is the number of elements in the sequence.\nThe transformed data compresses significantly better on average than the original data and for some cases there is a performance improvement in compression and decompression speed.\nYou can read a more detailed report here:\nhttps://drive.google.com/file/d/1wfLQyO2G5nofYFkS7pVbUW0-oJkQqBvv/view",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/144",
            "https://github.com/apache/parquet-mr/pull/705"
        ]
    },
    "PARQUET-1623": {
        "Key": "PARQUET-1623",
        "Summary": "[C++] Invalid memory access with a magic number of records",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Hatem Helal",
        "Reporter": "Hatem Helal",
        "Created": "11/Jul/19 15:46",
        "Updated": "12/Jul/19 08:50",
        "Resolved": "12/Jul/19 08:50",
        "Description": "I've observed a crash due to an invalid memory access when trying to read a parquet file that I created with a single column of double-precision values that occupies a fixed amount\u00a0of memory.\u00a0 After some experimentation I found that the\u00a0following unittest added to arrow-reader-writer-test.cc will fail when run in an ASAN build.\n\n\r\nTEST(TestArrowReadWrite, MultiDataPageMagicNumber) {\r\n\u00a0 const int num_rows = 262144;\u00a0 // 2^18\r\n\r\n\u00a0 std::shared_ptr<Table> table;\r\n\u00a0 ASSERT_NO_FATAL_FAILURE(MakeDoubleTable(1, num_rows, 1, &table));\r\n\r\n\u00a0 std::shared_ptr<Table> result;\r\n\u00a0 ASSERT_NO_FATAL_FAILURE(\r\n\u00a0 \u00a0 \u00a0 DoSimpleRoundtrip(table, false, table->num_rows(), {}, &result));\r\n\r\n\u00a0 ASSERT_NO_FATAL_FAILURE(::arrow::AssertTablesEqual(*table, *result));\r\n}",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/4857"
        ]
    },
    "PARQUET-1624": {
        "Key": "PARQUET-1624",
        "Summary": "ParquetFileReader.open ignores Hadoop configuration options",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0,                                            1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Blue",
        "Reporter": "Ryan Blue",
        "Created": "11/Jul/19 19:38",
        "Updated": "11/Jul/19 19:41",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/662"
        ]
    },
    "PARQUET-1625": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Update parquet thrift to align with spec",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.6.0",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "15/Jul/19 12:44",
        "Updated": "17/Jul/19 07:39",
        "Resolved": "17/Jul/19 07:39",
        "Description": "current parquet.thrift lags behind with spec, we need to udpate it.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/145"
        ]
    },
    "PARQUET-1626": {
        "Key": "PARQUET-1626",
        "Summary": "[C++] Ability to concat parquet files",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.3.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Nileema Shingte",
        "Created": "15/Jul/19 21:59",
        "Updated": "06/Aug/19 22:03",
        "Resolved": null,
        "Description": "Ability to concat the parquet files is something we've wanted for some time too. When we generate parquet files partitioned by an expression, we often end up with tiny files and would like to add a post-processing step to concat these files together.\nIs there a plan to add this ability to the library any time soon?\u00a0\nIf not, it would be great if someone can provide a somewhat detailed pseudocode (expanding on what\u00a0xhochy\u00a0mentioned in the comment in PARQUET-1022) as a guideline for conditions/scenarios that need to be handled with extra care, so we can contribute this as a PR.",
        "Issue Links": []
    },
    "PARQUET-1627": {
        "Key": "PARQUET-1627",
        "Summary": "Update specification so that legacy timestamp logical types can be written for local semantics as well",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "18/Jul/19 11:47",
        "Updated": "09/Sep/19 12:21",
        "Resolved": "08/Aug/19 07:55",
        "Description": "The rules for TIMESTAMP forward-compatibility were created based on the assumption that TIMESTAMP_MILLIS and TIMESTAMP_MICROS have only been used in the instant aka. UTC-normalized semantics so far.\nFrom this false premise it followed that TIMESTAMPs with local semantics were a new type and did not need to be annotated with the old types to maintain compatibility. In fact, annotating them with the old types were considered to be harmful, since it would have mislead older readers into thinking that they can read TIMESTAMPs with local semantics, when in reality they would have misinterpreted them as TIMESTAMPs with instant semantics. This would have lead to a difference of several hours, corresponding to the time zone offset.\nIn reality, however, this misinterpretation of timestamps has already been going on for a while, since Arrow annotates local timestamps with TIMESTAMP_MILLIS or TIMESTMAP_MICROS.\nTo maintain forward compatibilty of local timestamps, the specification should allow annotating them with the legacy timestamp logical types.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/148",
            "https://lists.apache.org/thread.html/04b9a8a156939adae1aa4cf2d4e3abd2137f541cbf92ade6d775dc06@%3Cdev.parquet.apache.org%3E"
        ]
    },
    "PARQUET-1628": {
        "Key": "PARQUET-1628",
        "Summary": "Accept local timestamps annotated with the legacy timestamp types",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "Zoltan Ivanfi",
        "Created": "18/Jul/19 11:50",
        "Updated": "18/Jul/19 11:51",
        "Resolved": null,
        "Description": "The rules for TIMESTAMP forward-compatibility were created based on the assumption that TIMESTAMP_MILLIS and TIMESTAMP_MICROS have only been used in the instant aka. UTC-normalized semantics so far.\nFrom this false premise it followed that TIMESTAMPs with local semantics were a new type and did not need to be annotated with the old types to maintain compatibility. In fact, annotating them with the old types were considered to be harmful, since it would have mislead older readers into thinking that they can read TIMESTAMPs with local semantics, when in reality they would have misinterpreted them as TIMESTAMPs with instant semantics. This would have lead to a difference of several hours, corresponding to the time zone offset.\nIn reality, however, this misinterpretation of timestamps has already been going on for a while, since Arrow annotates local timestamps with TIMESTAMP_MILLIS or TIMESTMAP_MICROS.\nTo maintain forward compatibilty of local timestamps, Arrow annotates them with the legacy timestamp logical types. However, the Java library considers these logical types to be incompatible and discards the new type in favour of the legacy ones (since doing the other way around would change the behaviour). Parquet-mr should be updated so that it accepts this combination of new and old logical types.",
        "Issue Links": [
            "https://lists.apache.org/thread.html/04b9a8a156939adae1aa4cf2d4e3abd2137f541cbf92ade6d775dc06@%3Cdev.parquet.apache.org%3E"
        ]
    },
    "PARQUET-1629": {
        "Key": "PARQUET-1629",
        "Summary": "Page-level CRC checksum verification for DataPageV2",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Boudewijn Braams",
        "Created": "19/Jul/19 16:44",
        "Updated": "22/Jun/23 16:12",
        "Resolved": null,
        "Description": "In https://jira.apache.org/jira/browse/PARQUET-1580 (Github PR: https://github.com/apache/parquet-mr/pull/647) we implemented page level CRC checksum verification for DataPageV1. As a follow up, we should add support for DataPageV2 that follows the spec (see see https://jira.apache.org/jira/browse/PARQUET-1539).\nWhat needs to be done:\n\nAdd writing out checksums for DataPageV2\nAdd checksum verification for DataPageV2\nCreate new test suite\nCreate new benchmarks",
        "Issue Links": []
    },
    "PARQUET-1630": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "Resolve Bloom filter spec concerns",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.7.0",
        "Component/s": "parquet-format",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "04/Aug/19 14:13",
        "Updated": "14/May/20 12:55",
        "Resolved": "31/Aug/19 01:51",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/146",
            "https://github.com/apache/parquet-format/pull/147",
            "https://github.com/apache/parquet-format/pull/147",
            "https://github.com/apache/parquet-format/pull/149",
            "https://github.com/apache/parquet-format/pull/150"
        ]
    },
    "PARQUET-1631": {
        "Key": "PARQUET-1631",
        "Summary": "[C++] ParquetInputWrapper::GetSize always returns 0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Zherui Cao",
        "Reporter": "Zherui Cao",
        "Created": "05/Aug/19 20:06",
        "Updated": "06/Aug/19 19:07",
        "Resolved": "06/Aug/19 19:07",
        "Description": "::arrow::Status ParquetInputWrapper::GetSize(int64_t* size)\n\n{ PARQUET_CATCH_NOT_OK(*size = source_->Tell()); return ::arrow::Status::OK(); }\n\nThis must be\u00a0\n ::arrow::Status ParquetInputWrapper::GetSize(int64_t* size) \n{\r\n PARQUET_CATCH_NOT_OK(*size = source_->Size());\r\n return ::arrow::Status::OK();\r\n }",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5017"
        ]
    },
    "PARQUET-1632": {
        "Key": "PARQUET-1632",
        "Summary": "Negative initial size when writing large values in parquet-mr",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Ivan Sadikov",
        "Created": "06/Aug/19 12:49",
        "Updated": "17/Feb/22 11:24",
        "Resolved": null,
        "Description": "I encountered an issue when writing large string values to Parquet.\nHere is the code to reproduce the issue:\n\n\r\nimport org.apache.spark.sql.functions._\r\n\r\ndef longString: String = \"a\" * (64 * 1024 * 1024)\r\nval long_string = udf(() => longString)\r\n\r\nval df = spark.range(0, 40, 1, 1).withColumn(\"large_str\", long_string())\r\n\r\nspark.conf.set(\"parquet.enable.dictionary\", \"false\")\r\ndf.write.option(\"compression\", \"uncompressed\").mode(\"overwrite\").parquet(\"/tmp/large.parquet\")\n\n\u00a0\nThis Spark job fails with the exception:\n\n\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10861.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10861.0 (TID 671168, 10.0.180.13, executor 14656): org.apache.spark.SparkException: Task failed while writing rows. at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1526) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) \r\n\r\nCaused by: java.lang.IllegalArgumentException: Negative initial size: -1610612543 at java.io.ByteArrayOutputStream.<init>(ByteArrayOutputStream.java:74) at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:234) at org.apache.parquet.bytes.BytesInput$BAOS.<init>(BytesInput.java:232) at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:202) at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33) at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:126) at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147) at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235) at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122) at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172) at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114) at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165) at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42) at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57) at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247) at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1560) at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248) ... 11 more\n\n\u00a0\nWould appreciate if you could help addressing the problem. Thanks!",
        "Issue Links": []
    },
    "PARQUET-1633": {
        "Key": "PARQUET-1633",
        "Summary": "Integer overflow in ParquetFileReader.ConsecutiveChunkList",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Edward Wright",
        "Reporter": "Ivan Sadikov",
        "Created": "06/Aug/19 12:59",
        "Updated": "22/Jun/21 07:55",
        "Resolved": "22/Jun/21 07:55",
        "Description": "When reading a large Parquet file (2.8GB), I encounter the following exception:\n\n\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file dbfs:/user/hive/warehouse/demo.db/test_table/part-00014-tid-1888470069989036737-593c82a4-528b-4975-8de0-5bcbc5e9827d-10856-1-c000.snappy.parquet\r\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\r\nat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\nat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\r\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:228)\r\n... 14 more\r\nCaused by: java.lang.IllegalArgumentException: Illegal Capacity: -212\r\nat java.util.ArrayList.<init>(ArrayList.java:157)\r\nat org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:1169)\n\n\u00a0\nThe file metadata is:\n\nblock 1 (3 columns)\n\n\n\n\nrowCount: 110,100\n\n\n\n\n\n\ntotalByteSize: 348,492,072\n\n\n\n\n\n\ncompressedSize: 165,689,649\n\n\n\n\nblock 2 (3 columns)\n\n\n\n\nrowCount: 90,054\n\n\n\n\n\n\ntotalByteSize: 3,243,165,541\n\n\n\n\n\n\ncompressedSize: 2,509,579,966\n\n\n\n\nblock 3 (3 columns)\n\n\n\n\nrowCount: 105,119\n\n\n\n\n\n\ntotalByteSize: 350,901,693\n\n\n\n\n\n\ncompressedSize: 144,952,177\n\n\n\n\nblock 4 (3 columns)\n\n\n\n\nrowCount: 48,741\n\n\n\n\n\n\ntotalByteSize: 1,275,995\n\n\n\n\n\n\ncompressedSize: 914,205\n\n\n\nI don't have the code to reproduce the issue, unfortunately; however, I\u00a0looked at the code and it seems that integer length\u00a0field in\u00a0ConsecutiveChunkList overflows, which results in negative capacity for array list in readAll method:\n\n\r\nint fullAllocations = length / options.getMaxAllocationSize();\r\nint lastAllocationSize = length % options.getMaxAllocationSize();\r\n\t\r\nint numAllocations = fullAllocations + (lastAllocationSize > 0 ? 1 : 0);\r\nList<ByteBuffer> buffers = new ArrayList<>(numAllocations);\n\n\u00a0\nThis is caused by cast to integer in readNextRowGroup method in ParquetFileReader:\n\n\r\ncurrentChunks.addChunk(new ChunkDescriptor(columnDescriptor, mc, startingPos, (int)mc.getTotalSize()));\r\n\n\nwhich overflows when total size of the column is larger than Integer.MAX_VALUE.\nI would appreciate if you could help addressing the issue. Thanks!",
        "Issue Links": []
    },
    "PARQUET-1634": {
        "Key": "PARQUET-1634",
        "Summary": "[C++] Factor out data/dictionary page writes to allow for page buffering",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "06/Aug/19 21:38",
        "Updated": "22/Aug/22 11:35",
        "Resolved": null,
        "Description": "Logic that eagerly writes out data pages is hard-coded into the column writer implementation\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc#L565\nFor higher-latency file systems like Amazon S3, it makes more sense to buffer pages in memory and write them in larger batches (and preferably asynchronously). We should refactor this logic so we have the ability to choose rather than have the behavior hard-coded",
        "Issue Links": []
    },
    "PARQUET-1635": {
        "Key": "PARQUET-1373 Encryption key management tools",
        "Summary": "[C++] Key rotation tool",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Gidon Gershinsky",
        "Created": "08/Aug/19 06:24",
        "Updated": "08/Aug/19 06:24",
        "Resolved": null,
        "Description": "C++ version of the tool that rotates the master key, for both single and double wrappers.\nFor the latter, enables support for a single KMS call per column, in readers of any data sets.",
        "Issue Links": []
    },
    "PARQUET-1636": {
        "Key": "PARQUET-1636",
        "Summary": "[C++] Incompatibility due to moving from Parquet to Arrow IO interfaces",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Deepak Majeti",
        "Created": "08/Aug/19 14:22",
        "Updated": "15/Aug/19 16:45",
        "Resolved": "15/Aug/19 16:45",
        "Description": "We moved to the Arrow IO interfaces as part of https://issues.apache.org/jira/browse/PARQUET-1422\nHowever, the BufferedInputStream implementations between Parquet and Arrow are different.\nParquet's BufferedInputStream used to takes a RandomAccessSource. Arrow's implementation takes an InputStream. As a result, the\u00a0::arrow::io::BufferedInputStream::Peek(which invokes Read()) implementation causes the raw source (input to BufferedInputStream) to change its offset on Peek(). This did not happen in the Parquet's BufferedInputStream implementation.",
        "Issue Links": [
            "/jira/browse/ARROW-6180"
        ]
    },
    "PARQUET-1637": {
        "Key": "PARQUET-1637",
        "Summary": "Builds are failing because default jdk changed to openjdk11 on Travis",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "N\u00e1ndor Koll\u00e1r",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "10/Aug/19 08:45",
        "Updated": "24/Oct/19 12:42",
        "Resolved": "15/Aug/19 14:04",
        "Description": "The default distribution on Travis recently changed from Trusy to Xenial. It appears that the default JDK also changed from JDK8 to JDK11, despite the doc says the default is openjdk8, it appears that it isn't correct (see related discussion)\nSince Parquet still doesn't support Java 11 (PARQUET-1551), we should explicitly tell in Travis config which JDK to use, at lease as long as PARQUET-1551 is still open.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/665",
            "https://github.com/apache/parquet-mr/pull/665"
        ]
    },
    "PARQUET-1638": {
        "Key": "PARQUET-1638",
        "Summary": "ParquetFileReader.readFooter and ParquetFileReader.readNextRowGroup may be hang",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.8.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yuming Wang",
        "Created": "15/Aug/19 07:39",
        "Updated": "15/Aug/19 13:51",
        "Resolved": "15/Aug/19 13:51",
        "Description": "It's a Spark SQL application. These 3 tasks hang for more than 1.5 hours.",
        "Issue Links": []
    },
    "PARQUET-1639": {
        "Key": "PARQUET-1639",
        "Summary": "[C++] Remove regex dependency for parsing ApplicationVersion",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Hatem Helal",
        "Created": "16/Aug/19 13:16",
        "Updated": "07/May/20 12:00",
        "Resolved": null,
        "Description": "This is a follow up task to ARROW-6096.  As fsaintjacques points out, the parsing can be done in a single pass without using the regex library.  See discussion:\nhttps://github.com/apache/arrow/pull/4985#issuecomment-517393619",
        "Issue Links": []
    },
    "PARQUET-1640": {
        "Key": "PARQUET-1640",
        "Summary": "[C++] parquet-encoding-benchmark crashes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "19/Aug/19 15:02",
        "Updated": "19/Aug/19 21:26",
        "Resolved": "19/Aug/19 21:26",
        "Description": "[...]\r\nBM_DictDecodingByteArray/DecodeArrowNonNull_Dict/65536     42199236 ns   42186639 ns          1   8.91692MB/s\r\npure virtual method called\r\nterminate called without an active exception",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5120"
        ]
    },
    "PARQUET-1641": {
        "Key": "PARQUET-1641",
        "Summary": "Parquet pages for different columns cannot be read in parallel",
        "Type": "Improvement",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Samarth Jain",
        "Reporter": "Samarth Jain",
        "Created": "20/Aug/19 21:19",
        "Updated": "29/Aug/19 09:48",
        "Resolved": null,
        "Description": "All ColumnChunkPageReader instances use the same decompressor.\u00a0\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L1286\n\n\r\nBytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(descriptor.metadata.getCodec());\r\nreturn new ColumnChunkPageReader(decompressor, pagesInChunk, dictionaryPage);\r\n\n\nThe CodecFactory caches the decompressors for every codec type returning the same instance on every getCompressor(codecName) call. See the caching happening here:\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java#L197\n\n\r\n@Override\r\n public BytesDecompressor getDecompressor(CompressionCodecName codecName) {\r\n    BytesDecompressor decomp = decompressors.get(codecName);\r\n    if (decomp == null){ \r\n       decomp = createDecompressor(codecName); decompressors.put(codecName, decomp); \r\n    }\r\n    return decomp;\r\n }\r\n\u00a0\r\n\n\n\u00a0\nIf multiple threads try to read the pages belonging to different columns, they run into thread\nsafety issues. This issue prevents increasing the throughput at which applications can read parquet data by parallelizing page reads.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/668",
            "https://github.com/apache/parquet-mr/pull/670"
        ]
    },
    "PARQUET-1642": {
        "Key": "PARQUET-1642",
        "Summary": "[C++] Provide for readahead-buffering in column readers",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "23/Aug/19 21:25",
        "Updated": "23/Aug/19 21:25",
        "Resolved": null,
        "Description": "Currently we support two modes of reading a column chunk:\n\nRead entire chunk into memory before beginning to deserialize\nUse arrow::io::BufferedInputStream to buffer reads, where IO calls are triggered when a buffer is consumed\n\nOne downside to the latter case is that the IO may sit idle while deserialization is happening. It might be preferable to allow \"lookahead buffering\" (possibly using \"ReadaheadSpooler\") so that data will continue to be requested in the background",
        "Issue Links": []
    },
    "PARQUET-1643": {
        "Key": "PARQUET-1643",
        "Summary": "Use airlift non-native implementations for GZIP, LZ0 and LZ4 codecs",
        "Type": "Improvement",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Samarth Jain",
        "Reporter": "Samarth Jain",
        "Created": "28/Aug/19 18:21",
        "Updated": "29/Jun/20 18:21",
        "Resolved": null,
        "Description": "rdblue\u00a0pointed me to\u00a0https://github.com/airlift/aircompressor\u00a0which provides\u00a0non-native implementations of compression codecs. It claims to be much faster than native wrappers that parquet uses. This Jira is to track the work needed for exploring using these codecs, getting benchmark results and making changes including not needing to pool compressors and decompressors anymore. Note that this doesn't include SNAPPY since Parquet already has its own non-hadoopy implementation for it.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/671"
        ]
    },
    "PARQUET-1644": {
        "Key": "PARQUET-1644",
        "Summary": "Clean up some benchmark code and docs.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ryan Skraba",
        "Reporter": "Ryan Skraba",
        "Created": "29/Aug/19 13:46",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "24/Sep/19 10:23",
        "Description": "Strictly following the instructions on the parquet-benchmarks module doesn't give meaningful results.\nIt appears some new benchmarks enter into conflict with the globs specified for others, not all benchmarks are run, and some iterations of write benchmarks aren't evalulated due to unexpected \"file already exists ...\" fail-fast returns in the data generator.\nThis should be cleaned up to encourage using and implementing benchmarks on Parquet code.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/672"
        ]
    },
    "PARQUET-1645": {
        "Key": "PARQUET-1645",
        "Summary": "Bump Apache Avro to 1.9.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "02/Sep/19 09:29",
        "Updated": "13/Feb/20 16:09",
        "Resolved": "08/Nov/19 07:19",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1661"
        ]
    },
    "PARQUET-1646": {
        "Key": "PARQUET-1646",
        "Summary": "[C++] Use arrow::Buffer for buffered dictionary indices in DictEncoder instead of std::vector",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-13.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "02/Sep/19 16:32",
        "Updated": "11/May/23 01:22",
        "Resolved": null,
        "Description": "Follow up to ARROW-6411",
        "Issue Links": [
            "/jira/browse/ARROW-6411"
        ]
    },
    "PARQUET-1647": {
        "Key": "PARQUET-1647",
        "Summary": "[Java] support for Arrow's float16",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-thrift",
        "Assignee": null,
        "Reporter": "The Alchemist",
        "Created": "03/Sep/19 19:09",
        "Updated": "15/Nov/22 00:26",
        "Resolved": null,
        "Description": "DESCRIPTION\n\u00a0\nI'm wondering if there's any interest in supporting Arrow's float16 type in Parquet.\nThere seem to be one or two float16 / halffloat tickets here (e.g.,\u00a0PARQUET-1403) but nothing that speaks to adding half-float support to Parquet in-general.\n\u00a0\nPLANS\nI'm able to spend some time on this, if someone points me\u00a0 in the right direction.\n\u00a0\n\nAdd the HALFFLOAT or FLOAT16 enum (any preferred naming convention?) to\u00a0https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift#L32\nAdd HALFFLOAT to\u00a0org.apache.parquet.schema.PrimitiveType\nAdd HALFFLOAT support to\u00a0org.apache.parquet.arrow.schema.SchemaConverter\nAdd encoding for new type at\u00a0org.apache.parquet.column.Encoding\n??\n\nIf anyone has any interest in this, pointers, or comments, they would be greatly appreciated!",
        "Issue Links": [
            "/jira/browse/PARQUET-758"
        ]
    },
    "PARQUET-1648": {
        "Key": "PARQUET-1648",
        "Summary": "[C++] Add accessors for ColumnChunk KeyValue metadata",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "04/Sep/19 02:47",
        "Updated": "04/Sep/19 02:47",
        "Resolved": null,
        "Description": "Per mailing list discussion on dev@arrow.apache.org",
        "Issue Links": []
    },
    "PARQUET-1649": {
        "Key": "PARQUET-1649",
        "Summary": "Bump Jackson Databind to 2.9.9.3",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "09/Sep/19 07:54",
        "Updated": "21/Sep/19 07:34",
        "Resolved": "21/Sep/19 07:34",
        "Description": "Due to CVE's",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/674"
        ]
    },
    "PARQUET-1650": {
        "Key": "PARQUET-1650",
        "Summary": "Implement unit test to validate column/offset indexes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Sep/19 12:36",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "18/Oct/19 06:38",
        "Description": "Implement end-to-end unit test for column indexes by writing random values to parquet files and validate the generated column/offset indexes.",
        "Issue Links": [
            "/jira/browse/PARQUET-1434",
            "https://github.com/apache/parquet-mr/pull/675"
        ]
    },
    "PARQUET-1651": {
        "Key": "PARQUET-1651",
        "Summary": "Typos in parquet.thrift",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Gary Fredericks",
        "Reporter": "Gary Fredericks",
        "Created": "14/Sep/19 22:44",
        "Updated": "16/Sep/19 07:34",
        "Resolved": "16/Sep/19 07:34",
        "Description": "\"meaningful\" is spelled \"meainful\" in two places.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/152"
        ]
    },
    "PARQUET-1652": {
        "Key": "PARQUET-1652",
        "Summary": "[C++] ColumnWriter writes incorrect \"num_values\" metadata for nested types",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "16/Sep/19 16:40",
        "Updated": "16/Sep/19 20:39",
        "Resolved": "16/Sep/19 20:39",
        "Description": "While investigating ARROW-5630, I discovered that we are writing incorrect \"num_values\" metadata in DataPageHeader when writing nested types. Instead of writing \"Number of values, including NULLs, in this data page\" as the specification in parquet.thrift says, we are writing the number of definition levels. For flat types, the number of definition levels and number of values with nulls in the same, but for nested types the number of values with nulls will generally be smaller.",
        "Issue Links": [
            "/jira/browse/ARROW-5630"
        ]
    },
    "PARQUET-1653": {
        "Key": "PARQUET-1653",
        "Summary": "[C++] Deprecated BIT_PACKED level decoding is probably incorrect",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "16/Sep/19 17:46",
        "Updated": "22/Aug/22 11:35",
        "Resolved": null,
        "Description": "In working on PARQUET-1652, I noticed that our implementation of BIT_PACKED almost certainly does not line up with apache/parquet-format. I'm going to disable it in our tests until it can be validated",
        "Issue Links": []
    },
    "PARQUET-1654": {
        "Key": "PARQUET-1654",
        "Summary": "Remove unnecessary options when building thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "18/Sep/19 04:38",
        "Updated": "21/Sep/19 07:30",
        "Resolved": "21/Sep/19 07:30",
        "Description": "In dev/travis-before_install.sh, configure options for thrift are defined as follows:\n\n\r\n./configure --disable-gen-erl --disable-gen-hs --without-ruby --without-haskell --without-erlang --without-php --without-nodejs\r\n\n\nAnd the following libraries are enabled on Travis:\n\n\r\nthrift 0.12.0\r\n\r\nBuilding C (GLib) Library .... : yes\r\nBuilding C# (Mono) Library ... : no\r\nBuilding C++ Library ......... : yes\r\nBuilding Common Lisp Library.. : no\r\nBuilding D Library ........... : no\r\nBuilding Dart Library ........ : no\r\nBuilding dotnetcore Library .. : no\r\nBuilding Erlang Library ...... : no\r\nBuilding Go Library .......... : yes\r\nBuilding Haskell Library ..... : no\r\nBuilding Haxe Library ........ : no\r\nBuilding Java Library ........ : yes\r\nBuilding Lua Library ......... : no\r\nBuilding NodeJS Library ...... : no\r\nBuilding Perl Library ........ : no\r\nBuilding PHP Library ......... : no\r\nBuilding Plugin Support ...... : no\r\nBuilding Python Library ...... : yes\r\nBuilding Py3 Library ......... : yes\r\nBuilding Ruby Library ........ : no\r\nBuilding Rust Library ........ : no\r\n\n\nBut Parquet requires only the C++ library to be installed, so we can disable C, Go, Java, and Python2 (Python3 doesn't seem to be disabled). This will save about 30 seconds on each CI run.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/676"
        ]
    },
    "PARQUET-1655": {
        "Key": "PARQUET-1655",
        "Summary": "[C++] Decimal comparisons used for min/max statistics are not correct",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Philip Felton",
        "Created": "06/Aug/19 15:39",
        "Updated": "23/Mar/21 20:16",
        "Resolved": "04/Mar/21 16:18",
        "Description": "The Parquet Format specifications says\nIf the column uses int32 or int64 physical types, then signed comparison of the integer values produces the correct ordering. If the physical type is fixed, then the correct ordering can be produced by flipping the most-significant bit in the first byte and then using unsigned byte-wise comparison.\nHowever this isn't followed in the C++ Parquet code. 16-byte decimal comparison is implemented using a lexicographical comparison of signed chars.\nThis appears to be because the function https://github.com/apache/arrow/blob/master/cpp/src/parquet/statistics.cc#L183 just goes off the sort_order (signed) and physical_type (FIXED_LENGTH_BYTE_ARRAY), there is no override for decimal.",
        "Issue Links": [
            "/jira/browse/ARROW-12054",
            "https://github.com/apache/arrow/pull/9582"
        ]
    },
    "PARQUET-1656": {
        "Key": "PARQUET-1656",
        "Summary": "Schema change  results in exception - java.lang.ClassCastException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1,                                            1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Xinli Shang",
        "Reporter": "Balajee Nagasubramaniam",
        "Created": "18/Sep/19 18:14",
        "Updated": "21/Dec/19 00:53",
        "Resolved": null,
        "Description": "Following exception was seen with parquet 1.8.1 (and in parquet 1.12.0, when trying to reproduce it).\nException in thread \"main\" java.lang.ClassCastException: optional binary phone_number (STRING) is not a group\nat com.uber.komondor.shaded.org.apache.parquet.schema.Type.asGroupType(Type.java:250)\nat com.uber.komondor.shaded.org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:279)\nat com.uber.komondor.shaded.org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:232)\nat com.uber.komondor.shaded.org.apache.parquet.avro.AvroRecordConverter.access$100(AvroRecordConverter.java:78)\nat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter$ElementConverter.<init>(AvroRecordConverter.java:536)\nat org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter.<init>(AvroRecordConverter.java:486)\nat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:289)\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:141)\nat org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:279)\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:141)\nat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\nat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\nat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\nat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\nat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\nat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\nat util.ParquetToAvroSchemaConverter$.convert(ParquetToAvroSchemaConverter.scala:46)\nat util.ParquetToAvroSchemaConverter$.main(ParquetToAvroSchemaConverter.scala:20)\nat util.ParquetToAvroSchemaConverter.main(ParquetToAvroSchemaConverter.scala)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nOriginal exception was triggered by the following schema change.\nSchema Before change:\n                         {\n                            \"default\": null,\n                            \"name\": \"master_cluster\",\n                            \"type\": [\n                                \"null\",\n                                {\n                                    \"fields\": [\n\n{\r\n                                            \"name\": \"uuid\",\r\n                                            \"type\": \"string\"\r\n                                        }\n,\n\n{\r\n                                            \"name\": \"namespace\",\r\n                                            \"type\": \"string\"\r\n                                        }\n,\n                                        {\r\n                                            \"name\": \"version\",\r\n                                            \"type\": \"long\"\r\n                                        }\n                                    ],\n                                    \"name\": \"master_cluster\",\n                                    \"type\": \"record\"\n                                }\n                            ]\n                        },\nAfter schema change:\n                        {\n                            \"default\": null,\n                            \"name\": \"master_cluster\",\n                            \"type\": [\n                                \"null\",\n                                {\n                                    \"fields\": [\n\n{\r\n                                            \"default\": null,\r\n                                            \"name\": \"uuid\",\r\n                                            \"type\": [\r\n                                                \"null\",\r\n                                                \"string\"\r\n                                            ]\r\n                                        }\n,\n\n{\r\n                                            \"default\": null,\r\n                                            \"name\": \"namespace\",\r\n                                            \"type\": [\r\n                                                \"null\",\r\n                                                \"string\"\r\n                                            ]\r\n                                        }\n,\n                                        {\r\n                                            \"default\": null,\r\n                                            \"name\": \"version\",\r\n                                            \"type\": [\r\n                                                \"null\",\r\n                                                \"long\"\r\n                                            ]\r\n                                        }\n                                    ],\n                                    \"name\": \"VORGmaster_cluster\",\n                                    \"type\": \"record\"\n                                }\n                            ]\n                        },\nWe were suspecting PARQUET-1441 could be in play and tried to reproduce the issue on parquet-1.12.0 and seeing the same exception.\nDuring the repro noticed that issue could be with avroSchema conversion (field name was substituted with generic name \"array\").  While we look into this further, want to get community input on whether this is a known issue and any thoughts on path forward.\n19/09/12 22:34:37 DEBUG avro.SchemaCompatibility: Checking compatibility of reader {\"type\":\"record\",\"name\":\"IDphones_items\",\"fields\":[\n{\"name\":\"phone_number\",\"type\":[\"null\",\"string\"],\"default\":null}\n]} with writer {\"type\":\"record\",\"name\":\"array\",\"fields\":[\n{\"name\":\"phone_number\",\"type\":[\"null\",\"string\"],\"default\":null}\n]}",
        "Issue Links": []
    },
    "PARQUET-1657": {
        "Key": "PARQUET-1657",
        "Summary": "[C++] Change Bloom filter implementation to use xxhash",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "18/Sep/19 22:27",
        "Updated": "22/Aug/22 11:34",
        "Resolved": null,
        "Description": "I also strongly recommend doing away with the virtual function calls if possible. We have vendored xxhash in Apache Arrow so we should also remove the murmur3 code while we are at it",
        "Issue Links": [
            "/jira/browse/ARROW-3298"
        ]
    },
    "PARQUET-1658": {
        "Key": "PARQUET-1658",
        "Summary": "travis preparing script for bloom-filter branch failed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "20/Sep/19 08:35",
        "Updated": "20/Sep/19 08:35",
        "Resolved": null,
        "Description": "The dev/travis-before_install-bloom-filter.sh will download the parquet format repo and install, It failed recently because the parquet-format repo introduces a file with no license.\u00a0 See https://travis-ci.org/apache/parquet-mr/builds/587361529?utm_source=github_status&utm_medium=notification.",
        "Issue Links": []
    },
    "PARQUET-1659": {
        "Key": "PARQUET-1659",
        "Summary": "Add AES-CTR to Parquet Encryption",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Do",
        "Affects Version/s": "format-2.6.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "20/Sep/19 18:20",
        "Updated": "30/Mar/21 15:59",
        "Resolved": "30/Mar/21 15:59",
        "Description": "AES-GCM-CTR perform GCM encryption on metadata and CTR encryption on data.\nAES-CTR would perform CTR encryption on both.\u00a0\nDuring Perf testing, we found AES-CTR can improve read/write performance by ~10% comparing with AES-GCM-CTR.\n\u00a0\nI checked with Gidon and the initial assumption was that AES-GCM-CTR would have similar performance as AES-CTR. But with recent performance benchmarking, we found it is worthy to introduce AES-CTR. Since many companies strive for parquet performance improvement.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/153"
        ]
    },
    "PARQUET-1660": {
        "Key": "PARQUET-1660",
        "Summary": "[java] Align Bloom filter implementation with format",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "23/Sep/19 03:26",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/680",
            "https://github.com/apache/parquet-mr/pull/686"
        ]
    },
    "PARQUET-1661": {
        "Key": "PARQUET-1661",
        "Summary": "Upgrade to Avro 1.9.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-avro",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "23/Sep/19 09:35",
        "Updated": "08/Nov/19 07:19",
        "Resolved": "23/Sep/19 10:46",
        "Description": "Avro 1.9.1 fixes some regression issues from 1.9.0",
        "Issue Links": [
            "/jira/browse/PARQUET-1645",
            "/jira/browse/PARQUET-1576",
            "https://github.com/apache/parquet-mr/pull/682"
        ]
    },
    "PARQUET-1662": {
        "Key": "PARQUET-1662",
        "Summary": "Upgrade Jackson to version 2.9.10",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cli,                                            parquet-thrift",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "23/Sep/19 15:11",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "24/Sep/19 08:05",
        "Description": "Jackson 2.9.10 addresses multiple CVE issues from previous Jackson versions, so we need to upgrade it.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/683"
        ]
    },
    "PARQUET-1663": {
        "Key": "PARQUET-1663",
        "Summary": "[C++] Provide API to check the presence of complex data types",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Zherui Cao",
        "Reporter": "Zherui Cao",
        "Created": "23/Sep/19 19:01",
        "Updated": "17/Mar/20 10:10",
        "Resolved": "11/Mar/20 03:41",
        "Description": "we need functions like\nhasMapType()\nhasArrayType()",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5490"
        ]
    },
    "PARQUET-1664": {
        "Key": "PARQUET-1664",
        "Summary": "[C++] Provide API to return metadata string from FileMetadata.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Zherui Cao",
        "Reporter": "Zherui Cao",
        "Created": "23/Sep/19 19:14",
        "Updated": "05/Nov/19 02:13",
        "Resolved": "05/Nov/19 02:13",
        "Description": "we need to add a function like\nFileMetaData::ToString()\nthis is useful when people want a string format of metadata.\nIf loads of file sharing the same fileMetaData, ToString will provide a binary string that stay in memory to reduce redundant metadata reding time.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5481"
        ]
    },
    "PARQUET-1665": {
        "Key": "PARQUET-1665",
        "Summary": "Upgrade zstd-jni to 1.4.0-1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "24/Sep/19 00:50",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "24/Sep/19 08:14",
        "Description": "PARQUET-1661 upgrades Avro's version to 1.9.1, which uses zstd-jni 1.4.0-1. We should also upgrade that, as specified in the discussion on https://github.com/apache/parquet-mr/pull/653.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/684"
        ]
    },
    "PARQUET-1666": {
        "Key": "PARQUET-1666",
        "Summary": "Remove Unused Modules",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Xinli Shang",
        "Created": "24/Sep/19 15:10",
        "Updated": "12/Jan/21 10:37",
        "Resolved": "12/Jan/21 10:37",
        "Description": "In the last two meetings, Ryan Blue proposed to remove some unused Parquet modules. This is to open a task to track it.\u00a0\nHere are the related meeting notes for the discussion on this.\u00a0\nRemove old Parquet modules\nHive modules - sounds good\nScooge - Julien will reach out to twitter\nTools - undecided - Cloudera may still use the parquet-tools according to Gabor.\nCascading - undecided\nWe can change the module as deprecated as description.",
        "Issue Links": [
            "/jira/browse/PARQUET-1676"
        ]
    },
    "PARQUET-1667": {
        "Key": "PARQUET-1667",
        "Summary": "Close InputStream after usage",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "25/Sep/19 07:18",
        "Updated": "13/Nov/19 12:32",
        "Resolved": "07/Nov/19 08:49",
        "Description": "Make sure that the streams are closed using try-with-resources",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/685"
        ]
    },
    "PARQUET-1668": {
        "Key": "PARQUET-1668",
        "Summary": "Show string min/max in parquet-cli/tools meta command",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "EdisonWang",
        "Created": "30/Sep/19 14:19",
        "Updated": "09/Dec/19 08:22",
        "Resolved": null,
        "Description": "It's better to show min/max of string type by default when using parquet-cli/parquet-tools meta command, by setting 'parquet.strings.signed-min-max.enabled' to true",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/687"
        ]
    },
    "PARQUET-1669": {
        "Key": "PARQUET-1669",
        "Summary": "Disable compiling all libraries when building thrift",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "30/Sep/19 20:52",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "02/Oct/19 13:08",
        "Description": "PARQUET-1654 disabled compiling all Thrift libraries except C++ and Python3 for speeding up the CI process. But actually, the parquet build process requires only the Thrift IDL compiler to be installed and doesn't need any language-specific libraries (I misunderstood before).\nSo we can disable compiling all Thrift libraries using the --disable-libs option. It will save around 3 minutes for each CI run.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/688"
        ]
    },
    "PARQUET-1670": {
        "Key": "PARQUET-1670",
        "Summary": "parquet-tools merge extremely slow with block-option",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alexander Gunkel",
        "Created": "01/Oct/19 09:00",
        "Updated": "01/Oct/19 14:18",
        "Resolved": null,
        "Description": "parquet-tools merge is extremely time- and memory-consuming when used with block-option.\n\u00a0\nThe merge function builds a bigger file out of several smaller parquet-files. Used without the block-option it just concatenates the files into a bigger one without building larger row-groups. That doesn't help with query-performance-issues. With block-option, parquet-tools build bigger row-groups which improves the query-performance, but the merge-process itself is extremely slow and memory-consuming.\n\u00a0\nConsider a case in which you have many small parquet files, e.g. 1000 files with a size of 100kb. Merging them into one file fails on my machine because even 20GB of memory are not enough for the process (the total amount of data as well as the resulting file should be smaller than 100MB).\n\u00a0\nDifferent situation: Consider having 100 files of size 1MB. Then merging them is possible with 20GB of RAM, but it takes almoust half an hour to process, which is to much for many use-cases.\n\u00a0\nIs there any possibility to accelerate the merge and reduce the need of memory?",
        "Issue Links": []
    },
    "PARQUET-1671": {
        "Key": "PARQUET-1671",
        "Summary": "Upgrade Yetus to 0.11.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "01/Oct/19 09:05",
        "Updated": "10/Dec/19 00:35",
        "Resolved": "04/Oct/19 08:59",
        "Description": "parquet-common is currently using Yetus 0.7.0 for annotating methods, but that version has already been EOL'd in YETUS-861. We should upgrade it to the latest one.",
        "Issue Links": [
            "/jira/browse/PARQUET-1699",
            "https://github.com/apache/parquet-mr/pull/689"
        ]
    },
    "PARQUET-1672": {
        "Key": "PARQUET-1672",
        "Summary": "[DOC] Broken link to \"How To Contribute\" section in Parquet-MR project",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.8.0",
        "Component/s": "None",
        "Assignee": "Tarek Allam",
        "Reporter": "Tarek Allam",
        "Created": "03/Oct/19 13:55",
        "Updated": "11/Dec/19 10:45",
        "Resolved": "07/Oct/19 10:27",
        "Description": "Link the \"How To Contribute\" section in Parquet-MR project returns 404 error as the URL is expanded incorrectly.\nA small change to https://github.com/apache/parquet-mr#how-to-contribute\u00a0should correct this.",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/154"
        ]
    },
    "PARQUET-1673": {
        "Key": "PARQUET-1673",
        "Summary": "Upgrade parquet-mr format version to 2.7.0",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "N\u00e1ndor Koll\u00e1r",
        "Created": "04/Oct/19 14:57",
        "Updated": "24/Oct/19 12:41",
        "Resolved": "06/Oct/19 20:55",
        "Description": "Latest format release has several important additions like Bloom filters, encryption, as well as fixes which unblock Java 11 compatibility efforts.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/690"
        ]
    },
    "PARQUET-1674": {
        "Key": "PARQUET-1674",
        "Summary": "The announcement email on the web site does not comply with ASF rules",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-site",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Jim Apple",
        "Created": "05/Oct/19 03:39",
        "Updated": "13/Nov/19 09:47",
        "Resolved": "13/Nov/19 09:47",
        "Description": "After following the instructions in the release guide on the Parquet website, my mail to announce@apache.org\u00a0was rejected with the following message:\n\u00a0\nSorry, but the announce email cannot be accepted as it stands.\nAnnouncements of Apache project releases must contain a link to the\nrelevant download page. [1]\nThe download page must provide public download links where current official\nsource releases and accompanying cryptographic files may be obtained. [2]\nIt must also link to the KEYS file at\nhttps://www.apache.org/dist/<project>/KEYS,\nand provide details of how to verify a download using the signature or a\nhash [3]\nNote also that MD5 and SHA1 hashes are deprecated and should not be used\nfor new releases. [4]\nAnnouncements that contain a link to the dyn/closer page alone will be\nrejected by the moderators.\nAnnouncements that contain a link to the dist.apache.org host will be\nrejected by the moderators.\nAnnouncements that contain a link to a web page that does not include a\nlink to a mirror to the artifact plus links to the signature and at least\none sha checksum will be rejected.\n[1] https://www.apache.org/legal/release-policy.html#release-announcements\n<http://www.apache.org/legal/release-policy.html#release-announcements>\n[2] https://www.apache.org/dev/release-distribution#download-links\n[3] https://www.apache.org/dev/release-download-pages.html#download-page\n[4] https://www.apache.org/dev/release-distribution#sigs-and-sums",
        "Issue Links": [
            "https://github.com/apache/parquet-site/pull/1"
        ]
    },
    "PARQUET-1675": {
        "Key": "PARQUET-1675",
        "Summary": "Switch to git for website",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Jim Apple",
        "Created": "05/Oct/19 03:54",
        "Updated": "30/Oct/19 12:36",
        "Resolved": "30/Oct/19 12:27",
        "Description": "Using git for the website, rather than SVN, would allow website changes to be proposed by non-committers as pull requests and reviewed in Github. For more, see:\n\u00a0\nhttps://blogs.apache.org/infra/entry/git_based_websites_available\nhttps://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories\nhttps://www.apache.org/dev/project-site.html",
        "Issue Links": [
            "/jira/browse/PARQUET-1686"
        ]
    },
    "PARQUET-1676": {
        "Key": "PARQUET-1676",
        "Summary": "Remove hive modules",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "07/Oct/19 07:33",
        "Updated": "27/Jan/21 10:54",
        "Resolved": "27/Jan/21 10:54",
        "Description": "Remove the hive modules as discusses in the Parquet sync.",
        "Issue Links": [
            "/jira/browse/PARQUET-1666",
            "https://github.com/apache/parquet-mr/pull/691"
        ]
    },
    "PARQUET-1677": {
        "Key": "PARQUET-1677",
        "Summary": "Bump Apache Pig from 0.16.0 to 0.17.0",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-pig",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "15/Oct/19 08:33",
        "Updated": "27/Jan/21 10:55",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/692"
        ]
    },
    "PARQUET-1678": {
        "Key": "PARQUET-1678",
        "Summary": "[C++] Provide classes for reading/writing using input/output operators",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "15/Oct/19 20:33",
        "Updated": "07/Nov/19 21:30",
        "Resolved": "07/Nov/19 03:18",
        "Description": "The current Parquet APIs allow for reading/writing data using either:\n\nA high level API whereby all data for an each column is given to an arrow::*Builder class.\nOr a low-level API using\u00a0parquet::*Writer classes which allows for a column to be selected and data items added to the column as needed.\n\nUsing the low-level approach gives great flexibility but makes for cumbersome code and requires casting each column to the required\u00a0 type.\nI propose offering StreamReader and StreamWriter classes with C++ input/output operators allowing for data to be written like this:\n\n\r\n// N.B. schema has 3 columns of type std::string, std::int32_t and float.\r\nauto file_writer{ parquet:ParquetFileWriter::Open(...) };\r\nStreamWriter sw{ file_writer };\r\n// Write to output file using output operator.\r\nsw << \"A string\" << 3 << 4.5f;\r\nsw.nextRow();\r\n...\n\n\u00a0\nSimilary reading would be done as follows:\n\n\r\nauto file_reader{ parquet::ParquetFileReader::Open(...) };\r\nStreamReader sr{ file_reader };\r\nstd::string s; std::int32_t i; float f;\r\nsr >> s >> i >> f;\r\nsr.nextRow();\n\nI have written such classes and an example file which shows how to use them.\nI think that they allow for a more simple and natural API since:\n\nNo casting is needed.\nCode is simple, easy to read.\nUser defined types are easily be accommodated by having the user provide the input/output operator for the type.\nRow groups can be created \"automatically\" when a given amount of user data has been written, or explicitly by a StreamWriter method such as \"createNewRowGroup()\"\n\nI have created this ticket because where I work (www.cfm.fr) we are very interested in using Parquet, but our users have requested a stream like API.\u00a0 \u00a0We think others might also be interested in this functionality.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5699"
        ]
    },
    "PARQUET-1679": {
        "Key": "PARQUET-1679",
        "Summary": "Invalid SchemaException for UUID while using AvroParquetWriter",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Felix Kizhakkel Jose",
        "Created": "16/Oct/19 16:04",
        "Updated": "05/Nov/19 15:51",
        "Resolved": null,
        "Description": "Hi,\nI am getting\u00a0org.apache.parquet.schema.InvalidSchemaException: Cannot write a schema with an empty group: optional group id {} while I include a UUID field on my POJO object. Without UUID everything worked fine. I have seen Parquet suports UUID as part of PR-71\u00a0on 2.4 release. \n But I am getting\u00a0InvalidSchemaException on UUID. Is there anything that I am missing or its a known issue?\nMy setup details:\ngradle dependency :\ndependencies\n\n{ compile group: 'org.springframework.boot', name: 'spring-boot-starter' compile group: 'org.projectlombok', name: 'lombok', version: '1.16.6' compile group: 'com.amazonaws', name: 'aws-java-sdk-bundle', version: '1.11.271' compile group: 'org.apache.parquet', name: 'parquet-avro', version: '1.10.1' compile group: 'org.apache.hadoop', name: 'hadoop-common', version: '3.1.1' compile group: 'org.apache.hadoop', name: 'hadoop-aws', version: '3.1.1' compile group: 'org.apache.hadoop', name: 'hadoop-client', version: '3.1.1' compile group: 'joda-time', name: 'joda-time' compile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version: '2.6.5' compile group: 'com.fasterxml.jackson.datatype', name: 'jackson-datatype-joda', version: '2.6.5' }\n\nModel used:\n@Data\n public class Employee\n\n{ private UUID id; private String name; private int age; private Address address; }\n\n@Data\n public class Address\n\n{ private String streetName; private String city; private Zip zip; }\n\n@Data\n public class Zip\n\n{ private int zip; private int ext; }\n\n\u00a0\nMy Serializer Code:\npublic void serialize(List<D> inputDataToSerialize, CompressionCodecName compressionCodecName) throws IOException {\nPath path = new Path(\"s3a://parquetpoc/data_\"+compressionCodecName+\".parquet\");\n Class clazz = inputDataToSerialize.get(0).getClass();\ntry (ParquetWriter<D> writer = AvroParquetWriter.<D>builder(path)\n .withSchema(ReflectData.AllowNull.get().getSchema(clazz)) // generate nullable fields\n .withDataModel(ReflectData.get())\n .withConf(parquetConfiguration)\n .withCompressionCodec(compressionCodecName)\n .withWriteMode(OVERWRITE)\n .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_2_0)\n .build()) {\nfor (D input : inputDataToSerialize)\n\n{ writer.write(input); }\n\n}\n }\nprivate List<Employee>\u00a0getInputDataToSerialize(){\nAddress address = new Address();\naddress.setStreetName(\"Murry Ridge Dr\");\naddress.setCity(\"Murrysville\");\nZip zip = new Zip();\nzip.setZip(15668);\nzip.setExt(1234);\naddress.setZip(zip);\nList<Employee> employees = new ArrayList<>();\nIntStream.range(0, 100000).forEach(i->\n\n{ Employee employee = new Employee(); // employee.setId(UUID.randomUUID()); employee.setAge(20); employee.setName(\"Test\"+i); employee.setAddress(address); employees.add(employee); }\n\n);\nreturn employees;\n}\n**Where generic Type D is Employee",
        "Issue Links": []
    },
    "PARQUET-1680": {
        "Key": "PARQUET-1680",
        "Summary": "Parquet Java Serialization is  very slow",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Felix Kizhakkel Jose",
        "Created": "16/Oct/19 18:56",
        "Updated": "28/Oct/19 20:02",
        "Resolved": null,
        "Description": "Hi,\n I am doing a POC to compare different data formats and its performance in terms of serialization/deserialization speed, storage size, compatibility between different language etc.\u00a0\n When I try to serialize a simple java object to parquet file,\u00a0 it takes 6-7 seconds vs same object's serialization to JSON is 100 milliseconds.\nCould you help me to resolve this issue?\n+My Configuration and code snippet:\nGradle dependencies\n dependencies\n\n{ compile group: 'org.springframework.boot', name: 'spring-boot-starter' compile group: 'org.projectlombok', name: 'lombok', version: '1.16.6' compile group: 'com.amazonaws', name: 'aws-java-sdk-bundle', version: '1.11.271' compile group: 'org.apache.parquet', name: 'parquet-avro', version: '1.10.0' compile group: 'org.apache.hadoop', name: 'hadoop-common', version: '3.1.1' compile group: 'org.apache.hadoop', name: 'hadoop-aws', version: '3.1.1' compile group: 'org.apache.hadoop', name: 'hadoop-client', version: '3.1.1' compile group: 'joda-time', name: 'joda-time' compile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version: '2.6.5' compile group: 'com.fasterxml.jackson.datatype', name: 'jackson-datatype-joda', version: '2.6.5' }\n\nCode snippet:+\npublic void serialize(List<D> inputDataToSerialize, CompressionCodecName compressionCodecName) throws IOException {\nPath path = new Path(\"s3a://parquetpoc/data_\"+compressionCodecName+\".parquet\");\n Path path1 = new Path(\"/Downloads/data_\"+compressionCodecName+\".parquet\");\n Class clazz = inputDataToSerialize.get(0).getClass();\ntry (ParquetWriter<D> writer = AvroParquetWriter.<D>builder(path1)\n .withSchema(ReflectData.AllowNull.get().getSchema(clazz)) // generate nullable fields\n .withDataModel(ReflectData.get())\n .withConf(parquetConfiguration)\n .withCompressionCodec(compressionCodecName)\n .withWriteMode(OVERWRITE)\n .withWriterVersion(ParquetProperties.WriterVersion.PARQUET_2_0)\n .build()) {\nfor (D input : inputDataToSerialize)\n\n{ writer.write(input); }\n\n}\n }\n+Model Used:\n @Data\n public class Employee\n\n{ //private UUID id; private String name; private int age; private Address address; }\n\n@Data\n public class Address\n\n{ private String streetName; private String city; private Zip zip; }\n\n@Data\n public class Zip\n\n{ private int zip; private int ext; }\n\n\u00a0\nprivate List<Employee> getInputDataToSerialize(){\n Address address = new Address();\n address.setStreetName(\"Murry Ridge Dr\");\n address.setCity(\"Murrysville\");\n Zip zip = new Zip();\n zip.setZip(15668);\n zip.setExt(1234);\n address.setZip(zip);\n List<Employee> employees = new ArrayList<>();\n IntStream.range(0, 100000).forEach(i->\n{\r\n Employee employee = new Employee();\r\n // employee.setId(UUID.randomUUID());\r\n employee.setAge(20);\r\n employee.setName(\"Test\"+i);\r\n employee.setAddress(address);\r\n employees.add(employee);\r\n }\n);\nreturn employees;\n}\nNote:\nI have tried to save the data into local file system as well as AWS S3, but both are having same result - very slow.",
        "Issue Links": []
    },
    "PARQUET-1681": {
        "Key": "PARQUET-1681",
        "Summary": "Avro's isElementType() change breaks the reading of some parquet(1.8.1) files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0,                                            1.9.1,                                            1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "18/Oct/19 19:16",
        "Updated": "21/Apr/22 15:48",
        "Resolved": null,
        "Description": "When using the Avro schema below to write a parquet(1.8.1) file and then read back by using parquet 1.10.1 without passing any schema, the reading throws an exception \"XXX is not a group\" . Reading through parquet 1.8.1 is fine.\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"phones\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"null\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"array\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"items\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"record\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"phones_items\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"fields\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n{ \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"phone_number\", \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": [ \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"null\", \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"string\" \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0], \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"default\": null \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"default\": null\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\nThe code to read is as below\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0val reader = AvroParquetReader.builder[SomeRecordType](parquetPath).withConf(new \u00a0 Configuration).build()\n\u00a0\u00a0\u00a0\u00a0reader.read()\nPARQUET-651 changed the method isElementType() by relying on Avro's checkReaderWriterCompatibility() to check the compatibility. However, checkReaderWriterCompatibility() consider the ParquetSchema and the AvroSchema(converted from File schema) as not compatible(the name in avro schema is \u2018phones_items\u2019, but the name is \u2018array\u2019 in Parquet schema, hence not compatible) . Hence return false and caused the \u201cphone_number\u201d field in the above schema to be considered as group type which is not true. Then the exception throws as .asGroupType().\u00a0\nI didn\u2019t try writing via parquet 1.10.1 would reproduce the same problem or not. But it could because the translation of Avro schema to Parquet schema is not changed(didn\u2019t verify yet).\u00a0\n\u00a0I hesitate to revert PARQUET-651 because it solved several problems. I would like to hear the community's thoughts on it.",
        "Issue Links": [
            "/jira/browse/PARQUET-651"
        ]
    },
    "PARQUET-1682": {
        "Key": "PARQUET-1682",
        "Summary": "Maintain forward compatibility for TIME/TIMESTAMP",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "21/Oct/19 10:56",
        "Updated": "22/Oct/19 14:36",
        "Resolved": "22/Oct/19 14:36",
        "Description": "Update parquet-mr implementation according to PARQUET-1627.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/694"
        ]
    },
    "PARQUET-1683": {
        "Key": "PARQUET-1683",
        "Summary": "Remove unnecessary string converting in readFooter method",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "21/Oct/19 16:47",
        "Updated": "24/Oct/19 12:27",
        "Resolved": "24/Oct/19 12:27",
        "Description": "The method (String filePath = file.toString()) is always called even filePath is not used(it is only used when an exception is thrown which is rare).\u00a0 This kind of string conversion should be avoided when it is not used in terms of memory & CPU efficiency.\u00a0\n\u00a0\nhttps://github.com/apache/parquet-mr/pull/695",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/695"
        ]
    },
    "PARQUET-1684": {
        "Key": "PARQUET-1684",
        "Summary": "[parquet-protobuf] default protobuf field values are stored as nulls",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0,                                            1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Priyank Bagrecha",
        "Reporter": "George Haddad",
        "Created": "23/Oct/19 12:11",
        "Updated": "25/Sep/20 14:58",
        "Resolved": "02/Jun/20 16:10",
        "Description": "When the source is a protobuf3 message, and the target file is Parquet, all the default values are stored in the output parquet as `null` instead of the actual type's default value.\n For example, if the field is of type `int32`, `double` or `enum` and it hasn't been set, the parquet value is `null` instead of `0`. When the field's type is a `string` that hasn't been set, the parquet value is `null` instead of an empty string.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/702"
        ]
    },
    "PARQUET-1685": {
        "Key": "PARQUET-1685",
        "Summary": "Truncate the stored min and max for String statistics to reduce the footer size",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "26/Oct/19 01:53",
        "Updated": "13/Nov/19 09:15",
        "Resolved": "13/Nov/19 09:15",
        "Description": "Iceberg has a cool feature that truncates the stored min, max\u00a0statistics to minimize the metadata size. We can borrow to truncate them in Parquet also to reduce the size of the footer, or even the page header. Here is the code in IceBerg\u00a0https://github.com/apache/incubator-iceberg/blob/master/api/src/main/java/org/apache/iceberg/util/UnicodeUtil.java.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/696",
            "https://docs.google.com/document/d/1Mgb0dDXQJkgjouboDrGa9v06hWGJ0oPiwnmffXShQ_M"
        ]
    },
    "PARQUET-1686": {
        "Key": "PARQUET-1686",
        "Summary": "Automate site generation",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-site",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "30/Oct/19 12:36",
        "Updated": "30/Oct/19 14:12",
        "Resolved": null,
        "Description": "We moved our site source to github. It is much better than svn but still not working as it should. Currently, we have to generate the site manually before checking in. It would be much better if the site generation would be automatic so we can simply accept PRs on the source files.\n One option to achieve this is\u00a0the\u00a0Pelican CMS System\u00a0as described at\u00a0.asf.yaml features for git repositories. Not sure if this is the best solution though. Another solution might be to trigger a jenkins build for the changes on master and after generating the site with middleman commit the files to the branch asf-site.",
        "Issue Links": [
            "/jira/browse/PARQUET-1675"
        ]
    },
    "PARQUET-1687": {
        "Key": "PARQUET-1687",
        "Summary": "Update release process",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0,                                            format-2.8.0",
        "Component/s": "parquet-format,                                            parquet-mr,                                            parquet-site",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "04/Nov/19 09:13",
        "Updated": "13/Nov/19 12:03",
        "Resolved": "12/Nov/19 15:57",
        "Description": "Our current tagging policy in the release process requires to use the same tag for all the release candidates which means at RC2 we remove the tag from RC1 head and adds again to the RC2 head and so on. It is not a good practice. Hard to track RCs and rewriting git history is usually not a good idea.\nUpdate the related scripts and docs so we will create RC tags (e.g. apache-parquet-1.11.0-rc6) first and add the final release tag (e.g. apache-parquet-1.11.0) after the vote passes.\nIt is also very confusing that we always use SNAPSHOT version of the next release even if the previous one is not released yet. For example we have not released 1.11.0 yet (we are after the failed vote of 1.11.0-rc6) still, we use the version number 1.12.0-SNAPSHOT. This is very confusing.\nUpdate the release scripts and docs to use the same SNAPSHOT version at RC phase and only step to the next version number after the vote passed and the final release tag is added.",
        "Issue Links": [
            "https://github.com/apache/parquet-site/pull/2",
            "https://github.com/apache/parquet-format/pull/155",
            "https://github.com/apache/parquet-mr/pull/697"
        ]
    },
    "PARQUET-1688": {
        "Key": "PARQUET-1688",
        "Summary": "[C++] StreamWriter/StreamReader can't be built with g++ 4.8.5 on CentOS 7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Kouhei Sutou",
        "Created": "07/Nov/19 21:23",
        "Updated": "08/Nov/19 17:01",
        "Resolved": "08/Nov/19 17:00",
        "Description": "cc gawain_bolton\nThis is caused since PARQUET-1678 is merged.\nIt seems that g++ 4.8.5 on CentOS 7 doesn't have the default implementation of operator=() noexcept:\nhttps://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=2562&view=logs&jobId=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&taskId=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&lineStart=5469&lineEnd=5484&colStart=1&colEnd=1\n\nIn file included from /root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_reader.h:31:0,\r\n                 from /root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_reader.cc:18:\r\n/root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_writer.h:67:17: error: function 'parquet::StreamWriter& parquet::StreamWriter::operator=(parquet::StreamWriter&&)' defaulted on its first declaration with an exception-specification that differs from the implicit declaration 'parquet::StreamWriter& parquet::StreamWriter::operator=(parquet::StreamWriter&&)'\r\n   StreamWriter& operator=(StreamWriter&&) noexcept = default;\r\n                 ^\r\nIn file included from /root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_reader.cc:18:0:\r\n/root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_reader.h:61:17: error: function 'parquet::StreamReader& parquet::StreamReader::operator=(parquet::StreamReader&&)' defaulted on its first declaration with an exception-specification that differs from the implicit declaration 'parquet::StreamReader& parquet::StreamReader::operator=(parquet::StreamReader&&)'\r\n   StreamReader& operator=(StreamReader&&) noexcept = default;\r\n                 ^\r\nmake[2]: *** [src/parquet/CMakeFiles/parquet_objlib.dir/stream_reader.cc.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nIn file included from /root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_writer.cc:18:0:\r\n/root/rpmbuild/BUILD/apache-arrow-0.15.0.dev227/cpp/src/parquet/stream_writer.h:67:17: error: function 'parquet::StreamWriter& parquet::StreamWriter::operator=(parquet::StreamWriter&&)' defaulted on its first declaration with an exception-specification that differs from the implicit declaration 'parquet::StreamWriter& parquet::StreamWriter::operator=(parquet::StreamWriter&&)'\r\n   StreamWriter& operator=(StreamWriter&&) noexcept = default;\r\n                 ^",
        "Issue Links": [
            "/jira/browse/ARROW-7088",
            "https://github.com/apache/arrow/pull/5792"
        ]
    },
    "PARQUET-1689": {
        "Key": "PARQUET-1689",
        "Summary": "[C++] Stream API: Allow for columns/rows to be skipped when reading",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "09/Nov/19 14:53",
        "Updated": "22/Nov/19 18:56",
        "Resolved": "22/Nov/19 18:56",
        "Description": "It can be useful to be able to skip rows and/or columns when reading data.\nThe ColumnReader class already allows for data to be skipped.\nThis new StreamReader class could use this functionality to allow for users to skip columns and rows when using the StreamReader API.\nI will propose this functionality by submitting a PR.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5797"
        ]
    },
    "PARQUET-1690": {
        "Key": "PARQUET-1690",
        "Summary": "Integer Overflow of BinaryStatistics#isSmallerThan()",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "10/Nov/19 16:35",
        "Updated": "17/Mar/23 14:13",
        "Resolved": null,
        "Description": "\"(min.length() + max.length()) < size\" didn't handle integer overflow\u00a0\nhttps://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/statistics/BinaryStatistics.java#L103",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/704",
            "https://github.com/apache/parquet-mr/pull/721",
            "https://github.com/apache/parquet-mr/pull/722"
        ]
    },
    "PARQUET-1691": {
        "Key": "PARQUET-1691",
        "Summary": "Build fails due to missing hadoop-lzo",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "11/Nov/19 12:57",
        "Updated": "13/Nov/19 12:03",
        "Resolved": "12/Nov/19 14:43",
        "Description": "Travis builds fail due to missing com.hadoop.gplcompression:hadoop-lzo:jar:0.4.16. See details in the Travis log. Issue is reproducible locally be removing hadoop-lzo from the local maven repository and starting the build.\nSee more details about the issue in the hadoop-lzo bug tracking.\nhadoop-lzo is not required for build/test parquet-mr; it is a transitive dependency pulled in by elephant-bird-core. Possible fix is to exclude hadoop-lzo from the dependency tree.",
        "Issue Links": [
            "/jira/browse/PARQUET-1556",
            "https://github.com/apache/parquet-mr/pull/698"
        ]
    },
    "PARQUET-1692": {
        "Key": "PARQUET-1692",
        "Summary": "[C++] LogicalType::FromThrift error on Centos 7 RPM",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Or Ozeri",
        "Reporter": "Or Ozeri",
        "Created": "14/Nov/19 08:55",
        "Updated": "28/Jan/20 21:49",
        "Resolved": "05/Dec/19 21:36",
        "Description": "When using\u00a0FileMetaDataBuilder with a PrimitiveNode with converted type INT_64, we get the following exception message:\n\"Metadata contains Thrift LogicalType that is not recognized\"\n\u00a0\nThis happens when using the Centos 7 RPMs.\n\u00a0\nSee attached Dockerfile which reproduces the bug.",
        "Issue Links": [
            "/jira/browse/ARROW-7339",
            "https://github.com/apache/arrow/pull/5900",
            "https://github.com/apache/arrow/pull/5946",
            "https://github.com/apache/arrow/pull/5958"
        ]
    },
    "PARQUET-1693": {
        "Key": "PARQUET-1693",
        "Summary": "[C++] Build examples don't account for CMAKE compression feature flags",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Francois Saint-Jacques",
        "Created": "14/Nov/19 14:39",
        "Updated": "09/Dec/19 16:01",
        "Resolved": "15/Nov/19 16:55",
        "Description": "Examples will fail at runtime if compression flags are off (which is now the default).",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5833"
        ]
    },
    "PARQUET-1694": {
        "Key": "PARQUET-1694",
        "Summary": "Restore ColumnChunkPageWriter constructors",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "17/Nov/19 19:56",
        "Updated": "31/Dec/19 11:25",
        "Resolved": "10/Dec/19 13:17",
        "Description": "Restore previous constructors to maintain backward compatibility.",
        "Issue Links": [
            "/jira/browse/PARQUET-1733",
            "https://github.com/apache/parquet-mr/pull/699"
        ]
    },
    "PARQUET-1695": {
        "Key": "PARQUET-1695",
        "Summary": "Disable crc checksums by default",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "17/Nov/19 20:22",
        "Updated": "02/Jan/20 12:16",
        "Resolved": null,
        "Description": "CRC checksums have been introduced in 1.11.0. This negatively impacts performance, and I would like to make it an opt-in choice.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/700"
        ]
    },
    "PARQUET-1696": {
        "Key": "PARQUET-1696",
        "Summary": "Remove unused hadoop-1 profile",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "19/Nov/19 09:17",
        "Updated": "31/Dec/19 18:15",
        "Resolved": "31/Dec/19 18:15",
        "Description": "It seems the tests do not run tests based on the old Hadoop 1 versions profile so we can remove it 'safely'. (I could not find a ticket referring to the removal of Hadoop 1.x support but I suppose it is something it already happened but this is a leftover. Probably not supporting Hadoop 1 is not a big thing anymore given that latest maintained release of Hadoop 1 happened 6y ago.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/701"
        ]
    },
    "PARQUET-1697": {
        "Key": "PARQUET-1697",
        "Summary": "Make Parquet Hadoop 3 compatible",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "19/Nov/19 09:26",
        "Updated": "19/Nov/19 10:33",
        "Resolved": null,
        "Description": "It seems Hadoop 3 is getting more mature and the downstream projects and distros (e.g. Spark, CDP) are slowly transitioning to use it.\nSince Hadoop 2 is probably the standard for a long term maybe we can just benefit of having an extra test configuration that provides the Hadoop 3.x dependencies to ensure that Parquet's current code base is 'forwards' compatible.",
        "Issue Links": []
    },
    "PARQUET-1698": {
        "Key": "PARQUET-1698",
        "Summary": "[C++] Add reader option to pre-buffer entire serialized row group into memory",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "David Li",
        "Reporter": "Wes McKinney",
        "Created": "21/Nov/19 04:15",
        "Updated": "24/Mar/21 16:09",
        "Resolved": "24/Mar/21 16:09",
        "Description": "In some scenarios (example: reading datasets from Amazon S3), reading columns independently and allowing unbridled Read calls to the underlying file handle can yield suboptimal performance. In such cases, it may be preferable to first read the entire serialized row group into memory then deserialize the constituent columns from this\nNote that such an option would not be appropriate as a default behavior for all file handle types since low-selectivity reads (example: reading only 3 columns out of a file with 100 columns)  will be suboptimal in some cases. I think it would be better for \"high latency\" file systems to opt into this option\ncc fsaintjacques bkietz apitrou",
        "Issue Links": [
            "/jira/browse/PARQUET-1820",
            "/jira/browse/PARQUET-1820",
            "/jira/browse/ARROW-11601",
            "/jira/browse/ARROW-8763",
            "/jira/browse/ARROW-7995",
            "https://github.com/apache/arrow/pull/6138"
        ]
    },
    "PARQUET-1699": {
        "Key": "PARQUET-1699",
        "Summary": "Could not resolve org.apache.yetus:audience-annotations:0.11.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Priyank Bagrecha",
        "Reporter": "Priyank Bagrecha",
        "Created": "23/Nov/19 02:47",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "20/Apr/20 13:16",
        "Description": "Trying to use parquet-protobuf and get this via parquet-common. I'm using latest on master branch\n\n\r\n> Could not resolve org.apache.yetus:audience-annotations:0.11.0.\r\n  Required by:\r\n      project : > org.apache.parquet:parquet-common:1.11.0-SNAPSHOT\r\n   > Could not resolve org.apache.yetus:audience-annotations:0.11.0.\r\n      > Could not parse POM /Users/pbagrecha/.m2/repository/org/apache/yetus/audience-annotations/0.11.0/audience-annotations-0.11.0.pom\r\n         > Unable to resolve version for dependency 'jdk.tools:jdk.tools:jar'\r\n   > Could not resolve org.apache.yetus:audience-annotations:0.11.0.\r\n      > Could not parse POM https://repo1.maven.org/maven2/org/apache/yetus/audience-annotations/0.11.0/audience-annotations-0.11.0.pom\r\n         > Unable to resolve version for dependency 'jdk.tools:jdk.tools:jar'\r\n   > Could not resolve org.apache.yetus:audience-annotations:0.11.0.\r\n      > Could not parse POM https://jcenter.bintray.com/org/apache/yetus/audience-annotations/0.11.0/audience-annotations-0.11.0.pom\r\n         > Unable to resolve version for dependency 'jdk.tools:jdk.tools:jar'",
        "Issue Links": [
            "/jira/browse/YETUS-897",
            "/jira/browse/PARQUET-1671"
        ]
    },
    "PARQUET-1700": {
        "Key": "PARQUET-1700",
        "Summary": "[C++] Stream API: Add support for repeated fields",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "23/Nov/19 21:04",
        "Updated": "26/Nov/19 05:04",
        "Resolved": null,
        "Description": "The parquet::StreamReader and parquet::StreamWriter classes currently only support required fields.\nSupport must be added to this API in order for it to be usable when the schema has repeated fields.",
        "Issue Links": [
            "/jira/browse/PARQUET-1701"
        ]
    },
    "PARQUET-1701": {
        "Key": "PARQUET-1701",
        "Summary": "[C++] Stream API: Add support for optional fields",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "24/Nov/19 15:07",
        "Updated": "14/Jan/20 16:58",
        "Resolved": "14/Jan/20 16:58",
        "Description": "The parquet::StreamReader and parquet::StreamWriter classes currently only support required fields.\nSupport must be added to this API in order for it to be usable when the schema has optional\u00a0 fields.",
        "Issue Links": [
            "/jira/browse/PARQUET-1700",
            "https://github.com/apache/arrow/pull/5928"
        ]
    },
    "PARQUET-1702": {
        "Key": "PARQUET-1702",
        "Summary": "[C++] Make BufferedRowGroupWriter compatible with parquet encryption",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-4.0.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Or Ozeri",
        "Reporter": "Or Ozeri",
        "Created": "26/Nov/19 08:50",
        "Updated": "04/Dec/19 17:40",
        "Resolved": "04/Dec/19 17:40",
        "Description": "The newly added parquet encryption feature currently works only with SerializedRowGroupWriter.\nThere are several issues preventing the use of BufferedRowGroupWriter with encryption enabled:\n1. Meta encryptor not passed on to ColumnChunkMetaDataBuilder::Finish. This can trigger a null-pointer dereference (reported as segmentation fault).\n2. UpdateEncryption not called on Close, resulting in an incorrect AAD string when encrypting the column chunk metadata.\n3. The column ordinal passed on to PageWriter::Open is always zero, resulting in a wrong AAD string when encrypting the columns data (except for the first column).\n4. When decrypting a column chunk with no dictionary pages, PARQUET-1706 confuses the decryptor to think it is decrypting a dictionary page, which again causes a wrong AAD string to be used when decrypting.\nWe propose a patch (few dozen lines) to fix the above issues.\nWe also extend the current parquet-encryption-test unit test, which tests SerializedRowGroupWriter, to test also with BufferedRowGroupWriter.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5903"
        ]
    },
    "PARQUET-1703": {
        "Key": "PARQUET-1703",
        "Summary": "Update API compatibility check",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "26/Nov/19 09:52",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "07/Jan/20 08:59",
        "Description": "The current API compatibility check is comparing the current version of parquet-mr to the release 1.7.0. It is not correct because several changes made in the public API since then which is not verified. Also, many packages are excluded from the check which are part of the public API. The semver plugin is also out of date and not maintained any more. The following tasks are to be done:\n\nFind a good tool to check API compatibility\nAlways compare to the previous minor release on master (e.g. 1.11.0 before releasing 1.12.0)\nExclude only packages/classes that are clear to not being used by our clients",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/709"
        ]
    },
    "PARQUET-1704": {
        "Key": "PARQUET-1704",
        "Summary": "[C++] Add re-usable encryption buffer to SerializedPageWriter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Or Ozeri",
        "Reporter": "Or Ozeri",
        "Created": "27/Nov/19 09:19",
        "Updated": "02/Dec/19 14:47",
        "Resolved": "02/Dec/19 11:47",
        "Description": "SerializedPageWriter currently allocates a one-time buffer for each page encrypted.\n We add a re-usable buffer to avoid the allocation and free overhead of these buffers.\n This optimization already exists when decrypting (in SerializedPageReader).\n Testing this optimization we found that it improves running time performance by ~25% (when encryption is on).\nSee attached screenshot of a profiler analysis, showing the overhead of re-allocating and freeing the buffer.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5913"
        ]
    },
    "PARQUET-1705": {
        "Key": "PARQUET-1705",
        "Summary": "[C++] Disable shrink-to-fit on the re-usable decryption buffer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Or Ozeri",
        "Reporter": "Or Ozeri",
        "Created": "27/Nov/19 09:24",
        "Updated": "02/Dec/19 14:47",
        "Resolved": "28/Nov/19 14:06",
        "Description": "We suggest to disable shrink-to-fit on the re-usable decryption buffer.\nThis can improve performance as buffer reallocations are avoided.\nSimilar use buffers (such as the compression buffers) also disable shrink-to-fit.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5914"
        ]
    },
    "PARQUET-1706": {
        "Key": "PARQUET-1706",
        "Summary": "[C++] Wrong dictionary_page_offset when writing only data pages via BufferedPageWriter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "cpp-1.5.0",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Or Ozeri",
        "Reporter": "Or Ozeri",
        "Created": "28/Nov/19 12:26",
        "Updated": "03/Dec/19 00:30",
        "Resolved": "03/Dec/19 00:30",
        "Description": "The dictionary_page_offset should be set to 0 to indicate that a column chunk has no dictionary pages.\nThis is indeed the case when using SerializedPageWriter.\nHowever, when using BufferedPageWriter, the dictionary_page_offset is wrongfully set to the offset of the first data page.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5922"
        ]
    },
    "PARQUET-1707": {
        "Key": "PARQUET-1707",
        "Summary": "[C++] parquet-arrow-test fails with undefined behaviour sanitizer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "01/Dec/19 17:27",
        "Updated": "03/Dec/19 20:26",
        "Resolved": "03/Dec/19 04:18",
        "Description": "parquet-arrow-test fails with undefined behaviour sanitizer:\n\narrow/cpp/src/parquet/arrow/test_util.h:84:15: runtime error: signed integer overflow: 37 * 86400000 cannot be represented in type 'int'\r\narrow/cpp/src/parquet/arrow/test_util.h:223:15: runtime error: signed integer overflow: 49 * 86400000 cannot be represented in type 'int'",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5929"
        ]
    },
    "PARQUET-1708": {
        "Key": "PARQUET-1708",
        "Summary": "Fix Thrift compiler warning",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.8.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Jiajia Li",
        "Reporter": "Jiajia Li",
        "Created": "03/Dec/19 08:32",
        "Updated": "31/Mar/21 15:50",
        "Resolved": "11/Dec/19 10:43",
        "Description": "Encountered the compiler warning when building:\n[WARNING:/arrow/cpp/src/parquet/parquet.thrift:297] The \"byte\" type is a compatibility alias for \"i8\". Use \"i8\" to emphasize the signedness of this type.",
        "Issue Links": [
            "/jira/browse/ARROW-7269",
            "/jira/browse/PARQUET-1425",
            "https://github.com/apache/parquet-format/pull/156"
        ]
    },
    "PARQUET-1709": {
        "Key": "PARQUET-1709",
        "Summary": "[C++] Avoid unnecessary temporary std::shared_ptr copies",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "03/Dec/19 20:23",
        "Updated": "09/Dec/19 11:05",
        "Resolved": "09/Dec/19 11:05",
        "Description": "There are several occurences of copying of std::shared_ptr objects which are easily avoided.\nCopying of std::shared_ptr objects can be expensive due to the atomic operations involved in incrementing/decrementing the reference counter.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5949"
        ]
    },
    "PARQUET-1710": {
        "Key": "PARQUET-1710",
        "Summary": "Use Objects.requireNonNull",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "04/Dec/19 01:55",
        "Updated": "26/Jan/20 20:07",
        "Resolved": "26/Jan/20 20:07",
        "Description": "https://docs.oracle.com/javase/8/docs/api/java/util/Objects.html#requireNonNull-T-java.lang.String-",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/703"
        ]
    },
    "PARQUET-1711": {
        "Key": "PARQUET-1711",
        "Summary": "[parquet-protobuf] stack overflow when work with well known json type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lawrence He",
        "Created": "05/Dec/19 02:12",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "26/Mar/23 06:04",
        "Description": "Writing following protobuf message as parquet file is not possible:\u00a0\n\n\r\nsyntax = \"proto3\";\r\n\r\nimport \"google/protobuf/struct.proto\";\r\n\r\npackage test;\r\noption java_outer_classname = \"CustomMessage\";\r\n\r\nmessage TestMessage {\r\n    map<string, google.protobuf.ListValue> data = 1;\r\n} \n\nProtobuf introduced \"well known json type\" such like ListValue\u00a0to work around json schema conversion.\u00a0\nHowever writing above messages traps parquet writer into an infinite loop due to the \"general type\" support in protobuf. Current implementation will keep referencing 6 possible types defined in protobuf (null, bool, number, string, struct, list) and entering infinite loop when referencing \"struct\".\n\n\r\njava.lang.StackOverflowErrorjava.lang.StackOverflowError at java.base/java.util.Arrays$ArrayItr.<init>(Arrays.java:4418) at java.base/java.util.Arrays$ArrayList.iterator(Arrays.java:4410) at java.base/java.util.Collections$UnmodifiableCollection$1.<init>(Collections.java:1044) at java.base/java.util.Collections$UnmodifiableCollection.iterator(Collections.java:1043) at org.apache.parquet.proto.ProtoSchemaConverter.convertFields(ProtoSchemaConverter.java:64) at org.apache.parquet.proto.ProtoSchemaConverter.addField(ProtoSchemaConverter.java:96) at org.apache.parquet.proto.ProtoSchemaConverter.convertFields(ProtoSchemaConverter.java:66) at org.apache.parquet.proto.ProtoSchemaConverter.addField(ProtoSchemaConverter.java:96) at org.apache.parquet.proto.ProtoSchemaConverter.convertFields(ProtoSchemaConverter.java:66) at org.apache.parquet.proto.ProtoSchemaConverter.addField(ProtoSchemaConverter.java:96) at org.apache.parquet.proto.ProtoSchemaConverter.convertFields(ProtoSchemaConverter.java:66) at org.apache.parquet.proto.ProtoSchemaConverter.addField(ProtoSchemaConverter.java:96) at org.apache.parquet.proto.ProtoSchemaConverter.convertFields(ProtoSchemaConverter.java:66) at org.apache.parquet.proto.ProtoSchemaConverter.addField(ProtoSchemaConverter.java:96)",
        "Issue Links": []
    },
    "PARQUET-1712": {
        "Key": "PARQUET-1712",
        "Summary": "[C++] Stop using deprecated APIs in examples",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kenta Murata",
        "Reporter": "Kenta Murata",
        "Created": "09/Dec/19 07:25",
        "Updated": "10/Dec/19 09:39",
        "Resolved": "09/Dec/19 20:19",
        "Description": "Some Status-returning APIs used in example files have been deprecated recently.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5992"
        ]
    },
    "PARQUET-1713": {
        "Key": "PARQUET-1713",
        "Summary": "[C++] Refactor Parquet Code Samples to use Result<T> APIs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gal Lushi",
        "Created": "09/Dec/19 14:26",
        "Updated": "09/Dec/19 15:49",
        "Resolved": "09/Dec/19 14:35",
        "Description": "Currently, the Parquet code samples use the (now deprecated by ARROW-7235) `Status`-returning functions.\nSee https://github.com/apache/arrow/pull/5994\nthis also closes ARROW-7352 which was opened is the wrong JIRA by mistake.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/5994"
        ]
    },
    "PARQUET-1714": {
        "Key": "PARQUET-1714",
        "Summary": "Release parquet format 2.8.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.8.0",
        "Fix Version/s": "format-2.8.0",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Dec/19 13:21",
        "Updated": "02/Dec/20 16:59",
        "Resolved": "02/Dec/20 16:59",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1715": {
        "Key": "PARQUET-1715",
        "Summary": "[C++] Add the Parquet code samples to CI + Refactor Parquet Encryption Samples",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Gal Lushi",
        "Reporter": "Gal Lushi",
        "Created": "10/Dec/19 13:34",
        "Updated": "12/Dec/19 22:53",
        "Resolved": "11/Dec/19 17:37",
        "Description": "This is a refix for PARQUET-1712 , refactoring the Parquet Encryption code samples as well + Added the code samples to the CI (the previous issue added only the tools, not the samples).",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6007"
        ]
    },
    "PARQUET-1716": {
        "Key": "PARQUET-1716",
        "Summary": "[C++] Add support for BYTE_STREAM_SPLIT encoding",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Martin Radev",
        "Reporter": "Martin Radev",
        "Created": "11/Jul/19 14:19",
        "Updated": "05/Feb/20 01:13",
        "Resolved": "05/Feb/20 01:13",
        "Description": "From the Parquet issue ( https://issues.apache.org/jira/browse/PARQUET-1622\u00a0):\nApache Parquet does not have any encodings suitable for FP data and the available text compressors (zstd, gzip, etc) do not handle FP data very well.\nIt is possible to apply a simple data transformation named \"stream splitting\". Such could be \"byte stream splitting\" which creates K streams of length N where K is the number of bytes in the data type (4 for floats, 8 for doubles) and N is the number of elements in the sequence.\nThe transformed data compresses significantly better on average than the original data and for some cases there is a performance improvement in compression and decompression speed.\nYou can read a more detailed report here:\n [https://drive.google.com/file/d/1wfLQyO2G5nofYFkS7pVbUW0-oJkQqBvv/view\nApache Arrow can benefit from the reduced requirements for storing FP parquet column data and improvements in decompression speed.",
        "Issue Links": [
            "/jira/browse/ARROW-6282",
            "https://github.com/apache/arrow/pull/6005"
        ]
    },
    "PARQUET-1717": {
        "Key": "PARQUET-1717",
        "Summary": "parquet-thrift converts Thrift i16 to parquet INT32 instead of INT_16",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Emmanuel Brard",
        "Reporter": "Emmanuel Brard",
        "Created": "11/Dec/19 14:52",
        "Updated": "27/Jan/21 14:30",
        "Resolved": "12/Dec/19 16:43",
        "Description": "Currently when converting Thrift to Parquet, the ThriftSchemaConverter will convert a i16 thrift data type as a INT32 Parquet data type even if the logical type INT(16, false) would be more representative, especially for downstream consumers of the parquet files.\nUsing the logical data type requires a change in this method:\n\n\r\n @Override\r\n  public ConvertedField visit(I16Type i16Type, State state) {\r\n    return visitPrimitiveType(INT32, state);\r\n  }\r\n\n\nfrom the ThriftSchemaConvertVisitor",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/706"
        ]
    },
    "PARQUET-1718": {
        "Key": "PARQUET-1718",
        "Summary": "Store int16 as int16",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Viacheslav Shalamov",
        "Created": "11/Dec/19 15:18",
        "Updated": "11/Dec/19 20:46",
        "Resolved": "11/Dec/19 15:25",
        "Description": "When writing a POJO with `short` field, it ends up in parquet file as 32-bit int because of:\n16-bit ints are not explicitly supported in the storage format since they are covered by 32-bit ints with an efficient encoding.\nhttps://github.com/apache/parquet-format#types\nHow about annotating it with logical type `IntType (bitWidth = 16, isSigned = true)` ?",
        "Issue Links": []
    },
    "PARQUET-1719": {
        "Key": "PARQUET-1719",
        "Summary": "Make ParquetReader(List<InputFile>, ParquetReadOptions, ReadSupport<T>) constructor public",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dmitry Sysolyatin",
        "Created": "12/Dec/19 08:46",
        "Updated": "30/May/22 12:22",
        "Resolved": null,
        "Description": "My application uses s3 storage to store parquet files. The problem is that there is not ability to get rid from `new Configuration(true);` which load configuration from files: \n`core-default.xml` and `core-site.xml` every time when ParquetReader instance is created.\nI suggest just make ParquetReader(List<InputFile>, ParquetReadOptions, ReadSupport<T>) constructor public then there will be ability to do custom builder.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/707"
        ]
    },
    "PARQUET-1720": {
        "Key": "PARQUET-1720",
        "Summary": "[C++] Parquet JSONPrint not showing version correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Zherui Cao",
        "Reporter": "Zherui Cao",
        "Created": "19/Dec/19 16:37",
        "Updated": "24/Dec/19 02:50",
        "Resolved": "24/Dec/19 02:50",
        "Description": "ParquetFilePrinter::JSONPrint in printer.cc\nline 190:\nstream << \" \\\"Version\\\": \\\"\" << file_metadata->version() << \"\\\",\\n\";\nshould be\nParquetVersionToString(file_metadata->version())",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6065"
        ]
    },
    "PARQUET-1721": {
        "Key": "PARQUET-1721",
        "Summary": "[C++] Arrow dependency is missing in parquet.pc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kouhei Sutou",
        "Reporter": "Kouhei Sutou",
        "Created": "21/Dec/19 22:19",
        "Updated": "23/Dec/19 18:45",
        "Resolved": "22/Dec/19 10:54",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6084"
        ]
    },
    "PARQUET-1722": {
        "Key": "PARQUET-1722",
        "Summary": "Scala maven plugin is deprecated",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Walid Gara",
        "Created": "21/Dec/19 22:44",
        "Updated": "06/Feb/20 22:52",
        "Resolved": "06/Feb/20 22:52",
        "Description": "The scala-maven-plugin in parquet-scala is deprecated.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/710"
        ]
    },
    "PARQUET-1723": {
        "Key": "PARQUET-1723",
        "Summary": "Read From Maps Without Using Contains",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "23/Dec/19 22:01",
        "Updated": "02/Jan/20 12:47",
        "Resolved": "02/Jan/20 12:47",
        "Description": "I see a few places with the following pattern...\n\u00a0\n\n\r\n\r\nif (map.contains(key)) {\r\n\r\n\u00a0 \u00a0return map.get(key);\r\n\r\n}\r\n\n\nBetter to just call get() and then check the return value for 'null' to determine if the key is there.  This prevents the need to traverse the Map twice,... once for contains and once for get.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/711"
        ]
    },
    "PARQUET-1724": {
        "Key": "PARQUET-1724",
        "Summary": "Use ConcurrentHashMap for Cache in DictionaryPageReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "23/Dec/19 22:35",
        "Updated": "02/Jan/20 12:53",
        "Resolved": "02/Jan/20 12:53",
        "Description": "Use ConcurrentHashMap for Cache in DictionaryPageReader\nUse Java 1.8 APIs",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/712"
        ]
    },
    "PARQUET-1725": {
        "Key": "PARQUET-1725",
        "Summary": "Replace Usage of Strings.join with JDK Functionality in ColumnPath Class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "23/Dec/19 22:40",
        "Updated": "08/Jan/20 17:00",
        "Resolved": "08/Jan/20 17:00",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/713"
        ]
    },
    "PARQUET-1726": {
        "Key": "PARQUET-1726",
        "Summary": "Use Java 8 Multi Exception Handling",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "23/Dec/19 23:44",
        "Updated": "02/Jan/20 13:00",
        "Resolved": "02/Jan/20 13:00",
        "Description": "Simplify the code and removes lines of code",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/714"
        ]
    },
    "PARQUET-1727": {
        "Key": "PARQUET-1727",
        "Summary": "Do Not Swallow InterruptedException in ParquetLoader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Dec/19 16:17",
        "Updated": "05/Jan/20 18:24",
        "Resolved": "05/Jan/20 18:24",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/715"
        ]
    },
    "PARQUET-1728": {
        "Key": "PARQUET-1728",
        "Summary": "Simplify NullPointerException Handling in AvroWriteSupport",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Dec/19 16:36",
        "Updated": "07/May/20 15:57",
        "Resolved": "07/May/20 15:57",
        "Description": "Use Java Collection API to simplify\nRemove new-line character from logging to play nice with 'grep'",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/716"
        ]
    },
    "PARQUET-1729": {
        "Key": "PARQUET-1729",
        "Summary": "Avoid AutoBoxing in EncodingStats",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Dec/19 16:52",
        "Updated": "10/Jan/20 12:55",
        "Resolved": "10/Jan/20 12:55",
        "Description": "Use AtomicInteger instead of a Java immutable Integer type which must be un-boxed, and re-boxed each time.\n\u00a0\nhttps://www.programcreek.com/2013/10/efficient-counter-in-java/",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/717"
        ]
    },
    "PARQUET-1730": {
        "Key": "PARQUET-1730",
        "Summary": "Use switch Statement in AvroIndexedRecordConverter for Enums",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Dec/19 17:06",
        "Updated": "05/Jan/20 20:12",
        "Resolved": "05/Jan/20 20:12",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/718"
        ]
    },
    "PARQUET-1731": {
        "Key": "PARQUET-1731",
        "Summary": "Use JDK 8 Facilities to Simplify FilteringRecordMaterializer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "27/Dec/19 18:03",
        "Updated": "05/Jan/20 18:26",
        "Resolved": "05/Jan/20 18:26",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/719"
        ]
    },
    "PARQUET-1732": {
        "Key": "PARQUET-1732",
        "Summary": "Call toArray With Empty Array",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "27/Dec/19 18:20",
        "Updated": "05/Jan/20 18:25",
        "Resolved": "05/Jan/20 18:25",
        "Description": "https://stackoverflow.com/questions/9572795/convert-list-to-array-in-java\n\u00a0\nIt is recommended now to use list.toArray(new Foo[0]);, not list.toArray(new Foo[list.size()]);.\n... less code too",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/720"
        ]
    },
    "PARQUET-1733": {
        "Key": "PARQUET-1733",
        "Summary": "[java]keep ColumnChunkPageWriteStore constructor from 1.10.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "31/Dec/19 02:47",
        "Updated": "31/Dec/19 11:25",
        "Resolved": "31/Dec/19 11:25",
        "Description": "The constructor of ColumnChunkPageWriteStore in 1.10.1 should be kept for backward compatibility.",
        "Issue Links": [
            "/jira/browse/PARQUET-1694"
        ]
    },
    "PARQUET-1734": {
        "Key": "PARQUET-1734",
        "Summary": "[C++] Fix typos",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kazuaki Ishizaki",
        "Created": "01/Jan/20 04:20",
        "Updated": "02/Jan/20 01:50",
        "Resolved": "02/Jan/20 01:49",
        "Description": "Fix typos in files under `arrow/cpp/src/parquet` directory",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6111"
        ]
    },
    "PARQUET-1735": {
        "Key": "PARQUET-1735",
        "Summary": "Clean Up parquet-columns Module",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "06/Jan/20 04:36",
        "Updated": "05/Aug/21 06:41",
        "Resolved": "09/Jan/20 08:32",
        "Description": "Remove unused imports\r\nRemove unused local variables\r\nAdd missing '@Override' annotations\r\nAdd missing '@Override' annotations to implementations of interface methods\r\nAdd missing '@Deprecated' annotations\r\nRemove unnecessary casts\r\nRemove redundant semicolons\r\nRemove unnecessary '$NON-NLS$' tags\r\nRemove redundant type arguments",
        "Issue Links": [
            "/jira/browse/PARQUET-2073",
            "https://github.com/apache/parquet-mr/pull/723"
        ]
    },
    "PARQUET-1736": {
        "Key": "PARQUET-1736",
        "Summary": "Use StringBuilder instead of StringBuffer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "06/Jan/20 14:37",
        "Updated": "11/Mar/21 16:11",
        "Resolved": "27/Jan/21 20:55",
        "Description": "StringBuffer is synchronized and therefore incurs the overhead even when it's not being used in a multi-threaded way.\u00a0 Use the unsynchronized StringBuilder instead.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/724"
        ]
    },
    "PARQUET-1737": {
        "Key": "PARQUET-1737",
        "Summary": "Replace Test Class RandomStr with Apache Commons Lang",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "06/Jan/20 15:39",
        "Updated": "03/Feb/20 13:57",
        "Resolved": "03/Feb/20 13:57",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/725"
        ]
    },
    "PARQUET-1738": {
        "Key": "PARQUET-1738",
        "Summary": "Remove unused imports in parquet-column",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Walid Gara",
        "Created": "06/Jan/20 21:03",
        "Updated": "15/Jan/20 12:00",
        "Resolved": "15/Jan/20 12:00",
        "Description": "Remove unused imports from the\u00a0parquet-column module.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/726"
        ]
    },
    "PARQUET-1739": {
        "Key": "PARQUET-1739",
        "Summary": "Make Spark SQL support Column indexes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.1",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "08/Jan/20 10:56",
        "Updated": "30/Jan/21 07:03",
        "Resolved": "30/Jan/21 07:03",
        "Description": "Make Spark SQL support\u00a0Column indexes.",
        "Issue Links": [
            "/jira/browse/SPARK-26346",
            "/jira/browse/PARQUET-1201"
        ]
    },
    "PARQUET-1740": {
        "Key": "PARQUET-1739 Make Spark SQL support Column indexes",
        "Summary": "Make ParquetFileReader.getFilteredRecordCount public",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "08/Jan/20 10:58",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "09/Jan/20 14:10",
        "Description": "Please see\u00a0\u00a0https://github.com/apache/spark/pull/26804/commits/4756e67dddbbf891c445efb78b202706e133cb46 for more details.",
        "Issue Links": [
            "/jira/browse/PARQUET-1774",
            "https://github.com/apache/parquet-mr/pull/728"
        ]
    },
    "PARQUET-1741": {
        "Key": "PARQUET-1741",
        "Summary": "APIs backward compatibility issues cause master branch build failure",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "08/Jan/20 12:00",
        "Updated": "08/Jan/20 15:14",
        "Resolved": "08/Jan/20 15:14",
        "Description": "The master branch build is broken due to API compatibility issues and the recent added API compatibility checking plugin.\n\u00a0\nJan 08, 2020 7:52:40 PM japicmp.output.incompatible.IncompatibleErrorOutput warn\nWARNING: Incompatibility detected: Requires semantic version level MAJOR: JApiMethod [oldMethod=org.apache.parquet.filter2.recordlevel.FilteringRecordMaterializer.getIndexFieldPathList(org.apache.parquet.io.PrimitiveColumnIO), newMethod=n.a., returnType=JApiReturnType [oldReturnTypeOptional=java.util.List, newReturnTypeOptional=value absent, changeStatus=REMOVED], getCompatibilityChanges()=[METHOD_REMOVED]]\nJan 08, 2020 7:52:40 PM japicmp.output.incompatible.IncompatibleErrorOutput warn\nWARNING: Incompatibility detected: Requires semantic version level MAJOR: JApiMethod [oldMethod=org.apache.parquet.filter2.recordlevel.FilteringRecordMaterializer.intArrayToList(int[]), newMethod=n.a., returnType=JApiReturnType [oldReturnTypeOptional=java.util.List, newReturnTypeOptional=value absent, changeStatus=REMOVED], getCompatibilityChanges()=[METHOD_REMOVED]]",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/729"
        ]
    },
    "PARQUET-1742": {
        "Key": "PARQUET-1742",
        "Summary": "Introduce Spotbugs",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "08/Jan/20 15:10",
        "Updated": "26/Jan/21 12:26",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/730"
        ]
    },
    "PARQUET-1743": {
        "Key": "PARQUET-1743",
        "Summary": "Add equals to BlockSplitBloomFilter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Walid Gara",
        "Reporter": "Walid Gara",
        "Created": "08/Jan/20 23:31",
        "Updated": "31/Mar/20 07:27",
        "Resolved": "03/Feb/20 14:01",
        "Description": "The method equals can be used to compare Bloom filters in tests since we can't access its bitset.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/731",
            "https://github.com/apache/parquet-mr/pull/773"
        ]
    },
    "PARQUET-1744": {
        "Key": "PARQUET-1739 Make Spark SQL support Column indexes",
        "Summary": "Some filters throws ArrayIndexOutOfBoundsException",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Yuming Wang",
        "Created": "09/Jan/20 04:34",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "10/Jan/20 06:09",
        "Description": "How to reproduce:\n\nBuild Spark\n\n\r\ngit clone https://github.com/apache/spark.git && cd spark\r\ngit fetch origin pull/26804/head:PARQUET-1744\r\ngit checkout PARQUET-1744\r\nbuild/sbt  package\r\nbin/spark-shell\r\n\n\nPrepare data:\n\n\r\nspark.sql(\"create table t1(a int, b int, c int) using parquet\")\r\nspark.sql(\"insert into t1 values(1,0,0)\")\r\nspark.sql(\"insert into t1 values(2,0,1)\")\r\nspark.sql(\"insert into t1 values(3,1,0)\")\r\nspark.sql(\"insert into t1 values(4,1,1)\")\r\nspark.sql(\"insert into t1 values(5,null,0)\")\r\nspark.sql(\"insert into t1 values(6,null,1)\")\r\nspark.sql(\"insert into t1 values(7,null,null)\")\r\n\n\nRun test 1\n\n\r\nscala> spark.sql(\"select a+120 from t1 where b<10 OR c=1\").show\r\njava.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:155)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:131)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:319)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:486)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:339)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 0\r\n\tat org.apache.parquet.internal.column.columnindex.IntColumnIndexBuilder$IntColumnIndex$1.compareValueToMin(IntColumnIndexBuilder.java:74)\r\n\tat org.apache.parquet.internal.column.columnindex.BoundaryOrder$2.lt(BoundaryOrder.java:123)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:262)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:64)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.lambda$visit$2(ColumnIndexFilter.java:131)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.applyPredicate(ColumnIndexFilter.java:176)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:131)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Lt.accept(Operators.java:209)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:186)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Or.accept(Operators.java:321)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\t... 29 more\r\n+---------+\r\n|(a + 120)|\r\n+---------+\r\n|      124|\r\n|      121|\r\n|      122|\r\n|      123|\r\n+---------+\r\n\n\n\n\nRun test 2\n\n\n\r\nscala> spark.sql(\"select a+140 from t1 where not (b<10 AND c=1)\").show\r\njava.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:155)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:131)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:319)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:486)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:339)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0\r\n\tat org.apache.parquet.internal.column.columnindex.IntColumnIndexBuilder$IntColumnIndex$1.compareValueToMax(IntColumnIndexBuilder.java:79)\r\n\tat org.apache.parquet.internal.column.columnindex.BoundaryOrder$2.gtEq(BoundaryOrder.java:107)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:257)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:64)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.lambda$visit$5(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.applyPredicate(ColumnIndexFilter.java:176)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:249)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:186)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Or.accept(Operators.java:321)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\t... 29 more\r\njava.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:155)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:131)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:319)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:486)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:339)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0\r\n\tat org.apache.parquet.internal.column.columnindex.IntColumnIndexBuilder$IntColumnIndex$1.compareValueToMax(IntColumnIndexBuilder.java:79)\r\n\tat org.apache.parquet.internal.column.columnindex.BoundaryOrder$2.gtEq(BoundaryOrder.java:107)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:257)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:64)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.lambda$visit$5(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.applyPredicate(ColumnIndexFilter.java:176)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:249)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:186)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Or.accept(Operators.java:321)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\t... 29 more\r\n+---------+\r\n|(a + 140)|\r\n+---------+\r\n|      141|\r\n|      143|\r\n+---------+\r\n\n\n\nRun test 3\n\n\n\r\nscala> spark.sql(\"select a+150 from t1 where not (c=1 AND b<10)\").show\r\njava.lang.reflect.InvocationTargetException\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:155)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:131)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:319)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:486)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:339)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0\r\n\tat org.apache.parquet.internal.column.columnindex.IntColumnIndexBuilder$IntColumnIndex$1.compareValueToMax(IntColumnIndexBuilder.java:79)\r\n\tat org.apache.parquet.internal.column.columnindex.BoundaryOrder$2.gtEq(BoundaryOrder.java:107)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:257)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:64)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.lambda$visit$5(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.applyPredicate(ColumnIndexFilter.java:176)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:249)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:186)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Or.accept(Operators.java:321)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\t... 28 more\r\njava.lang.reflect.InvocationTargetException\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:155)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:131)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:319)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:486)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:339)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Index 0 out of bounds for length 0\r\n\tat org.apache.parquet.internal.column.columnindex.IntColumnIndexBuilder$IntColumnIndex$1.compareValueToMax(IntColumnIndexBuilder.java:79)\r\n\tat org.apache.parquet.internal.column.columnindex.BoundaryOrder$2.gtEq(BoundaryOrder.java:107)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:257)\r\n\tat org.apache.parquet.internal.column.columnindex.ColumnIndexBuilder$ColumnIndexBase.visit(ColumnIndexBuilder.java:64)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.lambda$visit$5(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.applyPredicate(ColumnIndexFilter.java:176)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:146)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$GtEq.accept(Operators.java:249)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:186)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.visit(ColumnIndexFilter.java:56)\r\n\tat org.apache.parquet.filter2.predicate.Operators$Or.accept(Operators.java:321)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:86)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter$1.visit(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\t... 28 more\r\n+---------+\r\n|(a + 150)|\r\n+---------+\r\n|      151|\r\n|      153|\r\n+---------+",
        "Issue Links": [
            "/jira/browse/PARQUET-1774",
            "https://github.com/apache/parquet-mr/pull/732"
        ]
    },
    "PARQUET-1745": {
        "Key": "PARQUET-1739 Make Spark SQL support Column indexes",
        "Summary": "No result for partition key included in Parquet file",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yuming Wang",
        "Created": "09/Jan/20 05:10",
        "Updated": "23/Jan/20 09:35",
        "Resolved": "20/Jan/20 14:17",
        "Description": "How to reproduce:\n\n\r\ngit clone https://github.com/apache/spark.git && cd spark\r\ngit fetch origin pull/26804/head:PARQUET-1745\r\ngit checkout PARQUET-1745\r\nbuild/sbt \"sql/test-only *ParquetV2PartitionDiscoverySuite\"\r\n\n\noutput:\n\n[info] - read partitioned table - partition key included in Parquet file *** FAILED *** (1 second, 57 milliseconds)\r\n[info]   Results do not match for query:\r\n[info]   Timezone: sun.util.calendar.ZoneInfo[id=\"America/Los_Angeles\",offset=-28800000,dstSavings=3600000,useDaylight=true,transitions=185,lastRule=java.util.SimpleTimeZone[id=America/Los_Angeles,offset=-28800000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]\r\n[info]   Timezone Env:\r\n[info]\r\n[info]   == Parsed Logical Plan ==\r\n[info]   'Project [*]\r\n[info]   +- 'Filter ('pi = 1)\r\n[info]      +- 'UnresolvedRelation [t]\r\n[info]\r\n[info]   == Analyzed Logical Plan ==\r\n[info]   intField: int, stringField: string, pi: int, ps: string\r\n[info]   Project [intField#1788, stringField#1789, pi#1790, ps#1791]\r\n[info]   +- Filter (pi#1790 = 1)\r\n[info]      +- SubqueryAlias `t`\r\n[info]         +- RelationV2[intField#1788, stringField#1789, pi#1790, ps#1791] parquet file:/root/opensource/apache-spark/target/tmp/spark-c7e85130-3e1f-4137-ac7c-32f48be3b74a\r\n[info]\r\n[info]   == Optimized Logical Plan ==\r\n[info]   Filter (isnotnull(pi#1790) AND (pi#1790 = 1))\r\n[info]   +- RelationV2[intField#1788, stringField#1789, pi#1790, ps#1791] parquet file:/root/opensource/apache-spark/target/tmp/spark-c7e85130-3e1f-4137-ac7c-32f48be3b74a\r\n[info]\r\n[info]   == Physical Plan ==\r\n[info]   *(1) Project [intField#1788, stringField#1789, pi#1790, ps#1791]\r\n[info]   +- *(1) Filter (isnotnull(pi#1790) AND (pi#1790 = 1))\r\n[info]      +- *(1) ColumnarToRow\r\n[info]         +- BatchScan[intField#1788, stringField#1789, pi#1790, ps#1791] ParquetScan Location: InMemoryFileIndex[file:/root/opensource/apache-spark/target/tmp/spark-c7e85130-3e1f-4137-ac7c-32f..., ReadSchema: struct<intField:int,stringField:string>, PushedFilters: [IsNotNull(pi), EqualTo(pi,1)]\r\n[info]\r\n[info]   == Results ==\r\n[info]\r\n[info]   == Results ==\r\n[info]   !== Correct Answer - 20 ==   == Spark Answer - 0 ==\r\n[info]    struct<>                    struct<>\r\n[info]   ![1,1,1,bar]\r\n[info]   ![1,1,1,foo]\r\n[info]   ![10,10,1,bar]\r\n[info]   ![10,10,1,foo]\r\n[info]   ![2,2,1,bar]\r\n[info]   ![2,2,1,foo]\r\n[info]   ![3,3,1,bar]\r\n[info]   ![3,3,1,foo]\r\n[info]   ![4,4,1,bar]\r\n[info]   ![4,4,1,foo]\r\n[info]   ![5,5,1,bar]\r\n[info]   ![5,5,1,foo]\r\n[info]   ![6,6,1,bar]\r\n[info]   ![6,6,1,foo]\r\n[info]   ![7,7,1,bar]\r\n[info]   ![7,7,1,foo]\r\n[info]   ![8,8,1,bar]\r\n[info]   ![8,8,1,foo]\r\n[info]   ![9,9,1,bar]\r\n[info]   ![9,9,1,foo] (QueryTest.scala:248)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)\r\n[info]   at org.apache.spark.sql.QueryTest$.newAssertionFailedException(QueryTest.scala:238)\r\n[info]   at org.scalatest.Assertions.fail(Assertions.scala:1091)\r\n[info]   at org.scalatest.Assertions.fail$(Assertions.scala:1087)\r\n[info]   at org.apache.spark.sql.QueryTest$.fail(QueryTest.scala:238)\r\n[info]   at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:248)\r\n[info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:156)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite.$anonfun$new$194(ParquetPartitionDiscoverySuite.scala:1232)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView(SQLTestUtils.scala:260)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView$(SQLTestUtils.scala:258)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite.withTempView(ParquetPartitionDiscoverySuite.scala:53)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite.$anonfun$new$190(ParquetPartitionDiscoverySuite.scala:1212)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite.$anonfun$new$190$adapted(ParquetPartitionDiscoverySuite.scala:1200)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1(SQLTestUtils.scala:76)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1$adapted(SQLTestUtils.scala:75)\r\n[info]   at org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:161)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite.org$apache$spark$sql$test$SQLTestUtils$$super$withTempDir(ParquetPartitionDiscoverySuite.scala:53)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir(SQLTestUtils.scala:75)\r\n[info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir$(SQLTestUtils.scala:74)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite.withTempDir(ParquetPartitionDiscoverySuite.scala:53)\r\n[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite.$anonfun$new$189(ParquetPartitionDiscoverySuite.scala:1200)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)\r\n[info]   at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)\r\n[info]   at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)\r\n[info]   at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)\r\n[info]   at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)\r\n[info]   at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:392)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)\r\n[info]   at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)\r\n[info]   at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)\r\n[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1124)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1106)\r\n[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)\r\n[info]   at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:518)\r\n[info]   at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)\r\n[info]   at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)\r\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)\r\n[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:834)",
        "Issue Links": [
            "/jira/browse/PARQUET-1774"
        ]
    },
    "PARQUET-1746": {
        "Key": "PARQUET-1739 Make Spark SQL support Column indexes",
        "Summary": "Changed the data order after DataFrame reuse",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Yuming Wang",
        "Created": "09/Jan/20 05:23",
        "Updated": "26/Jan/21 04:28",
        "Resolved": "20/Jan/20 14:14",
        "Description": "How to reproduce:\n\n\r\ngit clone https://github.com/apache/spark.git && cd spark\r\ngit fetch origin pull/26804/head:PARQUET-1746\r\ngit checkout PARQUET-1746\r\nbuild/sbt \"sql/test-only *StreamSuite\"\r\n\n\noutput:\n\nsbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: \r\nDecoded objects do not match expected objects:\r\nexpected: WrappedArray(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\r\nactual:   WrappedArray(0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 2)\r\nassertnotnull(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"))\r\n+- upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\")\r\n   +- getcolumnbyordinal(0, LongType)\r\n\r\n         \r\n\tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)\r\n\tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)\r\n\tat org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)\r\n\tat org.scalatest.Assertions.fail(Assertions.scala:1091)\r\n\tat org.scalatest.Assertions.fail$(Assertions.scala:1087)\r\n\tat org.scalatest.FunSuite.fail(FunSuite.scala:1560)\r\n\tat org.apache.spark.sql.QueryTest.checkDataset(QueryTest.scala:73)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$22(StreamSuite.scala:215)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$22$adapted(StreamSuite.scala:208)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1(SQLTestUtils.scala:76)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1$adapted(SQLTestUtils.scala:75)\r\n\tat org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:161)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.org$apache$spark$sql$test$SQLTestUtils$$super$withTempDir(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.withTempDir(SQLTestUtils.scala:75)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.withTempDir$(SQLTestUtils.scala:74)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.withTempDir(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$21(StreamSuite.scala:208)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$21$adapted(StreamSuite.scala:207)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1(SQLTestUtils.scala:76)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1$adapted(SQLTestUtils.scala:75)\r\n\tat org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:161)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.org$apache$spark$sql$test$SQLTestUtils$$super$withTempDir(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.withTempDir(SQLTestUtils.scala:75)\r\n\tat org.apache.spark.sql.test.SQLTestUtils.withTempDir$(SQLTestUtils.scala:74)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.withTempDir(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.assertDF$1(StreamSuite.scala:207)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$25(StreamSuite.scala:226)\r\n\tat org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:52)\r\n\tat org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:36)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:231)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:229)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.withSQLConf(StreamSuite.scala:51)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$24(StreamSuite.scala:225)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$24$adapted(StreamSuite.scala:224)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.streaming.StreamSuite.$anonfun$new$20(StreamSuite.scala:224)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n\tat org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\r\n\tat org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)\r\n\tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)\r\n\tat org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)\r\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)\r\n\tat org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)\r\n\tat org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)\r\n\tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)\r\n\tat org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)\r\n\tat org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)\r\n\tat org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)\r\n\tat org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)\r\n\tat org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)\r\n\tat org.scalatest.FunSuite.runTests(FunSuite.scala:1560)\r\n\tat org.scalatest.Suite.run(Suite.scala:1124)\r\n\tat org.scalatest.Suite.run$(Suite.scala:1106)\r\n\tat org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)\r\n\tat org.scalatest.SuperEngine.runImpl(Engine.scala:518)\r\n\tat org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)\r\n\tat org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)\r\n\tat org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n\tat org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n\tat org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n\tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)\r\n\tat org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:317)\r\n\tat org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:510)\r\n\tat sbt.ForkMain$Run$2.call(ForkMain.java:296)\r\n\tat sbt.ForkMain$Run$2.call(ForkMain.java:286)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)",
        "Issue Links": [
            "/jira/browse/PARQUET-1774",
            "/jira/browse/PARQUET-1580"
        ]
    },
    "PARQUET-1747": {
        "Key": "PARQUET-1747",
        "Summary": "[C++] Access to ColumnChunkMetaData fails when encryption is on",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gal Lushi",
        "Reporter": "Gal Lushi",
        "Created": "09/Jan/20 12:06",
        "Updated": "22/Jan/20 16:51",
        "Resolved": "22/Jan/20 16:51",
        "Description": "When encryption is on, can't access  ColumnChunkMetaData  from the RowGroupMetaData.\nFor example, this code won't work with encryption on.\n\n\r\nreader->metadata()\r\n ->RowGroup(0)\r\n ->ColumnChunk(0)\r\n ->num_values();\r\n\n\n\u00a0\n\u00a0One implication is that the Parquet Arrow API doesn't work with encryption on.\nTests for the Parquet Arrow API (with encryption) are soon to follow in a separate PR.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6150"
        ]
    },
    "PARQUET-1748": {
        "Key": "PARQUET-1748",
        "Summary": "Update current release version in README.md",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "10/Jan/20 17:03",
        "Updated": "12/Jan/20 18:30",
        "Resolved": "12/Jan/20 18:30",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/733"
        ]
    },
    "PARQUET-1749": {
        "Key": "PARQUET-1749",
        "Summary": "Use Java 8 Streams for Empty PrimitiveIterator",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 17:06",
        "Updated": "26/Jan/20 20:11",
        "Resolved": "26/Jan/20 20:11",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/734"
        ]
    },
    "PARQUET-1750": {
        "Key": "PARQUET-1750",
        "Summary": "Reduce Memory Usage of RowRanges Class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 18:46",
        "Updated": "07/May/20 15:55",
        "Resolved": "07/May/20 15:55",
        "Description": "RowRanges maintains an internal ArrayList with a default capacity (10).  However, sometimes it is known ahead of time that only a single instance of Range will be added.  For these cases, do not instantiate an ArrayList",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/735"
        ]
    },
    "PARQUET-1751": {
        "Key": "PARQUET-1751",
        "Summary": "Fix Protobuf Build Warning",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 20:07",
        "Updated": "03/Feb/20 13:59",
        "Resolved": "03/Feb/20 13:59",
        "Description": "[libprotobuf WARNING google/protobuf/compiler/parser.cc:546] No syntax specified for the proto file: TestProtobuf.proto. Please use 'syntax = \"proto2\";' or 'syntax = \"proto3\";' to specify a syntax version. (Defaulted to proto2 syntax.)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/736"
        ]
    },
    "PARQUET-1752": {
        "Key": "PARQUET-1752",
        "Summary": "Remove slf4j-log4j12 Binding from parquet-protobuf Module",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 21:23",
        "Updated": "10/Jan/20 21:28",
        "Resolved": null,
        "Description": "Running org.apache.parquet.proto.ProtoInputOutputFormatTest\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/m2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/m2/org/slf4j/slf4j-simple/1.7.22/slf4j-simple-1.7.22.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n\n\nThere are two bindings being included and it produces this warning.  slf4j-log4j12 is coming in as a transient dependency.  There is also a log4j properties file in the test resources, but all it does is produce logging to the console.  Just stick with the slf4j-simple logger for testing (which is already explicitly specified for testing)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/737"
        ]
    },
    "PARQUET-1753": {
        "Key": "PARQUET-1753",
        "Summary": "Ensure Parquet Version slf4j Libraries Are Included In parquet-thrift Module",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 22:46",
        "Updated": "10/Jan/20 22:48",
        "Resolved": null,
        "Description": "### parquet-thrift\r\n[1;34mINFO] Excluding com.google.code.findbugs:jsr305:jar:3.0.0 from the shaded jar.\r\n[1;34mINFO] Excluding com.twitter.elephantbird:elephant-bird-core:jar:4.4 from the shaded jar.\r\n[1;34mINFO] Excluding com.twitter.elephantbird:elephant-bird-hadoop-compat:jar:4.4 from the shaded jar.\r\n***[1;34mINFO] Excluding org.slf4j:slf4j-api:jar:1.6.4 from the shaded jar.***\r\n[1;34mINFO] Excluding commons-lang:commons-lang:jar:2.4 from the shaded jar.\r\n[1;34mINFO] Excluding com.google.guava:guava:jar:11.0.1 from the shaded jar.\r\n\n\nYou can see that slf4j-api is version 1.6.4.  All other parquet modules are using 1.7.x.\n1.6.4 is being brought in by some old dependencies (primarily com.twitter.elephantbird).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/738"
        ]
    },
    "PARQUET-1754": {
        "Key": "PARQUET-1754",
        "Summary": "Include SLF4J Logger For parquet-format-structures Tests",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 22:51",
        "Updated": "10/Jan/20 23:19",
        "Resolved": null,
        "Description": "### /home/apache/parquet/parquet-mr/parquet-format-structures\r\n\r\n-------------------------------------------------------\r\n T E S T S\r\n-------------------------------------------------------\r\nRunning org.apache.parquet.format.TestUtil\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/739"
        ]
    },
    "PARQUET-1755": {
        "Key": "PARQUET-1755",
        "Summary": "Remove slf4j-simple From parquet-benchmarks Module",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "10/Jan/20 23:34",
        "Updated": "10/Jan/20 23:36",
        "Resolved": null,
        "Description": "The parquet-benchmarks module ships with the Log4J logger and the SLF4J \"simple\" logger.  Since this is a stand-alone application and needs Log4J, there is no reason to also use the \"simple\" logger.\n\n\r\n### parquet-benchmarks\r\n\r\n[1;34mINFO] Including org.slf4j:slf4j-simple:jar:1.7.22 in the shaded jar.\r\n[1;34mINFO] Including org.slf4j:slf4j-api:jar:1.7.22 in the shaded jar.\r\n[1;34mINFO] Including log4j:log4j:jar:1.2.17 in the shaded jar.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/740"
        ]
    },
    "PARQUET-1756": {
        "Key": "PARQUET-1756",
        "Summary": "Remove Dependency on Maven Plugin semantic-versioning",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "11/Jan/20 02:10",
        "Updated": "28/Apr/20 07:21",
        "Resolved": "28/Apr/20 07:21",
        "Description": "https://github.com/jeluard/semantic-versioning\nAccording to their github page:\n\nThis library is in dormant state and won't add any new feature. \nAlso, looking at their README file, it looks like the Parquet library is including their library in the Maven build process, but is not actually calling it.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/741"
        ]
    },
    "PARQUET-1757": {
        "Key": "PARQUET-1757",
        "Summary": "Upgrade Apache POM Parent Version to 22",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "11/Jan/20 02:12",
        "Updated": "11/Jan/20 02:12",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1758": {
        "Key": "PARQUET-1758",
        "Summary": "InternalParquetRecordReader Logging is Too Verbose",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 15:40",
        "Updated": "01/Apr/21 09:46",
        "Resolved": null,
        "Description": "A low-level library like Parquet should be pretty quiet.  It should just do its work and keep quiet.  Most issues should be addressed by throwing Exceptions, and the occasional warning message otherwise it will clutter the logging for the top-level application.  If debugging is required, administrator can enable it for the specific workload.\nWarning: This is my opinion. No stats to back it up.",
        "Issue Links": [
            "/jira/browse/PARQUET-1761",
            "https://github.com/apache/parquet-mr/pull/742"
        ]
    },
    "PARQUET-1759": {
        "Key": "PARQUET-1759",
        "Summary": "InternalParquetRecordReader Use Singleton Set",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 15:47",
        "Updated": "25/Feb/20 13:25",
        "Resolved": "25/Feb/20 13:25",
        "Description": "https://github.com/apache/parquet-mr/blob/d85a8f5dcfc1381655fcccaa81a2e83ba812f6a4/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java#L260-L262\nCode currently instantiates a HashSet (with a default internal data structure of size 16}} and then makes it immutable.  Use Collection#singleton to achieve this same goal with fewer lines of code and less memory requirements.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/743"
        ]
    },
    "PARQUET-1760": {
        "Key": "PARQUET-1760",
        "Summary": "Use SLF4J Logger for TestStatistics",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 15:56",
        "Updated": "12/Jan/20 16:00",
        "Resolved": null,
        "Description": "It is dumping a lot of logging into STDOUT and STDERR.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/744"
        ]
    },
    "PARQUET-1761": {
        "Key": "PARQUET-1761",
        "Summary": "Lower Logging Level in ParquetOutputFormat",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 16:08",
        "Updated": "18/May/21 08:23",
        "Resolved": "18/May/21 08:23",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1758",
            "https://github.com/apache/parquet-mr/pull/745"
        ]
    },
    "PARQUET-1762": {
        "Key": "PARQUET-1762",
        "Summary": "Move BitPackingPerfTest to parquet-benchmarks Module",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 16:13",
        "Updated": "12/Jan/20 16:13",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1763": {
        "Key": "PARQUET-1763",
        "Summary": "Add SLF4J to TestCircularReferences",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "12/Jan/20 16:17",
        "Updated": "26/Apr/20 09:44",
        "Resolved": "26/Apr/20 09:44",
        "Description": "Currently prints to STDOUT.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/746"
        ]
    },
    "PARQUET-1764": {
        "Key": "PARQUET-1764",
        "Summary": "The ParquetProperties constructor parameter list is so long",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "13/Jan/20 05:24",
        "Updated": "08/Feb/20 06:42",
        "Resolved": "08/Feb/20 06:42",
        "Description": "The ParquetProperties constructor parameter list is so long. We can refactor it like below, so that it is easier to use and less error-prone.\u00a0\n.withPageWriteChecksumEnabled()\n...",
        "Issue Links": []
    },
    "PARQUET-1765": {
        "Key": "PARQUET-1765",
        "Summary": "Invalid filteredRowCount in InternalParquetRecordReader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Jan/20 16:24",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "16/Jan/20 13:31",
        "Description": "The record count is retrieved before setting the projection schema so the value might be invalid if the projection impacts the filter.\nIn normal cases it does not cause any issue because the record filter will filter correctly only that we are filtering the records one-by-one instead of dropping the related pages.",
        "Issue Links": [
            "/jira/browse/PARQUET-1774",
            "https://github.com/apache/parquet-mr/pull/747"
        ]
    },
    "PARQUET-1766": {
        "Key": "PARQUET-1766",
        "Summary": "[C++] parquet NaN/null double statistics can result in endless loop",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Pierre Belzile",
        "Created": "11/Dec/19 19:22",
        "Updated": "24/Jun/21 19:31",
        "Resolved": "21/Jan/20 22:50",
        "Description": "There is a bug in the doubles column statistics computation when writing to parquet an array with only NaNs and nulls. It loops endlessly if the last cell of a write group is a Null. The line in error is\u00a0https://github.com/apache/arrow/blob/master/cpp/src/parquet/statistics.cc#L633\u00a0which checks for NaN but not for Null. Code then falls through and loops endlessly and causes the program to appear frozen.\nThis code snippet repeats:\n\nTEST(parquet, nans) {\r\n  /* Create a small parquet structure */\r\n  std::vector<std::shared_ptr<::arrow::Field>> fields;\r\n  fields.push_back(::arrow::field(\"doubles\", ::arrow::float64()));\r\n  std::shared_ptr<::arrow::Schema> schema = ::arrow::schema(std::move(fields));  std::unique_ptr<::arrow::RecordBatchBuilder> builder;\r\n  ::arrow::RecordBatchBuilder::Make(schema, ::arrow::default_memory_pool(), &builder);\r\n  builder->GetFieldAs<::arrow::DoubleBuilder>(0)->Append(std::numeric_limits<double>::quiet_NaN());\r\n  builder->GetFieldAs<::arrow::DoubleBuilder>(0)->AppendNull();  std::shared_ptr<::arrow::RecordBatch> batch;\r\n  builder->Flush(&batch);\r\n  arrow::PrettyPrint(*batch, 0, &std::cout);  std::shared_ptr<arrow::Table> table;\r\n  arrow::Table::FromRecordBatches({batch}, &table);  /* Attempt to write */\r\n  std::shared_ptr<::arrow::io::FileOutputStream> os;\r\n  arrow::io::FileOutputStream::Open(\"/tmp/test.parquet\", &os);\r\n  parquet::WriterProperties::Builder writer_props_bld;\r\n  // writer_props_bld.disable_statistics(\"doubles\");\r\n  std::shared_ptr<parquet::WriterProperties> writer_props = writer_props_bld.build();\r\n  std::shared_ptr<parquet::ArrowWriterProperties> arrow_props =\r\n      parquet::ArrowWriterProperties::Builder().store_schema()->build();\r\n  std::unique_ptr<parquet::arrow::FileWriter> writer;\r\n  parquet::arrow::FileWriter::Open(\r\n      *table->schema(), arrow::default_memory_pool(), os,\r\n      writer_props, arrow_props, &writer);\r\n  writer->WriteTable(*table, 1024);\r\n}",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6167"
        ]
    },
    "PARQUET-1767": {
        "Key": "PARQUET-1767",
        "Summary": "InternalParquetRecordWriter doesn't immediately limit current row group to threshold",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Brian Mwambazi",
        "Created": "14/Jan/20 22:23",
        "Updated": "15/Jan/20 13:32",
        "Resolved": null,
        "Description": "The MemoryManager adjust the row group size threshold of writers when the allocated memory pool fills up.\nProblem: However InternalParquetRecordWriter only re-adjusts the row group size on the next flush meaning they still use the old size. \nThis opens up a possibility of getting an OOM error if all writers are started at relatively the same time and progress in tandem(I saw this when investigating failing jobs while writing to disk in Spark)",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/748"
        ]
    },
    "PARQUET-1768": {
        "Key": "PARQUET-1768",
        "Summary": "InternalParquetRecordWriter doesn't immediately limit current row group to threshold",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Brian Mwambazi",
        "Created": "14/Jan/20 22:23",
        "Updated": "14/Jan/20 22:51",
        "Resolved": "14/Jan/20 22:32",
        "Description": "The MemoryManager adjust the row group size threshold of writers when the allocated memory pool fills up.\nProblem: However InternalParquetRecordWriter only re-adjusts the row group size on the next flush meaning they still use the old size. \nThis opens up a possibility of getting an OOM error if all writers are started at relatively the same time and progress in tandem(I saw this when investigating failing jobs while writing to disk in Spark)",
        "Issue Links": []
    },
    "PARQUET-1769": {
        "Key": "PARQUET-1769",
        "Summary": "[C++] Update to parquet-format 2.8.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "15/Jan/20 03:31",
        "Updated": "15/Jan/20 17:48",
        "Resolved": "15/Jan/20 17:48",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6200"
        ]
    },
    "PARQUET-1770": {
        "Key": "PARQUET-1770",
        "Summary": "[C++][CI] Add fuzz target for reading Parquet files",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "15/Jan/20 19:10",
        "Updated": "12/Feb/20 21:44",
        "Resolved": "12/Feb/20 18:36",
        "Description": "Now that Arrow has been accepted on OSS-Fuzz, we should check for crashes and potential vulnerabilities when reading Parquet files.\nThe Parquet fuzz target should use similar conventions as the IPC fuzz targets in cpp/src/arrow/ipc/. An executable to generate a seed corpus should be added as well, to make fuzzing more efficient.",
        "Issue Links": [
            "/jira/browse/ARROW-6273",
            "https://github.com/apache/arrow/pull/6405"
        ]
    },
    "PARQUET-1771": {
        "Key": "PARQUET-1771",
        "Summary": "Support configurable for DirectByteBufferAllocator from Hadoop Configuration",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "ShuMing Li",
        "Created": "16/Jan/20 07:06",
        "Updated": "01/Jan/23 01:43",
        "Resolved": null,
        "Description": "Now in HadoopReadOptions\u00a0class, we cannot change default allocator from Hadoop Configuration.\n\u00a0\nAdd a config `parquet.allocator.direct.enabled` to enable `DirectByteBufferAllocator` in `ParuqetFileReader`.",
        "Issue Links": [
            "/jira/browse/PARQUET-1006",
            "https://github.com/apache/parquet-mr/pull/749"
        ]
    },
    "PARQUET-1772": {
        "Key": "PARQUET-1772",
        "Summary": "[C++] ParquetFileWriter: Data overwritten when output stream opened in append mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "18/Jan/20 21:28",
        "Updated": "22/Jan/20 16:24",
        "Resolved": "22/Jan/20 16:24",
        "Description": "An arrow::io::FileOutputStream can be opened in append mode.\nHowever, when the output stream is used by the\u00a0ParquetFileWriter the data already present in the file is overwritten instead of being appended.\nFrom what I can see, Parquet does not have currently the functionality to append data.\u00a0 As such I suggest detecting when an append is attempted to give an error rather than overwrite existing data.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6228"
        ]
    },
    "PARQUET-1773": {
        "Key": "PARQUET-1773",
        "Summary": "Parquet file in invalid state while writing to S3 when calling ParquetWriter.write",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.10.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tristan Davolt",
        "Created": "20/Jan/20 21:55",
        "Updated": "22/Sep/20 23:07",
        "Resolved": "22/Sep/20 23:07",
        "Description": "This may be related to\u00a0PARQUET-632. I am also writing parquet to S3, but I am calling ParquetWriter.write directly. I have multiple containerized instances consuming messages from Kafka, converting them to Parquet, and then writing to S3. One instance will begin to throw this exception for all new messages. Sometimes, the container will recover. Other times, it must be restarted manually to recover. I am unable to find any \"error thrown previously.\"\nException:\n java.io.IOException\n Message:\n The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: BLOCK\n Stacktrace:\n\n\r\norg.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:168)org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:160)org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:291)org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:171)org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)org.apache.parquet.hadoop.ParquetWriter.close(ParquetWriter.java:308)",
        "Issue Links": []
    },
    "PARQUET-1774": {
        "Key": "PARQUET-1774",
        "Summary": "Release parquet 1.11.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.11.1",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Jan/20 10:05",
        "Updated": "25/Aug/20 13:37",
        "Resolved": "25/Aug/20 13:37",
        "Description": "Some issues are discovered during the migration to the parquet-mr release 1.11.0 in Spark. These issues are to be fixed and release in the minor release 1.11.1.",
        "Issue Links": [
            "/jira/browse/PARQUET-1740",
            "/jira/browse/PARQUET-1744",
            "/jira/browse/PARQUET-1745",
            "/jira/browse/PARQUET-1746",
            "/jira/browse/PARQUET-1765",
            "/jira/browse/PARQUET-1853"
        ]
    },
    "PARQUET-1775": {
        "Key": "PARQUET-1775",
        "Summary": "Deprecate AvroParquetWriter Builder Hadoop Path",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Jan/20 15:08",
        "Updated": "07/May/20 15:58",
        "Resolved": "07/May/20 15:58",
        "Description": "Trying to write a sample program with Parquet and came across the following quark:\n\u00a0\nThe AvroParquetWriter has no qualms about building one using org.apache.hadoop.fs.Path.\u00a0 However, doing so in AvroParquetReader is deprecated.\u00a0 I think it's appropriate to remove all dependencies of Hadoop from this simple reader/writer API.\n\u00a0\nTo make it consistent, also deprecate the use of org.apache.hadoop.fs.Path in the AvroParquetWriter.\n\u00a0\nhttps://github.com/apache/parquet-mr/blob/8c1bc9bcdeeac8178fecf61d18dc56913907fd46/parquet-avro/src/main/java/org/apache/parquet/avro/AvroParquetWriter.java#L38\n\u00a0\nhttps://github.com/apache/parquet-mr/blob/8c1bc9bcdeeac8178fecf61d18dc56913907fd46/parquet-avro/src/main/java/org/apache/parquet/avro/AvroParquetReader.java#L47",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/750"
        ]
    },
    "PARQUET-1776": {
        "Key": "PARQUET-1776",
        "Summary": "Add Java NIO Avro OutputFile InputFile",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "24/Jan/20 15:15",
        "Updated": "15/Dec/20 21:01",
        "Resolved": null,
        "Description": "Add a wrapper around Java NIO Path for org.apache.parquet.io.OutputFile and org.apache.parquet.io.InputFile",
        "Issue Links": [
            "/jira/browse/PARQUET-1905",
            "/jira/browse/PARQUET-1126"
        ]
    },
    "PARQUET-1777": {
        "Key": "PARQUET-1777",
        "Summary": "Add Parquet logo vector files to repo",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Julien Le Dem",
        "Reporter": "Julien Le Dem",
        "Created": "24/Jan/20 17:22",
        "Updated": "07/Apr/21 08:41",
        "Resolved": "28/Jan/20 05:38",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/157"
        ]
    },
    "PARQUET-1778": {
        "Key": "PARQUET-1778",
        "Summary": "Do Not Consider Class for Avro Generic Record Reader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "24/Jan/20 20:02",
        "Updated": "22/Jul/20 20:07",
        "Resolved": "22/Jul/20 20:07",
        "Description": "Example Code\n\r\nfinal ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(path).build();\r\nfinal GenericRecord genericRecord = reader.read();\r\n\n\nIt fails with...\n\n\r\njava.lang.NoSuchMethodException: io.github.belugabehr.app.Record.<init>()\r\n\tat java.lang.Class.getConstructor0(Class.java:3082) ~[na:1.8.0_232]\r\n\tat java.lang.Class.getDeclaredConstructor(Class.java:2178) ~[na:1.8.0_232]\r\n\tat org.apache.avro.specific.SpecificData$1.computeValue(SpecificData.java:63) ~[avro-1.9.1.jar:1.9.1]\r\n\tat org.apache.avro.specific.SpecificData$1.computeValue(SpecificData.java:58) ~[avro-1.9.1.jar:1.9.1]\r\n\tat java.lang.ClassValue.getFromHashMap(ClassValue.java:227) ~[na:1.8.0_232]\r\n\tat java.lang.ClassValue.getFromBackup(ClassValue.java:209) ~[na:1.8.0_232]\r\n\tat java.lang.ClassValue.get(ClassValue.java:115) ~[na:1.8.0_232]\r\n\tat org.apache.avro.specific.SpecificData.newInstance(SpecificData.java:470) ~[avro-1.9.1.jar:1.9.1]\r\n\tat org.apache.avro.specific.SpecificData.newRecord(SpecificData.java:491) ~[avro-1.9.1.jar:1.9.1]\r\n\tat org.apache.parquet.avro.AvroRecordConverter.start(AvroRecordConverter.java:404) ~[parquet-avro-1.11.0.jar:1.11.0]\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:392) ~[parquet-column-1.11.0.jar:1.11.0]\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226) ~[parquet-hadoop-1.11.0.jar:1.11.0]\r\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132) ~[parquet-hadoop-1.11.0.jar:1.11.0]\r\n\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136) ~[parquet-hadoop-1.11.0.jar:1.11.0]\r\n\n\nI was surprised because it should just load a GenericRecord view of the data. But alas, I have the Avro Schema defined with the namespace and name fields pointing to io.github.belugabehr.app.Record which just so happens to be a real class on the class path, so it is trying to call the public constructor on the class and this constructor does does not exist.\u00a0 Regardless, the\u00a0GenericRecordReader should just ignore this Avro Schema namespace information.\nI am putting GenericRecords into the Parquet file, I expect to get GenericRecords back out when I read it.\nIf I hack the information in a Schema and change the namespace or name fields to something bogus, it works as I would expect it to.  It successfully reads and returns a GenericRecord.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/751"
        ]
    },
    "PARQUET-1779": {
        "Key": "PARQUET-1779",
        "Summary": "format: Update merge script",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "28/Jan/20 05:39",
        "Updated": "30/Mar/21 10:37",
        "Resolved": null,
        "Description": "The current merge script is Python 3 incompatible, copy over the merge_script from the Arrow project which is a development that initially started from merge_parquet.py",
        "Issue Links": [
            "https://github.com/apache/parquet-format/pull/158"
        ]
    },
    "PARQUET-1780": {
        "Key": "PARQUET-1780",
        "Summary": "[C++] Set ColumnMetadata.encoding_stats field",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gamage Omega Ishendra",
        "Reporter": "Wes McKinney",
        "Created": "28/Jan/20 19:32",
        "Updated": "03/Mar/20 03:35",
        "Resolved": "03/Mar/20 02:25",
        "Description": "This metadata field is not set in the C++ library.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6370"
        ]
    },
    "PARQUET-1781": {
        "Key": "PARQUET-1781",
        "Summary": "[C++] 1.4.0+ reader ignore stats created by 1.3.* writer",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.4.0,                                            cpp-1.5.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Milos Sukovic",
        "Created": "03/Feb/20 10:24",
        "Updated": "04/Feb/20 22:20",
        "Resolved": null,
        "Description": "https://github.com/apache/arrow/commit/d257a88ed612301c0411894dfa783fcbff1bc867\nIn referenced commit, change to metadata.cc file changed the way for checking if new stats (min_value/max_value) are used.\nFrom\nif (metadata.statistics._isset.max_value || metadata.statistics._isset.min_value)\nto\nif (descr->column_order().get_order() == ColumnOrder::TYPE_DEFINED_ORDER)\n\u00a0\nThis change is breaking backward compat - all files which contain new stats (min_value/max_value), and are created before this change are valid, but they do not set column order flag.\nAfter this change, those stats are ignored, because column order flag is checked.\nPossible fix would be something like:\nif (descr->column_order().get_order() == ColumnOrder::TYPE_DEFINED_ORDER || (version == parquetcpp 1.3.* && (metadata.statistics._isset.max_value || metadata.statistics._isset.min_value)))\nI checked parquet-mr, and it seems like there, columnOrder is introduced as part of the same change as min_value and max_value, so issue shouldn't happen for files created by java code, but probably, stats are ignored by their reader too for files created by parquet-cpp 1.3.*.",
        "Issue Links": []
    },
    "PARQUET-1782": {
        "Key": "PARQUET-1782",
        "Summary": "Use Switch Statement in AvroRecordConverter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "03/Feb/20 23:30",
        "Updated": "10/Feb/20 12:35",
        "Resolved": "10/Feb/20 12:35",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/752"
        ]
    },
    "PARQUET-1783": {
        "Key": "PARQUET-1783",
        "Summary": "[C++] Parquet statistics wrong for dictionary type",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-4.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Florian Jetter",
        "Created": "31/Jan/20 10:22",
        "Updated": "23/Apr/21 10:08",
        "Resolved": null,
        "Description": "Observed behaviour\nStatistics for categorical data are equivalent for all row groups and refer to the entire CategoricalDtype instead of the data included in the row group.\nExpected behaviour\nThe row group statistics should only include data which is part of the actual row group, not the entire CategoricalDtype\nMinimal example\n\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\ntest_df = pd.DataFrame({\"categorical\": pd.Categorical([\"1\", \"42\"])})\r\ntable = pa.Table.from_pandas(test_df)\r\npq.write_table(\r\n    table,\r\n    \"test_parquet\",\r\n    chunk_size=1,\r\n)\r\ntest_parquet = pq.ParquetFile(\"test_parquet\")\r\ntest_parquet.metadata.row_group(0).column(0).statistics\r\n\n\n\n\r\nOut[1]:\r\n<pyarrow._parquet.Statistics object at 0x1163b5280>\r\n  has_min_max: True\r\n  min: 1\r\n  max: 42\r\n  null_count: 0\r\n  distinct_count: 0\r\n  num_values: 1\r\n  physical_type: BYTE_ARRAY\r\n  logical_type: String\r\n  converted_type (legacy): UTF8\r\n\n\nExpected would be\nmin:1 max:1\u00a0instead of max: 42 for the first row group\n\u00a0\nTested with \n pandas==1.0.0\n pyarrow==bd08d0ecbe355b9e0de7d07e8b9ff6ccdb150e73 (current master / essentially 0.16.0)",
        "Issue Links": [
            "/jira/browse/ARROW-11634"
        ]
    },
    "PARQUET-1784": {
        "Key": "PARQUET-1784",
        "Summary": "Column-wise configuration",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "05/Feb/20 10:19",
        "Updated": "15/Nov/21 08:23",
        "Resolved": "26/Feb/20 11:39",
        "Description": "After adding some new statistics and encodings into Parquet it is getting very hard to be smart and choose the best configs automatically. For example for which\u00a0columns should we save column index and/or bloom-filters? Is it worth using dictionary for a column that we know will fall back to another encoding?\nThe idea of this feature is to allow the library user to fine-tune the configuration by setting it column-wise. To support this we extend the existing configuration keys by a suffix to identify the related column. (From now on we introduce new keys following the same syntax.)\n {key of the configuration}#{column path in the file schema}\n For example: parquet.enable.dictionary#column.path.col_1\nThis jira covers the framework to support the column-wise configuration with the implementation of some existing configs where it make sense (e.g. parquet.enable.dictionary). Implementing new configuration is not part of this effort.",
        "Issue Links": [
            "/jira/browse/PARQUET-1805",
            "https://github.com/apache/parquet-mr/pull/754"
        ]
    },
    "PARQUET-1785": {
        "Key": "PARQUET-1785",
        "Summary": "[C++] Improve code reusability in encoding-test.cc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Martin Radev",
        "Reporter": "Martin Radev",
        "Created": "05/Feb/20 21:49",
        "Updated": "26/Feb/20 15:10",
        "Resolved": "26/Feb/20 15:09",
        "Description": "The code for testing the BYTE_STREAM_SPLIT encoding can be improved by reusing already existing test classes and providing equally good coverage for floats and doubles.",
        "Issue Links": [
            "/jira/browse/ARROW-7944",
            "https://github.com/apache/arrow/pull/6471"
        ]
    },
    "PARQUET-1786": {
        "Key": "PARQUET-1786",
        "Summary": "[C++] Use simd to improve BYTE_STREAM_SPLIT decoding performance",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Martin Radev",
        "Reporter": "Martin Radev",
        "Created": "05/Feb/20 21:51",
        "Updated": "24/Mar/20 18:33",
        "Resolved": "24/Mar/20 18:18",
        "Description": "BYTE_STREAM_SPLIT essentially does a scatter/gather operation in the encode/decoder paths. Unfortunately, it is not as fast as memcpy when the data is cached. That can be improved through using simd intrinsics.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6679"
        ]
    },
    "PARQUET-1787": {
        "Key": "PARQUET-1787",
        "Summary": "Expected distinct numbers is not parsed correctly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Walid Gara",
        "Reporter": "Walid Gara",
        "Created": "05/Feb/20 23:00",
        "Updated": "25/Mar/20 08:47",
        "Resolved": null,
        "Description": "In the bloom filter feature, when I pass the expected distinct numbers as below, I got null values instead of 1000 and 200.\n\n\r\nimport org.apache.hadoop.conf.Configuration;\r\n\r\nConfiguration conf = new Configuration();\r\n\r\nconf.set(\"parquet.bloom.filter.column.names\", \"content,line\"); conf.set(\"parquet.bloom.filter.expected.ndv\",\"1000,200\");\r\n\n\n\u00a0\n The issue is coming from getting the system property of expected distinct numbers through Long.getLong(expectedNDVs[i]).\n\u00a0\nIt's possible to fix it by parsing the string with\u00a0Long.parseLong(expectedNDVs[i]).",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/753"
        ]
    },
    "PARQUET-1788": {
        "Key": "PARQUET-1788",
        "Summary": "[C++] ColumnWriter has undefined behavior when writing arrow chunks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "06/Feb/20 06:18",
        "Updated": "10/Feb/20 17:04",
        "Resolved": "10/Feb/20 15:45",
        "Description": "We blindly add offset to dep_level and rep_level inside chunking callbacks when these are nullptrs (I believe this occurs if the schema is flat) we still apply the offset which triggers UBSan.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6378"
        ]
    },
    "PARQUET-1789": {
        "Key": "PARQUET-1789",
        "Summary": "Upgrade Hadoop3 to version 3.2.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "06/Feb/20 13:55",
        "Updated": "06/Feb/20 13:56",
        "Resolved": "06/Feb/20 13:56",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1790": {
        "Key": "PARQUET-1790",
        "Summary": "ParquetFileWriter missing Api for  DataPageV2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Brian Mwambazi",
        "Reporter": "Brian Mwambazi",
        "Created": "08/Feb/20 19:34",
        "Updated": "12/Feb/20 11:56",
        "Resolved": "12/Feb/20 11:56",
        "Description": "The\u00a0ParquetFileWriter class currently does not\u00a0 have an API for writing a DataPageV2 page.\u00a0 \nA similar API is already defined in\u00a0ColumnChunkPageWriteStore and inspiration can/should be derived from there in implementing this.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/756"
        ]
    },
    "PARQUET-1791": {
        "Key": "PARQUET-1791",
        "Summary": "Add 'prune' command to parquet-tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "09/Feb/20 14:20",
        "Updated": "25/Feb/20 09:53",
        "Resolved": "25/Feb/20 09:53",
        "Description": "During data retention, there is a need to remove unused or personal columns. Adding a 'prune' command in Parquet-tool to remove columns and retain all other columns unchanged. The way it works should be like 'merge' command in Parquet-tools, for example moving the column chunks as a whole.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/755"
        ]
    },
    "PARQUET-1792": {
        "Key": "PARQUET-1792",
        "Summary": "Add 'mask' command to parquet-tools/parquet-cli",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "10/Feb/20 16:17",
        "Updated": "01/Jul/21 20:17",
        "Resolved": null,
        "Description": "Some personal data columns need to be masked instead of being pruned(Parquet-1791). We need a tool to replace the raw data columns with masked value. The masked value could be hash, null, redact etc.\u00a0 For the unchanged columns, they should be moved as a whole like 'merge', 'prune' command in Parquet-tools.\u00a0\n\u00a0\nImplementing this feature in file format is 10X faster than doing it by rewriting the table data in the query engine.",
        "Issue Links": []
    },
    "PARQUET-1793": {
        "Key": "PARQUET-1793",
        "Summary": "Support writing INT96 timestamp from avro",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tamas Palfy",
        "Created": "11/Feb/20 10:33",
        "Updated": "11/Aug/20 12:48",
        "Resolved": "11/Aug/20 12:48",
        "Description": "Add support for writing avro LONG/timestamp-millis data in INT96 (or int the current INT64) format in parquet.\nAdd a config flag to select the required timestamp output format (INT96 or INT64).",
        "Issue Links": []
    },
    "PARQUET-1794": {
        "Key": "PARQUET-1794",
        "Summary": "Random data generation may cause flaky tests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "12/Feb/20 11:32",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "17/Feb/20 09:08",
        "Description": "Some code parts uses BigInteger objects to generate FIX_LEN_BYTE_ARRAY or INT96 values. The problem is with BigInteger.toByteArray() which creates the shortest byte array which can hold the value so the array might be shorter than expected.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/758"
        ]
    },
    "PARQUET-1795": {
        "Key": "PARQUET-41 Add bloom filters to parquet statistics",
        "Summary": "merge bloom filter feature branch to master",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "12/Feb/20 13:13",
        "Updated": "26/Feb/20 16:00",
        "Resolved": "26/Feb/20 16:00",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/757"
        ]
    },
    "PARQUET-1796": {
        "Key": "PARQUET-1796",
        "Summary": "Bump Apache Avro to 1.9.2",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-avro",
        "Assignee": "Ryan Skraba",
        "Reporter": "Ryan Skraba",
        "Created": "13/Feb/20 16:08",
        "Updated": "19/Feb/20 09:29",
        "Resolved": "14/Feb/20 08:40",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/759"
        ]
    },
    "PARQUET-1797": {
        "Key": "PARQUET-1797",
        "Summary": "[C++] Fix fuzzing errors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Francois Saint-Jacques",
        "Created": "14/Feb/20 14:46",
        "Updated": "25/Feb/20 22:11",
        "Resolved": "25/Feb/20 15:41",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6440"
        ]
    },
    "PARQUET-1798": {
        "Key": "PARQUET-1798",
        "Summary": "[C++] Review logic around automatic assignment of field_id's",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-5.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Weston Pace",
        "Reporter": "Wes McKinney",
        "Created": "14/Feb/20 16:53",
        "Updated": "27/May/21 15:56",
        "Resolved": "27/May/21 15:55",
        "Description": "At schema deserialization (from Thrift) time, we are assigning a default field_id to the Schema node based on a depth-first ordering of notes. This means that a round trip (load, then write) will cause field_id's to be written that weren't there before. I'm not sure this is the desired behavior.\nWe should examine this in more detail and possible change it. See also discussion in ARROW-7080 https://github.com/apache/arrow/pull/6408",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/10289"
        ]
    },
    "PARQUET-1799": {
        "Key": "PARQUET-1799",
        "Summary": "[C++] Stream API: Relax schema checking when reading",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gawain BOLTON",
        "Reporter": "Gawain BOLTON",
        "Created": "15/Feb/20 09:05",
        "Updated": "19/Feb/20 10:52",
        "Resolved": "19/Feb/20 10:51",
        "Description": "When reading Parquet files, the schema converted type may not match the currently expected converted type.\nFor example when reading a C++ uint32_t the converted type expected is INT_32 when in fact it may be NONE which causes the read to fail.\nThere are a few similar cases I have found and I will submit a PR to address this issue.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6434"
        ]
    },
    "PARQUET-1800": {
        "Key": "PARQUET-1800",
        "Summary": "Add 'prune' command to parquet-cli",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "17/Feb/20 06:30",
        "Updated": "02/Dec/20 17:10",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1801": {
        "Key": "PARQUET-1801",
        "Summary": "Add column index support for 'prune' command in Parquet-tools/cli",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli,                                            parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "17/Feb/20 06:31",
        "Updated": "15/Jan/21 10:07",
        "Resolved": "15/Jan/21 10:07",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1802": {
        "Key": "PARQUET-1802",
        "Summary": "CompressionCodec class not found if the codec class is not in the same defining classloader as the CodecFactory class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Terence Yim",
        "Reporter": "Terence Yim",
        "Created": "19/Feb/20 16:39",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "24/Feb/20 08:37",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/760"
        ]
    },
    "PARQUET-1803": {
        "Key": "PARQUET-1803",
        "Summary": "Could not find FilleInputSplit in ParquetInputSplit",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Shankar Koirala",
        "Reporter": "Shankar Koirala",
        "Created": "24/Feb/20 16:06",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "28/Feb/20 14:24",
        "Description": "@deprecated will be removed in 2.0.0. use FileInputSplit instead.\u00a0\nThis is confusion where we can't find FileInputSplit, It should be\u00a0FileSplit, provided by Hadoop:\u00a0https://hadoop.apache.org/docs/r2.7.3/api/index.html?org/apache/hadoop/mapred/FileSplit.html",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/761"
        ]
    },
    "PARQUET-1804": {
        "Key": "PARQUET-1804",
        "Summary": "Provide pluggable APIs to support user customized compression codec",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "XinDong",
        "Created": "25/Feb/20 06:26",
        "Updated": "26/Feb/20 06:42",
        "Resolved": null,
        "Description": "In demand of better performance, quite some end users want to leverage accelerators (e.g. FPGA, Intel QAT) to offload compression computation. Parquet, as a well adopted data format, should allow compression codec implementation customization via a pluggable mechanism for those standard compression codec. This JIRA is intended for introducing this set of APIs.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/762"
        ]
    },
    "PARQUET-1805": {
        "Key": "PARQUET-1805",
        "Summary": "Refactor the configuration for bloom filters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "26/Feb/20 13:23",
        "Updated": "03/Feb/21 09:34",
        "Resolved": "30/Mar/20 08:51",
        "Description": "Refactor the hadoop configuration for bloom filters according to PARQUET-1784.",
        "Issue Links": [
            "/jira/browse/PARQUET-41",
            "/jira/browse/PARQUET-1784",
            "https://github.com/apache/parquet-mr/pull/763"
        ]
    },
    "PARQUET-1806": {
        "Key": "PARQUET-1806",
        "Summary": "[C++] [CI] Improve fuzzing seed corpus",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "26/Feb/20 20:09",
        "Updated": "04/Mar/20 09:54",
        "Resolved": "04/Mar/20 09:54",
        "Description": "We should use the parquet-testing repo to seed the OSS-Fuzz setup with more Parquet files, to help exploring the domain space.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6526"
        ]
    },
    "PARQUET-1807": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Encryption: Interop and Function test suite for Java version",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch,                                            1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Maya Anderson",
        "Reporter": "Gidon Gershinsky",
        "Created": "27/Feb/20 07:36",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "13/Jul/20 05:46",
        "Description": "Interop: test parquet-mr reading of encrypted files produced by parquet-cpp (fetched from parquet-testing)\nFunction: test writing/reading in a number of encryption and decryption configurations",
        "Issue Links": [
            "/jira/browse/PARQUET-1834",
            "https://github.com/apache/parquet-mr/pull/782"
        ]
    },
    "PARQUET-1808": {
        "Key": "PARQUET-1808",
        "Summary": "SimpleGroup.toString() uses String += and so has poor performance",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Shankar Koirala",
        "Reporter": "Randy Tidd",
        "Created": "02/Mar/20 18:24",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "07/May/20 12:33",
        "Description": "This method in SimpleGroup uses `+=` for String concatenation which is a known performance problem in Java, the performance degrades exponentially the more strings that are added.\nhttps://github.com/apache/parquet-mr/blob/d69192809d0d5ec36c0d8c126c8bed09ee3cee35/parquet-column/src/main/java/org/apache/parquet/example/data/simple/SimpleGroup.java#L50\nWe ran into a performance problem whereby a single column in a Parquet file was defined as a group:\n\n\r\n    optional group customer_ids (LIST) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 repeated group list\u00a0{ \r\n\u00a0 \u00a0 \u00a0 \u00a0 optional binary element (STRING); \r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\n\n\u00a0\nand had over 31,000 values. Reading this single column took over 8 minutes due to time spent in the `toString()` method.\u00a0 Using a different implementation that uses `StringBuffer` like this:\n\n\r\n StringBuffer result = new StringBuffer();\r\n int i = 0;\r\n for (Type field : schema.getFields()) {\r\n   String name = field.getName();\r\n   List<Object> values = data[i];\r\n   ++i;\r\n   if (values != null) {\r\n     if (values.size() > 0) {\r\n       for (Object value : values) {\r\n         result.append(indent);\r\n         result.append(name);\r\n         if (value == null) { \r\n           result.append(\": NULL\\n\");\r\n         } else if (value instanceof Group){ \r\n           result.append(\"\\n\"); \r\n           result.append(betterToString((SimpleGroup)value, indent+\" \"));\r\n         } else { \r\n           result.append(\": \"); \r\n           result.append(value.toString()); \r\n           result.append(\"\\n\"); \r\n         }\r\n       }\r\n     }\r\n   }\r\n }\r\n return result.toString();\n\nreduced that time to less than 500 milliseconds.\u00a0\nThe existing implementation is really poor and exhibits an infamous Java string performance issue and should be fixed.\nThis was a significant problem for us but we were able to work around it so I am marking this issue as \"Minor\".",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/766"
        ]
    },
    "PARQUET-1809": {
        "Key": "PARQUET-1809",
        "Summary": "Add new APIs for nested predicate pushdown",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "DB Tsai",
        "Created": "03/Mar/20 19:23",
        "Updated": "15/Feb/21 01:24",
        "Resolved": null,
        "Description": "Currently, Parquet's org.apache.parquet.filter2.predicate.FilterApi is using dot to split the column name into multi-parts of nested fields. The drawback is that this causes issues when the field name contains dot.\nThe new APIs that will be added will take array of string directly for multi-parts of nested fields, so no confusion as using dot as a separator.  \nSee https://github.com/apache/spark/pull/27728 and SPARK-17636 for details.",
        "Issue Links": []
    },
    "PARQUET-1810": {
        "Key": "PARQUET-1810",
        "Summary": "[C++] Fix undefined behaviour on invalid enum values (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "04/Mar/20 18:49",
        "Updated": "06/Mar/20 04:09",
        "Resolved": "05/Mar/20 21:26",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/20",
            "https://github.com/apache/arrow/pull/6537"
        ]
    },
    "PARQUET-1811": {
        "Key": "PARQUET-1811",
        "Summary": "Update download links",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-site",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "05/Mar/20 10:06",
        "Updated": "18/Mar/20 08:38",
        "Resolved": "18/Mar/20 08:38",
        "Description": "Based on the following mail sent to the private list we shall update the download links on our site.\n\nHello, Apache PMCs,\nIn order to better provide our millions of users with downloads, the\nApache Infrastructure Team has been restructuring the way downloads work\nfor our main distribution channels in the past few weeks. For users,\nthis will largely go unnoticed, and for projects likely the same, but we\ndid want to reach out to projects and inform them of the changes we've\nmade:\nAs of March 2020, we are deprecating www.apache.org/dist/ in favor of\nhttps://downloads.apache.org/ for backup downloads as well as signature\nand checksum verification. The primary driver has been splitting up web\nsite visits and downloads to gain better control and offer a better\nservice for both downloads and web site visits.\nAs stated, this does not impact end-users, and should have a minimal\nimpact on projects, as our download selectors as well as visits to\nwww.apache.org/dist/ have been adjusted to make use of\ndownloads.apache.org instead. We do however ask that projects, in their\nown time-frame, change references on their own web sites from\nwww.apache.org/dist/ to downloads.apache.org wherever such references\nmay exist, to complete the switch in full. We will NOT be turning off\nwww.apache.org/dist/ in the near future, but would greatly appreciate if\nprojects could help us transition away from the old URLs in their\ndocumentation and on their download pages.\nThe standard way of uploading releases[1] will STILL apply, however\nthere may be a short delay (<= 15 minutes) between releasing and\nreleases showing up on downloads.apache.org for technical reasons.\nIf you have any questions about this change, please do not hesitate\nto reach out to us at users@infra.apache.org.\nWith regards,\nDaniel on behalf of ASF Infrastructure.\n[1] https://www.apache.org/legal/release-policy.html#upload-ci",
        "Issue Links": [
            "https://github.com/apache/parquet-site/pull/3"
        ]
    },
    "PARQUET-1812": {
        "Key": "PARQUET-1812",
        "Summary": "Use airlift codecs for zstd",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "06/Mar/20 01:26",
        "Updated": "06/Mar/20 01:26",
        "Resolved": null,
        "Description": "Just like Parquet-1643, which will add lz4, lz0, gzip with airlift implementation, it would be great to add airlift support for zstd.\nI synced with\u00a0Samarth Jain\u00a0the author of Parquet-1643 for this effort. He is addressing the comments of the PR\u00a0https://github.com/apache/parquet-mr/pull/671/commits/4feb36943209a796a51dc0dcaa7eb30c2453ccdd\u00a0to get it merged. I have zstd with airlift implementation working on my end. I will create a PR for the zstd with airlift implementation after his PR is merged.",
        "Issue Links": []
    },
    "PARQUET-1813": {
        "Key": "PARQUET-1813",
        "Summary": "[C++] Remove logging statement in unit test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Antoine Pitrou",
        "Created": "12/Mar/20 17:08",
        "Updated": "13/Mar/20 01:45",
        "Resolved": "12/Mar/20 23:43",
        "Description": "It doesn't appear to fail the test, but I still get this weird output on Windows:\n\n\r\n[ RUN      ] TestConvertArrowSchema.ParquetMaps\r\nC:/t/arrow/cpp/src/parquet/arrow/arrow_schema_test.cc:989: my_map: map<string, string> not null\r\n[       OK ] TestConvertArrowSchema.ParquetMaps (0 ms)",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6605"
        ]
    },
    "PARQUET-1814": {
        "Key": "PARQUET-1814",
        "Summary": "[C++] TestInt96ParquetIO failure on Windows",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "12/Mar/20 17:09",
        "Updated": "22/Aug/22 11:34",
        "Resolved": null,
        "Description": "[ RUN      ] TestInt96ParquetIO.ReadIntoTimestamp\r\nC:/t/arrow/cpp/src/arrow/testing/gtest_util.cc(77): error: Failed\r\n\r\n@@ -0, +0 @@\r\n-1970-01-01 00:00:00.145738543\r\n+1970-01-02 11:35:00.145738543\r\n\r\nC:/t/arrow/cpp/src/parquet/arrow/arrow_reader_writer_test.cc(1034): error: Expected: this->ReadAndCheckSingleColumnFile(*values) doesn't generate new fatal failures in the current thread.\r\n  Actual: it does.\r\n[  FAILED  ] TestInt96ParquetIO.ReadIntoTimestamp (47 ms)",
        "Issue Links": []
    },
    "PARQUET-1815": {
        "Key": "PARQUET-1815",
        "Summary": "Add union API to BloomFilter interface",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Walid Gara",
        "Reporter": "Junjie Chen",
        "Created": "13/Mar/20 01:16",
        "Updated": "25/Mar/20 08:46",
        "Resolved": null,
        "Description": "Sometimes, one may want to build a file-level bloom filter by union all row groups bloom filters so that to save some memory. Add a union API that could make it easy to use.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/768",
            "https://github.com/apache/parquet-mr/pull/770"
        ]
    },
    "PARQUET-1816": {
        "Key": "PARQUET-1816",
        "Summary": "Add intersection API to BloomFilter interface",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Walid Gara",
        "Reporter": "Walid Gara",
        "Created": "15/Mar/20 21:49",
        "Updated": "25/Mar/20 08:45",
        "Resolved": null,
        "Description": "The intersection of Bloom Filter is a useful operation if we manipulate just bloom filters.\nNote: The intersection of two bloom filters have a higher false-positive rate than a bloom filter constructed from the intersection of two sets.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/772"
        ]
    },
    "PARQUET-1817": {
        "Key": "PARQUET-1817",
        "Summary": "Crypto Properties Factory",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Gidon Gershinsky",
        "Created": "16/Mar/20 06:51",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "30/Mar/20 08:58",
        "Description": "Basic common interface (abstract class) for loading of file encryption and decryption properties - making them transparent to analytic frameworks, so they can leverage Parquet modular encryption (PARQUET-1178) without any code changes . This interface depends on passing of Hadoop configuration - already done by frameworks that work with parquet-mr. The \"write\" part of the interface can also utilize the name/path of the file being written, and its WriteContext, that contains the schema with extensions.",
        "Issue Links": [
            "/jira/browse/PARQUET-1568",
            "https://docs.google.com/document/d/1OQeukjp2yv2v7m7cDnch1tgnL7wTPE2FLGkCMU-KK50/edit?usp=sharing",
            "https://github.com/apache/parquet-mr/pull/769"
        ]
    },
    "PARQUET-1818": {
        "Key": "PARQUET-1818",
        "Summary": "Fix collision of encryption and bloom filters in format-structure Util",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "encryption-feature-branch,                                            1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "17/Mar/20 09:38",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "20/Mar/20 13:34",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/771"
        ]
    },
    "PARQUET-1819": {
        "Key": "PARQUET-1819",
        "Summary": "[C++] Fix crashes on corrupt IPC input (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "18/Mar/20 17:50",
        "Updated": "24/Mar/20 10:52",
        "Resolved": "18/Mar/20 22:37",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/21",
            "https://github.com/apache/arrow/pull/6659",
            "https://github.com/apache/arrow/pull/6685"
        ]
    },
    "PARQUET-1820": {
        "Key": "PARQUET-1820",
        "Summary": "[C++] Use a column filter hint to inform read prefetching in Arrow reads",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "David Li",
        "Reporter": "David Li",
        "Created": "18/Mar/20 18:36",
        "Updated": "24/Mar/21 16:09",
        "Resolved": "01/May/20 18:45",
        "Description": "As a follow up to PARQUET-1698 and ARROW-7995, we should use the I/O coalescing facility (where available and enabled), in combination with a column filter hint, to compute and prefetch the exact byte ranges we will be reading (using the metadata). This should further improve performance on remote object stores like Amazon S3.",
        "Issue Links": [
            "/jira/browse/PARQUET-1698",
            "/jira/browse/PARQUET-1698",
            "/jira/browse/ARROW-11601",
            "/jira/browse/ARROW-7995",
            "https://github.com/apache/arrow/pull/6744"
        ]
    },
    "PARQUET-1821": {
        "Key": "PARQUET-1821",
        "Summary": "Add 'column-size' command to parquet-cli and parquet-tools",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "19/Mar/20 15:56",
        "Updated": "12/Jun/20 03:35",
        "Resolved": "12/Jun/20 03:35",
        "Description": "To determine which column to be removed when running the 'prune' command, one of the factors is to check the size of each column. This Jira is to add a command to get the size in both bytes and the percentage of each column. It can be considered as the width of each column.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/774"
        ]
    },
    "PARQUET-1822": {
        "Key": "PARQUET-1822",
        "Summary": "Parquet without Hadoop dependencies",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "mark juchems",
        "Created": "19/Mar/20 20:04",
        "Updated": "04/Jul/23 02:13",
        "Resolved": null,
        "Description": "I have been trying for weeks to create a parquet file from avro and write to S3 in Java.\u00a0 This has been incredibly frustrating and odd as Spark can do it easily (I'm told).\nI have assembled the correct jars through luck and diligence, but now I find out that I have to have hadoop installed on my machine. I am currently developing in Windows and it seems a dll and exe can fix that up but am wondering about Linus as the code will eventually run in Fargate on AWS.\nWhy do I need external dependencies and not pure java?\nThe thing really is how utterly complex all this is.\u00a0 I would like to create an avro file and convert it to Parquet and write it to S3, but I am trapped in \"ParquetWriter\" hell!\u00a0\nWhy can't I get a normal OutputStream and write it wherever I want?\nI have scoured the web for examples and there are a few but we really need some documentation on this stuff.\u00a0 I understand that there may be reasons for all this but I can't find them on the web anywhere.\u00a0 Any help?\u00a0 Can't we get the \"SimpleParquet\" jar that does this:\n\u00a0\nParquetWriter writer = AvroParquetWriter.<GenericData.Record>builder(outputStream)\n .withSchema(avroSchema)\n .withConf(conf)\n .withCompressionCodec(CompressionCodecName.SNAPPY)\n .withWriteMode(Mode.OVERWRITE)//probably not good for prod. (overwrites files).\n .build();",
        "Issue Links": [
            "/jira/browse/PARQUET-1905",
            "http://mail-archives.apache.org/mod_mbox/parquet-dev/202001.mbox/%3cCAO4re1m-Y9X3yQABX1_XaSaof4NZWBb8Tg_TBXgepK8rCJfU-g@mail.gmail.com%3e"
        ]
    },
    "PARQUET-1823": {
        "Key": "PARQUET-1823",
        "Summary": "[C++] Invalid RowGroup returned when reading with parquet::arrow::FileReader->RowGroup(i)->Column(j)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Francois Saint-Jacques",
        "Created": "20/Mar/20 14:01",
        "Updated": "27/Mar/20 12:26",
        "Resolved": "20/Mar/20 16:56",
        "Description": "Originally reported as ARROW-8138",
        "Issue Links": [
            "/jira/browse/ARROW-8138",
            "https://github.com/apache/arrow/pull/6674"
        ]
    },
    "PARQUET-1824": {
        "Key": "PARQUET-1824",
        "Summary": "[C++] Fix crashes on invalid input (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "23/Mar/20 19:08",
        "Updated": "08/Oct/20 14:34",
        "Resolved": "08/Oct/20 14:34",
        "Description": "Fix yet more issues found by OSS-Fuzz.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6690"
        ]
    },
    "PARQUET-1825": {
        "Key": "PARQUET-1825",
        "Summary": "[C++] Fix compilation error in column_io_benchmark.cc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Uwe Korn",
        "Reporter": "Uwe Korn",
        "Created": "24/Mar/20 15:08",
        "Updated": "24/Mar/20 20:32",
        "Resolved": "24/Mar/20 20:32",
        "Description": "Leftover of\u00a0https://github.com/apache/arrow/pull/6690",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6701"
        ]
    },
    "PARQUET-1826": {
        "Key": "PARQUET-1826",
        "Summary": "Document hadoop configuration options",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Walid Gara",
        "Reporter": "Gabor Szadovszky",
        "Created": "25/Mar/20 08:34",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Apr/20 10:17",
        "Description": "The currently available hadoop configuration options is not documented properly. The only documentation we have is the javadoc comment and the implementation of\u00a0ParquetOutputFormat.\nWe shall investigate all the possible options and their usage/default values and document them properly in a way that it is easily accessible by our users.\nI would suggest creating a `README.md` file in the sub-module parquet-hadoop that would describe the purpose of the module and would have a section that lists the possible hadoop configuration options. (Later on we shall extend this document with other descriptions about the purpose and usage of our library in the hadoop ecosystem. These efforts shall be covered by other jiras.)\nBy adding the description to the source code it would be easy to extend it by the new features we implement so it will be up-to-date for every release.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/781"
        ]
    },
    "PARQUET-1827": {
        "Key": "PARQUET-1827",
        "Summary": "UUID type currently not supported by parquet-mr",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Brad Smith",
        "Created": "25/Mar/20 19:55",
        "Updated": "02/Apr/21 02:30",
        "Resolved": "04/Jun/20 06:56",
        "Description": "The parquet-format project introduced a new UUID logical type in version 2.4:\nhttps://github.com/apache/parquet-format/blob/master/CHANGES.md\nThis would be a useful type to have available in some circumstances, but it currently isn't supported in the parquet-mr library. Hopefully this feature can be implemented at some point.",
        "Issue Links": [
            "/jira/browse/DRILL-7825",
            "/jira/browse/DRILL-7829",
            "https://github.com/apache/parquet-mr/pull/778"
        ]
    },
    "PARQUET-1828": {
        "Key": "PARQUET-1828",
        "Summary": "[C++] Add a SSE2 path for the ByteStreamSplit encoder implementation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Martin Radev",
        "Reporter": "Martin Radev",
        "Created": "25/Mar/20 22:54",
        "Updated": "20/Apr/20 01:56",
        "Resolved": "30/Mar/20 11:20",
        "Description": "The encode path for the byte stream split encoding can have better performance if SSE2 intrinsics are used.\nThe decode path already uses sse2 intrinsics.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6723"
        ]
    },
    "PARQUET-1829": {
        "Key": "PARQUET-1829",
        "Summary": "[C++] Fix crashes on invalid input (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "26/Mar/20 13:17",
        "Updated": "26/Mar/20 23:15",
        "Resolved": "26/Mar/20 23:15",
        "Description": "There are remaining issues open in OSS-Fuzz. We should fix most of them (except some out-of-memory conditions which may not easily be fixable).",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/23",
            "https://github.com/apache/arrow/pull/6728"
        ]
    },
    "PARQUET-1830": {
        "Key": "PARQUET-1830",
        "Summary": "Vectorized API to support Column Index in Apache Spark",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Felix Kizhakkel Jose",
        "Created": "26/Mar/20 19:27",
        "Updated": "21/Jul/20 01:07",
        "Resolved": null,
        "Description": "As per the comment on https://issues.apache.org/jira/browse/SPARK-26345.\u00a0Its seems like Apache Spark doesn't support Column Index until we disable vectorizedReader in Spark - which will have other performance implications. As per\u00a0zi\u00a0, parquet-mr should implement a Vectorized API. Is it already implemented or any pull request for the same?",
        "Issue Links": []
    },
    "PARQUET-1831": {
        "Key": "PARQUET-1831",
        "Summary": "[C++] Fix crashes on invalid input (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "31/Mar/20 12:51",
        "Updated": "31/Mar/20 17:14",
        "Resolved": "31/Mar/20 17:14",
        "Description": "Fix more issues with input validation found by OSS-Fuzz.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/24",
            "https://github.com/apache/arrow-testing/pull/25",
            "https://github.com/apache/arrow/pull/6781"
        ]
    },
    "PARQUET-1832": {
        "Key": "PARQUET-1832",
        "Summary": "Travis fails with too long output",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "01/Apr/20 10:08",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "15/Apr/20 12:55",
        "Description": "Travis fails with the error message \"The job exceeded the maximum log length, and has been terminated\". We have to cut down our useless output during build/test.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/777"
        ]
    },
    "PARQUET-1833": {
        "Key": "PARQUET-1833",
        "Summary": "InternalParquetRecordWriter - Too much memory used",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "02/Apr/20 10:24",
        "Updated": "02/Apr/20 10:24",
        "Resolved": null,
        "Description": "Our logs are full of entries starting with InternalParquetRecordWriter - Too much memory used: (...). We shall investigate why this is logged and whether it is a logging issue or some regression in our memory consumption.",
        "Issue Links": []
    },
    "PARQUET-1834": {
        "Key": "PARQUET-1834",
        "Summary": "Add Apache 2.0 license to README.md files in parquet-testing",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Maya Anderson",
        "Reporter": "Maya Anderson",
        "Created": "06/Apr/20 09:45",
        "Updated": "06/Apr/20 20:56",
        "Resolved": "06/Apr/20 20:56",
        "Description": "parquet-testing files can be used for interop tests in parquet-mr. \nHowever, if it is added as a submodule, then the 3 README.md files fail the license check and hence fail build of parquet-mr.",
        "Issue Links": [
            "/jira/browse/PARQUET-1807",
            "https://github.com/apache/parquet-testing/pull/12"
        ]
    },
    "PARQUET-1835": {
        "Key": "PARQUET-1835",
        "Summary": "[C++] Fix crashes on invalid input (OSS-Fuzz)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "06/Apr/20 12:20",
        "Updated": "06/Apr/20 22:13",
        "Resolved": "06/Apr/20 22:13",
        "Description": "Fix more issues found by OSS-Fuzz.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/26",
            "https://github.com/apache/arrow/pull/6848"
        ]
    },
    "PARQUET-1836": {
        "Key": "PARQUET-1836",
        "Summary": "why the last chunk might be larger than descriptor.size?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zhenglin luo",
        "Created": "09/Apr/20 11:55",
        "Updated": "13/Apr/20 03:23",
        "Resolved": "13/Apr/20 02:48",
        "Description": "i don't know why the last chunk might be larger than descriptor.size.\nI saw the annotation saying \"It is for reading old files\".So there is no problem with the new file\uff0cisn't there?\nby the way ,How to distinguish old and new files.",
        "Issue Links": []
    },
    "PARQUET-1837": {
        "Key": "PARQUET-1837",
        "Summary": "[C++] Expose an API that surface RLE information for rep/def levels when reading parquet files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "11/Apr/20 06:22",
        "Updated": "11/Apr/20 06:23",
        "Resolved": null,
        "Description": "When reading data from parquet it can potentially be more efficient to have a direct interface to read back RLE values, instead of exploding them to int16_t",
        "Issue Links": []
    },
    "PARQUET-1838": {
        "Key": "PARQUET-1838",
        "Summary": "[C++] Expose an API that allows direct writing of RLE information for rep/def levels when writing parquet files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "11/Apr/20 06:23",
        "Updated": "11/Apr/20 06:24",
        "Resolved": null,
        "Description": "When\u00a0 writing data to parquet it can potentially be more efficient to have a direct interface to write RLE values, instead of passing the exploded int16_t* values.",
        "Issue Links": []
    },
    "PARQUET-1839": {
        "Key": "PARQUET-1839",
        "Summary": "[C++] values_read not updated in ReadBatchSpaced",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Nileema Shingte",
        "Created": "11/Apr/20 20:01",
        "Updated": "12/Jul/20 17:06",
        "Resolved": "12/Jul/20 17:06",
        "Description": "values_read is not updated in some cases in the `TypedColumnReaderImpl::ReadBatchSpaced` API\nwe probably need to add\u00a0\n\n\r\n*values_read = total_values;\n\nAfter\u00a0https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_reader.cc#L906",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7717"
        ]
    },
    "PARQUET-1840": {
        "Key": "PARQUET-1840",
        "Summary": "[C++] DecodeSpaced copies more values then necessary",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "13/Apr/20 03:24",
        "Updated": "14/Apr/20 09:52",
        "Resolved": "14/Apr/20 09:52",
        "Description": "It potentially touches memory twice with memset\nThe stopping condition should be when the indices are equal\n\nChanging these could provide some marginal performance improvements depending on data shape.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6908"
        ]
    },
    "PARQUET-1841": {
        "Key": "PARQUET-1841",
        "Summary": "[C++] Experiment to see if using SIMD shuffle operations for DecodeSpaced improves performance",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "14/Apr/20 04:41",
        "Updated": "27/Apr/20 03:23",
        "Resolved": null,
        "Description": "Followup from\u00a0PARQUET-1840 for current benchmarks it seems that doing removing the memset somehow either has no impact or is slightly worse.\u00a0 We should investigate using SIMD operations to speed up spacing.\u00a0\n\u00a0\nAs part of this we can see if moving the memset to only cover uninitialized values after moving all required values provides any speedup.",
        "Issue Links": []
    },
    "PARQUET-1842": {
        "Key": "PARQUET-1842",
        "Summary": "Update Jackson Databind version to address CVE",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Patrick OFriel",
        "Created": "16/Apr/20 17:06",
        "Updated": "03/Jun/20 12:32",
        "Resolved": "22/Apr/20 18:48",
        "Description": "The current version of jackson-databind in parquet-mr has several CVEs associated with it:\u00a0https://nvd.nist.gov/vuln/detail/CVE-2020-10673,\u00a0https://nvd.nist.gov/vuln/detail/CVE-2020-10672,\u00a0https://nvd.nist.gov/vuln/detail/CVE-2020-10969,\u00a0https://nvd.nist.gov/vuln/detail/CVE-2020-11111,\u00a0https://nvd.nist.gov/vuln/detail/CVE-2020-11113, (and a few more). We should update to jackson-databind 2.9.10.4",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/785"
        ]
    },
    "PARQUET-1843": {
        "Key": "PARQUET-1843",
        "Summary": "[C++] Unnecessary assignment in DictDecoderImpl::Decode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Kazuaki Ishizaki",
        "Created": "17/Apr/20 01:20",
        "Updated": "17/Apr/20 03:05",
        "Resolved": "17/Apr/20 03:05",
        "Description": "The following duplicated assignments are unnecessary.\n\n\r\n   int DecodeIndices(int num_values, arrow::ArrayBuilder* builder) override {\r\n     num_values = std::min(num_values, num_values_);\r\n     num_values = std::min(num_values, num_values_);\r\n     if (num_values > 0) {",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6960"
        ]
    },
    "PARQUET-1844": {
        "Key": "PARQUET-1844",
        "Summary": "Removed Hadoop transitive dependency on commons-lang",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "17/Apr/20 09:49",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Apr/20 07:33",
        "Description": "Some of our code parts are using commons-lang without declaring direct dependency on it. It comes as a transitive dependency from Hadoop. From Hadoop 3.3 they migrated from commons-lang to commons-lang3 which fails the build if parquet-mr is built against it.\nWe shall either properly declare our direct dependency to commons-lang (or with also migrating to commons-lang3) or refactor the code to not use commons-lang at all.",
        "Issue Links": [
            "/jira/browse/HADOOP-17008"
        ]
    },
    "PARQUET-1845": {
        "Key": "PARQUET-1845",
        "Summary": "[C++] Int96 memory images in test cases assume only little-endian",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Kazuaki Ishizaki",
        "Reporter": "Kazuaki Ishizaki",
        "Created": "19/Apr/20 18:17",
        "Updated": "03/Aug/20 16:49",
        "Resolved": "03/Aug/20 16:49",
        "Description": "Int96 is used as a pair of uint_64 and uint_32. Both elements can be handled using a native endian for effectiveness.\nInt96 memory images in parquet-internal-tests assume only little-endian.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/6981"
        ]
    },
    "PARQUET-1846": {
        "Key": "PARQUET-1846",
        "Summary": "[C++] Remove deprecated IO classes and related functions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Wes McKinney",
        "Created": "20/Apr/20 00:04",
        "Updated": "16/Mar/22 09:27",
        "Resolved": "25/Mar/21 15:58",
        "Description": "These were added almost a year ago, so there has been ample time for users to migrate",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/9792"
        ]
    },
    "PARQUET-1847": {
        "Key": "PARQUET-1847",
        "Summary": "Filter out github notification from dev mail list",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "21/Apr/20 12:53",
        "Updated": "24/Jul/20 09:25",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1848": {
        "Key": "PARQUET-1404 [C++] Add index pages to the format to support efficient page skipping to parquet-cpp",
        "Summary": "Add Index support in the read path",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "23/Apr/20 18:11",
        "Updated": "23/Apr/20 18:58",
        "Resolved": null,
        "Description": "The scope of this Jira is to add support for reading indexes from a Parquet file.\nThe changes will involve de-serializing indexes and adding API to return them.\nTo test the implementation, we can get Parquet files with indexes generated via Impala or parquet-mr and see that the parquet-cpp tools can print them.",
        "Issue Links": []
    },
    "PARQUET-1849": {
        "Key": "PARQUET-1404 [C++] Add index pages to the format to support efficient page skipping to parquet-cpp",
        "Summary": "Add index support in the write path",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Deepak Majeti",
        "Reporter": "Deepak Majeti",
        "Created": "23/Apr/20 18:13",
        "Updated": "23/Apr/20 18:57",
        "Resolved": null,
        "Description": "The scope of this Jira is to add index support in the write path.\nThe changes will involve computing the indexes followed by serializing them and writing to the file.",
        "Issue Links": []
    },
    "PARQUET-1850": {
        "Key": "PARQUET-1850",
        "Summary": "toParquetMetadata method in ParquetMetadataConverter does not set dictionary page offset bit",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.1,                                            1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Srinivas S T",
        "Reporter": "Srinivas S T",
        "Created": "27/Apr/20 12:25",
        "Updated": "01/Jun/20 17:33",
        "Resolved": "01/Jun/20 17:33",
        "Description": "toParquetMetadata method converts org.apache.parquet.hadoop.metadata.ParquetMetadata to org.apache.parquet.format.FileMetaData but this does not set the dictionary page offset bit in FileMetaData.\nWhen a FileMetaData object is serialized while writing to the footer and then deserialized, the dictionary offset is lost as the dictionary page offset bit was never set.",
        "Issue Links": []
    },
    "PARQUET-1851": {
        "Key": "PARQUET-1851",
        "Summary": "ParquetMetadataConveter throws NPE in an Iceberg unit test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Junjie Chen",
        "Reporter": "Junjie Chen",
        "Created": "28/Apr/20 10:31",
        "Updated": "13/Apr/21 06:12",
        "Resolved": "13/Jan/21 12:02",
        "Description": "When writing data to parquet in an Iceberg unit test, it throws NPE as below\n\n\r\njava.lang.NullPointerExceptionjava.lang.NullPointerException at org.apache.parquet.format.converter.ParquetMetadataConverter.addRowGroup(ParquetMetadataConverter.java:476) at org.apache.parquet.format.converter.ParquetMetadataConverter.toParquetMetadata(ParquetMetadataConverter.java:177) at org.apache.parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:914) at org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:864) at org.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:206) at org.apache.iceberg.data.TestLocalScan.writeFile(TestLocalScan.java:429)",
        "Issue Links": [
            "/jira/browse/PARQUET-2026"
        ]
    },
    "PARQUET-1852": {
        "Key": "PARQUET-1852",
        "Summary": "Array Index OutOf Bounds Exception when fall Back Dictionary Encoded Data",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "jiangbo",
        "Created": "29/Apr/20 02:47",
        "Updated": "04/Apr/21 09:31",
        "Resolved": null,
        "Description": "java.lang.ArrayIndexOutOfBoundsException: 39782\n\\n\\tat org.apache.parquet.column.values.dictionary.DictionaryValuesWriter$PlainBinaryDictionaryValuesWriter.fallBackDictionaryEncodedData(DictionaryValuesWriter.java:284)\n\\n\\tat org.apache.parquet.column.values.dictionary.DictionaryValuesWriter.fallBackAllValuesTo(DictionaryValuesWriter.java:123)\n\\n\\tat org.apache.parquet.column.values.fallback.FallbackValuesWriter.fallBack(FallbackValuesWriter.java:147)\n\\n\\tat org.apache.parquet.column.values.fallback.FallbackValuesWriter.checkFallback(FallbackValuesWriter.java:141)\n\\n\\tat org.apache.parquet.column.values.fallback.FallbackValuesWriter.writeBytes(FallbackValuesWriter.java:163)\n\\n\\tat org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:201)\n\\n\\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:467)\n\\n\\tat org.apache.parquet.io.RecordConsumerLoggingWrapper.addBinary(RecordConsumerLoggingWrapper.java:119)\n\\n\\tat org.apache.parquet.example.data.simple.BinaryValue.writeValue(BinaryValue.java:45)\n\\n\\tat org.apache.parquet.example.data.simple.SimpleGroup.writeValue(SimpleGroup.java:229)\n\\n\\tat org.apache.parquet.example.data.GroupWriter.writeGroup(GroupWriter.java:51)\n\\n\\tat org.apache.parquet.example.data.GroupWriter.write(GroupWriter.java:37)\n\\n\\tat org.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:79)\n\\n\\tat org.apache.parquet.hadoop.example.GroupWriteSupport.write(GroupWriteSupport.java:36)\n\\n\\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)\n\\n\\tat org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:293)",
        "Issue Links": []
    },
    "PARQUET-1853": {
        "Key": "PARQUET-1853",
        "Summary": "Minimize the parquet-avro fastutil shaded jar",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-avro",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "30/Apr/20 21:00",
        "Updated": "07/Jul/20 08:12",
        "Resolved": "07/Jul/20 08:12",
        "Description": "The current jar is 71MB+ bigger due to the shaded fastutil",
        "Issue Links": [
            "/jira/browse/PARQUET-1774"
        ]
    },
    "PARQUET-1854": {
        "Key": "PARQUET-1854",
        "Summary": "Properties-Driven Interface to Parquet Encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "04/May/20 06:10",
        "Updated": "24/Sep/20 06:14",
        "Resolved": "13/Jul/20 05:48",
        "Description": "A high-level interface to Parquet encryption layer, based on configuration properties (table properties, Hadoop configuration, writer/reader options, etc)\u00a0 -\u00a0 will\u00a0 simplify the activation and configuration of data encryption.",
        "Issue Links": [
            "/jira/browse/PARQUET-1373",
            "/jira/browse/PARQUET-1568",
            "https://docs.google.com/document/d/1boH6HPkG0ZhgxcaRkGk3QpZ8X_J91uXZwVGwYN45St4/edit?usp=sharing"
        ]
    },
    "PARQUET-1855": {
        "Key": "PARQUET-1855",
        "Summary": "[C++] Improve documentation on MetaData ownership",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Francois Saint-Jacques",
        "Created": "04/May/20 15:32",
        "Updated": "24/May/20 20:11",
        "Resolved": "24/May/20 20:11",
        "Description": "I had to look at the implementation to understand what are the lifetime relationship for the following objects:\n\nFileMetaData\nRowGroupMetaData\nColumnChunkMetaData\n\nFrom what I gather, a reference to the top-level FileMetaData must be hold for any of the children objects (RowGroupMetaData and ColumnChunkMetaData) lifetime. It is unclear if the original buffer from which the metadata was deserialized must be hold for the lifetime of the FIleMetaData object, I suspect it does not need to be kept.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7244"
        ]
    },
    "PARQUET-1856": {
        "Key": "PARQUET-1856",
        "Summary": "[C++] Test suite assumes that Snappy support is built",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-7.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Wes McKinney",
        "Created": "04/May/20 16:49",
        "Updated": "09/Nov/21 23:44",
        "Resolved": "09/Nov/21 16:29",
        "Description": "The test suite fails if -DARROW_WITH_SNAPPY=OFF\n\n\r\n[----------] 1 test from TestStatisticsSortOrder/0, where TypeParam = parquet::PhysicalType<(parquet::Type::type)1>\r\n[ RUN      ] TestStatisticsSortOrder/0.MinMax\r\nunknown file: Failure\r\nC++ exception with description \"NotImplemented: Snappy codec support not built\" thrown in the test body.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/11642"
        ]
    },
    "PARQUET-1857": {
        "Key": "PARQUET-1857",
        "Summary": "[C++][Parquet] ParquetFileReader unable to read files with more than 32767 row groups",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Wes McKinney",
        "Reporter": "Novice",
        "Created": "02/May/20 23:14",
        "Updated": "06/May/20 16:28",
        "Resolved": "06/May/20 16:28",
        "Description": "I am using Rust to write Parquet file and read from Python.\nWhen write_batch with 10000 batch size, reading the Parquet file from Python gives the error below:\n```\n>>> pd.read_parquet(\"some.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home//.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home//.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home//miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1537, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home//miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1262, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home//miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 707, in read\n table = reader.read(**options)\n File \"/home//miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 337, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1130, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n OSError: Unexpected end of stream\n```\nAlso, when using batch size 1 and then read from Python, there is error too:\u00a0\n```\n>>> pd.read_parquet(\"some.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1537, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1262, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 707, in read\n table = reader.read(**options)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 337, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1130, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n OSError: The file only has 0 columns, requested metadata for column: 6\n```\nUsing batch size 1000 is fine.\nNote that my data has 450047 rows. Schema:\n```\nmessage schema\n\n{ REQUIRED INT32 a; REQUIRED INT32 b; REQUIRED INT32 c; REQUIRED INT64 d; REQUIRED INT32 e; REQUIRED BYTE_ARRAY f (UTF8); REQUIRED BOOLEAN g; }\n\n```\n\u00a0\nEDIT: as I add more rows (estimated 80 millions), using batch size 1000 does not work too:\n```\n>>> df = pd.read_parquet(\"data/ping_pong.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1537, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1262, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 707, in read\n table = reader.read(**options)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 337, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1130, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n OSError: The file only has 0 columns, requested metadata for column: 6\n```\nUnless I am using it wrong (which doesn't seem to be, since the API is simple), this is not usable at all \n\u00a0\nEDIT: some more logs, using 1000 batch size, a lot of rows:\n```\n>>> df = pd.read_parquet(\"ping_pong.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1537, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1262, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 707, in read\n table = reader.read(**options)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 337, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1130, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n OSError: The file only has -959432807 columns, requested metadata for column: 6\n```\n\u00a0\nEDIT:\nI wanted to try fastparquet, but seems fastparquet does not support .set_dictionary_enabled(true), so I set it to false.\nTurns out fastparquet is fine, so likely a problem with pyarrow.\n```\n>>> df = pd.read_parquet(\"data/ping_pong.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1281, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1137, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 605, in read\n table = reader.read(**options)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 253, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1136, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\n OSError: The file only has -580697109 columns, requested metadata for column: 5\n >>> df = pd.read_parquet(\"data/ping_pong.parquet\", engine=\"fastparquet\")\n```",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7108"
        ]
    },
    "PARQUET-1858": {
        "Key": "PARQUET-1858",
        "Summary": "[Python] [Rust] Parquet read file fails with batch size 1_000_000 and 41 row groups",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Novice",
        "Created": "05/May/20 17:07",
        "Updated": "26/Feb/21 05:23",
        "Resolved": null,
        "Description": "Here is the error I got:\nPyarrow:\n```\n>>> df = pd.read_parquet(\"test.parquet\", engine=\"pyarrow\")\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 125, in read\n path, columns=columns, **kwargs\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1281, in read_table\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1137, in read\n use_pandas_metadata=use_pandas_metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 605, in read\n table = reader.read(**options)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/pyarrow/parquet.py\", line 253, in read\n use_threads=use_threads)\n File \"pyarrow/_parquet.pyx\", line 1136, in pyarrow._parquet.ParquetReader.read_all\n File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\n OSError: Unexpected end of stream\n```\nfastparquet:\n```\n >>> df = pd.read_parquet(\"test.parquet\", engine=\"fastparquet\")\n /home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/encoding.py:222: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n Numpy8 = numba.jitclass(spec8)(NumpyIO)\n /home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/encoding.py:224: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n Numpy32 = numba.jitclass(spec32)(NumpyIO)\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 296, in read_parquet\n return impl.read(path, columns=columns, **kwargs)\n File \"/home/.local/lib/python3.7/site-packages/pandas/io/parquet.py\", line 201, in read\n return parquet_file.to_pandas(columns=columns, **kwargs)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/api.py\", line 399, in to_pandas\n index=index, assign=parts)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/api.py\", line 228, in read_row_group\n scheme=self.file_scheme)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/core.py\", line 354, in read_row_group\n cats, selfmade, assign=assign)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/core.py\", line 331, in read_row_group_arrays\n catdef=out.get(name+'-catdef', None))\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/core.py\", line 245, in read_col\n skip_nulls, selfmade=selfmade)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/core.py\", line 99, in read_data_page\n raw_bytes = _read_page(f, header, metadata)\n File \"/home/miniconda3/envs/ds/lib/python3.7/site-packages/fastparquet/core.py\", line 31, in _read_page\n page_header.uncompressed_page_size)\n AssertionError: found 120016208 raw bytes (expected None)\n```\nThe corresponding Rust code is:\n```\nuse parquet::{\n column::writer::ColumnWriter::BoolColumnWriter,\n column::writer::ColumnWriter::Int32ColumnWriter,\nfile::\n{ properties::WriterProperties, writer::\n\n{FileWriter, SerializedFileWriter}\n\n,\n },\n schema::parser::parse_message_type,\n };\n use std::{fs, rc::Rc};\nfn main() {\n let schema = \"\n message schema\n\n{ REQUIRED INT32 a; REQUIRED BOOLEAN b; }\n\n\";\nlet schema = Rc::new(parse_message_type(schema).unwrap());\n let props = Rc::new(\n WriterProperties::builder()\n .set_statistics_enabled(false)\n .set_dictionary_enabled(false)\n .build(),\n );\n let file = fs::File::create(\"test.parquet\").unwrap();\n let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n let batch_size = 1_000_000;\n let mut data = vec![];\n let mut data_bool = vec![];\n for i in 0..batch_size\n\n{ data.push(i); data_bool.push(true); }\n\nlet mut j = 0;\n loop {\n let mut row_group_writer = writer.next_row_group().unwrap();\n let mut col_writer = row_group_writer.next_column().unwrap().unwrap();\n if let Int32ColumnWriter(ref mut typed_writer) = col_writer\n\n{ typed_writer.write_batch(&data, None, None).unwrap(); }\n\nelse\n\n{ panic!(); }\n\nrow_group_writer.close_column(col_writer).unwrap();\n let mut col_writer = row_group_writer.next_column().unwrap().unwrap();\n if let BoolColumnWriter(ref mut typed_writer) = col_writer { typed_writer.write_batch(&data_bool, None, None).unwrap(); } else { panic!(); }\nrow_group_writer.close_column(col_writer).unwrap();\n writer.close_row_group(row_group_writer).unwrap();\nj += 1;\n if j * batch_size > 40_000_000\n\n{ break; }\n\n}\n writer.close().unwrap()\n }\n```",
        "Issue Links": [
            "/jira/browse/PARQUET-1859"
        ]
    },
    "PARQUET-1859": {
        "Key": "PARQUET-1859",
        "Summary": "[C++] Require error message when using ParquetException::EofException",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Wes McKinney",
        "Created": "05/May/20 21:19",
        "Updated": "22/Aug/22 11:34",
        "Resolved": null,
        "Description": "\"Unexpected end of stream\" (the defaults) gives no clue where the failure occurred",
        "Issue Links": [
            "/jira/browse/PARQUET-1858"
        ]
    },
    "PARQUET-1860": {
        "Key": "PARQUET-1860",
        "Summary": "Builder Support for ProtoParquetWriter",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Santosh Lade",
        "Created": "08/May/20 05:26",
        "Updated": "08/May/20 06:24",
        "Resolved": null,
        "Description": "ProtoParquetWriter does not extend the Builder class of ParquetWriter super class. Only deprecated constructors are used.",
        "Issue Links": []
    },
    "PARQUET-1861": {
        "Key": "PARQUET-1861",
        "Summary": "[Documentation][C++] Explain ReaderProperters.buffer_stream*",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Francois Saint-Jacques",
        "Reporter": "Francois Saint-Jacques",
        "Created": "08/May/20 17:46",
        "Updated": "22/May/20 01:14",
        "Resolved": "22/May/20 01:14",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7221"
        ]
    },
    "PARQUET-1862": {
        "Key": "PARQUET-1862",
        "Summary": "Fix comment on statistics field in Thrift file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Liam",
        "Reporter": "Liam",
        "Created": "14/May/20 07:22",
        "Updated": "07/Apr/21 08:42",
        "Resolved": "14/May/20 09:20",
        "Description": "A comment of\u00a0DataPageHeaderV2 in the src/main/thrift/parquet.thrift is wrong.\n\u00a0\n\n\r\n  /** optional statistics for this column chunk */\r\n  8: optional Statistics statistics;\r\n\n\n\u00a0\n should be\n\n\r\n  /** optional statistics for the data in this page */\r\n  8: optional Statistics statistics;",
        "Issue Links": []
    },
    "PARQUET-1863": {
        "Key": "PARQUET-1863",
        "Summary": "Remove use of add-test-source mojo in parquet-protobuf",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Laurent Goujon",
        "Reporter": "Laurent Goujon",
        "Created": "15/May/20 04:15",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "18/May/20 13:20",
        "Description": "parquet-protobuf uses build-helper-maven-plugin:add-test-source maven mojo to add protobuf test classes to the test sources path, but protoc-jar-maven-plugin also adds these classes to the main sources path. It is unnecessary (protoc-jar-maven-plugin could be configured to add to the test classes path) and actually causes confusion with some IDE (Eclipse).",
        "Issue Links": []
    },
    "PARQUET-1864": {
        "Key": "PARQUET-1864",
        "Summary": "How to generate a file with UUID as a Logical type",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Information Provided",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Khasim Shaik",
        "Created": "20/May/20 10:24",
        "Updated": "06/Jul/20 08:14",
        "Resolved": "06/Jul/20 08:14",
        "Description": "I want to generate a file with UUID as a Logical type,\u00a0\nplease provide schema or any hint to use UUID as logical type",
        "Issue Links": []
    },
    "PARQUET-1865": {
        "Key": "PARQUET-1865",
        "Summary": "[C++] Failure from C++17 feature used in parquet/encoding_benchmark.cc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "None",
        "Assignee": "Wes McKinney",
        "Reporter": "Wes McKinney",
        "Created": "20/May/20 22:23",
        "Updated": "20/May/20 23:13",
        "Resolved": "20/May/20 23:13",
        "Description": "ir/encoding_benchmark.cc.o -c ../src/parquet/encoding_benchmark.cc\r\n../src/parquet/encoding_benchmark.cc:242:53: error: static_assert with no message is a C++17 extension [-Werror,-Wc++17-extensions]\r\n  static_assert(sizeof(CType) == sizeof(*raw_values));\r\n                                                    ^\r\n                                                    , \"\"\r\n../src/parquet/encoding_benchmark.cc:286:53: error: static_assert with no message is a C++17 extension [-Werror,-Wc++17-extensions]\r\n  static_assert(sizeof(CType) == sizeof(*raw_values));",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7237"
        ]
    },
    "PARQUET-1866": {
        "Key": "PARQUET-1866",
        "Summary": "Replace Hadoop ZSTD with JNI-ZSTD",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "21/May/20 19:30",
        "Updated": "23/Jul/20 18:30",
        "Resolved": "03/Jun/20 07:17",
        "Description": "The parquet-mr repo has been using ZSTD-JNI for the parquet-cli project. It is a cleaner approach to use this JNI than using Hadoop ZSTD compression, because 1) on the developing box, installing Hadoop is cumbersome, 2) Older version of Hadoop doesn't support ZSTD. Upgrading Hadoop is another pain. This Jira is to replace Hadoop ZSTD with ZSTD-JNI for parquet-hadoop project. \nAccording to the author of ZSTD-JNI, Flink, Spark, Cassandra all use ZSTD-JNI for ZSTD.\nAnother approach is to use https://github.com/airlift/aircompressor which is a pure Java implementation. But it seems the compression level is not adjustable in aircompressor.",
        "Issue Links": [
            "/jira/browse/PARQUET-1876"
        ]
    },
    "PARQUET-1867": {
        "Key": "PARQUET-1867",
        "Summary": "[C++] Fix MetaData children object lifetime issues",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Francois Saint-Jacques",
        "Created": "22/May/20 13:25",
        "Updated": "22/May/20 13:25",
        "Resolved": null,
        "Description": "FileMetaData::RowGroup(int i) and RowGroupMetaData::ColumnChunk  returns both a unique_ptr<T> which gives the impression that the object are self-contained and safe to own without any lifetime issue.\nThe reality is that the caller must hold the lifetime of the parent object. We should change the signature to a const-ref instead.",
        "Issue Links": []
    },
    "PARQUET-1868": {
        "Key": "PARQUET-1868",
        "Summary": "Parquet reader options toggle for bloom filter toggles dictionary filtering",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Ryan Rupp",
        "Reporter": "Ryan Rupp",
        "Created": "30/May/20 04:43",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "02/Jun/20 07:46",
        "Description": "Looks like the variable names just got swapped so this is toggling dictionary filtering instead of bloom filter usage. Note, this is against current master and not a released version.",
        "Issue Links": []
    },
    "PARQUET-1869": {
        "Key": "PARQUET-1869",
        "Summary": "[C++] Large decimal values don't roundtrip correctly",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Joris Van den Bossche",
        "Created": "02/Jun/20 07:51",
        "Updated": "02/Jun/20 12:40",
        "Resolved": null,
        "Description": "Reproducer with python:\n\n\r\nimport decimal\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\narr = pa.array([decimal.Decimal('9223372036854775808'), decimal.Decimal('1.111')])\r\nprint(arr)\r\n\r\npq.write_table(pa.table({'a': arr}), \"test_decimal.parquet\") \r\nresult = pq.read_table(\"test_decimal.parquet\")\r\nprint(result.column('a'))\r\n\n\ngives\n\n\r\n# before writing\r\n<pyarrow.lib.Decimal128Array object at 0x7fd07d79a468>\r\n[\r\n  9223372036854775808.000,\r\n  1.111\r\n]\r\n# after reading\r\n<pyarrow.lib.ChunkedArray object at 0x7fd0711e9f98>\r\n[\r\n\u00a0 [\r\n\u00a0 \u00a0 -221360928884514619.392,\r\n\u00a0 \u00a0 1.111\r\n\u00a0 ]\r\n]\r\n\n\nI tried reading the file with a different parquet implementation (fastparquet python package), and that gives the same values on read, so the issue might possibly rather be on the write side.",
        "Issue Links": []
    },
    "PARQUET-1870": {
        "Key": "PARQUET-1870",
        "Summary": "Handle INT96 more gracefully in parquet-avro",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Ben Watson",
        "Created": "05/Jun/20 21:42",
        "Updated": "08/Jun/20 09:52",
        "Resolved": "08/Jun/20 09:52",
        "Description": "The parquet-avro library does not support INT96 columns (PARQUET-323), and any attempt to process a file containing such a column results in:\n\n\r\nthrow new IllegalArgumentException(\"INT96 not implemented and is deprecated\");\n\nINT96 is still used in many legacy datasets, and so it would be useful to be able to process Parquet files containing these records, even if the INT96 values themselves aren't rendered.\nThe same functionality has already been re-added into parquet-pig (PARQUET-1133).",
        "Issue Links": []
    },
    "PARQUET-1871": {
        "Key": "PARQUET-1871",
        "Summary": "ProtoReader does not iterate over the parquet file correctly.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Pau Alarcon",
        "Created": "06/Jun/20 08:47",
        "Updated": "06/Jun/20 08:47",
        "Resolved": null,
        "Description": "The `ProtoParquetReader` does not iterate over the parquet file correctly, but it gets stuck in the first element and keeps reading as many times as elements the file contained.\nIn my Scala example I am just reading from a local file that I know for sure it contains right data.\n```\nval hadoopCOnf = new Configuration()\nval outfile: String = genTemporaryFile()\nval r: ParquetReader[Event.Builder] = \n{\r\n ProtoParquetReader.builder[Event.Builder](new Path(outfile)).withConf(hadoopCOnf).build()\r\n}\n\n```\nNotice that the proto schema that I am using is generated from\u00a0https://scalapb.github.io/\nThe generated proto implements com.google.protobuf.GeneratedMessageV3.\nSee an example on how the ProtoParquetReader is created line(65 and 69):\u00a0https://github.com/monix/monix-connect/blob/master/parquet/src/test/scala/monix/connect/parquet/ProtoParquetFixture.scala#L69\nand here how it is used (notice that is defined for only one record, the same one for multiple records would fail)\u00a0https://github.com/monix/monix-connect/blob/master/parquet/src/test/scala/monix/connect/parquet/ProtoParquetSpec.scala#L83",
        "Issue Links": []
    },
    "PARQUET-1872": {
        "Key": "PARQUET-1872",
        "Summary": "Add TransCompression Feature",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "12/Jun/20 03:41",
        "Updated": "14/Jan/22 21:32",
        "Resolved": "14/Jan/22 21:32",
        "Description": "When ZSTD becomes more popular, there is a need to translate existing data to ZSTD compressed which can achieve a higher compression ratio. It would be useful if we can have a tool to convert a Parquet file directly by just decompressing/compressing each page without decoding/encoding or assembling the record because it is much faster. The initial result shows it is ~5 times faster.",
        "Issue Links": [
            "/jira/browse/PARQUET-1949",
            "https://github.com/apache/parquet-mr/pull/796"
        ]
    },
    "PARQUET-1873": {
        "Key": "PARQUET-1872 Add TransCompression Feature",
        "Summary": "Add to Parquet-tools",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "12/Jun/20 03:46",
        "Updated": "04/Dec/21 04:19",
        "Resolved": "04/Dec/21 04:19",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1874": {
        "Key": "PARQUET-1872 Add TransCompression Feature",
        "Summary": "Add to parquet-cli",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "12/Jun/20 03:46",
        "Updated": "04/Dec/21 04:19",
        "Resolved": "04/Dec/21 04:19",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1875": {
        "Key": "PARQUET-1872 Add TransCompression Feature",
        "Summary": "Add bloom filter support",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "12/Jun/20 03:47",
        "Updated": "04/Dec/20 22:37",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1949"
        ]
    },
    "PARQUET-1876": {
        "Key": "PARQUET-1876",
        "Summary": "Port ZSTD-JNI support to 1.10.x brach",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.2",
        "Fix Version/s": "1.10.2",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "15/Jun/20 02:14",
        "Updated": "15/Jun/20 07:51",
        "Resolved": null,
        "Description": "I hear the need to port the zstd-jni support to 1.10.x because of easiness to use ZSTD. \ncc dbtsai",
        "Issue Links": [
            "/jira/browse/PARQUET-1866",
            "https://github.com/apache/parquet-mr/pull/797"
        ]
    },
    "PARQUET-1877": {
        "Key": "PARQUET-1877",
        "Summary": "[C++] Reconcile container size with string size for memory issues",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "17/Jun/20 04:51",
        "Updated": "18/Jun/20 00:31",
        "Resolved": "18/Jun/20 00:30",
        "Description": "Right now the size can cause allocations an order of magnitude larger then string size limits.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7465"
        ]
    },
    "PARQUET-1878": {
        "Key": "PARQUET-1878",
        "Summary": "[C++] lz4 codec is not compatible with Hadoop Lz4Codec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Patrick Pai",
        "Reporter": "Steve M. Kim",
        "Created": "18/Jun/20 13:50",
        "Updated": "27/Sep/22 08:49",
        "Resolved": "22/Sep/20 19:05",
        "Description": "As described in HADOOP-12990, the Hadoop\u00a0Lz4Codec\u00a0uses the lz4 block format, and it prepends 8 extra bytes before the compressed data. I believe that lz4 implementation in parquet-cpp\u00a0also uses the lz4 block format, but it does not prepend these 8 extra bytes.\n\u00a0\nUsing Java parquet-mr, I wrote a Parquet file with lz4 compression:\n\n\r\n$ parquet-tools meta /tmp/f4a1c7f57cb1c98c2b9da3b25b16d027df5d2f1cf55adb79374c154fbd79011f\r\nfile:        file:/tmp/f4a1c7f57cb1c98c2b9da3b25b16d027df5d2f1cf55adb79374c154fbd79011f\r\ncreator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)file schema:\r\n--------------------------------------------------------------------------------\r\nc1:          REQUIRED INT64 R:0 D:0\r\nc0:          REQUIRED BINARY R:0 D:0\r\nv0:          REQUIRED INT64 R:0 D:0row group 1: RC:5007 TS:28028 OFFSET:4\r\n--------------------------------------------------------------------------------\r\nc1:           INT64 LZ4 DO:0 FPO:4 SZ:24797/25694/1.04 VC:5007 ENC:DELTA_BINARY_PACKED ST:[min: 1566330126476659000, max: 1571211622650188000, num_nulls: 0]\r\nc0:           BINARY LZ4 DO:0 FPO:24801 SZ:279/260/0.93 VC:5007 ENC:PLAIN,RLE_DICTIONARY ST:[min: 0x7471732F62656566616C6F2F746F6D6163636F2D66782D6D6B74646174612D6C69766573747265616D, max: 0x7471732F62656566616C6F2F746F6D6163636F2D66782D6D6B74646174612D6C69766573747265616D, num_nulls: 0]\r\nv0:           INT64 LZ4 DO:0 FPO:25080 SZ:1348/2074/1.54 VC:5007 ENC:PLAIN,RLE_DICTIONARY ST:[min: 0, max: 9, num_nulls: 0] \n\nWhen I attempted to read this file with parquet-cpp, I got the following error:\n\n\r\n>>> import pyarrow.parquet as pq\r\n>>> pq.read_table('/tmp/f4a1c7f57cb1c98c2b9da3b25b16d027df5d2f1cf55adb79374c154fbd79011f')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/skim/miniconda3/envs/arrow/lib/python3.8/site-packages/pyarrow/parquet.py\", line 1536, in read_table\r\n    return pf.read(columns=columns, use_threads=use_threads,\r\n  File \"/home/skim/miniconda3/envs/arrow/lib/python3.8/site-packages/pyarrow/parquet.py\", line 1260, in read\r\n    table = piece.read(columns=columns, use_threads=use_threads,\r\n  File \"/home/skim/miniconda3/envs/arrow/lib/python3.8/site-packages/pyarrow/parquet.py\", line 707, in read\r\n    table = reader.read(**options)\r\n  File \"/home/skim/miniconda3/envs/arrow/lib/python3.8/site-packages/pyarrow/parquet.py\", line 336, in read\r\n    return self.reader.read_all(column_indices=column_indices,\r\n  File \"pyarrow/_parquet.pyx\", line 1130, in pyarrow._parquet.ParquetReader.read_all\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\nOSError: IOError: Corrupt Lz4 compressed data. \n\n\u00a0\nhttps://github.com/apache/arrow/issues/3491\u00a0reported incompatibility in the other direction, using Spark (which uses the Hadoop lz4 codec) to read a parquet file that was written with parquet-cpp.\n\u00a0\nGiven that the Hadoop lz4 codec has long been in use, and users have accumulated Parquet files that were written with this implementation, I propose changing parquet-cpp to match the Hadoop implementation.\n\u00a0\nSee also:\n\nhttps://issues.apache.org/jira/browse/PARQUET-1241?focusedCommentId=16574328#comment-16574328\nhttps://issues.apache.org/jira/browse/PARQUET-1241?focusedCommentId=16585288#comment-16585288",
        "Issue Links": [
            "/jira/browse/ARROW-9177",
            "/jira/browse/ARROW-11301",
            "/jira/browse/PARQUET-1515",
            "/jira/browse/PARQUET-1241",
            "/jira/browse/PARQUET-2196",
            "https://github.com/apache/parquet-testing/pull/13",
            "https://github.com/apache/parquet-testing/pull/14",
            "https://github.com/apache/arrow-testing/pull/47",
            "https://github.com/apache/arrow/pull/7789"
        ]
    },
    "PARQUET-1879": {
        "Key": "PARQUET-1879",
        "Summary": "Apache Arrow can not read a Parquet File written with Parqet-Avro 1.11.0 with a Map field",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0,                                            1.11.1",
        "Component/s": "parquet-avro,                                            parquet-format",
        "Assignee": "Matthew McMahon",
        "Reporter": "Matthew McMahon",
        "Created": "22/Jun/20 11:37",
        "Updated": "01/Sep/22 07:46",
        "Resolved": "06/Jul/20 07:36",
        "Description": "From my\u00a0StackOverflow in relation to an issue I'm having with getting Snowflake (Cloud DB) to load Parquet files written with version 1.11.0\n\nThe problem only appears when using a map schema field in the Avro schema. For example:\n\n\r\n    {\r\n      \"name\": \"FeatureAmounts\",\r\n      \"type\": {\r\n        \"type\": \"map\",\r\n        \"values\": \"records.MoneyDecimal\"\r\n      }\r\n    }\r\n\n\nWhen using Parquet-Avro to write the file, a bad Parquet schema ends up with, for example\n\n\r\nmessage record.ResponseRecord {\r\n  required binary GroupId (STRING);\r\n  required int64 EntryTime (TIMESTAMP(MILLIS,true));\r\n  required int64 HandlingDuration;\r\n  required binary Id (STRING);\r\n  optional binary ResponseId (STRING);\r\n  required binary RequestId (STRING);\r\n  optional fixed_len_byte_array(12) CostInUSD (DECIMAL(28,15));\r\n  required group FeatureAmounts (MAP) {\r\n    repeated group map (MAP_KEY_VALUE) {\r\n      required binary key (STRING);\r\n      required fixed_len_byte_array(12) value (DECIMAL(28,15));\r\n    }\r\n  }\r\n}\r\n\n\nFrom the great answer to my StackOverflow, it seems the issue is that the 1.11.0 Parquet-Avro is still using the legacy MAP_KEY_VALUE converted type, that has no logical type equivalent. From the comment on LogicalTypeAnnotation\n\n\r\n// This logical type annotation is implemented to support backward compatibility with ConvertedType.\r\n  // The new logical type representation in parquet-format doesn't have any key-value type,\r\n  // thus this annotation is mapped to UNKNOWN. This type shouldn't be used.\r\n\n\nHowever, it seems this is being written with the latest 1.11.0, which then causes Apache Arrow to fail with\n\n\r\nLogical type Null can not be applied to group node\r\n\n\nAs it appears that Arrow only looks for the new logical type of Map or List, therefore this causes an error.\nI have seen in Parquet Formats that LogicalTypes should be something like\n\n\r\n// Map<String, Integer>\r\nrequired group my_map (MAP) {\r\n  repeated group key_value {\r\n    required binary key (UTF8);\r\n    optional int32 value;\r\n  }\r\n}\r\n\n\nIs this on the correct path?",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/798"
        ]
    },
    "PARQUET-1880": {
        "Key": "PARQUET-1880",
        "Summary": "Convert JSON to Parquet directly",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Emmanuel Brard",
        "Created": "23/Jun/20 12:36",
        "Updated": "26/Jun/20 14:42",
        "Resolved": null,
        "Description": "In order to convert JSON payload to Parquet, the \"established\" way seems to be to go first through a JSON to Avro conversion, provided the JSON payload is covered by a JSON-Schema or part of a OpenAPI definition. The rest of the process relies on the parquet-avro implementation.\nSince JSON payload can be loaded to a generic JsonNode (jackson library) and since a JSON payload can be described and checked against a JSON-Schema or an OpenAPI definition, wouldn't it be theoretically possible to convert a JSON payload to a Parquet \"record\" directly (somehow implementing a similar approach than parquet-avro but this a JsonNode as a data holder)?",
        "Issue Links": []
    },
    "PARQUET-1881": {
        "Key": "PARQUET-1881",
        "Summary": "How to enable sorted array flag while writing a column",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Not A Bug",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro,                                            parquet-cpp,                                            parquet-format,                                            parquet-mr,                                            parquet-thrift",
        "Assignee": null,
        "Reporter": "Khasim Shaik",
        "Created": "07/Jul/20 11:27",
        "Updated": "04/Apr/21 09:30",
        "Resolved": "04/Apr/21 09:30",
        "Description": "I want to understand how can we enable the flag \"sortedArray\" information in metadata while writing a row group or column\nI am exploring parquet.thrift to understand more about metadata, \nI observed a field in metadata which is related to below struct in parquet.thrift\nI am wondering how to set these fields from parquet while writing a column or rowgroup \nstruct SortingColumn \n{\r\n  /** The column index (in this row group) **/\r\n  1: required i32 column_idx\r\n\r\n  /** If true, indicates this column is sorted in descending order. **/\r\n  2: required bool descending\r\n\r\n  /** If true, nulls will come before non-null values, otherwise,\r\n   * nulls go at the end. */\r\n  3: required bool nulls_first\r\n}",
        "Issue Links": []
    },
    "PARQUET-1882": {
        "Key": "PARQUET-1882",
        "Summary": "[C++] Writing an all-null column and then reading it with buffered_stream aborts the process",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Eric Gorelik",
        "Created": "09/Jul/20 16:10",
        "Updated": "12/Jul/20 21:14",
        "Resolved": "12/Jul/20 21:14",
        "Description": "When writing a column unbuffered that contains only nulls, a 0-byte dictionary page gets written. When then reading the resulting file with buffered_stream enabled, the column reader gets the length of the page (which is 0), and then tries to read that many bytes from the underlying input stream.\nparquet/column_reader.cc, SerializedPageReader::NextPage\n\u00a0\n\n\r\nint compressed_len = current_page_header_.compressed_page_size;\r\nint uncompressed_len = current_page_header_.uncompressed_page_size;\r\n\r\n// Read the compressed data page.\r\nstd::shared_ptr<Buffer> page_buffer;\r\nPARQUET_THROW_NOT_OK(stream_->Read(compressed_len, &page_buffer));\n\n\u00a0\nBufferedInputStream::Read, however, has an assertion that the bytes to read is strictly positive, so the assertion fails and aborts the process.\narrow/io/buffered.cc, BufferedInputStream::Impl\n\u00a0\n\n\r\nStatus Read(int64_t nbytes, int64_t* bytes_read, void* out) {        \r\n  ARROW_CHECK_GT(nbytes, 0);",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/7718"
        ]
    },
    "PARQUET-1883": {
        "Key": "PARQUET-1883",
        "Summary": "int96 support in parquet-avro",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "satish",
        "Created": "09/Jul/20 18:01",
        "Updated": "19/Oct/20 07:06",
        "Resolved": "10/Jul/20 22:06",
        "Description": "Hi\nIt looks like 'timestamp' is being converted to 'int64' primitive type in parquet-avro. This is incompatible with hive2. Hive throws below error \n\n\r\nError: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to org.apache.hadoop.hive.serde2.io.TimestampWritable (state=,code=0)\r\n\n\nWhat does it take to write timestamp field as 'int96'? \nHive seems to write timestamp field as int96.  See example below\n\n\r\n$ hadoop jar parquet-tools-1.9.0.jar meta hdfs://timestamp_test/000000_0\r\ncreator:     parquet-mr version 1.10.6 (build 098c6199a821edd3d6af56b962fd0f1558af849b)\r\n\r\nfile schema: hive_schema\r\n--------------------------------------------------------------------------------\r\nts:          OPTIONAL INT96 R:0 D:1\r\n\r\nrow group 1: RC:4 TS:88 OFFSET:4\r\n--------------------------------------------------------------------------------\r\nts:           INT96 UNCOMPRESSED DO:0 FPO:4 SZ:88/88/1.00 VC:4 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY\r\n\n\nWriting a spark dataframe into parquet format (without using avro) is also using int96.\n\n\r\nscala> testDS.printSchema()\r\nroot\r\n |-- ts: timestamp (nullable = true)\r\n\r\nscala> testDS.write.mode(Overwrite).save(\"/tmp/x\");\r\n\r\n$ parquet-tools meta /tmp/x/part-00000-99720ebd-0aea-45ac-9b8c-0eb7ad6f4e3c-c000.gz.parquet \r\nfile:        file:/tmp/x/part-00000-99720ebd-0aea-45ac-9b8c-0eb7ad6f4e3c-c000.gz.parquet \r\ncreator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1) \r\nextra:       org.apache.spark.sql.parquet.row.metadata = {\"type\":\"struct\",\"fields\":[{\"name\":\"ts\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]} \r\n\r\nfile schema: spark_schema \r\n--------------------------------------------------------------------------------\r\nts:          OPTIONAL INT96 R:0 D:1\r\n\r\nrow group 1: RC:4 TS:93 OFFSET:4 \r\n--------------------------------------------------------------------------------\r\nts:           INT96 GZIP DO:0 FPO:4 SZ:130/93/0.72 VC:4 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[no stats for this column]\r\n\r\n\r\n\n\nI saw some explanation for deprecating int96 support here from gszadovszky. But given hive and serialization in other parquet modules (non-avro) support int96, I'm trying to understand the reasoning for not implementing it in parquet-avro.\nA bit more context: we are trying to migrate some of our data to hudi format. Hudi adds a lot of efficiency for our use cases. But, when we write data using hudi, hudi uses parquet-avro and timestamp is being converted to int64. As mentioned earlier, this breaks compatibility with hive. A lot of columns in our tables have 'timestamp' as type in hive DDL.  It is almost impossible to change DDL to long as there are large number of tables and columns. \nWe are happy to contribute if there is a clear path forward to support int96 in parquet-avro. Please also let me know if you are aware of a workaround in hive that can read int64 correctly as timestamp.",
        "Issue Links": []
    },
    "PARQUET-1884": {
        "Key": "PARQUET-1178 Parquet modular encryption",
        "Summary": "Merge encryption branch into master",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "13/Jul/20 05:51",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "29/Jul/20 10:59",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1885": {
        "Key": "PARQUET-1885",
        "Summary": "[parquet-protobuf] Pass descriptor to ProtoWriteSupport constructor",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0,                                            1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Maulik Soneji",
        "Created": "20/Jul/20 18:31",
        "Updated": "26/May/22 17:55",
        "Resolved": null,
        "Description": "Currently, the ProtoWriteSupport class checks for Descriptor by calling `Protobufs.getMessageDescriptor` function which checks for descriptor in the classpath. There is no way to pass descriptor as an argument to the ProtoWriteSupport constructor.\nIn our approach to using parquet-mr library, we are using a descriptor that is not available in the classpath.\nI will be happy to work on adding this support to the parquet-mr library.",
        "Issue Links": []
    },
    "PARQUET-1886": {
        "Key": "PARQUET-1886",
        "Summary": "CompressionCodec Provider-aware Compression Codec Lookup for parquet-mr",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "XinDong",
        "Created": "21/Jul/20 00:36",
        "Updated": "22/Apr/21 16:26",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1887": {
        "Key": "PARQUET-1887",
        "Summary": "Exception thrown by AvroParquetWriter#write causes all subsequent calls to it to fail",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0,                                            1.8.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "\u00d8yvind Str\u00f8mmen",
        "Created": "22/Jul/20 13:06",
        "Updated": "27/Jul/20 11:32",
        "Resolved": null,
        "Description": "Please see sample code below:\n\n\r\nSchema schema = new Schema.Parser().parse(\"\"\"\r\n        {\r\n          \"type\": \"record\",\r\n          \"name\": \"person\",\r\n          \"fields\": [\r\n            {\r\n              \"name\": \"address\",\r\n              \"type\": [\r\n                \"null\",\r\n                {\r\n                  \"type\": \"array\",\r\n                  \"items\": \"string\"\r\n                }\r\n              ],\r\n              \"default\": null\r\n            }\r\n          ]\r\n        }\r\n        \"\"\"\r\n);\r\n\r\nParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(\"/tmp/person.parquet\"))\r\n        .withSchema(schema)\r\n        .build();\r\n\r\ntry {\r\n    // To trigger exception, add array with null element.\r\n    writer.write(new GenericRecordBuilder(schema).set(\"address\", Arrays.asList(\"first\", null, \"last\")).build());\r\n} catch (Exception e) {\r\n    e.printStackTrace(); // \"java.lang.NullPointerException: Array contains a null element at 1\"\r\n}\r\n\r\ntry {\r\n    // At this point all future calls to writer.write will fail\r\n    writer.write(new GenericRecordBuilder(schema).set(\"address\", Arrays.asList(\"foo\", \"bar\")).build());\r\n} catch (Exception e) {\r\n    e.printStackTrace(); // \"org.apache.parquet.io.InvalidRecordException: 1(r) > 0 ( schema r)\"\r\n}\r\n\r\nwriter.close();\r\n\n\nIt seems to me this is caused by state not being reset between writes.\u00a0Is this the indented behavior of the writer? And if so, does one have to create a new writer whenever a write fails?\nI'm able to reproduce this using both parquet 1.8.3 and 1.11.0, and have attached a sample parquet file for each version.",
        "Issue Links": []
    },
    "PARQUET-1888": {
        "Key": "ARROW-9584",
        "Summary": "[Rust] Provide guidance on number of file descriptors needed to read Parquet file",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Rust",
        "Assignee": null,
        "Reporter": "Adam Shirey",
        "Created": "28/Jul/20 03:54",
        "Updated": "11/Jan/23 08:07",
        "Resolved": null,
        "Description": "I have a series of Parquet files that are 181 columns wide, and I'm processing them in parallel (using rayon). I ran into the OS limit (default 1024 according to ulimit -n) of open file descriptors when doing this, but each file was consuming 208 descriptors.\nIs there a deterministic calculation for how many file descriptors will be used to process files so that one can determine appropriate multithreading in a situation like this?",
        "Issue Links": []
    },
    "PARQUET-1889": {
        "Key": "PARQUET-1889",
        "Summary": "Register a MIME type for the Parquet format.",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "format-2.7.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Mark Wood",
        "Created": "28/Jul/20 14:43",
        "Updated": "08/Mar/23 21:10",
        "Resolved": null,
        "Description": "There is currently\u00a0 no MIME type registered for Parquet.\u00a0 Perhaps this is intentional.\nIf it is not intentional, I suggest steps be taken to register a MIME type with IANA.\n\u00a0\nhttps://www.iana.org/assignments/media-types/media-types.xhtml",
        "Issue Links": []
    },
    "PARQUET-1890": {
        "Key": "PARQUET-1890",
        "Summary": "Upgrade to Avro 1.10.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-avro",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "29/Jul/20 10:24",
        "Updated": "30/Jul/20 07:36",
        "Resolved": "30/Jul/20 07:06",
        "Description": "The new major release 1.10.0 of Avro is out.\nMoving Avro from 1.8 to 1.9 requires significant efforts from the components using it and we already introduced 1.9 in the parquet release 1.11.0. We cannot step backward (downgrading Avro) and the Avro release 1.10.0 does not contain too many additional breaking changes so we should do the upgrade for the next 1.12.0 parquet release.",
        "Issue Links": []
    },
    "PARQUET-1891": {
        "Key": "PARQUET-1891",
        "Summary": "Encryption-related light fixes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "29/Jul/20 11:00",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": "hadoop/readme.md, travis.yaml, vault sample fixes",
        "Issue Links": []
    },
    "PARQUET-1892": {
        "Key": "PARQUET-1892",
        "Summary": "Explain CRC computation better",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "29/Jul/20 11:02",
        "Updated": "07/Apr/21 08:41",
        "Resolved": "07/Apr/21 08:39",
        "Description": "Mention that CRC is calculated after compression and encryption",
        "Issue Links": []
    },
    "PARQUET-1893": {
        "Key": "PARQUET-1893",
        "Summary": "H2SeekableInputStream readFully() doesn't respect start and len",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "29/Jul/20 20:43",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Oct/20 14:19",
        "Description": "The  readFully() throws away the parameters 'start' and 'len' as shown below. \npublic void readFully(byte[] bytes, int start, int len) throws IOException \n{\r\n    stream.readFully(bytes);\r\n  }\n\nIt should be corrected as below. \npublic void readFully(byte[] bytes, int start, int len) throws IOException \n{\r\n    stream.readFully(bytes, start, len);\r\n  }\n\nH1SeekableInputStream() has been fixed.",
        "Issue Links": []
    },
    "PARQUET-1894": {
        "Key": "PARQUET-1894",
        "Summary": "Please fix the related Shaded Jackson Databind CVEs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Rodney Aaron Stainback",
        "Created": "30/Jul/20 19:20",
        "Updated": "08/Feb/21 10:06",
        "Resolved": "08/Feb/21 10:06",
        "Description": "The following CVEs are all related to version 2.9.10 of Jackson databind which you shade\n\n\n\ncve\nseverity\ncvss\n\n\nCVE-2019-16942\ncritical\n9.8\n\n\nCVE-2019-16943\ncritical\n9.8\n\n\nCVE-2019-17531\ncritical\n9.8\n\n\nCVE-2019-20330\ncritical\n9.8\n\n\nCVE-2020-10672\nhigh\n8.8\n\n\nCVE-2020-10673\nhigh\n8.8\n\n\nCVE-2020-10968\nhigh\n8.8\n\n\nCVE-2020-10969\nhigh\n8.8\n\n\nCVE-2020-11111\nhigh\n8.8\n\n\nCVE-2020-11112\nhigh\n8.8\n\n\nCVE-2020-11113\nhigh\n8.8\n\n\nCVE-2020-11619\ncritical\n9.8\n\n\nCVE-2020-11620\ncritical\n9.8\n\n\nCVE-2020-14060\nhigh\n8.1\n\n\nCVE-2020-14061\nhigh\n8.1\n\n\nCVE-2020-14062\nhigh\n8.1\n\n\nCVE-2020-14195\nhigh\n8.1\n\n\nCVE-2020-8840\ncritical\n9.8\n\n\nCVE-2020-9546\ncritical\n9.8\n\n\nCVE-2020-9547\ncritical\n9.8\n\n\nCVE-2020-9548\ncritical\n9.8\n\n\n\n\u00a0\nOur security team is trying to block us from using parquet files because of this issue",
        "Issue Links": [
            "/jira/browse/PARQUET-1961"
        ]
    },
    "PARQUET-1895": {
        "Key": "PARQUET-1895",
        "Summary": "Update jackson-databind",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Patrick OFriel",
        "Created": "06/Aug/20 20:27",
        "Updated": "13/Oct/20 09:49",
        "Resolved": "13/Oct/20 08:31",
        "Description": "The jackson databind 2.9.10.4 has the following CVEs:\nhttps://nvd.nist.gov/vuln/detail/CVE-2020-14060\nhttps://nvd.nist.gov/vuln/detail/CVE-2020-14061\nhttps://nvd.nist.gov/vuln/detail/CVE-2020-14062\nhttps://nvd.nist.gov/vuln/detail/CVE-2020-14195\nThey should be resolved if we update to 2.9.10.5",
        "Issue Links": []
    },
    "PARQUET-1896": {
        "Key": "PARQUET-1896",
        "Summary": "[Maven] parquet-tools build is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Qinghui Xu",
        "Reporter": "Qinghui Xu",
        "Created": "09/Aug/20 16:49",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "24/Aug/20 12:42",
        "Description": "There is a compilation error when running `mvn clean install` on the parquet-mr project:\nEnvironment: macos 10.14.6 (Darwin Kernel Version 18.7.0), maven 3.6.3\n\n\r\n[ERROR] COMPILATION ERROR :\r\n[INFO] -------------------------------------------------------------\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleMapRecord.java:[21,43] package com.fasterxml.jackson.databind.node does not exist\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[29,38] package com.fasterxml.jackson.databind does not exist\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[30,43] package com.fasterxml.jackson.databind.node does not exist\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/json/JsonRecordFormatter.java:[22,38] package com.fasterxml.jackson.databind does not exist\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[72,23] cannot find symbol\r\n  symbol:   class BinaryNode\r\n  location: class org.apache.parquet.tools.read.SimpleRecord\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[108,5] cannot find symbol\r\n  symbol:   class ObjectMapper\r\n  location: class org.apache.parquet.tools.read.SimpleRecord\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[108,31] cannot find symbol\r\n  symbol:   class ObjectMapper\r\n  location: class org.apache.parquet.tools.read.SimpleRecord\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[125,18] cannot find symbol\r\n  symbol:   class BinaryNode\r\n  location: class org.apache.parquet.tools.read.SimpleRecord\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleMapRecord.java:[59,20] cannot find symbol\r\n  symbol:   class BinaryNode\r\n  location: class org.apache.parquet.tools.read.SimpleMapRecord\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/json/JsonRecordFormatter.java:[116,7] cannot find symbol\r\n  symbol:   class ObjectMapper\r\n  location: class org.apache.parquet.tools.json.JsonRecordFormatter.JsonGroupFormatter\r\n[ERROR] /Users/q.xu/Sources/thirdparty/parquet-mr/parquet-tools/src/main/java/org/apache/parquet/tools/json/JsonRecordFormatter.java:[116,33] cannot find symbol\r\n  symbol:   class ObjectMapper\r\n  location: class org.apache.parquet.tools.json.JsonRecordFormatter.JsonGroupFormatter",
        "Issue Links": [
            "/jira/browse/PARQUET-1897"
        ]
    },
    "PARQUET-1897": {
        "Key": "PARQUET-1897",
        "Summary": "Failing individual module build/test",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "12/Aug/20 09:18",
        "Updated": "25/Aug/20 13:32",
        "Resolved": "25/Aug/20 13:32",
        "Description": "The build/test of individual modules are failing. For example mvn test -pl parquet-tools fails.",
        "Issue Links": [
            "/jira/browse/PARQUET-1896"
        ]
    },
    "PARQUET-1898": {
        "Key": "PARQUET-1898",
        "Summary": "Release parquet-mr 1.12.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "13/Aug/20 09:26",
        "Updated": "25/Mar/21 14:34",
        "Resolved": "25/Mar/21 14:34",
        "Description": "This task is to track the parquet-mr release 1.12.0.\nAny open Jira that we would like to be added to this release shall be linked to this one as a blocker.",
        "Issue Links": [
            "/jira/browse/DRILL-7825",
            "/jira/browse/PARQUET-1966",
            "/jira/browse/PARQUET-1970",
            "/jira/browse/PARQUET-1971",
            "/jira/browse/PARQUET-1977",
            "/jira/browse/PARQUET-1979"
        ]
    },
    "PARQUET-1899": {
        "Key": "PARQUET-1899",
        "Summary": "[C++] Deprecated ReadBatchSpaced in parquet/column_reader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "20/Aug/20 05:18",
        "Updated": "01/Feb/21 12:23",
        "Resolved": "01/Feb/21 12:23",
        "Description": "This method is not used any place outside of unit tests and doesn't space elements properly in the context of deeply nested structures.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/8015"
        ]
    },
    "PARQUET-1900": {
        "Key": "PARQUET-1900",
        "Summary": "Run mvn clean in CI",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Qinghui Xu",
        "Reporter": "Qinghui Xu",
        "Created": "20/Aug/20 15:03",
        "Updated": "21/Oct/20 20:21",
        "Resolved": null,
        "Description": "Currently parquet-mr CI does not run `mvn clean`, modules without changes are not recompiled each time.",
        "Issue Links": [
            "/jira/browse/PARQUET-1902"
        ]
    },
    "PARQUET-1901": {
        "Key": "PARQUET-1901",
        "Summary": "Add filter null check for ColumnIndex",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "22/Aug/20 20:24",
        "Updated": "03/Dec/20 10:17",
        "Resolved": null,
        "Description": "This Jira is opened for discussion that should we add null checking for the filter when ColumnIndex is enabled. \nIn the ColumnIndexFilter#calculateRowRanges() method, the input parameter 'filter' is assumed to be non-null without checking. It throws NPE when ColumnIndex is enabled(by default) but there is no filter set in the ParquetReadOptions. The call stack is as below. \n    java.lang.NullPointerException\n        at org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\n        at org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\n        at org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:891)\nIf we don't add, the user might need to choose to call readNextRowGroup() or readFilteredNextRowGroup() accordingly based on filter existence. \nThoughts?",
        "Issue Links": []
    },
    "PARQUET-1902": {
        "Key": "PARQUET-1902",
        "Summary": "Invoke mvn clean in Travis",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "24/Aug/20 12:39",
        "Updated": "24/Aug/20 12:47",
        "Resolved": "24/Aug/20 12:47",
        "Description": "Currently we do not invoke mvn clean in the Travis build which may cause undetected issues in our CI. (See PR #809 for details.)",
        "Issue Links": [
            "/jira/browse/PARQUET-1900"
        ]
    },
    "PARQUET-1903": {
        "Key": "PARQUET-1903",
        "Summary": "Improve Parquet Protobuf Usability",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-protobuf",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "27/Aug/20 00:40",
        "Updated": "01/Apr/21 12:47",
        "Resolved": null,
        "Description": "Check out the PR for details.\n\u00a0\n\nMove away from passing around a Class object to take advantage of Java Templating\nMake parquet-proto library more usable and straight-forward\nProvide test examples\nLimited support for protocol buffer schema registry",
        "Issue Links": []
    },
    "PARQUET-1904": {
        "Key": "PARQUET-1904",
        "Summary": "[C++] Export file_offset in RowGroupMetaData",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Simon Bertron",
        "Reporter": "Simon Bertron",
        "Created": "21/Aug/20 23:32",
        "Updated": "27/Aug/20 21:41",
        "Resolved": "27/Aug/20 03:22",
        "Description": "In the C++ row group metadata object, the offset of the row group in the file is stored, but not exposed to users. RowGroupMetaDataImpl has a field file_offset and a method file_offset() that exposes it. But RowGroupMetaData does not have a file_offset() method. This seems odd, most other fields in RowGroupMetaDataImpl are exposed by RowGroupMetaData.\n\u00a0\nThis issue is similar to ARROW-3590, but that issue seems pretty stale and is requesting a python feature. I think this issue is more focused and detailed.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/8040"
        ]
    },
    "PARQUET-1905": {
        "Key": "PARQUET-1905",
        "Summary": "Use SeekableByteChannel instead of OutputFile/InputFile Classes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "2.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "27/Aug/20 20:44",
        "Updated": "01/Sep/20 15:38",
        "Resolved": null,
        "Description": "Use Java NIO\u00a0SeekableByteChannel for input to reader/writer instead of the current Parquet-only Output/InputFile Classes",
        "Issue Links": [
            "/jira/browse/PARQUET-1822",
            "/jira/browse/PARQUET-1776"
        ]
    },
    "PARQUET-1906": {
        "Key": "PARQUET-1906",
        "Summary": "CLONE - [C++] Parquet modular encryption",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Deepak Majeti",
        "Reporter": "Akshay",
        "Created": "07/Sep/20 07:00",
        "Updated": "07/Sep/20 13:15",
        "Resolved": "07/Sep/20 13:15",
        "Description": "CPP version of a mechanism for modular encryption and decryption of Parquet files. Allows to keep the data fully encrypted in the storage, while enabling a client to extract a required subset (footer, column(s), pages) and to authenticate / decrypt the extracted data.",
        "Issue Links": [
            "/jira/browse/PARQUET-1300",
            "/jira/browse/PARQUET-1178",
            "https://docs.google.com/document/d/1T89G7xR0zHFV1f2pjTO28jtfVm8qoNVGEJQ70Rsk-bY/edit?usp=sharing",
            "https://github.com/apache/parquet-cpp/pull/475",
            "https://github.com/apache/arrow/pull/2555",
            "https://github.com/apache/arrow/pull/4826"
        ]
    },
    "PARQUET-1907": {
        "Key": "PARQUET-1906 CLONE - [C++] Parquet modular encryption",
        "Summary": "CLONE - [C++] Crypto package in parquet-cpp",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Akshay",
        "Created": "07/Sep/20 07:00",
        "Updated": "07/Sep/20 13:15",
        "Resolved": "07/Sep/20 13:15",
        "Description": "The C++ implementation of basic AES-GCM encryption and decryption",
        "Issue Links": [
            "https://github.com/apache/parquet-cpp/pull/464"
        ]
    },
    "PARQUET-1908": {
        "Key": "PARQUET-1906 CLONE - [C++] Parquet modular encryption",
        "Summary": "CLONE - [C++] Update cpp crypto package to match signed-off specification",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-5.0.0",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Akshay",
        "Created": "07/Sep/20 07:00",
        "Updated": "03/Aug/21 08:26",
        "Resolved": "03/Aug/21 08:26",
        "Description": "An initial version of crypto package is merged. This Jira updates the crypto code to\u00a0\n\nconform the signed off specification (wire protocol updates, signature tag creation, AAD support, etc)\nimprove performance by extending cipher lifecycle to file writing/reading - instead of creating cipher on each encrypt/decrypt operation",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/3520"
        ]
    },
    "PARQUET-1909": {
        "Key": "PARQUET-1906 CLONE - [C++] Parquet modular encryption",
        "Summary": "CLONE - [C++] Add encrypted parquet files to apache parquet-testing repository",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Revital Sur",
        "Reporter": "Akshay",
        "Created": "07/Sep/20 07:00",
        "Updated": "07/Sep/20 13:15",
        "Resolved": "07/Sep/20 13:15",
        "Description": "Add encrypted parquet files to apache parquet-testing repository (https://github.com/apache/parquet-testing) for the purpose of testing.",
        "Issue Links": [
            "https://github.com/apache/parquet-testing/pull/7"
        ]
    },
    "PARQUET-1910": {
        "Key": "PARQUET-1910",
        "Summary": "Parquet-cli is broken after TransCompressionCommand was added",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli",
        "Assignee": "Grisha Weintraub",
        "Reporter": "Grisha Weintraub",
        "Created": "07/Sep/20 12:41",
        "Updated": "22/Oct/20 16:39",
        "Resolved": "21/Oct/20 20:15",
        "Description": "Scenario\nRun parquet-cli\n\n\r\njava -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main\r\n\n\n\u00a0\nExpected result\u00a0\nparquet-cli usage is presented\n\n\r\nUsage: parquet [options] [command] [command options]\r\n\n\n\u00a0\nActual result\nthe following error is presented\n\n\r\nException in thread \"main\" com.beust.jcommander.ParameterException: Only one @Parameter with no names attribute is allowed, found:com.beust.jcommander.JCommander$MainParameter@6442b0a6 and com.beust.jcommander.Parameterized@f5bfa59d\r\n        at com.beust.jcommander.JCommander.addDescription(JCommander.java:606)\r\n        at com.beust.jcommander.JCommander.createDescriptions(JCommander.java:587)\r\n        at com.beust.jcommander.JCommander.addCommand(JCommander.java:1533)\r\n        at com.beust.jcommander.JCommander.addCommand(JCommander.java:1512)\r\n        at org.apache.parquet.cli.Main.<init>(Main.java:95)\r\n        at org.apache.parquet.cli.Main.main(Main.java:181)\r\n\n\n\u00a0\nNotes\nThe error is related to the recent changes in\u00a0PARQUET-1872.\nThe fix is trivial - see PR.",
        "Issue Links": []
    },
    "PARQUET-1911": {
        "Key": "PARQUET-1911",
        "Summary": "Add way to disables statistics on a per column basis",
        "Type": "New Feature",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Anthony Pessy",
        "Created": "08/Sep/20 12:58",
        "Updated": "02/Jan/23 06:54",
        "Resolved": null,
        "Description": "When you write dataset with BINARY columns that can be fairly large (several Mbs) you can often end with an OutOfMemory error where you either have to:\n\u00a0\n\u00a0- Throw more RAM\n\u00a0- Increase number of output files\n\u00a0- Play with Block size\n\u00a0\nUsing a fork with increased checks frequency for row group size help but it is not enough. (PR: https://github.com/apache/parquet-mr/pull/470)\n\u00a0\n\u00a0\nThe OutOfMemory error is now caused due to the accumulation of min/max values for those columns for each BlockMetaData.\n\u00a0\nThe \"parquet.statistics.truncate.length\" configuration is of no help because it is applied during the footer serialization whereas the OOM occurs before that.\n\u00a0\nI think it would be nice to have, like for dictionary or bloom filter, a way to disable the statistic on a per-column basis.\n\u00a0\nCould be very useful to lower memory consumption when stats of huge binary column are unnecessary.",
        "Issue Links": []
    },
    "PARQUET-1912": {
        "Key": "PARQUET-1912",
        "Summary": "ParquetReader.read(InputFile) always causes exception on build",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "M. Justin",
        "Created": "16/Sep/20 15:49",
        "Updated": "16/Sep/20 15:51",
        "Resolved": null,
        "Description": "The ParquetReader.read(InputFile file) static factory method in parquet-hadoop creates a builder from an InputFile.\u00a0 This method always throws an IllegalArgumentException when .build() is subsequently called.\n\n\r\n            java.nio.Path parquetFile = getParquetFile();\r\n            ParquetReader.read(HadoopInputFile.fromPath(new org.apache.hadoop.fs.Path(parquetFile.toUri()), new Configuration()))\r\n                    .build();\r\n\n\n\njava.lang.IllegalArgumentException: [BUG] Classes that extend Builder should override getReadSupport()\r\n\r\n\tat org.apache.parquet.Preconditions.checkArgument(Preconditions.java:53)\r\n\tat org.apache.parquet.hadoop.ParquetReader$Builder.getReadSupport(ParquetReader.java:310)\r\n\tat org.apache.parquet.hadoop.ParquetReader$Builder.build(ParquetReader.java:337)\r\n\n\nThe issue appears to be that the build() method enforces that a ReadSupport value was set on the builder, but ParquetReader.read(InputFile file) doesn't take accept a ReadSupport, nor is there a way to set it after the builder has been created.\nFor context, my use case is reading Parquet files directly from Java.\nExpected behavior\nI wouldn't expect a method to exist that always results in an exception being thrown. I would expect the ParquetReader.read(InputFile file) to be fixed, replaced, or removed.\nWorkaround\nI am able to achieve my goal by using ParquetFileReader.open(InputFile) instead of ParquetReader.read(InputFile).\n\n\r\n            java.nio.Path parquetFile = getParquetFile();\r\n            ParquetFileReader reader = ParquetFileReader.open(\r\n                    HadoopInputFile.fromPath(new org.apache.hadoop.fs.Path(parquetFile.toUri()), new Configuration()));",
        "Issue Links": []
    },
    "PARQUET-1913": {
        "Key": "PARQUET-1913",
        "Summary": "ParquetReader Should Support InputFile",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "21/Sep/20 18:30",
        "Updated": "22/Sep/20 03:34",
        "Resolved": "21/Sep/20 18:52",
        "Description": "When creating a ParquetReader, a \"read support\" object is required.\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java#L325-L330\nHowever, when building from an InputFile, 'readSupport' is always 'null' and therefore will never work.\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java#L202\nAdd the read support option just as is done with a Path object.",
        "Issue Links": [
            "/jira/browse/PARQUET-1914"
        ]
    },
    "PARQUET-1914": {
        "Key": "PARQUET-1914",
        "Summary": "Allow ProtoParquetReader To Support InputFile",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "21/Sep/20 18:51",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Oct/20 14:57",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1913"
        ]
    },
    "PARQUET-1915": {
        "Key": "PARQUET-1792 Add 'mask' command to parquet-tools/parquet-cli",
        "Summary": "Add null command",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "24/Sep/20 03:22",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1916": {
        "Key": "PARQUET-1792 Add 'mask' command to parquet-tools/parquet-cli",
        "Summary": "Add hash functionality",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "24/Sep/20 03:23",
        "Updated": "24/Sep/20 03:23",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1917": {
        "Key": "PARQUET-1917",
        "Summary": "[parquet-proto] default values are stored in oneOf fields that aren't set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-protobuf",
        "Assignee": "Aaron Blake Niskode-Dossett",
        "Reporter": "Aaron Blake Niskode-Dossett",
        "Created": "29/Sep/20 15:55",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "22/Oct/20 14:55",
        "Description": "SCHEMA\n--------\n\nmessage Person {\r\n\u00a0 int32 foo = 1;\r\n\u00a0\u00a0oneof optional_bar {\r\n\u00a0\u00a0 \u00a0int32 bar_int = 200;\r\n\u00a0 \u00a0 int32 bar_int2 = 201;\r\n\u00a0 \u00a0 string bar_string = 300;\r\n\u00a0 }\r\n}\n\n\u00a0\nCODE\n--------\nI set values for foo and bar_string\n\u00a0\n\nfor (int i = 0; i < 3; i += 1) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 com.etsy.grpcparquet.Person message = Person.newBuilder()\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setFoo(i)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setBarString(\"hello world\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 message.writeDelimitedTo(out);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n\nAnd then I write the protobuf file out to parquet.\n\u00a0\nRESULT\n-----------\n\n$ parquet-tools show example.parquet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n+-------+-----------+------------+--------------+\r\n| \u00a0 foo | \u00a0 bar_int | \u00a0 bar_int2 | bar_string \u00a0 |\r\n|-------+-----------+------------+--------------|\r\n| \u00a0 \u00a0 0 | \u00a0 \u00a0 \u00a0 \u00a0 0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 | hello world \u00a0|\r\n| \u00a0 \u00a0 1 | \u00a0 \u00a0 \u00a0 \u00a0 0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 | hello world \u00a0|\r\n| \u00a0 \u00a0 2 | \u00a0 \u00a0 \u00a0 \u00a0 0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 | hello world \u00a0|\r\n+-------+-----------+------------+--------------+\n\n\u00a0\nbar_int and bar_int2 should be EMPTY for all three rows since only bar_string is set in the oneof.\u00a0 0 is the default value for int, but it should not be stored.",
        "Issue Links": []
    },
    "PARQUET-1918": {
        "Key": "PARQUET-1918",
        "Summary": "Avoid Copy of Bytes in Protobuf BinaryWriter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "02/Oct/20 13:57",
        "Updated": "22/Oct/20 14:49",
        "Resolved": null,
        "Description": "ProtoWriteSupport.java\n\r\n  class BinaryWriter extends FieldWriter {\r\n    @Override\r\n    final void writeRawValue(Object value) {\r\n      ByteString byteString = (ByteString) value;\r\n      Binary binary = Binary.fromConstantByteArray(byteString.toByteArray());\r\n      recordConsumer.addBinary(binary);\r\n    }\r\n  }\r\n\n\ntoByteArray() creates a copy of the buffer.  There is already support with Parquet and Protobuf to pass instead a ByteBuffer which avoids the copy.",
        "Issue Links": [
            "/jira/browse/THRIFT-5288"
        ]
    },
    "PARQUET-1919": {
        "Key": "PARQUET-1919",
        "Summary": "Buffer int overflow in CapacityByteArrayOutputStream, SnappyCompressor",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Artem Shnayder",
        "Created": "07/Oct/20 19:49",
        "Updated": "07/Oct/20 19:49",
        "Resolved": null,
        "Description": "During an attempted write operation, a buffer position integer overflow is resulting in a IllegalArgumentException: Negative capacity: -2147336621 exception.\n\u00a0\n\n20/10/06 15:30:39 INFO HadoopRDD: Input split: s3a://<prefix>/part-00015-96362e5d-d047-4f31-812b-38ff79f6919c-c000.txt.bz2:268435456+33554432\r\n20/10/06 17:23:37 ERROR Utils: Aborting task\r\njava.lang.IllegalArgumentException: Negative capacity: -2147336621\r\n\tat java.nio.Buffer.<init>(Buffer.java:199)\r\n\tat java.nio.ByteBuffer.<init>(ByteBuffer.java:281)\r\n\tat java.nio.ByteBuffer.<init>(ByteBuffer.java:289)\r\n\tat java.nio.MappedByteBuffer.<init>(MappedByteBuffer.java:89)\r\n\tat java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:119)\r\n\tat java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)\r\n\tat org.apache.parquet.hadoop.codec.SnappyCompressor.setInput(SnappyCompressor.java:97)\r\n\tat org.apache.parquet.hadoop.codec.NonBlockedCompressorStream.write(NonBlockedCompressorStream.java:48)\r\n\tat org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeToOutput(CapacityByteArrayOutputStream.java:227)\r\n\tat org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeTo(CapacityByteArrayOutputStream.java:247)\r\n\tat org.apache.parquet.bytes.BytesInput$CapacityBAOSBytesInput.writeAllTo(BytesInput.java:405)\r\n\tat org.apache.parquet.bytes.BytesInput$SequenceBytesIn.writeAllTo(BytesInput.java:296)\r\n\tat org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:164)\r\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:95)\r\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147)\r\n\tat org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235)\r\n\tat org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.checkBlockSizeReached(InternalParquetRecordWriter.java:148)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:130)\r\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)\r\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\n\n2,147,483,647 (max int) - 2147336621 (negative capacity) = 147026.\nThe input bz2 files are all roughly 900 MiB in size. The target parquet part files are 1.7 GiB in size.\nIncreasing the partition count from 64 to 1024 fixes the issue. The output parquet part files drop to 100MiB in size. \nHowever, it's unclear to me what the root cause is and why increasing partition count helps. Was it an unlucky row grouping that bumped the buffer size over by 147KB, i.e, any change up or down in parittion count would have helped? Is it approaching the parquet part file size limit?\nThis issue seems related to PARQUET-1632 but it's not using the ConcatenatingByteArrayCollector, which potentially means a distinct root cause. The input dataset does have large string columns (up to 10MB) but nothing close to the signed int max of 2.4G that was produced in PARQUET-1632.",
        "Issue Links": []
    },
    "PARQUET-1920": {
        "Key": "PARQUET-1920",
        "Summary": "Fix issue with reading parquet files with too large column chunks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10.0,                                            1.11.0,                                            1.10.1,                                            1.12.0,                                            1.11.1",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Ashish Singh",
        "Reporter": "Ashish Singh",
        "Created": "07/Oct/20 22:13",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "12/Oct/20 08:22",
        "Description": "Fix Parquet writer's memory check while writing highly skewed data.\nParquet uses CapacityByteArrayOutputStream to hold column chunks in memory. This is similar to ByteArrayOutputStream, however it avoids copying of entire data while growing the array. It does so by creating and maintaining different arrays (slabs). The way the size grows is exponentially till it nears the configurable max capacity hint, and after that it grows very slowly. This along with the Parquet's logic to determine when to check if enough data is in memory to flush to disk, makes it possible for a highly skewed dataset to make Parquet's write really large column chunk and so row group, beyond the max expected size (in int) of the row group.\nIn Parquet 1.10, a change was made to make page size row check frequency configurable. However, there is a bug in the implementation that is leading to these configs to not help with memory checks calculation.",
        "Issue Links": []
    },
    "PARQUET-1921": {
        "Key": "PARQUET-1921",
        "Summary": "Use StringBuilder instead of StringBuffer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "08/Oct/20 16:31",
        "Updated": "08/Oct/20 16:31",
        "Resolved": null,
        "Description": "MessageTypeParser.java\n\r\n    private StringBuffer currentLine = new StringBuffer();\r\n\r\n    ....\r\n\r\n    public String nextToken() {\r\n      while (st.hasMoreTokens()) {\r\n        String t = st.nextToken();\r\n        if (t.equals(\"\\n\")) {\r\n          ++ line;\r\n          currentLine.setLength(0);\r\n        } else {\r\n          currentLine.append(t);\r\n        }\r\n        if (!isWhitespace(t)) {\r\n          return t;\r\n        }\r\n      }\r\n      throw new IllegalArgumentException(\"unexpected end of schema\");\r\n    }\r\n\n\nUse StringBuilder instead of StringBuffer as StringBuffer is synchronized (which is not required here).",
        "Issue Links": []
    },
    "PARQUET-1922": {
        "Key": "PARQUET-1922",
        "Summary": "Deprecate IOExceptionUtils",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "08/Oct/20 18:07",
        "Updated": "14/May/21 08:15",
        "Resolved": "14/May/21 08:15",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1923": {
        "Key": "PARQUET-1923",
        "Summary": "parquet-tools 1.11.0: TestSimpleRecordConverter fails with ExceptionInInitializerError on openjdk 15",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Alexander Bayandin",
        "Reporter": "Alexander Bayandin",
        "Created": "13/Oct/20 10:59",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "17/Oct/20 23:15",
        "Description": "mvn clean package -Plocal for\u00a0parquet-tools 1.11.1 fails with a failing test\u00a0testConverter(org.apache.parquet.tools.read.TestSimpleRecordConverter).\nmvn clean -Dtest=TestSimpleRecordConverter \"-Plocal\" test:\n\n\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------Test set: org.apache.parquet.tools.read.TestSimpleRecordConverter-------------------------------------------------------------------------------Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.338 sec <<< FAILURE!testConverter(org.apache.parquet.tools.read.TestSimpleRecordConverter)\u00a0 Time elapsed: 0.268 sec\u00a0 <<< ERROR!java.lang.ExceptionInInitializerError at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2823) at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2818) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2684) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:172) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:357) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295) at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58) at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:227) at org.apache.parquet.hadoop.ParquetWriter.<init>(ParquetWriter.java:192) at org.apache.parquet.tools.read.TestSimpleRecordConverter.createTestParquetFile(TestSimpleRecordConverter.java:114) at org.apache.parquet.tools.read.TestSimpleRecordConverter.setUp(TestSimpleRecordConverter.java:90) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110) at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175) at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:107) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end 3, length 2 at java.base/java.lang.String.checkBoundsBeginEnd(String.java:3734) at java.base/java.lang.String.substring(String.java:1903) at org.apache.hadoop.util.Shell.<clinit>(Shell.java:52) ... 44 more\n\nThe error looks similar to SPARK-26134, so I suppose a fix should be the same.\nNotes:\n\nI use parquet-tools 1.11.1 with https://github.com/apache/parquet-mr/commit/b6d07ae0744ba47aa9a8868ef2d7cbb232a60b22 patch\nI can't build current master at all: https://issues.apache.org/jira/browse/PARQUET-1896?focusedCommentId=17203146&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17203146",
        "Issue Links": []
    },
    "PARQUET-1924": {
        "Key": "PARQUET-1924",
        "Summary": "Do not Instantiate a New LongHashFunction",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "13/Oct/20 15:48",
        "Updated": "21/Oct/20 20:13",
        "Resolved": "21/Oct/20 20:13",
        "Description": "XxHash.java\n\r\n/**\r\n * The implementation of HashFunction interface. The XxHash uses XXH64 version xxHash\r\n * with a seed of 0.\r\n */\r\npublic class XxHash implements HashFunction {\r\n  @Override\r\n  public long hashBytes(byte[] input) {\r\n    return LongHashFunction.xx(0).hashBytes(input);\r\n  }\r\n\r\n  @Override\r\n  public long hashByteBuffer(ByteBuffer input) {\r\n    return LongHashFunction.xx(0).hashBytes(input);\r\n  }\r\n\n\nSince the seed is always zero, the static implementation provided by the library can be used here.",
        "Issue Links": []
    },
    "PARQUET-1925": {
        "Key": "PARQUET-1925",
        "Summary": "Introduce Velocity Template Engine to Parquet Generator",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "15/Oct/20 15:25",
        "Updated": "12/Nov/21 17:24",
        "Resolved": null,
        "Description": "Much easier than the current setup of manually outputting the strings.",
        "Issue Links": []
    },
    "PARQUET-1926": {
        "Key": "PARQUET-1926",
        "Summary": "Add LogicalType support to ThriftType.I64Type",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-thrift",
        "Assignee": "Joshua Martone",
        "Reporter": "Joshua Martone",
        "Created": "16/Oct/20 05:12",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "27/Jan/21 08:57",
        "Description": "Adds a LogicalTypeAnnotation to the I64Type.\nThis allows you to serialize timestamps and times.",
        "Issue Links": []
    },
    "PARQUET-1927": {
        "Key": "PARQUET-1927",
        "Summary": "ColumnIndex should provide number of records skipped",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "17/Oct/20 14:42",
        "Updated": "27/Jan/21 10:55",
        "Resolved": null,
        "Description": "When integrating Parquet ColumnIndex, I found we need to know from Parquet that how many records that we skipped due to ColumnIndex filtering. When rowCount is 0,\u00a0readNextFilteredRowGroup() just advance to next without telling the caller. See code here https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L969\n\u00a0\nIn Iceberg, it reads Parquet record with an iterator. The hasNext() has the following code():\nvaluesRead + skippedValues < totalValues\nSee (https://github.com/apache/iceberg/pull/1566/commits/cd70cac279d3f14ba61f0143f9988d4cc9413651#diff-d80c15b3e5376265436aeab8b79d5a92fb629c6b81f58ad10a11b9b9d3bfcffcR115).\u00a0\nSo without knowing the skipped values, it is hard to determine hasNext() or not.\u00a0\n\u00a0\nCurrently, we can workaround by using a flag. When\u00a0readNextFilteredRowGroup() returns null, we consider it is done for the whole file. Then hasNext() just retrun false.",
        "Issue Links": []
    },
    "PARQUET-1928": {
        "Key": "PARQUET-1928",
        "Summary": "Interpret Parquet INT96 type as FIXED[12] AVRO Schema",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-avro",
        "Assignee": "Anant Damle",
        "Reporter": "Anant Damle",
        "Created": "19/Oct/20 07:16",
        "Updated": "22/Apr/22 17:11",
        "Resolved": "07/Dec/20 09:48",
        "Description": "Reading Parquet files in Apache Beam using ParquetIO uses `AvroParquetReader` causing it to throw `IllegalArgumentException(\"INT96 not implemented and is deprecated\")`\nCustomers have large datasets which can't be reprocessed again to convert into a supported type. An easier approach would be to convert into a byte array of 12 bytes, that can then be interpreted by the developer in any way they want to interpret it.",
        "Issue Links": [
            "/jira/browse/PARQUET-323",
            "/jira/browse/BEAM-11527",
            "https://github.com/apache/parquet-mr/pull/831"
        ]
    },
    "PARQUET-1929": {
        "Key": "PARQUET-1929",
        "Summary": "Bump Snappy to 1.1.8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Oct/20 19:09",
        "Updated": "22/Oct/20 16:45",
        "Resolved": "22/Oct/20 16:45",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1930": {
        "Key": "PARQUET-1930",
        "Summary": "Bump Apache Thrift to 0.13.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Oct/20 19:39",
        "Updated": "07/Apr/21 08:40",
        "Resolved": "22/Oct/20 16:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1931": {
        "Key": "PARQUET-1931",
        "Summary": "Bump Junit 4.13.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Oct/20 19:59",
        "Updated": "22/Oct/20 16:44",
        "Resolved": "22/Oct/20 16:44",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1932": {
        "Key": "PARQUET-1932",
        "Summary": "Bump Fastutil to 8.4.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "21/Oct/20 20:28",
        "Updated": "22/Oct/20 17:53",
        "Resolved": "22/Oct/20 17:53",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1933": {
        "Key": "PARQUET-1933",
        "Summary": "[Format] Clarify encodings and data page guidance.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "22/Oct/20 03:55",
        "Updated": "30/Jan/21 21:38",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1934": {
        "Key": "PARQUET-1934",
        "Summary": "Dictionary page is not decrypted in predicate pushdown path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "22/Oct/20 06:33",
        "Updated": "01/Nov/20 06:06",
        "Resolved": "01/Nov/20 06:06",
        "Description": "Predicate pushdown, based on dictionary pages, uses a page parsing code that doesn't support decryption yet. Will add a few lines to decrypt the dictionary page header and page (for encrypted columns).",
        "Issue Links": []
    },
    "PARQUET-1935": {
        "Key": "PARQUET-1935",
        "Summary": "[C++][Parquet] nullptr access violation when writing arrays of non-nullable values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Tanguy Fautre",
        "Created": "23/Oct/20 11:31",
        "Updated": "10/Nov/20 15:36",
        "Resolved": "10/Nov/20 15:36",
        "Description": "I'm updating ParquetSharp to build against Arrow 2.0.0 (currently using Arrow 1.0.1). One of our unit test is now throwing a nullptr access violation.\nI have narrowed it down to writing arrays of non-nullable values (in this case the column contains int[]) . If the values are nullable, the test passes.\nThe parquet file schema is as following:\n\nGroupNode(\"schema\", LogicalType.None, Repetition.Required)\n\nGroupNode(\"array_of_ints_column\", LogicalType.List, Repetition.Optional)\n\nGroupNode(\"list\", LogicalType.None, Repetition.Repeated)\n\nPrimitiveNode(\"item\", LogicalType.Int(32, signed), Repetition.Required)\n\n\n\n\n\n\n\nThe test crashes when calling TypedColumnWriter::WriteBatchSpaced with the following  arguments:\n\nnum_values = 1\ndef_levels = {0}\nrep_levels = {0}\nvalid_bits = {0}\nvalid_bit_offset = 0\nvalues = {} (i.e. nullptr)\n\nThis call is effectively trying to write a null array, and therefore (to my understanding) does not need to pass any values. Yet further down the callstack, the implementation tries to read one value out of values (which is nullptr).\nI believe the problem lies with\n\n\r\n  void MaybeCalculateValidityBits(\r\n    const int16_t* def_levels,\r\n    int64_t batch_size,\r\n    int64_t* out_values_to_write,\r\n    int64_t* out_spaced_values_to_write,\r\n    int64_t* null_count) {\r\n    if (bits_buffer_ == nullptr) {\r\n      if (!level_info_.HasNullableValues()) {\r\n        *out_values_to_write = batch_size;\r\n        *out_spaced_values_to_write = batch_size;\r\n        *null_count = 0;\r\n      } else {\r\n        for (int x = 0; x < batch_size; x++) {\r\n          *out_values_to_write += def_levels[x] == level_info_.def_level ? 1 : 0;\r\n          *out_spaced_values_to_write +=\r\n              def_levels[x] >= level_info_.repeated_ancestor_def_level ? 1 : 0;\r\n        }\r\n        *null_count = *out_values_to_write - *out_spaced_values_to_write;\r\n      }\r\n      return;\r\n    }\r\n\r\n    // ...\r\n  }\r\n\n\nIn particular, level_info_.HasNullableValues() returns false given that the arrays cannot contain null-values. My understanding is that this is wrong, since the arrays themselves are nullable.\nThis code appears to have been introduced by ARROW-9603.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/8516"
        ]
    },
    "PARQUET-1936": {
        "Key": "PARQUET-1936",
        "Summary": "WriteBatchSpaced writes incorrect value for parquet when input contains NULL list",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Ruta Dhaneshwar",
        "Created": "31/Oct/20 00:54",
        "Updated": "16/Nov/20 04:04",
        "Resolved": null,
        "Description": "When trying to write a column of parquet lists, if there is a NULL list, WriteBatchSpaced will either throw an error (case 1 below) or incorrectly write the last value in the last list as the first value from the first list (case 2 below).\n\nCASE 1\n Data (3 lists):\n [\n \u00a0 \u00a0\"one\"\n ]\n null\n [\n \u00a0 \u00a0\"two\"\n ]\n \u00a0\n Parameters to TypedColumnWriter<PhysicalType<parquet::Type::BYTE_ARRAY>>::WriteBatchSpaced:\n\nnum_values: 3\ndef_levels: [3, 0, 3]\nrep_levels: [0, 0, 0]\nvalid_bits: 0x05 (bit representation 101)\nvalid_bits_offset: 0\nvalues: [\"one\", nullptr, \"two\"]\n\nWhen I use WriteBatchSpaced(num_values, def_levels, rep_levels, valid_bits, valid_bits_offset, values), I get an error when running parquet-tools on the outputted parquet file:\n\n\nAdditionally, if I add another list into the data that I write, then the last element of that additional list is incorrectly written as the first element of the first list. See below.\n \u00a0\nCASE 2\n Data (4 lists):\n [\n \u00a0 \u00a0\"one\"\n ]\n null\n [\n \u00a0 \u00a0\"two\"\n ]\n [\n \u00a0 \u00a0\"three\",\n \u00a0 \u00a0\"four\"\n ]\n \u00a0\n Parameters to TypedColumnWriter<PhysicalType<parquet::Type::BYTE_ARRAY>>::WriteBatchSpaced:\n\nnum_values: 5\ndef_levels: [3, 0, 3, 3, 3]\nrep_levels: [0, 0, 0, 0, 1]\nvalid_bits: 0x29 (bit representation 11101)\nvalid_bits_offset: 0\nvalues: [\"one\", nullptr, \"two\", \"three\", \"four\"]\n\nOutputted Parquet File:\u00a0\n\n\n \u00a0\n Here we see that the \"four\" in the last list actually shows up as \"one\".",
        "Issue Links": []
    },
    "PARQUET-1937": {
        "Key": "PARQUET-1937",
        "Summary": "parquet-tools class-not-found failures are silent and therefore not helpful",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "john lilley",
        "Created": "31/Oct/20 12:36",
        "Updated": "31/Oct/20 12:40",
        "Resolved": null,
        "Description": "If you download a parquet-tools jar from e.g. maven central and simply run it without the \"hadoop\" command prefix:\n[jlilley@rpb-dev-cent7-1 ~]$ wget https://repo1.maven.org/maven2/org/apache/parquet/parquet-tools/1.11.1/parquet-tools-1.11.1.jar\n...\n[jlilley@rpb-dev-cent7-1 ~]$ java -jar parquet-tools-1.11.1.jar --help\n[jlilley@rpb-dev-cent7-1 ~]$\nit will fail because the hadoop dependencies are missing, and will fail silently because System.out and System.err have been replaced with VoidStream:\nSystem.setOut(VoidStream);\nSystem.setErr(VoidStream);\nBecause of this, the missing-class exceptions are not reported and user is left scratching head and wondering what happened.\u00a0 It is unclear why the message goes unreported, because the exception\u00a0should be printed to Main.err by die()\ncatch (Throwable th) {\n\u00a0 \u00a0if (debug) th.printStackTrace(Main.err);\n\u00a0 \u00a0die(th, false, name, command);\n }\nHowever, commenting out the System.setOut() and System.setErr() lines does indeed cause the stack trace to be shown.",
        "Issue Links": []
    },
    "PARQUET-1938": {
        "Key": "PARQUET-1938",
        "Summary": "Option to get KMS details from key material (in key rotation)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Maya Anderson",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/Nov/20 06:13",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": "Currently, key rotation uses explicit parameters to get the KMS details. Instead, it can extract these details from the key material files - this is more convenient for a user. Still, the explicit parameters (if provided) will override these values",
        "Issue Links": []
    },
    "PARQUET-1939": {
        "Key": "PARQUET-1939",
        "Summary": "Fix RemoteKmsClient API ambiguity",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/Nov/20 06:25",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": "Users complain that RemoteKmsClient name can be confusing, since this class covers both local and in-server (remote) key wrapping. Still, this class supports only remote KMS servers. But to remove any ambiguity, and to make the API simpler, we will rename this class to LocalWrapKmsClient; it will be used only in rare situations where in-server wrapping in not supported. In all other situations, the basic KmsClient interface will be used directly.",
        "Issue Links": []
    },
    "PARQUET-1940": {
        "Key": "PARQUET-1940",
        "Summary": "Make KeyEncryptionKey length configurable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "01/Nov/20 06:29",
        "Updated": "27/Jan/21 14:36",
        "Resolved": "27/Jan/21 14:36",
        "Description": "KEK length is hardcoded to 128 bits. It should be configurable, to any value allowed by AES (128, 192 or 256 bits).",
        "Issue Links": []
    },
    "PARQUET-1941": {
        "Key": "PARQUET-1941",
        "Summary": "Bump Commons CLI from 1.3.1 to 1.4",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "01/Nov/20 20:52",
        "Updated": "02/Dec/20 17:17",
        "Resolved": "02/Dec/20 17:17",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1942": {
        "Key": "PARQUET-1942",
        "Summary": "Bump Apache Arrow 2.0.0",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "01/Nov/20 21:13",
        "Updated": "14/Apr/21 12:58",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1943": {
        "Key": "PARQUET-1943",
        "Summary": "parquet-tools cat command does not consider logical type",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "john lilley",
        "Created": "02/Nov/20 14:39",
        "Updated": "02/Nov/20 14:39",
        "Resolved": null,
        "Description": "The parquet-tools cat command doesn't format timestamps, outputting them as numbers instead.\u00a0 The parquet-tools dump command does format as timestamps, so the code has been written.\nUsing the attached file:\n$ java jar parquet-tools<VERSION>.jar cat -j datetime_utc_adjust.parquet\n{\"ID\":1,\"DT\":1604297946277}\n\u00a0\n$ java jar parquet-tools<VERSION>.jar dump datetime_utc_adjust.parquet\n...\nINT32 ID\n--------------------------------------------------------------------------------\n\n\n\n\n\nrow group 1 of 1, values 1 to 1 ***\nvalue 1: R:0 D:1 V:1INT64 DT\n--------------------------------------------------------------------------------\nrow group 1 of 1, values 1 to 1 ***\nvalue 1: R:0 D:1 V:2020-11-02T06:19:06.277",
        "Issue Links": []
    },
    "PARQUET-1944": {
        "Key": "PARQUET-1944",
        "Summary": "Unable to download transitive dependency hadoop-lzo",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "11/Nov/20 09:45",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "12/Nov/20 08:32",
        "Description": "Seems that the solution for PARQUET-1691 is not complete. We have to exclude `hadoop-lzo` from the transitive dependencies of `elephant-bird-pig` as well. Not sure why we did not recognize this issue before.",
        "Issue Links": []
    },
    "PARQUET-1945": {
        "Key": "PARQUET-1945",
        "Summary": "Add an option to allow auto conversion from empty fields to NULL",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Zheng Shao",
        "Created": "11/Nov/20 18:58",
        "Updated": "04/Apr/21 09:46",
        "Resolved": null,
        "Description": "Right now, Parquet Writer throws out an exception:\nParquet record is malformed: empty fields are illegal, the field should be ommited completely instead\nwhen an empty field (array or struct or map I guess?) is written.\nThe suggestion here is to add an option \"auto_convert_empty_fields_to_null\" that convert empty fields to null automatically on write.\nThe LOC to change is here:\nif (emptyField) {\n{{ \u00a0 \u00a0throw new ParquetEncodingException(\"empty fields are illegal, the field should be ommited completely instead\");}}\n}",
        "Issue Links": []
    },
    "PARQUET-1946": {
        "Key": "PARQUET-1946",
        "Summary": "Parquet File not readable by Google big query (works with Spark)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Richard Grossman",
        "Created": "23/Nov/20 14:11",
        "Updated": "07/Dec/20 08:43",
        "Resolved": null,
        "Description": "Hi\nI'm trying to write Avro message to parquet on GCS. These parquet should be query by big query engine who support now parquet.\nTo do this I'm using Secor a kafka log persister tools from pinterest.\nFirst I didn't notice any problem using Spark the same file can be read without any problem all is working perfect.\nNow using Big query bring and error like this :\nError while reading table: , error message: Read less values than expected: Actual: 29333, Expected: 33827. Row group: 0, Column: , File:\nAfter investigation using parquet-tools I figured out that in parquet there is metadata regarding number total of unique values for each columns eg from parquet-tools\npage 0: DLE:BIT_PACKED RLE:BIT_PACKED [more]... CRC:[PAGE CORRUPT] VC:547\nSo the VC value indicate that the total number of unique value in the file is 547.\nNow when make a spark SQL like SELECT DISTINCT COUNT(column) FROM ... I get 421 mean this number in the metadata is incorrect.\nSo what is not a problem for Spark to read is a blocking problem for Big data because it relies on these values and found it incorrect.\nIs there any configuration of the writer\u00a0that can prevent these errors in the metadata ? Or is it a normal behavior that should be a problem ?\nThanks",
        "Issue Links": []
    },
    "PARQUET-1947": {
        "Key": "PARQUET-1947",
        "Summary": "DeprecatedParquetInputFormat in CombineFileInputFormat would produce wrong data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cascading",
        "Assignee": "Daniel Dai",
        "Reporter": "Daniel Dai",
        "Created": "30/Nov/20 18:43",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "07/Dec/20 11:08",
        "Description": "When we read parquet file using cascading 2, we observe wrong data in the file boundary when we turn on input combine in cascading (setUseCombinedInput to true).\nThis can be reproduced easily with two parquet input files, each containing one record. A simple cascading application (attached) read the two input with setUseCombinedInput(true). What we get is the duplicated record in the first input file and the missing record in the second input file.\nHere is the call sequence to understand what happen after the last record of first input:\n1. cascading invokes DeprecatedParquetInputFormat.createValue(), that's the last record of first input again\n2. CombineFileRecordReader invokes RecordReader.next and reach the EOF of first input\n3. CombineFileRecordReader creates a new DeprecatedParquetInputFormat.RecordReaderWrapper, which creates the new \"value\" variable containing the first record of second input\n4. CombineFileRecordReader invokes RecordReader.next on the new RecordReaderWrapper, but since firstRecord flag is on, next does not do anything\n5. Thus the \"value\" variable containing the first record of second input is lost, and cascading is reusing the last record of first input",
        "Issue Links": []
    },
    "PARQUET-1948": {
        "Key": "PARQUET-1948",
        "Summary": "TransCompressionCommand Inoperable",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Shelby Vanhooser",
        "Created": "03/Dec/20 17:32",
        "Updated": "19/Feb/21 02:43",
        "Resolved": null,
        "Description": "TransCompressionCommand in parquet-tools is intended to allow translation of compression types in parquet files.\u00a0 We are intending to use this functionality to debug a corrupted file, but this command fails to run at the moment entirely.\u00a0\nRunning the following command (on the uncorrupted file):\n\n\r\njava -jar ./parquet-tools-1.11.1.jar trans-compression ~/Downloads/part-00048-69f65188-94b5-4772-8906-5c78989240b5_00048.c000.snappy.parquet\n\nThis results in\u00a0\n\u00a0\n\n\r\nUnknown command: trans-compression\n\n\u00a0\nI believe this is due to the Registry class silently catching any errors to initialize\u00a0which subsequently is misinterpreted as an unknown command.\nWe need to:\u00a0\n\nWrite a test for the TransCompressionCommand to figure out why it's showing up as unknown command\nProbably expand these tests to cover all the other commands\n\n\u00a0\nThis will then unblock our debugging work on the suspect file.",
        "Issue Links": []
    },
    "PARQUET-1949": {
        "Key": "PARQUET-1949",
        "Summary": "Mark Parquet-1872 with not support bloom filter yet",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "04/Dec/20 22:36",
        "Updated": "15/Jan/21 10:05",
        "Resolved": "15/Jan/21 10:05",
        "Description": "To unblock the release of 1.12.0, we need to add comments in the trans-compression command to indicated 'not support bloom filter yet'",
        "Issue Links": [
            "/jira/browse/PARQUET-1875",
            "/jira/browse/PARQUET-1872"
        ]
    },
    "PARQUET-1950": {
        "Key": "PARQUET-1950",
        "Summary": "Define core features / compliance level",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Dec/20 14:32",
        "Updated": "3 days ago 20:32",
        "Resolved": null,
        "Description": "Parquet format is getting more and more features while the different implementations cannot keep the pace and left behind with some features implemented and some are not. In many cases it is also not clear if the related feature is mature enough to be used widely or more an experimental one.\nThese are huge issues that makes hard ensure interoperability between the different implementations.\nThe following idea came up in a discussion. Create a now document in the parquet-format repository that lists the \"core features\". This document is versioned by the parquet-format releases. This way a certain version of \"core features\" defines a level of compatibility between the different implementations. This version number can be written to a new field (e.g. complianceLevel) in the footer. If an implementation writes a file with a version in the field it must implement all the related \"core features\" (read and write) and must not use any other features at write because it makes the data unreadable by another implementation if only the same level of \"core features\" are implemented.\nFor example if we have encoding A listed in the version 1 \"core features\" but encoding B is not then at \"complianceLevel = 1\" we can use encoding A but we cannot use encoding B because it would make the related data unreadable.",
        "Issue Links": []
    },
    "PARQUET-1951": {
        "Key": "PARQUET-1951",
        "Summary": "Allow different strategies to combine key values when merging parquet files",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "satish",
        "Reporter": "satish",
        "Created": "14/Dec/20 02:39",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "07/Jan/21 09:46",
        "Description": "I work on Apache Hudi project. We store some additional metadata in parquet files (key range in the file, for example).  So the metadata is different in different parquet files that we want to merge these files. \nHere is what I'm thinking:\n1) Merge command takes additional command line option: --strategy <StrategyClassName>. \n2) We introduce new strategy class in parquet-hadoop to keep the same behavior as today.  \nWe can extend that class and provide our custom implementation.",
        "Issue Links": []
    },
    "PARQUET-1952": {
        "Key": "PARQUET-1952",
        "Summary": "Upgrade Avro to 1.10.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "14/Dec/20 13:39",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "14/Dec/20 14:36",
        "Description": "Avro 1.10.1 release notes:\nhttps://issues.apache.org/jira/issues/?jql=project%20%3D%20AVRO%20AND%20%20fixVersion%20%3D%201.10.1%20and%20status%20%3D%20Resolved%20%20%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC",
        "Issue Links": []
    },
    "PARQUET-1953": {
        "Key": "PARQUET-1953",
        "Summary": "hadoop-common is not an optional dependency",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sam Halliday",
        "Created": "15/Dec/20 12:27",
        "Updated": "15/Dec/20 18:29",
        "Resolved": "15/Dec/20 18:29",
        "Description": "parquet-hadoop provides the only mechanism to load .parquet files and has an optional (provided) dependency on hadoop-common, implying that it is possible to use parquet-hadoop without using hadoop. However, it is required.\nThe following code is needed to instantiate a ParquetFileReader\n\n\r\nfinal class LocalInputFile(file: File) extends InputFile {\r\n  def getLength() = file.length()\r\n  def newStream(): SeekableInputStream = {\r\n    val input = new FileInputStream(file)\r\n    new DelegatingSeekableInputStream(input) {\r\n      def getPos(): Long = input.getChannel().position()\r\n      def seek(bs: Long): Unit = {\r\n        val _ = input.getChannel().position(bs)\r\n      }\r\n    }\r\n  }\r\n}\r\n\n\nbut using this leads to a runtime exception because hadoop is missing transitive dependency on org.apache.hadoop.fs.PathFilter which then depends on org.apache.hadoop.fs.Path, both in hadoop-common.\nRequiring downstream users to depend on hadoop-common is an extremely large dependency and I would rather that this was not the case.\nA search for \"import org.apache.hadoop\" in src/main reveals a few more places where the dependency is hardwired, although often in deprecated static constructors and therefore benign.",
        "Issue Links": [
            "/jira/browse/PARQUET-1126"
        ]
    },
    "PARQUET-1954": {
        "Key": "PARQUET-1954",
        "Summary": "TCP connection leak in parquet dump",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "xiepengjie",
        "Reporter": "xiepengjie",
        "Created": "15/Dec/20 12:56",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "05/Jan/21 10:52",
        "Description": "Hi,\nWhen i'm trying to dump a parquet file, i find the TCP connection leak in\u00a0org.apache.parquet.tools.command.DumpCommand#dump :\n\u00a0\n\n\r\n...\r\nParquetFileReader freader = null;\r\nif (showmd) {\r\n    try {\r\n        long group = 0;\r\n        for (BlockMetaData block : blocks) {\r\n            ...\r\n            freader = new ParquetFileReader(\r\n                conf, meta.getFileMetaData(), inpath, rblocks, columns);\r\n            ...\r\n            out.flushColumns();\r\n        }\r\n    } finally {\r\n        if (freader != null) {\r\n            freader.close();\r\n        }\r\n    }\r\n}",
        "Issue Links": []
    },
    "PARQUET-1955": {
        "Key": "PARQUET-1955",
        "Summary": "ParquetThriftOutputFormat does not store any optional primitive values in parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Daniel Morales",
        "Created": "15/Dec/20 17:34",
        "Updated": "15/Dec/20 17:34",
        "Resolved": null,
        "Description": "As per title, I created a simple record with thrift IDL and used the\u00a0ParquetThriftOutputFormat.\u00a0\nThe library would not write any of the optional primitive even if set, it would write only if they are specified as required.\u00a0\n\u00a0\nparquet-mr: 1.11.1\nthrift 0.13.0",
        "Issue Links": []
    },
    "PARQUET-1956": {
        "Key": "PARQUET-1956",
        "Summary": "\"parquet-tools cat -j\" should not output metadata",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Dumitru Husleag",
        "Created": "16/Dec/20 17:33",
        "Updated": "16/Dec/20 17:36",
        "Resolved": null,
        "Description": "Parquet-tools cat -help says:\n\nparquet-tools cat:\r\nPrints the content of a Parquet file. The output contains only the data, no\r\nmetadata is displayed\r\n\n\nLet's say I have a structure like this in a Parquet file:\n\n\r\n{\r\n   \"account\": \r\n   [\r\n\t\t{\r\n\t\t\t\"accountDepositType\": \"NA\",\r\n\t\t\t\"accountNumber\": \"NA\",\r\n\t\t\t\"accountHolderType\": \"NA\"\r\n\t\t}\r\n\t]\r\n}\r\n\n\nit is extracted like this\n\n\r\n{\r\n   \"account\": {\r\n\t\t\"list\": [\r\n\t\t\t{\r\n\t\t\t\t\"element\": {\r\n\t\t\t\t\t\"accountDepositType\": \"NA\",\r\n\t\t\t\t\t\"accountNumber\": \"NA\",\r\n\t\t\t\t\t\"accountHolderType\": \"NA\"\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n}\r\n\n\n\"list\" and \"element\" is metadata or type information if you like and it should not appear in output.\nCan you please fix that ?",
        "Issue Links": []
    },
    "PARQUET-1957": {
        "Key": "PARQUET-1957",
        "Summary": "[c++] PARQUET_MINIMAL_DEPENDENCY incompatible with ARROW_DEPENDENCY_SOURCE=BUNDLED and parallel build",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Iurii Borisov",
        "Created": "19/Dec/20 08:26",
        "Updated": "19/Dec/20 08:26",
        "Resolved": null,
        "Description": "Trying to build with the following flags being set:\n\nARROW_DEPENDENCY_SOURCE=BUNDLED\nARROW_DEPENDENCY_USE_SHARED=OFF\nARROW_PARQUET=ON\nPARQUET_MINIMAL_DEPENDENCY=ON\n\nin parallel mode (-j4).\u00a0\nBuild failed with:\narrow/cpp/src/parquet/thrift_internal.h:36:10: fatal error: thrift/TApplicationException.h: No such file or directory \n \u00a0\u00a036 | #include <thrift/TApplicationException.h>\nParquet target does not have a dependency on\u00a0thrift::thrift, so it tries to build thrift and parquet in parallel.\nProbably source included: cpp/src/parquet/CMakeLists.txt:234",
        "Issue Links": []
    },
    "PARQUET-1958": {
        "Key": "PARQUET-1958",
        "Summary": "Forced UTF8 encoding of BYTE_ARRAY on stream::read/write",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-1.5.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "ian",
        "Created": "23/Dec/20 15:54",
        "Updated": "24/Feb/21 10:02",
        "Resolved": null,
        "Description": "StreamReader& StreamReader::operator>>(optional<std::string>& v) {\r\n CheckColumn(Type::BYTE_ARRAY, ConvertedType::UTF8);\r\n ByteArray ba;\n\n\u00a0\n\n\r\nStreamWriter& StreamWriter::WriteVariableLength(const char* data_ptr,\r\n std::size_t data_len) {\r\n CheckColumn(Type::BYTE_ARRAY, ConvertedType::UTF8);\n\n\u00a0\nThough the C++ Parquet::Schema::Node allows physical type of BYTE_ARRAY with ConvertedType=NONE, the stream reader/writer classes throw when ConvertedType != UTF8.\nstd::string is, unfortunately, the canonical byte buffer class in C++.\nA simple approach might be to create >>parquet::ByteArray.. with columnCheck(BYTE_ARRAY, NONE), and let the user take it from there.\u00a0 that would use the existing methods that >>std::string uses.. just an idea.\nI am new to this forum, and have assigned MAJOR to this bug, but gladly defer to those who have a better grasp of classification.",
        "Issue Links": []
    },
    "PARQUET-1959": {
        "Key": "PARQUET-1959",
        "Summary": "[Rust] LogicalType::TIMESTAMP_NANOS missing?",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ivan Smirnov",
        "Created": "08/Jan/21 12:58",
        "Updated": "08/Jan/21 13:03",
        "Resolved": "08/Jan/21 13:03",
        "Description": "There's UnitTime::NANOS in parquet-format, but no nanosecond timestamp support (seemingly) in schema's LogicalType. What is needed to add support for nanosecond timestamps in Rust Parquet?",
        "Issue Links": []
    },
    "PARQUET-1960": {
        "Key": "PARQUET-1960",
        "Summary": "Parquet tools build error in the latest released version `apache-parquet-1.11.1`",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jacinta Gichuhi",
        "Created": "11/Jan/21 07:26",
        "Updated": "14/Jan/21 10:38",
        "Resolved": null,
        "Description": "After following the instruction [here](https://github.com/apache/parquet-mr/tree/master/parquet-tools)\u00a0fro running parquet tools project am getting the following error\n\u00a0\n`[ERROR] /Users/jecihjoy/Downloads/parquet-mr-apache-parquet-1.11.1/parquet-tools/src/main/java/org/apache/parquet/tools/read/SimpleRecord.java:[29,38] package com.fasterxml.jackson.databind does not exist`\n\u00a0\n[HERE](https://pastebin.com/HUkU3Cgs)\u00a0is the full error log.\nThe same error also occurs in all versions greater than 1.9.0\nPlease help.",
        "Issue Links": []
    },
    "PARQUET-1961": {
        "Key": "PARQUET-1961",
        "Summary": "Bump Jackson to 2.11.4",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Jan/21 10:01",
        "Updated": "08/Feb/21 10:06",
        "Resolved": "13/Jan/21 12:50",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1894"
        ]
    },
    "PARQUET-1962": {
        "Key": "PARQUET-1962",
        "Summary": "Filtering dictionary data types using dplyr in Arrow package",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Soumya Ranjan Prusty",
        "Created": "18/Jan/21 08:11",
        "Updated": "18/Jan/21 08:11",
        "Resolved": null,
        "Description": "Is there any way where we can filter the dictionary data types using dplyr. Below is the example for illustration\u00a0\ntest %>% dplyr::filter(cols %in% c(\"A\",\"B\",\"C\") %>collect(). It gives me following error\nError in dataset__Scanner_ToTable(self) : \u00a0\u00a0NotImplemented: Function is_in has no kernel matching input types (array[dictionary<values=string, indices=int32, ordered=0>])\nHere cols is dictionary type value",
        "Issue Links": []
    },
    "PARQUET-1963": {
        "Key": "PARQUET-1963",
        "Summary": "DeprecatedParquetInputFormat in CombineFileInputFormat throw NPE when the first sub-split is empty",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Daniel Dai",
        "Reporter": "Daniel Dai",
        "Created": "19/Jan/21 00:10",
        "Updated": "27/Jan/21 14:33",
        "Resolved": "20/Jan/21 16:02",
        "Description": "A followup of PARQUET-1947, after the fix, when the first sub-split is empty in CombineFileInputFormat, there's a NPE:\n\n\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.parquet.hadoop.mapred.DeprecatedParquetInputFormat$RecordReaderWrapper.next(DeprecatedParquetInputFormat.java:154)\r\n\tat org.apache.parquet.hadoop.mapred.DeprecatedParquetInputFormat$RecordReaderWrapper.next(DeprecatedParquetInputFormat.java:73)\r\n\tat cascading.tap.hadoop.io.CombineFileRecordReaderWrapper.next(CombineFileRecordReaderWrapper.java:70)\r\n\tat org.apache.hadoop.mapred.lib.CombineFileRecordReader.next(CombineFileRecordReader.java:58)\r\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)\r\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)\r\n\tat cascading.tap.hadoop.util.MeasuredRecordReader.next(MeasuredRecordReader.java:61)\r\n\tat org.apache.parquet.cascading.ParquetTupleScheme.source(ParquetTupleScheme.java:160)\r\n\tat cascading.tuple.TupleEntrySchemeIterator.getNext(TupleEntrySchemeIterator.java:163)\r\n\tat cascading.tuple.TupleEntrySchemeIterator.hasNext(TupleEntrySchemeIterator.java:136)\r\n\t... 10 more\r\n\n\nThe reason is CombineFileInputFormat will use the result of createValue of the first sub-split as the value container. Since the first sub-split is empty, the value container is null.",
        "Issue Links": []
    },
    "PARQUET-1964": {
        "Key": "PARQUET-1964",
        "Summary": "Properly handle missing/null filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Yuming Wang",
        "Created": "19/Jan/21 05:00",
        "Updated": "03/Feb/21 08:32",
        "Resolved": "21/Jan/21 08:46",
        "Description": "How to reproduce this issue:\n\n\r\nval hadoopInputFile = HadoopInputFile.fromPath(new Path(\"/path/to/parquet/000.snappy.parquet\"), new Configuration())\r\nval reader = ParquetFileReader.open(hadoopInputFile)\r\nval recordCount = reader.getFilteredRecordCount\r\nreader.close()\r\n\n\nOutput:\n\njava.lang.NullPointerException was thrown.\r\njava.lang.NullPointerException\r\n\tat org.apache.parquet.internal.filter2.columnindex.ColumnIndexFilter.calculateRowRanges(ColumnIndexFilter.java:81)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:961)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:766)\r\n\n\nUPDATE: This is not only about the potential NPE if a null filter is set but to handle the missing/null filter in a better performing way. (Currently a NOOP filter implementation is used by default if no filter is set which requires to load the related data for column index/bloom filter even if no actual filtering will occur.)",
        "Issue Links": []
    },
    "PARQUET-1965": {
        "Key": "PARQUET-1965",
        "Summary": "ParquetRecordReader support building ParquetFileReader using incoming Footer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yang Jie",
        "Created": "27/Jan/21 06:01",
        "Updated": "27/Jan/21 06:01",
        "Resolved": null,
        "Description": "SPARK-33449 discusses adding file meta cache mechanism for Spark SQL to reduce data reading and speed up query.\nFor the scenarios that need to use ParquetRecordReader, we hope to add an interface to support passing an existing footer to ParquetRecordReader and use this instance to build ParquetFileReader.",
        "Issue Links": []
    },
    "PARQUET-1966": {
        "Key": "PARQUET-1966",
        "Summary": "Fix build with JDK11 for JDK8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "29/Jan/21 14:07",
        "Updated": "15/Feb/21 09:47",
        "Resolved": "15/Feb/21 09:47",
        "Description": "However the target is set to 1.8 it seems to be not enough as of building with JDK11 it fails at runtime with the following exception: \n\n\r\nava.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer;\r\n        at org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:197)\r\n        at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridEncoder.writeOrAppendBitPackedRun(RunLengthBitPackingHybridEncoder.java:193)\r\n        at org.apache.parquet.column.values.rle.RunLengthBitPackingHybridEncoder.writeInt(RunLengthBitPackingHybridEncoder.java:179)\r\n        at org.apache.parquet.column.values.dictionary.DictionaryValuesWriter.getBytes(DictionaryValuesWriter.java:167)\r\n        at org.apache.parquet.column.values.fallback.FallbackValuesWriter.getBytes(FallbackValuesWriter.java:74)\r\n        at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:60)\r\n        at org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:387)\r\n        at org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\r\n        at org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\r\n        at org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\r\n        at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endMessage(MessageColumnIO.java:307)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:465)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:148)\r\n        at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)\r\n        at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:138)\r\n\n\nTo reproduce execute the following.\n\n\r\nexport JAVA_HOME={the path to the JDK11 home}\r\nmvn clean install -Djvm={the path to the JRE8 java executable}",
        "Issue Links": [
            "/jira/browse/PARQUET-1898"
        ]
    },
    "PARQUET-1967": {
        "Key": "PARQUET-1967",
        "Summary": "Upgrade Zstd-jni to 1.4.8-3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-cli,                                            parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "30/Jan/21 04:43",
        "Updated": "05/Feb/21 10:34",
        "Resolved": "04/Feb/21 17:55",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1968": {
        "Key": "PARQUET-1968",
        "Summary": "FilterApi support In predicate",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Huaxin Gao",
        "Reporter": "Yuming Wang",
        "Created": "01/Feb/21 14:22",
        "Updated": "30/Sep/21 14:24",
        "Resolved": "30/Sep/21 07:35",
        "Description": "FilterApi should support native In predicate.\nSpark:\nhttps://github.com/apache/spark/blob/d6a68e0b67ff7de58073c176dd097070e88ac831/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala#L600-L605\nImpala:\nhttps://issues.apache.org/jira/browse/IMPALA-3654",
        "Issue Links": []
    },
    "PARQUET-1969": {
        "Key": "PARQUET-1969",
        "Summary": "Migrate CI to Github Actions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0,                                            format-2.9.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Yuming Wang",
        "Created": "01/Feb/21 15:22",
        "Updated": "07/Apr/21 08:41",
        "Resolved": "09/Feb/21 14:49",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1970": {
        "Key": "PARQUET-1970",
        "Summary": "Make minor releases source compatible",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "02/Feb/21 15:33",
        "Updated": "10/Feb/21 10:01",
        "Resolved": "10/Feb/21 10:01",
        "Description": "Currently the compatibility checker (japicmp-maven-plugin) is configured to allow source incompatible changes for minor releases. It does not seem to be a great burden not allow such changes.\nMeanwhile a potential bug is discovered in the tool. Let's wait for this issue to be clarified because if it is not a bug then changes in parquet-mr will be required.",
        "Issue Links": [
            "/jira/browse/PARQUET-1898"
        ]
    },
    "PARQUET-1971": {
        "Key": "PARQUET-1971",
        "Summary": "Flaky test in github action",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "03/Feb/21 08:48",
        "Updated": "05/Feb/21 10:31",
        "Resolved": "04/Feb/21 12:10",
        "Description": "It seems we have a flaky test in the newly configured github action. It is strange that this test was not flaky in Travis. Anyway:\n\n\r\ntestMemoryManagerUpperLimit(org.apache.parquet.hadoop.TestMemoryManager): Pool size should be within 10% of the expected value (expected = 453745044 actual = 505046624)",
        "Issue Links": [
            "/jira/browse/PARQUET-1898"
        ]
    },
    "PARQUET-1972": {
        "Key": "PARQUET-1972",
        "Summary": "[C++] Switch to format version 2 as default for writing Parquet",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Joris Van den Bossche",
        "Created": "03/Feb/21 10:08",
        "Updated": "03/Feb/21 10:08",
        "Resolved": null,
        "Description": "Related to the thread on the arrow dev mailing list: https://lists.apache.org/thread.html/rf1a377c66990ae5ac0693119d416c93a7e19228d3eaaea8bd90acb17%40%3Cdev.arrow.apache.org%3E\nCurrently, when writing parquet files with Arrow (parquet-cpp), we default to parquet format \"1.0\". In practice, this means that we don't use certain LogicalTypes (eg we don't write integers other than int32/int64, and we don't write the nanosecond timestamps).\nI think it would be nice to enable nanosecond timestamps by default, but I also have no idea how widely this is already supported by other readers.\nTo be clear, this is not about enabling data page version 2 by default, in Arrow that is governed by a separate option.\nWhile checking this, I made an overview of which types were introduced in\nwhich parquet format version, in case someone wants to see the details ->\nhttps://nbviewer.jupyter.org/gist/jorisvandenbossche/3cc9942eaffb53564df65395e5656702",
        "Issue Links": []
    },
    "PARQUET-1973": {
        "Key": "PARQUET-1973",
        "Summary": "Support ZSTD JNI BufferPool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "04/Feb/21 19:22",
        "Updated": "22/Apr/21 15:49",
        "Resolved": "11/Mar/21 16:06",
        "Description": null,
        "Issue Links": [
            "/jira/browse/SPARK-34651"
        ]
    },
    "PARQUET-1974": {
        "Key": "PARQUET-1974",
        "Summary": "LZ4 decoding is not working over hadoop",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "mario luzi",
        "Created": "05/Feb/21 07:16",
        "Updated": "27/Sep/22 08:49",
        "Resolved": null,
        "Description": "Hello , we just tried latest apache-arrow version 3.0.0 and the write example included in low level api example, but lz4 still seems not compatible with Hadoop . we got this error reading over hadoop file parquet produced with 3.0.0 library\u00a0 :\n\u00a0[leal@sulu parquet]$ ./hadoop-3.2.2/bin/hadoop jar apache-parquet-1.11.1/parquet-tools/target/parquet-tools-1.11.1.jar head --debug parquet_2_0_example2.parquet\n2021-02-04 21:24:36,354 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1500001 records.\n2021-02-04 21:24:36,355 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block\n2021-02-04 21:24:36,397 INFO compress.CodecPool: Got brand-new decompressor\u00a0[.lz4]\n2021-02-04 21:24:36,410 INFO hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 434436\norg.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file\u00a0file:/home/leal/parquet/parquet_2_0_example2.parquet\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:255)\nat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\nat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\nat org.apache.parquet.tools.command.HeadCommand.execute(HeadCommand.java:87)\nat org.apache.parquet.tools.Main.main(Main.java:223)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\nCaused by: java.lang.IllegalArgumentException\nat java.nio.Buffer.limit(Buffer.java:275)\nat org.apache.hadoop.io.compress.lz4.Lz4Decompressor.decompress(Lz4Decompressor.java:232)\nat org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:88)\nat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\nat java.io.DataInputStream.readFully(DataInputStream.java:195)\n\u00a0\nany advice ? we need to write Lz4 files by C++ and read oover Hadoop jobs but still stuck on this problem .",
        "Issue Links": [
            "/jira/browse/PARQUET-2196"
        ]
    },
    "PARQUET-1975": {
        "Key": "PARQUET-1975",
        "Summary": "Test failure on ARM64 CPU architecture",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Martin Tzvetanov Grigorov",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "09/Feb/21 11:17",
        "Updated": "11/Mar/21 16:11",
        "Resolved": "26/Feb/21 16:24",
        "Description": "Trying to build Apache Parquet MR on ARM64 fails with:\n\u00a0\n\n\r\n$ mvn clean verify\r\n...\r\nTests in error: \r\n  testReadWriteWithCountDeprecated(org.apache.parquet.hadoop.DeprecatedInputFormatTest): org.apache.hadoop.io.compress.CompressionCodec: Provider org.apache.hadoop.io.compress.BrotliCodec could not be instantiated\r\n\n\n\u00a0\nThe reason is that com.github.rdblue:brotli-codec has no binary for aarch64",
        "Issue Links": [
            "/jira/browse/PARQUET-1980"
        ]
    },
    "PARQUET-1976": {
        "Key": "PARQUET-1976",
        "Summary": "Use net.alchim31.maven:scala-maven-plugin instead of org.scala-tools:maven-scala-plugin",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "09/Feb/21 11:48",
        "Updated": "13/Jan/22 15:14",
        "Resolved": null,
        "Description": "org.scala-tools:maven-scala-plugin is not maintained since a long time.\nnet.alchim31.maven:scala-maven-plugin is the replacement.\nAlso Scala version could be upgraded from 2.12.8 to 2.12.13\nFew other Maven plugins also could be upgraded.",
        "Issue Links": [
            "https://github.com/apache/parquet-mr/pull/866"
        ]
    },
    "PARQUET-1977": {
        "Key": "PARQUET-1977",
        "Summary": "Invalid data_page_offset",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Feb/21 09:56",
        "Updated": "25/Feb/21 09:57",
        "Resolved": "25/Feb/21 09:57",
        "Description": "The thrift field data_page_offset\u00a0is filled with incorrect value. Currently, it always points to the beginning of the column chunk which is not correct according to the spec in case there is a dictionary page. This is not a regression as it was written incorrectly since the beginning of parquet-mr.\nMeanwhile PARQUET-1850 fixed that we never wrote the field dictionary_page_offset. After the fix we correctly write this field if there is a dictionary filter. The problem is we are using the same value to fill both fields. So there are two possibilities:\n\nThere is no dictionary page in the column chunk so data_page_offset is filled with the correct value while dictionary_page_offset is not filled which is still correct. We are good.\nThere is a dictionary page at the beginning of the column chunk so data_page_offset and dictionary_page_offset are both contains the same value. This is not only a regression but it causes issues in other implementations (e.g. Impala) where footer validation is more strict than in parquet-mr because dictionary_page_offset shall be less than data_page_offset at all time if it is filled.\n\nSo, we need to fill data_page_offset correctly.",
        "Issue Links": [
            "/jira/browse/PARQUET-1898"
        ]
    },
    "PARQUET-1978": {
        "Key": "PARQUET-1978",
        "Summary": "Provide a tool to show the complete footer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Feb/21 10:17",
        "Updated": "19/Mar/21 10:22",
        "Resolved": "19/Mar/21 10:22",
        "Description": "In many cases of debugging it would be nice to have a tool to check the whole footer of a parquet file. It would be also nice to see both the content of the parquet-mr internal footer object structure and the original thrift objects.\n\u00a0\n(As of parquet-tools is deprecated we should develop this in parquet-cli.)",
        "Issue Links": []
    },
    "PARQUET-1979": {
        "Key": "PARQUET-1979",
        "Summary": "Optional bloom_filter_offset is filled if no bloom filter is present",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Feb/21 14:46",
        "Updated": "17/Feb/21 09:05",
        "Resolved": "17/Feb/21 09:05",
        "Description": "Currently the field bloom_filter_offset is filled (with the value 0) even if no bloom filter is saved for the related column chunk. This is not correct as this field is optional so we should not fill it in this case.",
        "Issue Links": [
            "/jira/browse/PARQUET-1898",
            "/jira/browse/PARQUET-41"
        ]
    },
    "PARQUET-1980": {
        "Key": "PARQUET-1980",
        "Summary": "Build and test Apache Parquet on ARM64 CPU architecture",
        "Type": "Test",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-format",
        "Assignee": "Martin Tzvetanov Grigorov",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "16/Feb/21 11:05",
        "Updated": "10/Jan/23 18:57",
        "Resolved": null,
        "Description": "More and more deployments are being done on ARM64 machines.\nIt would be good to make sure Parquet MR project builds fine on it.\nThe project moved from TravisCI to GitHub Actions recently (PARQUET-1969) but .travis.yml could be re-intorduced for ARM64 until GitHub Actions provide aarch64 nodes!",
        "Issue Links": [
            "/jira/browse/PARQUET-1975",
            "https://github.com/apache/parquet-site/pull/4"
        ]
    },
    "PARQUET-1981": {
        "Key": "PARQUET-1981",
        "Summary": "Consider adding BloomFilterHeader to ColumnMetaData",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Csaba Ringhofer",
        "Created": "16/Feb/21 13:45",
        "Updated": "16/Feb/21 13:45",
        "Resolved": null,
        "Description": "Currently ColumnMetaData only contains bloom_filter_offset, which points to BloomFilterHeader followed by the bloom filter data.\nThis solution is not optimal during reading, as two IO reads are needed once we know bloom_filter_offset - one to read the header, which contains the size of the bloom filter, then another to read the actual bloom filter to a buffer. Having the size near bloom_filter_offset would allow to do this in a single read.\nHaving algorithm/hash/compression could be also useful by allowing skipping the read of the bloom filter if one of those parameters is not supported.",
        "Issue Links": []
    },
    "PARQUET-1982": {
        "Key": "PARQUET-1982",
        "Summary": "Allow random access to row groups in ParquetFileReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Felix Schmalzel",
        "Reporter": "Felix Schmalzel",
        "Created": "16/Feb/21 15:15",
        "Updated": "19/Apr/21 07:53",
        "Resolved": "19/Apr/21 07:53",
        "Description": "The used SeekableInputStream and all other components of the ParquetFileReader already support random access and row groups should be independent of each other.\nThis would allow reusing the opened InputStream when you want to go back a row group. It also makes accessing specific row groups a lot easier.\nI've already developed a patch that would enable this functionality. I will link the merge request in the next few days.\nIs there a related ticket that i have overlooked?",
        "Issue Links": []
    },
    "PARQUET-1983": {
        "Key": "PARQUET-1983",
        "Summary": "Pool SeekableInputStreams in ParquetFileReader",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Felix Schmalzel",
        "Created": "16/Feb/21 15:20",
        "Updated": "16/Feb/21 15:20",
        "Resolved": null,
        "Description": "If https://issues.apache.org/jira/browse/PARQUET-1982 goes through, then we could allow parallel reading of row groups with a pool of SeekableInputStreams. This would significantly boost performance for applications that read data at random positions from a large file.\nI've already developed a patch that would enable this functionality. I will link the merge request in the next few days.\nIs there a related ticket that i have overlooked?",
        "Issue Links": []
    },
    "PARQUET-1984": {
        "Key": "PARQUET-1984",
        "Summary": "Some tests fail on windows",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr,                                            parquet-thrift",
        "Assignee": "Felix Schmalzel",
        "Reporter": "Felix Schmalzel",
        "Created": "16/Feb/21 15:42",
        "Updated": "08/Jul/21 15:40",
        "Resolved": "24/Feb/21 11:14",
        "Description": "Reasons:\n\nExpecting \\n and getting \\r\\n\nUnclosed streams preventing a temporary file from being deleted\nFile layout differences \\ and /\nNo native library for brotli, because the brotli-codec dependency only shadows macos and linux native libraries.\n\n\u00a0\nI've already developed a patch that would fix all the problems excluding the brotli one. For that one we would have to wait until this\u00a0https://github.com/rdblue/brotli-codec/pull/2 request is merged. I will link the merge request for the other problems in the next few days.\nIs there a related ticket that i have overlooked?",
        "Issue Links": []
    },
    "PARQUET-1985": {
        "Key": "PARQUET-1985",
        "Summary": "Improve integration tests between implementations",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-testing",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "17/Feb/21 10:11",
        "Updated": "18/Feb/21 09:30",
        "Resolved": null,
        "Description": "We have a lack of proper integration tests between components. Fortunately, we already have a git repository to upload test data: https://github.com/apache/parquet-testing.\nThe idea is the following.\nCreate a directory structure for the different versions of the implementations containing parquet files with defined data. The structure definition shall be self-descriptive so we can write integration tests that reads the whole structure automatically and also works with files to be added later.\nThe following directory structure is an example for the previous requirements:\n\ntest-data/\r\n\u251c\u2500\u2500 impala\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3.2.0\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 basic-data.parquet\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3.3.0\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 basic-data.parquet\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 3.4.0\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 basic-data.lz4.parquet\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 basic-data.snappy.parquet\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 some-specific-issue-2.parquet\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 some-specific-issue-3.csv\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 some-specific-issue-3_mode1.parquet\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 some-specific-issue-3_mode2.parquet\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 some-specific-issue-3.schema\r\n\u251c\u2500\u2500 parquet-cpp\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.5.0\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.lz4.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 basic-data.parquet\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1.6.0\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 basic-data.lz4.parquet\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 some-specific-issue-2.parquet\r\n\u251c\u2500\u2500 parquet-mr\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.10.2\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 basic-data.parquet\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.11.1\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 some-specific-issue-1.parquet\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.12.0\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.br.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.lz4.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.snappy.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic-data.zstd.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 some-specific-issue-1.parquet\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 some-specific-issue-2.parquet\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 some-specific-issue-1.csv\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 some-specific-issue-1.schema\r\n\u251c\u2500\u2500 basic-data.csv\r\n\u251c\u2500\u2500 basic-data.schema\r\n\u251c\u2500\u2500 some-specific-issue-2.csv\r\n\u2514\u2500\u2500 some-specific-issue-2.schema\r\n\n\nParquet files are created at leaf level. The expected data is saved in a csv format (to be specified: separators, how to save binary etc.), the expected schema (to specify the data types independently from the parquet files) are saved in .schema files. The csv and schema files can be saved on the same level of the parquet files or upper levels if they are common to several parquet files.\nAny comments about the idea are welcomed.",
        "Issue Links": []
    },
    "PARQUET-1986": {
        "Key": "PARQUET-1986",
        "Summary": "Request for Parquet Documentation for Fixed Decimal Type in JSON data representation",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.8.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Divya",
        "Created": "19/Feb/21 15:57",
        "Updated": "05/Mar/21 17:03",
        "Resolved": "05/Mar/21 17:03",
        "Description": "We are trying to use a fixed type for logicalType Decimal. We create JSON and then convert it into Parquet. There, we are trying to call conversion.toFixed(), but it is returning in bytes (Array.toString(bytes)). We are using Avro 1.8 version.\n\u00a0\nBigDecimal d1\u00a0= new BigDecimal(\"124.56\")\nLogicalType decimal = LogicalTypes.decimal(9, 2);\nSchema fixedSchema = Schema.createFixed(Schema.Type.FIXED.name, null, null, 16);\nConversion<BigDecimal> conversion = new Conversions.DecimalConversion();\nGenericFixed d1fixed = conversion.toFixed(d1, fixedSchema, decimal);\n\u00a0\n\u00a0\nIs there any method that we can use to convert it into 'Fixed text' that JSONDecoder is expecting?\u00a0Any help will be appreciated.",
        "Issue Links": [
            "/jira/browse/AVRO-3050"
        ]
    },
    "PARQUET-1987": {
        "Key": "PARQUET-1987",
        "Summary": "Document how a schema can have columns splitted over different files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Carlos Diogo",
        "Created": "20/Feb/21 10:00",
        "Updated": "21/Feb/21 18:22",
        "Resolved": null,
        "Description": "In the design overview is stated that the format supports that columns of a given schema are splitted over several files .\u00a0\nTo date ,other than the reference in the parquet homepage there are no other references to these feature , not even if it is actually implemented.\u00a0\nAs it currently stands , I believe that for a given row group you cannot have some columns in one file and other column group in another file belonging to the same schema .\nIf these feature was actually implemented it would open the door to faster data updates which affect a single column in very wide tables, which is typical on big data use cases .",
        "Issue Links": []
    },
    "PARQUET-1988": {
        "Key": "PARQUET-1988",
        "Summary": "Upgrade to ZSTD 1.4.8-6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "23/Feb/21 21:26",
        "Updated": "28/Feb/21 19:36",
        "Resolved": "28/Feb/21 08:00",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-1989": {
        "Key": "PARQUET-1989",
        "Summary": "Deep verification of encrypted files",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-cli",
        "Assignee": "Maya Anderson",
        "Reporter": "Gidon Gershinsky",
        "Created": "28/Feb/21 08:05",
        "Updated": "12/Apr/23 12:54",
        "Resolved": null,
        "Description": "A tools that verifies encryption of parquet files in a given folder. Analyzes the footer, and then every module (page headers, pages, column indexes, bloom filters) - making sure they are encrypted (in relevant columns). Potentially checking the encryption keys.\nWe'll start with a design doc, open for discussion.",
        "Issue Links": [
            "https://docs.google.com/document/d/1c6Vc2X2CPTReWG3aHoBhOBUJ7ghk80wTzcqs04bBDX8/edit?usp=sharing"
        ]
    },
    "PARQUET-1990": {
        "Key": "PARQUET-1990",
        "Summary": "[C++] ConvertedType::NA is  written out in some cases",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Micah Kornfield",
        "Created": "01/Mar/21 05:52",
        "Updated": "01/Apr/21 03:57",
        "Resolved": "01/Apr/21 03:57",
        "Description": "This makes it an invalid thrift enum.\u00a0 ::NA is a placeholder enum internal to C++.\u00a0 There will right out an enum value of 24 to thrift which can cause errors for other readers.\u00a0 See:\u00a0https://issues.apache.org/jira/browse/ARROW-10553",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/9863"
        ]
    },
    "PARQUET-1991": {
        "Key": "PARQUET-1991",
        "Summary": "Reserve ConvertedType==24 due to bug in parquet-cpp implementation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Do",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "01/Mar/21 06:04",
        "Updated": "01/Apr/21 12:30",
        "Resolved": "01/Apr/21 12:30",
        "Description": "https://issues.apache.org/jira/browse/PARQUET-1990\u00a0sometimes a converted type of 24 is written out because we are leaking a C++ enum value that doesn't exist in the parquet thrift specification.\u00a0 We should reserve this so if any other converted type elements are added future readers of older files don't accidentally misinterpret this value.",
        "Issue Links": []
    },
    "PARQUET-1992": {
        "Key": "PARQUET-1992",
        "Summary": "Cannot build from tarball because of git submodules",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Maya Anderson",
        "Reporter": "Gabor Szadovszky",
        "Created": "01/Mar/21 10:42",
        "Updated": "11/Mar/21 16:11",
        "Resolved": "11/Mar/21 15:40",
        "Description": "Because we use git submodules (to get test parquet files) a simple \"mvn clean install\" fails from the unpacked tarball due to \"not a git repository\".\nI think we would have 2 options to solve this situation:\n\nInclude all the required files (even only for testing) in the tarball and somehow avoid the git submodule update in case of executed in a non-git envrionment\nMake the downloading of the parquet files and the related tests optional so it won't fail the build from the tarball",
        "Issue Links": []
    },
    "PARQUET-1993": {
        "Key": "PARQUET-1993",
        "Summary": "[C++] Expose when prefetching completes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "David Li",
        "Reporter": "David Li",
        "Created": "01/Mar/21 22:23",
        "Updated": "10/Mar/21 14:13",
        "Resolved": "10/Mar/21 14:13",
        "Description": "As a follow up to PARQUET-1820, we should let an application be notified when pre-buffering has completed (e.g. PreBuffer() should return Future<void>). This would let an application pre-buffer some amount of data (across multiple files and/or row groups) and then decode data as it becomes available instead of blocking.\nA more ergonomic API would be to expose Future<RecordBatchReader> or something along those lines.",
        "Issue Links": [
            "/jira/browse/ARROW-11770",
            "https://github.com/apache/arrow/pull/9613"
        ]
    },
    "PARQUET-1994": {
        "Key": "PARQUET-1994",
        "Summary": "Upgrade ZSTD JNI to 1.4.9-1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "09/Mar/21 07:00",
        "Updated": "11/Mar/21 16:11",
        "Resolved": "09/Mar/21 08:29",
        "Description": null,
        "Issue Links": [
            "/jira/browse/SPARK-34651"
        ]
    },
    "PARQUET-1995": {
        "Key": "PARQUET-1995",
        "Summary": "[C++][Parquet] Crash at parquet::TypedColumnWriterImpl<>::WriteBatchSpaced",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tham",
        "Created": "09/Mar/21 09:18",
        "Updated": "10/Mar/21 07:03",
        "Resolved": null,
        "Description": "I got this crash from our customer machine:\u00a0parquet_crash_WriteBatchSpaced.txt\u00a0, it crashes continuously every time my application opens a parquet file and write the first row group. It happens only on one machine and runs fine on others until now. I cannot reproduce it.\nI access the parquet file on only one thread, so you can eliminate the case of race condition. Do you have any idea about this crash report?",
        "Issue Links": []
    },
    "PARQUET-1996": {
        "Key": "PARQUET-1996",
        "Summary": "[Format] Add interoperable LZ4 codec, deprecate existing LZ4 codec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "09/Mar/21 18:17",
        "Updated": "27/Sep/22 08:49",
        "Resolved": "11/Mar/21 19:55",
        "Description": "The current LZ4 codec is non-interoperable for reasons explained in details on the parquet-dev mailing-list:\nhttps://mail-archives.apache.org/mod_mbox/parquet-dev/202102.mbox/%3c20210216151401.7647ce37@fsol%3e",
        "Issue Links": [
            "/jira/browse/PARQUET-2196",
            "/jira/browse/PARQUET-1998",
            "/jira/browse/PARQUET-1241"
        ]
    },
    "PARQUET-1997": {
        "Key": "PARQUET-1997",
        "Summary": "[C++] AesEncryptor and AesDecryptor primitives are unsafe",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "10/Mar/21 17:24",
        "Updated": "10/Mar/21 18:29",
        "Resolved": null,
        "Description": "AesEncryptor::Encrypt, AesDecryptor::Decrypt take a pointer to the output buffer but without the output buffer length. The caller is required to guess the expected output length. The functions also return the written output length, but at this point it's too late: data may have been written out of bounds.",
        "Issue Links": []
    },
    "PARQUET-1998": {
        "Key": "PARQUET-1998",
        "Summary": "[C++] Implement LZ4_RAW compression",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-5.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "11/Mar/21 19:57",
        "Updated": "22/Apr/21 16:29",
        "Resolved": "22/Apr/21 16:28",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-1996",
            "/jira/browse/PARQUET-1241",
            "https://github.com/apache/parquet-testing/pull/18",
            "https://github.com/apache/arrow/pull/9782"
        ]
    },
    "PARQUET-1999": {
        "Key": "PARQUET-1999",
        "Summary": "NPE might occur if OutputFile is implemented by the client",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "16/Mar/21 09:53",
        "Updated": "16/Mar/21 15:47",
        "Resolved": "16/Mar/21 15:47",
        "Description": "We have introduced a new default method in OutputFile which returns null by default. Unfortunately we not always prepared for this null causing NPE in case of this interface is implemented by our API client. (Our own implementations overrides this method and returns non-null values.)",
        "Issue Links": []
    },
    "PARQUET-2000": {
        "Key": "PARQUET-2000",
        "Summary": "build failed on AArch64, Fedora 33",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lutz Weischer",
        "Created": "16/Mar/21 12:18",
        "Updated": "07/Apr/21 14:11",
        "Resolved": null,
        "Description": "Apache Thrift 0.12.0 is required. Building it reports unsupported .NET, etc. \nInstalling 0.13.0 using yum results in an error on mvn package.",
        "Issue Links": []
    },
    "PARQUET-2001": {
        "Key": "PARQUET-2001",
        "Summary": "Bump Apache Avro to 1.10.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ryan Skraba",
        "Reporter": "Ryan Skraba",
        "Created": "17/Mar/21 13:02",
        "Updated": "17/Mar/21 14:13",
        "Resolved": "17/Mar/21 14:13",
        "Description": "Avro 1.10.2 release notes",
        "Issue Links": []
    },
    "PARQUET-2002": {
        "Key": "PARQUET-2002",
        "Summary": "Parquet 1.9 not forward compatible with Parquet 1.10",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nikhil Goyal",
        "Created": "18/Mar/21 18:43",
        "Updated": "18/Mar/21 18:43",
        "Resolved": null,
        "Description": "This patch broke forward compatibility\nhttps://issues.apache.org/jira/browse/PARQUET-357\nThis introduced a binary field which cannot be deserialized while reading from 1.9\nNew schema:\n \"type\": \n{ \"id\": \"STRING\", \"binary\": false }\nError message\n\n\r\nCaused by: java.lang.RuntimeException: shaded.parquet.org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field \"binary\" (Class org.apache.parquet.thrift.struct.ThriftType$StringType), not marked as ignorable at [Source: java.io.StringReader@611b35d6; line: 23, column: 23] (through reference chain: org.apache.parquet.thrift.struct.StringType[\"binary\"])\r\nat org.apache.parquet.thrift.struct.JSON.fromJSON(JSON.java:38) at org.apache.parquet.thrift.struct.ThriftType.fromJSON(ThriftType.java:89) at org.apache.parquet.thrift.ThriftMetaData.parseDescriptor(ThriftMetaData.java:121)\r\n... 19 more Caused by: shaded.parquet.org.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field \"binary\" (Class org.apache.parquet.thrift.struct.ThriftType$StringType), not marked as ignorable\n\nA simple fix to 1.9 should fix this\n\u00a0\n\n\r\n--- a/parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/ThriftType.java\r\n+++ b/parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/ThriftType.java\r\n@@ -641,11 +641,23 @@ public abstract class ThriftType {\r\n }\r\npublic static class StringType extends ThriftType {\r\n+ private boolean binary = false;\r\n+\r\n+ @JsonIgnore\r\n+ public boolean isBinary() {\r\n+ return binary;\r\n+ }\r\n+\r\n+ @JsonProperty\r\n+ public void setBinary(boolean binary) {\r\n+ this.binary = binary;\r\n+ }",
        "Issue Links": []
    },
    "PARQUET-2003": {
        "Key": "PARQUET-2003",
        "Summary": "Decimal Statistics emitted  by parquet-cpp are broken",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "19/Mar/21 03:21",
        "Updated": "19/Mar/21 03:55",
        "Resolved": null,
        "Description": "PARQUET-1655 min/max statistics for Decimal logical types that are encoded as byte arrays or fixed length byte arrays are potentially broken.\u00a0 All reader implementations should ignore/clear them for parquet for files written with a version included \"parquet-cpp\"\n\u00a0\nThis serves as a parent tracking issue. To fix various readers.",
        "Issue Links": []
    },
    "PARQUET-2004": {
        "Key": "PARQUET-2004",
        "Summary": "Bump Jackson  to version 2.12.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "19/Mar/21 08:09",
        "Updated": "22/Mar/21 08:36",
        "Resolved": "19/Mar/21 09:22",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2005": {
        "Key": "PARQUET-2005",
        "Summary": "Upgrade thrift to 0.14.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Mar/21 14:59",
        "Updated": "07/Feb/23 21:17",
        "Resolved": "08/Apr/21 20:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2006": {
        "Key": "PARQUET-2006",
        "Summary": "Column resolution by ID",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "23/Mar/21 15:40",
        "Updated": "26/Sep/22 19:01",
        "Resolved": null,
        "Description": "Parquet relies on the name. In a lot of usages e.g. schema resolution, this would be a problem. Iceberg uses ID and stored Id/name mappings. \nThis Jira is to add column ID resolution support.",
        "Issue Links": []
    },
    "PARQUET-2007": {
        "Key": "PARQUET-2007",
        "Summary": "Remove java related parts from parquet-format",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr,                                            parquet-site",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "23/Mar/21 17:24",
        "Updated": "23/Mar/21 17:24",
        "Resolved": null,
        "Description": "Since the java code generation part has moved to parquet-mr there is no reason (and it is misleading) to still keep these java files in the parquet-format repo and to release in a jar format.\nThe proposal is the following:\n\nRemove java code from the repository and update the pom.xml accordingly\nKeep the unit tests (to validate the PRs) but don't include them in the release. (We might release a separate -tests jar.)\nRelease in a zip format instead of a jar. The zip would contain all the documentation and the parquet.thrift file\nAlso move the dremel paper from parquet-mr to parquet-format\nUpdate the How to release doc",
        "Issue Links": []
    },
    "PARQUET-2008": {
        "Key": "PARQUET-2008",
        "Summary": "[C++] Wrong information written in RowGroup::total_byte_size",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-4.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "24/Mar/21 13:59",
        "Updated": "25/Mar/21 17:46",
        "Resolved": "25/Mar/21 17:46",
        "Description": "It seems parquet-cpp writes the compressed size, not the uncompressed size, in RowGroup::total_byte_size. It also writes the same value (correctly) in RowGroup::total_compressed_size.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/9791"
        ]
    },
    "PARQUET-2009": {
        "Key": "PARQUET-2009",
        "Summary": "Remove deprecated modules",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "25/Mar/21 14:49",
        "Updated": "22/Jun/21 10:03",
        "Resolved": "22/Jun/21 10:03",
        "Description": "We have deprecated a couple of modules. They were renamed to *-deprecated. These modules shall be removed for the next release if there are no objections come up in the community. (We might wait for the removal until the preparation of the next release.)",
        "Issue Links": [
            "/jira/browse/PARQUET-2020"
        ]
    },
    "PARQUET-2010": {
        "Key": "PARQUET-2010",
        "Summary": "Fix japicmp issues",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "25/Mar/21 14:56",
        "Updated": "30/Mar/21 17:15",
        "Resolved": "30/Mar/21 17:15",
        "Description": "japicmp (the compatibility checker plugin) cannot find the previous version (1.12.0) of the deprecated modules. The root cause is we renamed these modules to *-deprecated for the\u00a0release 1.12.0 and during the rename we've had to set the previous name for the plugin to be able to find the previous release 1.11.0 of these modules. Now, that we released 1.12.0 the previous name is the same as the current one so we need to remove these additional configs.",
        "Issue Links": []
    },
    "PARQUET-2011": {
        "Key": "PARQUET-2011",
        "Summary": "Update the doc for data types having parameters as precision instead of unit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Tanuja Dubey",
        "Reporter": "Tanuja Dubey",
        "Created": "30/Mar/21 11:30",
        "Updated": "31/Mar/21 16:34",
        "Resolved": "31/Mar/21 16:33",
        "Description": "https://github.com/apache/parquet-format/blob/master/LogicalTypes.md",
        "Issue Links": []
    },
    "PARQUET-2012": {
        "Key": "PARQUET-2012",
        "Summary": "ProtoParquetWriter constructors should be updated",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-protobuf",
        "Assignee": "Aaron Blake Niskode-Dossett",
        "Reporter": "Aaron Blake Niskode-Dossett",
        "Created": "31/Mar/21 12:43",
        "Updated": "01/Apr/21 14:03",
        "Resolved": "01/Apr/21 14:03",
        "Description": "The constructors should be marked as deprecated and internal uses of them switched to the Builder pattern",
        "Issue Links": []
    },
    "PARQUET-2013": {
        "Key": "PARQUET-2013",
        "Summary": "[Format] Mention that converted types are deprecated",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "01/Apr/21 12:29",
        "Updated": "04/Apr/21 22:43",
        "Resolved": "04/Apr/21 09:25",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2014": {
        "Key": "PARQUET-2014",
        "Summary": "Local key wrapping with rotation",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "04/Apr/21 11:52",
        "Updated": "04/Apr/21 11:52",
        "Resolved": null,
        "Description": "parquet-mr-1.12.0 has an experimental support for local wrapping of encryption keys, that doesn't handle master key versions and key rotation. This Jira will add these capabilities.",
        "Issue Links": []
    },
    "PARQUET-2015": {
        "Key": "PARQUET-2015",
        "Summary": "[Format] Update changelog for 2.9.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "07/Apr/21 08:31",
        "Updated": "07/Apr/21 12:45",
        "Resolved": "07/Apr/21 12:45",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2016": {
        "Key": "PARQUET-2016",
        "Summary": "Reference column_order field from column indexes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "07/Apr/21 11:31",
        "Updated": "22/Apr/21 11:15",
        "Resolved": "22/Apr/21 11:15",
        "Description": "We have created the field column_order to specify the ordering of a primitive type. This is used for the row group level statistics but we never referenced this from the column indexes feature while in both cases we heavily rely on the ordering.",
        "Issue Links": [
            "/jira/browse/PARQUET-686"
        ]
    },
    "PARQUET-2017": {
        "Key": "PARQUET-2017",
        "Summary": "Handle special values of floating point statistics",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "07/Apr/21 11:40",
        "Updated": "07/Apr/21 11:40",
        "Resolved": null,
        "Description": "Based on PARQUET-1251 we have implemented the suggested workaround but it is not complete in all situations. \n\nWe handle the special floating point values at row-group level in parquet-mr but only for the read path. For the write path we still write these values.\nFor column indexes we handle these values but only for the write path and not for the read path.\n\nWe should implement the workaround for both read and write paths for all cases so we not only handle potentially invalid values but also don't write them to the file.",
        "Issue Links": []
    },
    "PARQUET-2018": {
        "Key": "PARQUET-2018",
        "Summary": "ParquetThriftWriter uses deprecated constructors",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-thrift",
        "Assignee": null,
        "Reporter": "Aaron Blake Niskode-Dossett",
        "Created": "07/Apr/21 12:35",
        "Updated": "07/Apr/21 12:35",
        "Resolved": null,
        "Description": "ParquetThriftWriter only has constructors that rely on deprecated ParquetWriter constructors.\u00a0 It should implement a builder by extending ParquetWriter.builder similar to how other parquet writer extensions have.\n\u00a0\nThis would, at some point in the future, be a blocker for 2.0.0",
        "Issue Links": []
    },
    "PARQUET-2019": {
        "Key": "PARQUET-2019",
        "Summary": "[Format] Outdated KEYS file in git repo",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "07/Apr/21 13:45",
        "Updated": "07/Apr/21 16:47",
        "Resolved": "07/Apr/21 16:47",
        "Description": "For some reason, the parquet-format git repo has a KEYS file that's outdated compared to the one stored in the SVN repo. Unless there's a reason to keep a KEYS file in git, I would suggest removing it.",
        "Issue Links": []
    },
    "PARQUET-2020": {
        "Key": "PARQUET-2020",
        "Summary": "Remove deprecated modules",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cascading",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "08/Apr/21 20:42",
        "Updated": "13/Oct/22 07:04",
        "Resolved": "19/Apr/21 15:49",
        "Description": "Removes:\u00a0\n\nparquet-tools-deprecated\nparquet-scrooge-deprecated\nparquet-cascading-common23-deprecated\nparquet-cascading-deprecated\nparquet-cascading3-deprecated",
        "Issue Links": [
            "/jira/browse/PARQUET-2009"
        ]
    },
    "PARQUET-2021": {
        "Key": "PARQUET-2021",
        "Summary": "Add documentation about RLE-bitpack hybrid encoder",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Jorge Leit\u00e3o",
        "Created": "10/Apr/21 16:45",
        "Updated": "18/Sep/21 01:20",
        "Resolved": null,
        "Description": "Since this is a parquet-specific encoder, it would be good to have a more complete description of the encoding/decoding, so that implementations have a easier time implementing it.",
        "Issue Links": []
    },
    "PARQUET-2022": {
        "Key": "PARQUET-2022",
        "Summary": "ZstdDecompressorStream should close `zstdInputStream`",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "11/Apr/21 18:12",
        "Updated": "13/Sep/21 05:26",
        "Resolved": "19/Apr/21 08:23",
        "Description": "`ZstdDecompressorStream` should close its resource because `CompressionInputStream.close` closes only the inter stream.\n\n\r\npublic class ZstdDecompressorStream extends CompressionInputStream {\r\n\r\n  private ZstdInputStream zstdInputStream;\r\n\r\n  public ZstdDecompressorStream(InputStream stream) throws IOException {\r\n    super(stream);\r\n    zstdInputStream = new ZstdInputStream(stream);\r\n  }\r\n}",
        "Issue Links": []
    },
    "PARQUET-2023": {
        "Key": "PARQUET-2023",
        "Summary": "TravisCI builds do not fail even when there is a compilation error",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Martin Tzvetanov Grigorov",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "12/Apr/21 07:57",
        "Updated": "14/Apr/21 09:26",
        "Resolved": "14/Apr/21 09:26",
        "Description": "As noticed at https://github.com/apache/parquet-mr/pull/840 the build at TravisCI didn't fail despite the compilation errors.\n\u00a0\nThe reason is the piping to 'pv' for progress monitoring. The exit status of 'mvn' command is ignored and the exit status of 'pv' is 0, i.e. success.\nCurrently the build at TravisCI takes around 40 minutes (https://travis-ci.org/github/apache/parquet-mr/builds/765496614) so there are 10 more minutes before the upper limit at TravisCI.\nMaven will be run with '\u2013quiet' option to prevent another error - too much output.",
        "Issue Links": []
    },
    "PARQUET-2024": {
        "Key": "PARQUET-2024",
        "Summary": "Remove KEYS file from parquet-mr repo",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "12/Apr/21 15:07",
        "Updated": "14/Apr/21 09:24",
        "Resolved": "14/Apr/21 09:24",
        "Description": "The official KEYS file is maintained in the release svn repo. The others shall be removed to avoid confusion.",
        "Issue Links": []
    },
    "PARQUET-2025": {
        "Key": "PARQUET-2025",
        "Summary": "Bump snappy to 1.1.8.3 to support Mac m1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Junjie Chen",
        "Created": "13/Apr/21 01:59",
        "Updated": "19/Apr/21 16:10",
        "Resolved": "19/Apr/21 15:48",
        "Description": "When running unit tests of \u00a0iceberg on\u00a0Mac m1 , it throws: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n\u00a0\nCaused by:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.codec.SnappyCompressor.compress(SnappyCompressor.java:67)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:165)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:122)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:53)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:315)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.column.impl.ColumnWriteStoreBase.flush(ColumnWriteStoreBase.java:152)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:27)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ... 10 more",
        "Issue Links": []
    },
    "PARQUET-2026": {
        "Key": "PARQUET-2026",
        "Summary": "Allow empty row in parquet file",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Vitalii Diravka",
        "Created": "13/Apr/21 06:10",
        "Updated": "12/Apr/23 01:48",
        "Resolved": null,
        "Description": "PARQUET-1851 starts abandon to write parquet files with schema (meta information), but with 0 rows, aka empty files.\nIn result it prevent to store empty tables in DRILL by using parquet files, for example:\n\n\r\nCREATE TABLE dfs.tmp.%s AS SELECT * FROM cp.`employee.json` WHERE 1=0\n\n\n\r\nCREATE TABLE dfs.tmp.%s AS select * from dfs.`parquet/alltypes_required.parquet` where `col_int` = 0\n\n\n\r\ncreate table dfs.tmp.%s as select * from dfs.`parquet/empty/complex/empty_complex.parquet`\n\nSo PARQUET-1851\u00a0breaks the following test cases:\n\n\r\nTestUntypedNull.testParquetTableCreation   TestParquetWriterEmptyFiles.testComplexEmptyFileSchema   TestParquetWriterEmptyFiles.testWriteEmptyFile   TestParquetWriterEmptyFiles.testWriteEmptyFileWithSchema   TestParquetWriterEmptyFiles.testWriteEmptySchemaChange TestMetastoreCommands.testAnalyzeEmptyRequiredParquetTable  TestMetastoreCommands.testSelectEmptyRequiredParquetTable\n\n\u00a0I suggest to use warning in the process of creating empty parquet files or create alternative endBlock for backward compatibility with other tools:",
        "Issue Links": [
            "/jira/browse/PARQUET-1851",
            "/jira/browse/DRILL-7825",
            "/jira/browse/DRILL-7907",
            "/jira/browse/DRILL-7825"
        ]
    },
    "PARQUET-2027": {
        "Key": "PARQUET-2027",
        "Summary": "Merging parquet files created in 1.11.1 not possible using 1.12.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.1",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Matthew M",
        "Created": "16/Apr/21 11:53",
        "Updated": "27/Sep/21 16:50",
        "Resolved": "26/Apr/21 11:42",
        "Description": "I have parquet files created using 1.11.1. In the process I join two files (with the same schema) into a one output file. I create Hadoop writer:\n\n\r\nval hadoopWriter = new ParquetFileWriter(\r\n      HadoopOutputFile.fromPath(\r\n        new Path(outputPath.toString),\r\n        new Configuration()\r\n      ), outputSchema, Mode.OVERWRITE,\r\n      8 * 1024 * 1024,\r\n      2097152,\r\n      DEFAULT_COLUMN_INDEX_TRUNCATE_LENGTH,\r\n      DEFAULT_STATISTICS_TRUNCATE_LENGTH,\r\n      DEFAULT_PAGE_WRITE_CHECKSUM_ENABLED\r\n    )\r\n    hadoopWriter.start()\r\n\n\nand try to append one file into another:\n\n\r\nhadoopWriter.appendFile(HadoopInputFile.fromPath(new Path(file), new Configuration()))\r\n\n\nEverything works on 1.11.1. But when I've switched to 1.12.0 it fails with that error:\n\n\r\nSTDERR: Exception in thread \"main\" java.io.IOException: can not read class org.apache.parquet.format.PageHeader: Required field 'uncompressed_page_size' was not found in serialized data! Struct: org.apache.parquet.format.PageHeader$PageHeaderStandardScheme@b91d8c4\r\n at org.apache.parquet.format.Util.read(Util.java:365)\r\n at org.apache.parquet.format.Util.readPageHeader(Util.java:132)\r\n at org.apache.parquet.format.Util.readPageHeader(Util.java:127)\r\n at org.apache.parquet.hadoop.Offsets.readDictionaryPageSize(Offsets.java:75)\r\n at org.apache.parquet.hadoop.Offsets.getOffsets(Offsets.java:58)\r\n at org.apache.parquet.hadoop.ParquetFileWriter.appendRowGroup(ParquetFileWriter.java:998)\r\n at org.apache.parquet.hadoop.ParquetFileWriter.appendRowGroups(ParquetFileWriter.java:918)\r\n at org.apache.parquet.hadoop.ParquetFileReader.appendTo(ParquetFileReader.java:888)\r\n at org.apache.parquet.hadoop.ParquetFileWriter.appendFile(ParquetFileWriter.java:895)\r\n at [...]\r\nCaused by: shaded.parquet.org.apache.thrift.protocol.TProtocolException: Required field 'uncompressed_page_size' was not found in serialized data! Struct: org.apache.parquet.format.PageHeader$PageHeaderStandardScheme@b91d8c4\r\n at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:1108)\r\n at org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:1019)\r\n at org.apache.parquet.format.PageHeader.read(PageHeader.java:896)\r\n at org.apache.parquet.format.Util.read(Util.java:362)\r\n ... 14 more",
        "Issue Links": []
    },
    "PARQUET-2028": {
        "Key": "PARQUET-2028",
        "Summary": "The example in delta-encoding seems incorrect",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Jorge Leit\u00e3o",
        "Created": "17/Apr/21 20:27",
        "Updated": "17/Apr/21 20:27",
        "Resolved": null,
        "Description": "In the example using delta-encoded, encoding [1, 2, 3, 4, 5], we state that\n\n\r\nThe final encoded data is:\r\n\r\nheader: 8 (block size), 1 (miniblock count), 5 (value count), 1 (first value)\r\n\r\nblock 1 (minimum delta), 0 (bitwidth), (no data needed for bitwidth 0)\r\n\n\nI believe that the correct result should be\nheader: [8, 1, 5, 2]\nblock: [2, 0]\nI.e first_value and min_delta should be 2, not 1.\nThis is because the zig-zag ULEB128-encoding of 1 is 2: the ULEB-128 encoding of 1 is 1, but AFAIK the zig-zag encoding of 1 is 2 (see e.g. here).\nAlternatively, we could re-phrase \"The final encoded data is:\" to \"The final data prior to zig-zag encoding is:\"",
        "Issue Links": []
    },
    "PARQUET-2029": {
        "Key": "PARQUET-2029",
        "Summary": "Delta-bitpacked encoding Java implementation seems incorrect",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jorge Leit\u00e3o",
        "Created": "18/Apr/21 05:19",
        "Updated": "18/Apr/21 05:33",
        "Resolved": "18/Apr/21 05:33",
        "Description": "This assumes that spark==3.1.1 uses the java reference implementation. Consider\n\n\r\nimport pyspark.sql\r\nfrom pyspark.sql.types import *\r\n\r\nspark = (\r\n    pyspark.sql.SparkSession.builder.config(\r\n        \"spark.sql.parquet.compression.codec\",\r\n        \"uncompressed\",\r\n    )\r\n    .config(\"spark.hadoop.parquet.writer.version\", \"v2\")\r\n    .getOrCreate()\r\n)\r\n\r\nschema = StructType([StructField(\"label\", IntegerType(), False)])\r\n\r\ndf = spark.createDataFrame([[1], [2], [3], [4], [5]], schema).coalesce(1)\r\n\r\ndf.write.parquet(\"bla.parquet\", mode=\"overwrite\")\r\n\n\nThis has no def or rep levels, is encoded with DELTA_BINARY_PACKED = 5, and results in the following page buffer:\n\n\r\nbuffer: [\r\n            128,\r\n            1,\r\n            4,\r\n            5,\r\n            2,\r\n            2,\r\n            0,\r\n            0,\r\n            0,\r\n            0,\r\n        ],\r\n\n\nLet's use notation\n\n\r\n(encoded <=u> decoded via uleb128)\r\n(encoded <=z> decoded via uleb128 zig-zag)\r\n\n\nLet's decode the above buffer manually according to the specification.\n\n\r\nbyte 0: 128 <=u> 128, the <block size in values>\r\nbyte 1: 1 <=u> 1, the <number of miniblocks in a block>\r\nbyte 2: 4 <=u> 4, the <total value count>\r\nbyte 3: 5 <=z> -3, the <first value>\r\nbyte 4: 2 <=z> 1, the <min delta>\r\nbyte 5: 2 <=u> 2, the bit width of the (only) miniblock\r\n\n\nI think that this is incorrect: the first value should be 1, not -3; the total count should be 5, not 4, the bit width of the miniblock should be 0, not 2.\nNote that if byte 2 was removed, everything would be correct:\n\n\r\nbyte 0: 128 <=u> 128, the <block size in values>\r\nbyte 1: 1 <=u> 1, the <number of miniblocks in a block>\r\nbyte 2: 5 <=u> 5, the <total value count>\r\nbyte 3: 2 <=z> 1, the <first value>\r\nbyte 4: 2 <=z> 1, the <min delta>\r\nbyte 5: 0 <=u> 0, the bit width of the (only) miniblock\r\n\n\nwhich corresponds to the delta-bitpack encoding of [1,2,3,4,5]: \n\n5 elements\nfirst value is 1\nthe min delta is 1\nthe relative differences are zero (bit_width=0) and thus do not require a buffer (writing 0 is also fine)",
        "Issue Links": []
    },
    "PARQUET-2030": {
        "Key": "PARQUET-2030",
        "Summary": "Expose page size row check configurations to ParquetWriter.Builder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Mika Ristim\u00e4ki",
        "Reporter": "Mika Ristim\u00e4ki",
        "Created": "19/Apr/21 12:07",
        "Updated": "22/Apr/21 07:33",
        "Resolved": "22/Apr/21 07:33",
        "Description": "PARQUET-1920 makes it possible to configure \"page.size.row.check.max\" and \"page.size.row.check.max\". But those configurations are not exposed to \"org.apache.parquet.hadoop.ParquetWriter.Builder\".",
        "Issue Links": []
    },
    "PARQUET-2031": {
        "Key": "PARQUET-2031",
        "Summary": "Upgrade to parquet-format 2.9.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "19/Apr/21 13:41",
        "Updated": "20/Apr/21 08:25",
        "Resolved": "20/Apr/21 08:25",
        "Description": "Since parquet-format 2.9.0 is release we shall upgrade it in parquet-mr.",
        "Issue Links": []
    },
    "PARQUET-2032": {
        "Key": "PARQUET-2032",
        "Summary": "Deprecate LZ4, introduce new LZ4_RAW",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "19/Apr/21 14:26",
        "Updated": "19/Apr/21 15:06",
        "Resolved": null,
        "Description": "The currently implemented LZ4 compression is based on the hadoop codec which is now deprecated (see details at PARQUET-1996). Also, a new, properly specified LZ4 compression (LZ4_RAW) has been introduced in the format.\nThe idea is to use the new LZ4_RAW compression for all cases when we currently use LZ4 and introduce a new configuration where the user would be able to switch to the deprecated behavior in case of the selected codec is LZ4.\nFor LZ4_RAW we will need to add the proper libs that will provide the officially specified LZ4 raw format.",
        "Issue Links": []
    },
    "PARQUET-2033": {
        "Key": "PARQUET-2033",
        "Summary": "Make \"null decryptor\" exception more informative",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "20/Apr/21 12:11",
        "Updated": "20/Apr/21 12:11",
        "Resolved": null,
        "Description": "Forgetting to pass decryption properties when reading an encrypted column in files with plaintext footer, results in a \"null decryptor\" exception thrown in the ColumnChunkMetaData class. The exception text can/should be updated to point to the possible reason.",
        "Issue Links": []
    },
    "PARQUET-2034": {
        "Key": "PARQUET-2034",
        "Summary": "Document dictionary page position",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Apr/21 17:03",
        "Updated": "24/Jun/21 11:56",
        "Resolved": "24/Jun/21 11:56",
        "Description": "Dictionary page shall be always written to the first position of the column chunk. Unfortunately, we only have one statement about this \"hidden\" at the encodings doc:\nThe dictionary page is written first, before the data pages of the column chunk.\nThis statement is not emphasized enough and not prepared for the potential extension of the available page types. It also should be placed to a more central place of the specification and also in the thrift file.",
        "Issue Links": []
    },
    "PARQUET-2035": {
        "Key": "PARQUET-2035",
        "Summary": "Java module import error due to shaded package shaded.parquet.it.unimi.dsi.fastutil",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0,                                            1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Maxim Kolesnikov",
        "Created": "24/Apr/21 22:19",
        "Updated": "29/Apr/21 17:24",
        "Resolved": null,
        "Description": "Description:\nDue to collision of shaded packages\u00a0\n\n\r\nshaded.parquet.it.unimi.dsi.fastutil\n\nin\u00a0\n\n\r\norg.apache.parquet:parquet-avro\n\nand\u00a0\n\n\r\norg.apache.parquet:parquet-column\n\nit is not possible to use both these dependencies within a modularized java project at the same time.\n\u00a0\n\u00a0\nHow to reproduce:\n\ncreate a maven project with dependency\u00a0org.apache.parquet:parquet-avro:1.11.1\ndeclare java module that requires both\u00a0parquet.avro and\u00a0parquet.column\nrun\n\n\r\nmvn compile\n\n\n\u00a0\nExpected behaviour:\nProject should compile without errors.\n\u00a0\nActual behaviour:\nProject fails with compilation errors:\n\u00a0\n\n\r\n[ERROR] the unnamed module reads package shaded.parquet.it.unimi.dsi.fastutil from both parquet.column and parquet.avro\r\n...\n\n\u00a0\n\u00a0\nReproducible example (same code as in the attached zip file):\u00a0https://github.com/xCASx/parquet-example",
        "Issue Links": []
    },
    "PARQUET-2036": {
        "Key": "PARQUET-2036",
        "Summary": "implicitly defining DEBUG mode in MessageColumnIO causes 80% performance overhead",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0,                                            1.10.1,                                            1.12.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Elad Yosifon",
        "Created": "25/Apr/21 16:15",
        "Updated": "26/Apr/21 14:26",
        "Resolved": null,
        "Description": "parquet-column jar leverages slf4j and log4j as default logger, neglecting to define a log4j configuration, defaults to DEBUG log level.\n\u00a0\n\n\r\npublic class MessageColumnIO extends GroupColumnIO {\r\n  private static final Logger LOG = LoggerFactory.getLogger(MessageColumnIO.class);\r\n\r\n  private static final boolean DEBUG = LOG.isDebugEnabled(); // <------\r\n}\r\n\n\n\u00a0\nthis \"magic behavior\" defaults parquet library to be in DEBUG mode, without any notification or warnings. Unfortunately, the RecordConsumerLoggingWrapper implementation generates 5x performance overhead in comparison to the MessageColumnIORecordConsumer implementation, causing a massive hit in performance and wasteful server utilization.\n\u00a0\nIMHO there are two things that could prevent such issue:\n\nprinting a message to STDOUT notifying about DEBUG mode being set to active.\ndefaulting to MessageColumnIORecordConsumer implementation, and waiting for explicit configuration to define DEBUG mode, and use RecordConsumerLoggingWrapper.\n\n\u00a0\nIn the past 2 years, this issue probably cost my company 50,000$ in excessive cloud costs!",
        "Issue Links": []
    },
    "PARQUET-2037": {
        "Key": "PARQUET-2037",
        "Summary": "Write INT96 with parquet-avro",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro,                                            parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "27/Apr/21 17:03",
        "Updated": "12/May/21 08:08",
        "Resolved": "12/May/21 08:08",
        "Description": "This jira is about the write path of PARQUET-1928. \nThe issue here is how to identify an Avro FIXED type that was an INT96 before. Of course, this feature would be behind a configuration flag similarly to PARQUET-1928. But even with this flag it is not obvious to differentiate a \"simple\" FIXED[12] byte from one that was an INT96 before.\nTwo options to solve this issue:\n\nWrite the doc field of the avro schema that the FIXED value was an INT96.\nInstead of implementing a configuration flag let the user specify the names of the columns to be converted to INT96 via the configuration.",
        "Issue Links": []
    },
    "PARQUET-2038": {
        "Key": "PARQUET-2038",
        "Summary": "Upgrade Jackson version used in parquet encryption",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Maya Anderson",
        "Reporter": "Maya Anderson",
        "Created": "28/Apr/21 20:57",
        "Updated": "04/May/21 12:44",
        "Resolved": "04/May/21 12:44",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2039": {
        "Key": "PARQUET-2039",
        "Summary": "AvroReadSupport.setRequestedProjection in 1.11.1+ not backwards compatible with MAPS",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0,                                            1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Bil Bingham",
        "Created": "28/Apr/21 21:03",
        "Updated": "29/Apr/21 14:34",
        "Resolved": null,
        "Description": "using\u00a0AvroReadSupport.setRequestedProjection in 1.11.1 reading a 1.10.0 generated parquet file sets MAP<STRING,STRING> fields to null (and vice versa 1.10.0 reader against a 1.11.1 generated file)\u00a0\n\nNot using a projected schema works, the map fields are converted correctly.\nIn my case.\u00a0Parquet file is generated by hive\u00a0\n\n\r\nCREATE TABLE parquetmaptest (\r\n a string,\r\n b MAP<string,string>\r\n)\r\nSTORED AS PARQUET \r\ntblproperties(\r\n \"parquet.compression\"=\"SNAPPY\"\r\n);\r\ninsert into parquetmaptest select \"a\",map(\"k1\",\"v1\",\"k2\",\"v2\");\n\n\nUsing parquet-avro 1.11.1 (and appropriate dependencies) result in field \"b\" being null.\u00a0\n\n\r\ndata:null\r\nrow:{\"a\": \"a\", \"b\": null}\n\nUsing parquet-avro 1.11.0 (and appropriate dependencies) result in field \"b\" being\u00a0 the right map value.\u00a0\u00a0\n\n\r\ndata:{k1=v1, k2=v2}\r\nrow:{\"a\": \"a\", \"b\": {\"k1\": \"v1\", \"k2\": \"v2\"}}",
        "Issue Links": []
    },
    "PARQUET-2040": {
        "Key": "PARQUET-2040",
        "Summary": "Uniform encryption",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "29/Apr/21 12:34",
        "Updated": "28/Jul/22 08:18",
        "Resolved": "28/Jul/22 08:18",
        "Description": "PME low-level spec supports using the same encryption key for all columns, which is useful in a number of scenarios. However, this feature is not exposed yet in the high-level API, because its misuse can break the NIST limit on the number of AES GCM operations with one key. We will develop a limit-enforcing code and provide an API for uniform table encryption.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2041": {
        "Key": "PARQUET-2041",
        "Summary": "Add zstd to `parquet.compression` description of ParquetOutputFormat Javadoc",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "William Hyun",
        "Created": "29/Apr/21 23:54",
        "Updated": "24/May/21 03:09",
        "Resolved": null,
        "Description": "The current Javadoc doesn't mention zstd.\n\nhttps://javadoc.io/doc/org.apache.parquet/parquet-hadoop/latest/org/apache/parquet/hadoop/ParquetOutputFormat.html",
        "Issue Links": []
    },
    "PARQUET-2042": {
        "Key": "PARQUET-2042",
        "Summary": "Unwrap common Protobuf wrappers and logical Timestamps, Date, TimeOfDay",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-protobuf",
        "Assignee": null,
        "Reporter": "Michael Wong",
        "Created": "03/May/21 07:26",
        "Updated": "17/Oct/22 22:04",
        "Resolved": null,
        "Description": "Related to https://issues.apache.org/jira/browse/PARQUET-1595",
        "Issue Links": []
    },
    "PARQUET-2043": {
        "Key": "PARQUET-2043",
        "Summary": "Fail build for used but not declared direct dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "04/May/21 09:06",
        "Updated": "12/Nov/21 16:45",
        "Resolved": "16/Aug/21 09:19",
        "Description": "It is always a good practice to specify all the dependencies directly used (classes are imported from) by our modules. We have a couple of issues where classes are imported from transitive dependencies. It makes hard to validate the actual dependency tree and also may result in using wrong versions of classes (see PARQUET-2038 for example).\nIt would be good to enforce to reference such dependencies directly in the module poms. The maven-dependency-plugin analyze-only goal can be used for this purpose.",
        "Issue Links": []
    },
    "PARQUET-2044": {
        "Key": "PARQUET-2044",
        "Summary": "Enable ZSTD buffer pool by default",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "05/May/21 20:54",
        "Updated": "11/May/21 08:39",
        "Resolved": "11/May/21 08:39",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2045": {
        "Key": "PARQUET-2045",
        "Summary": "ConsecutiveChunkList's length field should be long instead of int",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "YE",
        "Reporter": "YE",
        "Created": "10/May/21 09:14",
        "Updated": "10/May/21 09:15",
        "Resolved": null,
        "Description": "Hi, we encountered some read failure for large column chunk(size > Int.MaxValue). After some debugging, the buggy code is that\u00a0ConsecutiveChunkList's length field is int, and it overflows when the uncompressed size of one ColumnChunk is large than Int.MaxValue.\n\u00a0\nBelow is the exception stack:\n\n\u00a0\nThe column size is some what:",
        "Issue Links": []
    },
    "PARQUET-2046": {
        "Key": "PARQUET-2046",
        "Summary": "Upgrade Apache POM to 23",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "13/May/21 19:37",
        "Updated": "17/May/21 18:29",
        "Resolved": "17/May/21 09:25",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2047": {
        "Key": "PARQUET-2047",
        "Summary": "Clean Up Code",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "13/May/21 20:14",
        "Updated": "14/May/21 09:31",
        "Resolved": null,
        "Description": "Removed unused code\nRemove unused imports\nAdd @Override annotations\n\nMostly throwing away superfluous stuff. Less is more.",
        "Issue Links": []
    },
    "PARQUET-2048": {
        "Key": "PARQUET-2048",
        "Summary": "Deprecate BaseRecordReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "13/May/21 20:38",
        "Updated": "17/May/21 18:08",
        "Resolved": "17/May/21 09:23",
        "Description": "No longer used.",
        "Issue Links": []
    },
    "PARQUET-2049": {
        "Key": "PARQUET-2049",
        "Summary": "Deprecate Exceptions Class",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "14/May/21 15:26",
        "Updated": "12/Nov/21 17:19",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2050": {
        "Key": "PARQUET-2050",
        "Summary": "Expose repetition & definition level from ColumnIO",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "14/May/21 22:53",
        "Updated": "19/May/21 10:24",
        "Resolved": "19/May/21 10:24",
        "Description": "ColumnIO is pretty useful for obtaining repetition and definition level info, for constructing nested records (the ColumnDescriptor only expose the info for leave nodes). However, currently getDefinitionLevel and getRepetitionLevel are both package-private and other applications depend on Parquet have to find workaround for this (e.g., ColumnIOUtil used by Presto).",
        "Issue Links": []
    },
    "PARQUET-2051": {
        "Key": "PARQUET-2051",
        "Summary": "AvroWriteSupport does not pass Configuration to AvroSchemaConverter on Creation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Andreas Hailu",
        "Reporter": "Andreas Hailu",
        "Created": "15/May/21 01:34",
        "Updated": "23/Jun/22 21:55",
        "Resolved": "22/Jun/21 08:00",
        "Description": "Because of this, we're unable to fully leverage the ThreeLevelListWriter functionality when trying to write Avro lists out using Parquet through the AvroParquetOutputFormat.\nThe following record is used for testing:\n\u00a0Schema:\n\n{ \"type\": \"record\", \"name\": \"NullLists\", \"namespace\": \"com.test\", \"fields\": [ \\{ \"name\": \"KeyID\", \"type\": \"string\" }\n, { \"name\": \"NullableList\", \"type\": [ \"null\", \n{ \"type\": \"array\", \"items\": [ \"null\", \"string\" ] }\n ], \"default\": null } ] }\nRecord (using basic JSON just for display purposes):\n\n{ \"KeyID\": \"0\", \"NullableList\": [ \"foo\", null, \"baz\" ] }\n\nDuring testing, we see the following exception:\nCaused by: java.lang.ClassCastException: repeated binary array (STRING) is not a group\n {{ at org.apache.parquet.schema.Type.asGroupType(Type.java:250)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport$ThreeLevelListWriter.writeCollection(AvroWriteSupport.java:612)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport$ListWriter.writeList(AvroWriteSupport.java:397)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport.writeValueWithoutConversion(AvroWriteSupport.java:355)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:278)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport.writeRecordFields(AvroWriteSupport.java:191)}}\n {{ at org.apache.parquet.avro.AvroWriteSupport.write(AvroWriteSupport.java:165)}}\n {{ at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128}}\nUpon review, it was found that the configuration option that was set in AvroWriteSupport for the ThreeLevelListWriter, parquet.avro.write-old-list-structure being set to false, was never shared with the AvroSchemaConverter.\nOnce we made this change and tested locally, we observe the record with nulls in the array being successfully written by AvroParquetOutputFormat.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2052": {
        "Key": "PARQUET-2052",
        "Summary": "Integer overflow when writing huge binary using dictionary encoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "21/May/21 01:57",
        "Updated": "06/Jul/22 22:27",
        "Resolved": "26/May/21 07:43",
        "Description": "To check whether it should fallback to plain encoding, DictionaryValuesWriter currently use two variables: dictionaryByteSize and maxDictionaryByteSize, both of which are integer. This will cause issue when one first writes a relatively small binary within the threshold and then write a huge string which cause dictionaryByteSize overflow and becoming negative.",
        "Issue Links": [
            "/jira/browse/PARQUET-2164"
        ]
    },
    "PARQUET-2053": {
        "Key": "PARQUET-2053",
        "Summary": "Pluggable key material store",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "25/May/21 08:52",
        "Updated": "25/May/21 08:52",
        "Resolved": null,
        "Description": "Encryption key material can be stored either inside Parquet files, or outside (configurable). For outside storage, Parquet already has a pluggable interface for custom implementations, FileKeyMaterialStore, but no mechanism to load them (currently, one implementation is packaged in parquet-mr, and always loaded when outside storage is configured). We will provide a way to load custom implementations of the\u00a0FileKeyMaterialStore.",
        "Issue Links": []
    },
    "PARQUET-2054": {
        "Key": "PARQUET-2054",
        "Summary": "TCP connection leaking when calling appendFile()",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Kai Jiang",
        "Reporter": "Xinli Shang",
        "Created": "01/Jun/21 17:40",
        "Updated": "22/Jun/21 08:20",
        "Resolved": "22/Jun/21 07:53",
        "Description": "When appendFile() is called, the file reader path is opened but not closed. It caused many TCP connections leaked.",
        "Issue Links": []
    },
    "PARQUET-2055": {
        "Key": "PARQUET-2055",
        "Summary": "Schema mismatch for reading Avro from parquet file with old schema version?",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Philip Wilcox",
        "Created": "03/Jun/21 19:36",
        "Updated": "07/Jun/21 09:35",
        "Resolved": null,
        "Description": "I ran into what looks like a bug in the Parquet Avro reading code, around trying to read a file written with a previous version of a schema with a new, evolved version of the schema.\nI'm using Apache Beam's ParquetIO library, which supports passing in schemas to use for \"projection\" and I was investigating if that would work for me here. However, it didn't work, complaining that my new reader schema had a field that wasn't in the writer schema.\n\u00a0\nI traced this through to a couple places in the parquet-avro code that don't look right to me:\n\u00a0\nFirst, in `prepareForRead` here: https://github.com/apache/parquet-mr/blob/master/parquet-avro/src/main/java/org/apache/parquet/avro/AvroReadSupport.java#L116\nThe `parquetSchema` var comes from `parquetSchema = readContext.getRequestedSchema();` while the `avroSchema` var comes from the parquet file itself with `avroSchema = new Schema.Parser().parse(keyValueMetaData.get(AVRO_SCHEMA_METADATA_KEY));`\nI can verify that `parquetSchema` is the schema I'm requesting it be projected to and that `avroSchema` is the schema from the file, but the naming looks backward, shouldn't `parquetSchema` be the one from the parquet file?\nFollowing the stack down, I was hitting this line: https://github.com/apache/parquet-mr/blob/master/parquet-avro/src/main/java/org/apache/parquet/avro/AvroIndexedRecordConverter.java#L91\nhere it was failing because the `avroSchema` didn't have a field that was in the `parquetSchema`, with the variables assigned in the same way as above. That's the case I was hoping to use this projection for, though - to get the record read with the new reader schema, using the default value from the new schema for the new field. In fact, the comment on line 101 \"store defaults for any new Avro fields from avroSchema that are not in the writer schema (parquetSchema)\" suggests that the intent was for this to work, but the actual code has the writer schema in avroSchema and the reader schema in parquetSchema.\n(Additionally, I'd want this to support schema evolution both for adding an optional field and also removing an old field - so just flipping the names around would result in this still breaking if the reader schema dropped a field from the writer schema...)\nLooking to understand if I'm interpreting this correctly, or if there's another path that's intended to be used.\nThank you!",
        "Issue Links": []
    },
    "PARQUET-2056": {
        "Key": "PARQUET-2056",
        "Summary": "[C++] Add ability  for retrieving dictionary and indices separately for ColumnReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-5.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Jinpeng Zhou",
        "Reporter": "Micah Kornfield",
        "Created": "08/Jun/21 16:31",
        "Updated": "17/Jun/21 07:21",
        "Resolved": "17/Jun/21 07:21",
        "Description": "In some contexts it is useful to be able to retrieve these separately instead of decoding.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/10537"
        ]
    },
    "PARQUET-2057": {
        "Key": "PARQUET-2057",
        "Summary": "Upgrade ZSTD-JNI to 1.5.0-1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "David Christle",
        "Reporter": "David Christle",
        "Created": "08/Jun/21 23:36",
        "Updated": "10/Jun/21 07:53",
        "Resolved": "10/Jun/21 07:53",
        "Description": "This issue tracks upgrading the zstd-jni dependency to version 1.5.0-1.",
        "Issue Links": [
            "/jira/browse/SPARK-35670",
            "/jira/browse/SPARK-34651"
        ]
    },
    "PARQUET-2058": {
        "Key": "PARQUET-2058",
        "Summary": "Parquet-tools is affected by multiple CVEs",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.0,                                            1.10.1,                                            1.11.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Tony Liu",
        "Created": "09/Jun/21 03:48",
        "Updated": "09/Jun/21 07:08",
        "Resolved": null,
        "Description": "The parquet-tools library is affected by multiple CVEs.\n\u00a0\n\n\n\nCVE-2018-10237\nhttps://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-10237\nUnbounded memory allocation in Google Guava 11.0 through 24.x before 24.1.1 allows remote attackers to conduct denial of service attacks against servers that depend on this library and deserialize attacker-provided data, because the AtomicDoubleArray class (when serialized with Java serialization) and the CompoundOrdering class (when serialized with GWT serialization) perform eager allocation without appropriate checks on what a client has sent and whether the data size is reasonable.\n\n\nCVE-2020-8908\nhttps://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2020-8908\nA temp directory creation vulnerability exists in all versions of Guava, allowing an attacker with access to the machine to potentially access data in a temporary directory created by the Guava API com.google.common.io.Files.createTempDir(). By default, on unix-like systems, the created directory is world-readable (readable by an attacker with access to the system). The method in question has been marked @Deprecated in versions 30.0 and later and should not be used. For Android developers, we recommend choosing a temporary directory API provided by Android, such as context.getCacheDir(). For other Java developers, we recommend migrating to the Java 7 API java.nio.file.Files.createTempDirectory() which explicitly configures permissions of 700, or configuring the Java runtime\\'s java.io.tmpdir system property to point to a location whose permissions are appropriately configured.\n\n\nCVE-2019-17571\nhttps://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2019-17571\nIncluded in Log4j 1.2 is a SocketServer class that is vulnerable to deserialization of untrusted data which can be exploited to remotely execute arbitrary code when combined with a deserialization gadget when listening to untrusted network traffic for log data. This affects Log4j versions up to 1.2 up to 1.2.17.\n\n\nCVE-2020-9488\nhttps://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2020-9488\nImproper validation of certificate with host mismatch in Apache Log4j SMTP appender. This could allow an SMTPS connection to be intercepted by a man-in-the-middle attack which could leak any log messages sent through that appender.\n\n\n\n\u00a0\n\u00a0\nIs it possible to upgrade the POM files to reference the latest version of log4j and guava library?\n\u00a0\nThanks\nTony",
        "Issue Links": []
    },
    "PARQUET-2059": {
        "Key": "PARQUET-2059",
        "Summary": "Tests require too much memory",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "10/Jun/21 12:28",
        "Updated": "16/Aug/21 09:21",
        "Resolved": "16/Aug/21 09:21",
        "Description": "For testing the solution of PARQUET-1633 we require ~3GB memory that is not always available. To solve this issue we temporarily disabled the implemented unit test.\nWe need to ensure somehow that this test (and maybe some other similar ones) are executed regularly. Some options we might have:\n\nExecute this test separately with a maven profile. I am not sure if the CI allows allocating such large memory but with Xmx options we might give a try and create a separate check for this test only.\nSimilar to the previous with the profile but not executing in the CI ever. Instead, we add some comments to the release doc so this test will be executed at least once per release.\nConfiguring the CI profile to skip this test but have it in the normal scenario meaning the devs will execute it locally. There are a couple of cons though. There is no guarantee that devs executes all the tests including this one. It also can cause issues if the dev doesn't have enough memory and don't know that the test failure is not related to the current change.",
        "Issue Links": []
    },
    "PARQUET-2060": {
        "Key": "PARQUET-2060",
        "Summary": "Parquet corruption can cause infinite loop with Snappy",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Marios Meimaris",
        "Created": "24/Jun/21 12:10",
        "Updated": "24/Jun/21 12:37",
        "Resolved": null,
        "Description": "I am attaching a valid and corrupt parquet file (datapageV2) that differ in one byte.\nWe hit an infinite loop when trying to read the corrupt file in https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnReaderBase.java#L698\u00a0and specifically in the `page.getData().toInputStream()` call.\u00a0\u00a0\nStack trace of infinite loop:\njava.io.DataInputStream.readFully(DataInputStream.java:195)\n java.io.DataInputStream.readFully(DataInputStream.java:169)\n org.apache.parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:287)\n org.apache.parquet.bytes.BytesInput.toByteBuffer(BytesInput.java:237)\n org.apache.parquet.bytes.BytesInput.toInputStream(BytesInput.java:246)\n org.apache.parquet.column.impl.ColumnReaderBase.readPageV2(ColumnReaderBase.java:698)\n org.apache.parquet.column.impl.ColumnReaderBase.access$400(ColumnReaderBase.java:57)\n org.apache.parquet.column.impl.ColumnReaderBase$3.visit(ColumnReaderBase.java:628)\n org.apache.parquet.column.impl.ColumnReaderBase$3.visit(ColumnReaderBase.java:620)\n org.apache.parquet.column.page.DataPageV2.accept(DataPageV2.java:192)\n org.apache.parquet.column.impl.ColumnReaderBase.readPage(ColumnReaderBase.java:620)\n org.apache.parquet.column.impl.ColumnReaderBase.checkRead(ColumnReaderBase.java:594)\n\u00a0\nThe call to `readFully` will underneath go through `NonBlockedDecompressorStream` which will always hit this path: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/NonBlockedDecompressorStream.java#L45.\u00a0This will cause `setInput` to not be called on the decompressor, and the subsequent calls to `decompress` will always hit this condition: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/SnappyDecompressor.java#L54.\u00a0Therefore, the 0 value will be returned by the read method, which will cause an infinite loop in https://github.com/openjdk-mirror/jdk7u-jdk/blob/master/src/share/classes/java/io/DataInputStream.java#L198\u00a0\n This originates from the corruption, which causes the input stream of the data page to be of size 0, which makes `getCompressedData` always return -1.\u00a0\nI am wondering whether this can be caught earlier so that the read fails in case of such corruptions.\u00a0\nSince this happens in `BytesInput.toInputStream`, I don't think it's only relevant to DataPageV2.\u00a0\n\u00a0\nIn https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java#L111,\u00a0if we call `bytes.toByteArray` and log its length, it is 0 in the case of the corrupt file, and 6 in the case of the valid file.\u00a0\nA potential fix is to check the array size there and fail early, but I am not sure if a zero-length byte array can ever be expected in the case of valid files.\n\u00a0\nAttached:\nValid file: `datapage_v2_snappy.parquet`\nCorrupt file:\u00a0`datapage_v2_snappy.parquet1383`",
        "Issue Links": []
    },
    "PARQUET-2061": {
        "Key": "PARQUET-2061",
        "Summary": "Add a new API in `PageReadStore` to return row ranges directly",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chao Sun",
        "Created": "28/Jun/21 17:39",
        "Updated": "28/Jun/21 17:39",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2062": {
        "Key": "PARQUET-2062",
        "Summary": "Data masking(null) for column encryption",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "01/Jul/21 04:09",
        "Updated": "05/Jul/21 15:20",
        "Resolved": null,
        "Description": "When user doesn't have permisson on a column that are encrypted by the column encryption feature (parquet-1178), returning a masked value could avoid an exception and let the call succeed. \nWe would like to introduce the data masking with null values. The idea is when the user gets key access denied and the user can accept null(via a reading option flag), we would return null for the encrypted columns. This solution doesn't need to save extra columns for masked value and doesn't need to translate existing data.",
        "Issue Links": []
    },
    "PARQUET-2063": {
        "Key": "PARQUET-2063",
        "Summary": "Remove Compile Warnings from MemoryManager",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "02/Jul/21 16:35",
        "Updated": "10/Aug/21 07:37",
        "Resolved": "10/Aug/21 07:37",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2064": {
        "Key": "PARQUET-2064",
        "Summary": "Make Range public accessible in RowRanges",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "09/Jul/21 20:49",
        "Updated": "16/Aug/21 09:24",
        "Resolved": "16/Aug/21 09:24",
        "Description": "When rolling out to Presto, I found we need to know the boundaries of each Range in RowRanges. It is still doable with Iterator but Presto has. batch reader, we cannot use iterator for each row.",
        "Issue Links": []
    },
    "PARQUET-2065": {
        "Key": "PARQUET-2065",
        "Summary": "parquet-cli not working in release 1.12.0",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Akshay Sundarraj",
        "Created": "15/Jul/21 04:36",
        "Updated": "16/Jul/21 10:31",
        "Resolved": null,
        "Description": "When I run parquet-cli getting\u00a0 java.lang.NoSuchMethodError\nSteps to repdouce:\n\nDownload parquet-mr 1.12.0 from https://github.com/apache/parquet-mr/archive/refs/tags/apache-parquet-1.12.0.tar.gz\nBuild and install using mvn clean install\ncd parquet-cli\nmvn dependency:copy-dependencies\njava -cp 'target/:target/dependency/' org.apache.parquet.cli.Main head <parquet file>\nGot below exception\n\nWARNING: An illegal reflective access operation has occurred\n WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/home/amsundar/hgroot/parquet-mr-apache-parquet-1.12.0/parquet-cli/target/dependency/hadoop-auth-2.10.1.jar) to method sun.security.krb5.Config.getInstance()\n WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n WARNING: All illegal access operations will be denied in a future release\n Exception in thread \"main\" java.lang.NoSuchMethodError: org.apache.parquet.avro.AvroSchemaConverter.convert(Lorg/apache/parquet/schema/MessageType;)Lorg/apache/avro/Schema;\n at org.apache.parquet.cli.util.Schemas.fromParquet(Schemas.java:89)\n at org.apache.parquet.cli.BaseCommand.getAvroSchema(BaseCommand.java:405)\n at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:66)\n at org.apache.parquet.cli.Main.run(Main.java:155)\n at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n at org.apache.parquet.cli.Main.main(Main.java:185)",
        "Issue Links": []
    },
    "PARQUET-2066": {
        "Key": "PARQUET-2066",
        "Summary": "[C++][Parquet] num_rows is incorrect for nested types",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Jorge Leit\u00e3o",
        "Created": "16/Jul/21 06:41",
        "Updated": "16/Jul/21 06:56",
        "Resolved": null,
        "Description": "Data pages v2 have:\n\nnum_rows\nnum_values\n\nwe write num_rows equal to the num_values. However, they represent different aspects.\nGiven a list such as \"[[0, 1], None, [2, None, 3]]\", num_rows = 3 and num_values = 6. We currently report 6 in both instances.",
        "Issue Links": []
    },
    "PARQUET-2067": {
        "Key": "PARQUET-2067",
        "Summary": "[C++]  null_count and num_nulls incorrect for repeated columns",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-6.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "Micah Kornfield",
        "Created": "16/Jul/21 06:58",
        "Updated": "07/Oct/21 07:45",
        "Resolved": "06/Oct/21 08:21",
        "Description": "Currently only nulls at the leaf are accounted for in the null count statstics.\u00a0 For nested lists this is incorrect because null lists have zero elements and don't show up in the leaf.\n\u00a0\nExample from mailing list discussion\n\u00a0\n[[0, 1], None, [2, None, 3]]\n\u00a0\nshould have a null count of 2 (it currently reports as 1).",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/11281"
        ]
    },
    "PARQUET-2068": {
        "Key": "PARQUET-2068",
        "Summary": "[C++] [Parquet] Use arrow compute to determine min/max of dictionaries (possibly other arrays?)",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Weston Pace",
        "Created": "17/Jul/21 00:42",
        "Updated": "17/Jul/21 00:43",
        "Resolved": null,
        "Description": "parquet::Comparator is currently used to calculate the min & max values of an array.\u00a0 This should be benchmarked against arrow::compute's MinMax kernel (once it supports all necessary data types).\u00a0 The latter should be more aggressive with SIMD resulting in better performance.\nEven if there is no performance difference the MinMax kernel should be used when computing dictionary statistics as the current implementation requires making a copy of the dictionary values array (see ARROW-12513)",
        "Issue Links": []
    },
    "PARQUET-2069": {
        "Key": "PARQUET-2069",
        "Summary": "Parquet file containing arrays, written by Parquet-MR, cannot be read again by Parquet-MR",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Devon Kozenieski",
        "Created": "20/Jul/21 19:49",
        "Updated": "08/Nov/22 06:25",
        "Resolved": null,
        "Description": "In the attached files, there is one original file, and one written modified file that results after reading the original file and writing it back with Parquet-MR, with a few values modified. The schema should not be modified, since the schema of the input file is used as the schema to write the output file. However, the output file has a slightly modified schema that then cannot be read back the same way again with Parquet-MR, resulting in the exception message:\u00a0\u00a0java.lang.ClassCastException: optional binary element (STRING) is not a group\nMy guess is that the issue lies in the Avro schema conversion.\nThe Parquet files attached have some arrays and some nested fields.",
        "Issue Links": [
            "/jira/browse/HUDI-864"
        ]
    },
    "PARQUET-2070": {
        "Key": "PARQUET-2070",
        "Summary": "Replace deprecated syntax in protobuf support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Svend Vanderveken",
        "Reporter": "Svend Vanderveken",
        "Created": "30/Jul/21 22:10",
        "Updated": "04/Aug/21 07:22",
        "Resolved": "04/Aug/21 07:02",
        "Description": "This is trivial change, though at the moment ProtoWriteSupport.java is producing a human-readable JSON output of the protobuf schema with\u00a0 the following deprecated syntax:\n\u00a0\n\n\r\nTextFormat.printToString(asProto)\n\n\u00a0\nAlso, the method where is code is present executed one reflection invocation to get the protobuf descriptor which is unnecesserary since the context from where it's called already has this descriptor.\n=> all minor and trivial stuff though well, housekeeping I guess",
        "Issue Links": []
    },
    "PARQUET-2071": {
        "Key": "PARQUET-2071",
        "Summary": "Encryption translation tool",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "05/Aug/21 01:16",
        "Updated": "14/Jan/22 21:32",
        "Resolved": "14/Jan/22 21:32",
        "Description": "When translating existing data to encryption state, we could develop a tool like TransCompression to translate the data at page level to encryption state without reading to record and rewrite. This will speed up the process a lot.",
        "Issue Links": [
            "/jira/browse/PARQUET-2082",
            "/jira/browse/PARQUET-2081"
        ]
    },
    "PARQUET-2072": {
        "Key": "PARQUET-2072",
        "Summary": "Do Not Determine Both Min/Max for Binary Stats",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "David Mollitor",
        "Reporter": "David Mollitor",
        "Created": "05/Aug/21 02:30",
        "Updated": "09/Aug/21 15:21",
        "Resolved": "09/Aug/21 15:21",
        "Description": "I'm looking at some benchmarking code of Apache ORC v.s. Apache Parquet and see that Parquet is quite a bit slower for writes (reads TBD).  Based on my investigation, I have noticed a significant amount of time spent in determining min/max for binary types.\nOne quick improvement is to bypass a \"max\" value determinization if the value has already been determined to be a \"min\".\nWhile I'm at it, remove calls to deprecated functions.",
        "Issue Links": []
    },
    "PARQUET-2073": {
        "Key": "PARQUET-2073",
        "Summary": "Is there something wrong calculate usedMem in ColumnWriteStoreBase.java",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "JiangYang",
        "Reporter": "JiangYang",
        "Created": "05/Aug/21 06:38",
        "Updated": "16/Aug/21 09:23",
        "Resolved": "16/Aug/21 09:23",
        "Description": "",
        "Issue Links": [
            "/jira/browse/PARQUET-1735"
        ]
    },
    "PARQUET-2074": {
        "Key": "PARQUET-2074",
        "Summary": "Upgrade to JDK 9+",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "05/Aug/21 14:23",
        "Updated": "09/Aug/21 15:35",
        "Resolved": null,
        "Description": "Moving to JDK 9 will provide a plethora of new compares/equals capabilities on arrays that are all based on vectorization and implement @IntrinsicCandidate\nhttps://docs.oracle.com/javase/9/docs/api/java/util/Arrays.html",
        "Issue Links": []
    },
    "PARQUET-2075": {
        "Key": "PARQUET-2075",
        "Summary": "Unified Rewriter Tool",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Xinli Shang",
        "Created": "05/Aug/21 15:00",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "12/Apr/23 01:29",
        "Description": "During the discussion of PARQUET-2071, we came up with the idea of a universal tool to translate the existing file to a different state while skipping some level steps like encoding/decoding, to gain speed. For example, only decompress pages and then compress directly. For PARQUET-2071, we only decrypt and then encrypt directly. This will be useful for the existing data to onboard Parquet features like column encryption, zstd etc. \nWe already have tools like trans-compression, column pruning etc. We will consolidate all these tools with this universal tool.",
        "Issue Links": []
    },
    "PARQUET-2076": {
        "Key": "PARQUET-2076",
        "Summary": "Improve Travis CI build Performance",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Chen Zhang",
        "Created": "17/Aug/21 07:27",
        "Updated": "12/Apr/23 01:34",
        "Resolved": "12/Apr/23 01:34",
        "Description": "According to\u00a0Common Build Problems - Travis CI (travis-ci.com), we should carefully use travis_wait, as it may make the build unstable and extend the build time.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2077": {
        "Key": "PARQUET-2077",
        "Summary": "The number of values in a miniblock should be multiple of 32 instead of 8 in DeltaBinaryPackingConfig",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Shan Huang",
        "Created": "19/Aug/21 13:35",
        "Updated": "19/Aug/21 13:35",
        "Resolved": null,
        "Description": "In the code of DeltaBinaryPackingValuesWriter, the parameters are always DEFAULT_NUM_BLOCK_VALUES(which is 128) and DEFAULT_NUM_MINIBLOCKS(which is 4). So if the file is written by parquet-mr,\u00a0the number of values in a miniblock is always 32. It is consistent with the spec.\n However, the code in DeltaBinaryPackingConfig\u00a0indicates that the number of values in a miniblock must be multiple of 8. Would it be better if the limitation was changed to 32?",
        "Issue Links": []
    },
    "PARQUET-2078": {
        "Key": "PARQUET-2078",
        "Summary": "Failed to read parquet file after writing with the same parquet version",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "1.13.0,                                            1.12.1",
        "Component/s": "parquet-mr",
        "Assignee": "Nemon Lou",
        "Reporter": "Nemon Lou",
        "Created": "26/Aug/21 09:57",
        "Updated": "02/Apr/22 07:45",
        "Resolved": "13/Sep/21 12:58",
        "Description": "Writing parquet\u00a0 file with version 1.12.0 in Apache Hive, then read that file, returns the following error:\n\nCaused by: java.lang.IllegalStateException: All of the offsets in the split should be found in the file. expected: [4, 133961161] found: [BlockMetaData{1530100, 133961157 [ColumnMetaData{UNCOMPRESSED [c_customer_sk] optional int64 c_customer_sk  [PLAIN, RLE, BIT_PACKED], 4}, ColumnMetaData{UNCOMPRESSED [c_customer_id] optional binary c_customer_id (STRING)  [PLAIN, RLE, BIT_PACKED], 12243647}, ColumnMetaData{UNCOMPRESSED [c_current_cdemo_sk] optional int64 c_current_cdemo_sk  [PLAIN, RLE, BIT_PACKED], 42848491}, ColumnMetaData{UNCOMPRESSED [c_current_hdemo_sk] optional int64 c_current_hdemo_sk  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 54868535}, ColumnMetaData{UNCOMPRESSED [c_current_addr_sk] optional int64 c_current_addr_sk  [PLAIN, RLE, BIT_PACKED], 57421932}, ColumnMetaData{UNCOMPRESSED [c_first_shipto_date_sk] optional int64 c_first_shipto_date_sk  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 69694809}, ColumnMetaData{UNCOMPRESSED [c_first_sales_date_sk] optional int64 c_first_sales_date_sk  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 72093040}, ColumnMetaData{UNCOMPRESSED [c_salutation] optional binary c_salutation (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 74461508}, ColumnMetaData{UNCOMPRESSED [c_first_name] optional binary c_first_name (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 75092758}, ColumnMetaData{UNCOMPRESSED [c_last_name] optional binary c_last_name (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 77626525}, ColumnMetaData{UNCOMPRESSED [c_preferred_cust_flag] optional binary c_preferred_cust_flag (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 80116456}, ColumnMetaData{UNCOMPRESSED [c_birth_day] optional int32 c_birth_day  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 80505351}, ColumnMetaData{UNCOMPRESSED [c_birth_month] optional int32 c_birth_month  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 81581772}, ColumnMetaData{UNCOMPRESSED [c_birth_year] optional int32 c_birth_year  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 82473740}, ColumnMetaData{UNCOMPRESSED [c_birth_country] optional binary c_birth_country (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 83921564}, ColumnMetaData{UNCOMPRESSED [c_login] optional binary c_login (STRING)  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 85457674}, ColumnMetaData{UNCOMPRESSED [c_email_address] optional binary c_email_address (STRING)  [PLAIN, RLE, BIT_PACKED], 85460523}, ColumnMetaData{UNCOMPRESSED [c_last_review_date_sk] optional int64 c_last_review_date_sk  [RLE, PLAIN_DICTIONARY, BIT_PACKED], 132146109}]}]\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:172) ~[parquet-hadoop-bundle-1.12.0.jar:1.12.0]\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140) ~[parquet-hadoop-bundle-1.12.0.jar:1.12.0]\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:95) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:60) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:89) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:96) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_292]\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_292]\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_292]\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_292]\r\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:254) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:214) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:342) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:716) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:175) ~[hadoop-mapreduce-client-core-3.1.4.jar:?]\r\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:444) ~[hadoop-mapreduce-client-core-3.1.4.jar:?]\r\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349) ~[hadoop-mapreduce-client-core-3.1.4.jar:?]\r\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271) ~[hadoop-mapreduce-client-common-3.1.4.jar:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]\r\n\tat java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]\r\n\r\n\n\nRepoduce\u00a0Scenario:\nTPC-DS table\u00a0customer, any parquet file witten by 1.12.0 that larger than 128MB(two row groups).\n\n\r\ncreate  table if not exists customer(\r\n      c_customer_sk bigint\r\n,     c_customer_id char(16)\r\n,     c_current_cdemo_sk bigint\r\n,     c_current_hdemo_sk bigint\r\n,     c_current_addr_sk bigint\r\n,     c_first_shipto_date_sk bigint\r\n,     c_first_sales_date_sk bigint\r\n,     c_salutation char(10)\r\n,     c_first_name char(20)\r\n,     c_last_name char(30)\r\n,     c_preferred_cust_flag char(1)\r\n,     c_birth_day int\r\n,     c_birth_month int\r\n,     c_birth_year int\r\n,     c_birth_country varchar(20)\r\n,     c_login char(13)\r\n,     c_email_address char(50)\r\n,     c_last_review_date_sk bigint\r\n)\r\nstored as parquet location 'file:///home/username/data/customer';\r\n--after add file:\r\nselect count(*) from\u00a0customer;",
        "Issue Links": [
            "/jira/browse/SPARK-34276",
            "/jira/browse/SPARK-36696",
            "/jira/browse/SPARK-36726",
            "https://lists.apache.org/thread.html/rde7db8c9e824a7017453b7fb03c3f0653e63f3238bbd29c993967489%40%3Cdev.parquet.apache.org%3E"
        ]
    },
    "PARQUET-2079": {
        "Key": "PARQUET-2079",
        "Summary": "Blog on bulk insert sort modes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sivabalan narayanan",
        "Created": "27/Aug/21 03:38",
        "Updated": "27/Aug/21 03:38",
        "Resolved": "27/Aug/21 03:38",
        "Description": "Blog on bulk insert sort modes",
        "Issue Links": []
    },
    "PARQUET-2080": {
        "Key": "PARQUET-2080",
        "Summary": "Deprecate RowGroup.file_offset",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gabor Szadovszky",
        "Created": "30/Aug/21 13:19",
        "Updated": "28/Sep/21 11:05",
        "Resolved": null,
        "Description": "Due to PARQUET-2078 RowGroup.file_offset is not reliable.\nThis field is also wrongly calculated in the C++ oss parquet implementation PARQUET-2089",
        "Issue Links": []
    },
    "PARQUET-2081": {
        "Key": "PARQUET-2081",
        "Summary": "Encryption translation tool - Parquet-hadoop",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "30/Aug/21 19:13",
        "Updated": "16/May/23 19:30",
        "Resolved": "12/Apr/23 01:41",
        "Description": "This is the implement the core part of the Encryption translation tool in parquet-hadoop. After this, we will have another Jira/PR for parquet-cli to integrate with key tools for encryption properties..",
        "Issue Links": [
            "/jira/browse/PARQUET-2071",
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2082": {
        "Key": "PARQUET-2082",
        "Summary": "Encryption translation tool - Parquet-cli",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "30/Aug/21 19:16",
        "Updated": "30/Aug/21 19:19",
        "Resolved": null,
        "Description": "This is to implement the parquet-cli part of the encryption translation tool. It integrates with key tools to build the encryption properties, handle the parameters and call the parquet-hadoop API to encrypt.",
        "Issue Links": [
            "/jira/browse/PARQUET-2071"
        ]
    },
    "PARQUET-2083": {
        "Key": "PARQUET-2083",
        "Summary": "Expose getFieldPath from ColumnIO",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "31/Aug/21 19:21",
        "Updated": "14/Sep/21 10:01",
        "Resolved": "14/Sep/21 10:01",
        "Description": "Similar to PARUQET-2050, this exposes getFieldPath from ColumnIO so downstream apps such as Spark can use it to assemble nested records.",
        "Issue Links": []
    },
    "PARQUET-2084": {
        "Key": "PARQUET-2084",
        "Summary": "Upgrade Thrift to 0.14.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "01/Sep/21 18:50",
        "Updated": "14/Sep/21 15:37",
        "Resolved": "14/Sep/21 10:01",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2085": {
        "Key": "PARQUET-2085",
        "Summary": "Formatting is broken for description of BIT_PACKED",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Alex Ott",
        "Created": "05/Sep/21 09:03",
        "Updated": "15/Sep/21 06:09",
        "Resolved": null,
        "Description": "The Nested Encoding section of documentation doesn't escape the _ character, so it looks as following:\nTwo encodings for the levels are supported BIT_PACKED and RLE. Only RLE is now used as it supersedes BIT_PACKED.\ninstead of\nTwo encodings for the levels are supported BIT_PACKED and RLE. Only RLE is now used as it supersedes BIT_PACKED.",
        "Issue Links": []
    },
    "PARQUET-2086": {
        "Key": "PARQUET-2086",
        "Summary": "[C++] Incremental decoding not tested",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "cpp-5.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "08/Sep/21 14:11",
        "Updated": "08/Sep/21 14:11",
        "Resolved": null,
        "Description": "TypedDecoder::Decode and its various implementations seem to support incremental decoding (by passing a max_values smaller than the num_values given to SetData), however that capability seems for the most part untested (except for the BYTE_STREAM_SPLIT encoding).\nIt should be relatively easy to put up incremental decoding tests for all supported encodings. That might expose bugs in the current decode implementations as well.",
        "Issue Links": []
    },
    "PARQUET-2087": {
        "Key": "PARQUET-2087",
        "Summary": "Release parquet-mr 1.12.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "10/Sep/21 00:05",
        "Updated": "17/Sep/21 17:00",
        "Resolved": "17/Sep/21 17:00",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2088": {
        "Key": "PARQUET-2088",
        "Summary": "Different created_by field values for application and library",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "format-2.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Joshua Howard",
        "Created": "10/Sep/21 19:33",
        "Updated": "16/Sep/21 08:55",
        "Resolved": null,
        "Description": "There seems to be a discrepancy in the Parquet format created_by field regarding how it should be filled out. The parquet-mr library uses this value to enable/disable features based on the parquet-mr version here. Meanwhile, users are encouraged to make use of the application version here. It seems like there are multiple fields needed for an application and library version.",
        "Issue Links": []
    },
    "PARQUET-2089": {
        "Key": "PARQUET-2089",
        "Summary": "[C++] RowGroupMetaData file_offset set incorrectly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-6.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Archie Menzies",
        "Created": "11/Aug/21 15:52",
        "Updated": "22/Sep/21 07:04",
        "Resolved": "22/Sep/21 07:04",
        "Description": "It appears that the RowGroupMetaData file_offset property is being set to the same value as the first ColumnMetaData file_offset property in https://github.com/apache/arrow/blob/8e43f23dcc6a9e630516228f110c48b64d13cec6/cpp/src/parquet/metadata.cc#L1557-L1565\n\u00a0\nThis is not consistent with the definition of these properties given in the Thrift file: https://github.com/apache/arrow/blob/master/cpp/src/parquet/parquet.thrift\n\n\r\nstruct ColumnChunk {\r\n  ...\r\n\r\n  /** Byte offset in file_path to the ColumnMetaData **/\r\n  2: required i64 file_offset\r\n  \r\n  ...\r\n}\r\n\r\n...\r\n\r\nstruct RowGroup {\r\n  ...\r\n\r\n  /** Byte offset from beginning of file to first page (data or dictionary)\r\n   * in this row group **/\r\n  5: optional i64 file_offset\r\n\r\n  ...\r\n}\r\n\n\nThis is causing issues when trying to read the file with the parquet-mr libraries, because the RowGroup's file offset is used to determine whether a RowGroup exists within a given file split: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L1226-L1251\n\u00a0\nThis issue is therefore resulting in Parquet files which cannot be read as the metadata is incorrect.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/11149"
        ]
    },
    "PARQUET-2090": {
        "Key": "PARQUET-2090",
        "Summary": "[C++] Parquet writes incorrect file_offset",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Chao Sun",
        "Created": "08/Sep/21 22:45",
        "Updated": "14/Sep/21 05:52",
        "Resolved": "14/Sep/21 05:52",
        "Description": "Currently the Parquet writer sets file_offset in the following way (from metadata.cc)\n\n\r\n    if (dictionary_page_offset > 0) {\r\n      column_chunk_->meta_data.__set_dictionary_page_offset(dictionary_page_offset);\r\n      column_chunk_->__set_file_offset(dictionary_page_offset + compressed_size);\r\n    } else {\r\n      column_chunk_->__set_file_offset(data_page_offset + compressed_size);\r\n    }\n\nThis doesn't look correct, as it shouldn't take compressed_size into consideration.\nThe file_offset is used when filtering row groups, and the above could cause correctness issue. See SPARK-36696.",
        "Issue Links": []
    },
    "PARQUET-2091": {
        "Key": "PARQUET-2091",
        "Summary": "Fix release build error introduced by PARQUET-2043",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "14/Sep/21 04:36",
        "Updated": "27/Jan/22 15:44",
        "Resolved": "27/Jan/22 15:44",
        "Description": "After PARQUET-2043 when building for a release like 1.12.1, there is build error complaining 'used undeclared dependency'.",
        "Issue Links": []
    },
    "PARQUET-2092": {
        "Key": "PARQUET-2092",
        "Summary": "[Go] Fix in go implementation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthew Topol",
        "Created": "14/Sep/21 14:33",
        "Updated": "14/Sep/21 17:25",
        "Resolved": "14/Sep/21 17:25",
        "Description": "Same fix, but for the go implementation",
        "Issue Links": [
            "/jira/browse/ARROW-13996",
            "https://github.com/apache/arrow/pull/11153"
        ]
    },
    "PARQUET-2093": {
        "Key": "PARQUET-2093",
        "Summary": "Add rewriter version to Parquet footer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "20/Sep/21 19:39",
        "Updated": "20/Sep/21 19:39",
        "Resolved": null,
        "Description": "Parquet footer records the writer's version in the field of 'create-by'. As we introduce several rewrites, the new file is written partially by the rewriter. In this case, we need to record the rewriter's version also. \nSome questions (about a common rewriter) we need to answer before step forward:\nWhat would be the place of the rewriter versions? (New specific field or key-value metadata? Which key shall we use?)\nShall we somehow also save what the rewriter has done? How?\nAt what level shall we copy the original created_by field and what level shall we write the version of the rewriter to that field instead? (What different levels are possible?)\nFrom the introduction of this rewriter(s) field in case of any related writer version dependent fix we need to check this field as well and not only the created_by one.",
        "Issue Links": []
    },
    "PARQUET-2094": {
        "Key": "PARQUET-2094",
        "Summary": "Handle negative values in page headers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11.2,                                            1.12.2",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "22/Sep/21 08:13",
        "Updated": "20/Dec/21 10:12",
        "Resolved": "30/Sep/21 08:07",
        "Description": "There are integer values in the page headers that should be always positive (e.g. length). I am not sure if we properly handle the cases if they are not positive.",
        "Issue Links": []
    },
    "PARQUET-2095": {
        "Key": "PARQUET-2095",
        "Summary": "[C++] Read Parquet file with MapArray",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "cpp-4.0.0",
        "Fix Version/s": "cpp-6.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "jiang,longshan",
        "Created": "25/Sep/21 15:58",
        "Updated": "24/Oct/21 05:05",
        "Resolved": "24/Oct/21 05:05",
        "Description": "Parquet format can reduce storage space effectively, and we use the format with hdfs+Hive Jni(call c++)Spark Jni(call c), and it works well. Now we are starting a new project only use c+ language with higher performance expectation, but we meet a blocking issue on how to read the parquet file with MapArray such as\u00a0\nlist<array_element: map<string, list<array_element: int64>>>\nlist<array_element: map<string, string>>\nmap<string, list<array_element: int64>>\n\u00a0\nAnd I know how to\u00a0 work well only without map struct such as\u00a0\nlist<array_element: string>,\u00a0list<array_element: list<array_element: string>>\nHere is the code example, please give me some advice on how to read parquet file with map type, thanks a lot!\n\u00a0\n\n\r\n// code placeholder\r\n#include \"gflags/gflags.h\"\r\n#include \"arrow/api.h\"\r\n#include \"arrow/array/builder_base.h\"\r\n#include \"arrow/filesystem/hdfs.h\"\r\n#include \"arrow/io/api.h\"\r\n#include \"parquet/arrow/reader.h\"\r\n#include \"parquet/column_reader.h\"\r\n#include \"parquet/exception.h\"\r\n#include \"parquet/arrow/reader.h\"\r\n\r\nint main(int argc, char** argv) {\r\n\u00a0 \u00a0 gflags::ParseCommandLineFlags(&argc, &argv, true);\r\n\u00a0 \u00a0 arrow::Status st;\r\n\u00a0 \u00a0 arrow::MemoryPool* pool = ::arrow::default_memory_pool();\r\n\u00a0 \u00a0 std::shared_ptr<arrow::io::RandomAccessFile> input = nullptr;\r\n\u00a0 \u00a0 std::shared_ptr<::arrow::io::RandomAccessFile> _infile;\r\n\u00a0 \u00a0 PARQUET_ASSIGN_OR_THROW(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 _infile,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ::arrow::io::ReadableFile::Open(FLAGS_input_file,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ::arrow::default_memory_pool()));\r\n\u00a0 \u00a0 // Open Parquet file reader\r\n\u00a0 \u00a0 std::unique_ptr<parquet::arrow::FileReader> arrow_reader;\r\n\u00a0 \u00a0 st = parquet::arrow::OpenFile(_infile, pool, &arrow_reader);\r\n\u00a0 \u00a0 if (!st.ok()) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 LOG(ERROR) << \"open file failed \" << FLAGS_input_file;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return 0;\r\n\u00a0 \u00a0 }\r\n\r\n\r\n\u00a0 \u00a0 // Read entire file as a single Arrow table\r\n\u00a0 \u00a0 std::shared_ptr<arrow::Table> table;\r\n\u00a0 \u00a0 st = arrow_reader->ReadTable(&table);\r\n\u00a0 \u00a0 if (!st.ok()) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 LOG(INFO) << \"read file to table successfully \" << FLAGS_input_file;\r\n\u00a0 \u00a0 }\u00a0 \u00a0\r\n     \r\n    size_t num_cols = table->num_columns();\r\n\u00a0 \u00a0 for (size_t idx = 0; idx < num_cols; idx++) {\r\n\u00a0 \u00a0 \u00a0 auto this_field = table->field(idx);\r\n\u00a0 \u00a0 \u00a0 auto this_column = table->column(idx);\r\n    if (this_field->name() == \"lls_column\") { // works well type: list<array_element: list<array_element: string>>\r\n\u00a0 \u00a0 \u00a0 \u00a0 for (size_t c_idx = 0; c_idx < this_column->num_chunks(); c_idx++) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto row_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::ListArray>(this_column->chunk(c_idx));\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto sample_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::ListArray>(row_array->values());\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto id_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::StringArray>(sample_array->values());\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (int64_t i = 0; i < table->num_rows(); i++) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto offset = row_array->value_offset(i);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto count = row_array->value_length(i);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (auto x = 0; x < count; x++) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::vector<std::string> result;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto sample_offset = sample_array->value_offset(offset+x);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto id_count = sample_array->value_length(offset+x);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (auto id = 0; id < id_count; id++) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 int32_t len;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const uint8_t* addr = id_array->GetValue(sample_offset + id, &len);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 result.push_back(std::string(reinterpret_cast<const char*>(addr), (int16_t)len));\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 LOG(INFO) << \"LLS \" << count << \" \" << this_field->name() << \" \" << to_string(result); // works well\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 }\r\n      else if (this_field->name() == \"ms2li_column\") { // MS2LI type: map<string, list<array_element: int64>>\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOG(INFO)\u00a0 << \"col name: \" << this_field->name() << \" type: \" << this_field->type()->ToString();\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOG(INFO)\u00a0 << \"length: \" << this_column->length() << \" chunk num: \" << this_column->num_chunks();\r\n\u00a0 \u00a0 \u00a0 \u00a0 for (size_t c_idx = 0; c_idx < this_column->num_chunks(); c_idx++) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto row_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::MapArray>(this_column->chunk(c_idx));\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto keys_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::StringArray>(row_array->keys());\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto item_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::ListArray>(row_array->items());\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto item_value_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::ListArray>(item_array->values());\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 auto id_array =\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 std::static_pointer_cast<arrow::Int64Array>(item_value_array->values());\r\n        // I've no idea how to traverse the map<string, list<array_element: int64>> to get key and value correctly, \r\n       }\r\n      }\r\n    }\r\n\r\n\n\nIt seems that arrow::MayArray :: keys() and items() lose each map pair's offset, and cannot find the right pair in\u00a0list<array_element: map<string, string>> format. Really need and appreciate your help.",
        "Issue Links": []
    },
    "PARQUET-2096": {
        "Key": "PARQUET-2096",
        "Summary": "Upgrade Thrift to 0.15.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Vinoo Ganesh",
        "Reporter": "Vinoo Ganesh",
        "Created": "27/Sep/21 02:29",
        "Updated": "28/Sep/21 20:19",
        "Resolved": "28/Sep/21 14:47",
        "Description": "Thrift 0.15.0 is currently the default in brew: https://github.com/Homebrew/homebrew-core/blob/82d03f657371e1541a9a5e5de57c5e1aa00acd45/Formula/thrift.rb#L4.",
        "Issue Links": []
    },
    "PARQUET-2097": {
        "Key": "PARQUET-2097",
        "Summary": "Clarified what is compressed in pages",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jorge Leit\u00e3o",
        "Created": "28/Sep/21 05:39",
        "Updated": "28/Sep/21 05:39",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2098": {
        "Key": "PARQUET-2098",
        "Summary": "Add more methods into interface of BlockCipher",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "30/Sep/21 02:08",
        "Updated": "26/Apr/22 13:09",
        "Resolved": null,
        "Description": "Currently BlockCipher interface has methods without letting caller to specify length/offset. In some use cases like Presto,  it is needed to pass in a byte array and the data to be encrypted only occupys partially of the array.  So we need to add a new methods something like below for decrypt. Similar methods might be needed for encrypt. \nbyte[] decrypt(byte[] ciphertext, int cipherTextOffset, int cipherTextLength, byte[] aad);",
        "Issue Links": []
    },
    "PARQUET-2099": {
        "Key": "PARQUET-2099",
        "Summary": "[C++] Statistics::num_values() is misleading",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-13.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "Micah Kornfield",
        "Created": "30/Sep/21 19:53",
        "Updated": "11/May/23 01:22",
        "Resolved": null,
        "Description": "num_values() in statistics seems to capture the number of encoded values.  This is misleading as everyplace else in parquet num_values() really indicates all values (null and not-null, i.e. the number of levels).  \nWe should likely remove this field, rename it or at the very least update the documentation.\nCC zeroshade",
        "Issue Links": []
    },
    "PARQUET-2100": {
        "Key": "PARQUET-2100",
        "Summary": "Merging two valid parquet files produces a corrupted result file in 1.12.1",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.1,                                            1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Matthew M",
        "Created": "07/Oct/21 09:57",
        "Updated": "19/Oct/21 09:27",
        "Resolved": null,
        "Description": "This ticket relates to PARQUET-2027. In the previous ticket for two parquet files produced by 1.11.x merging was failing in 1.12.0. For 1.12.1 merging was fixed, i. e. it doesn't fail. But in the same time it results with a corrupted output file. The error:\n\n\r\nDictionary page must be before data page.\r\n\n\nis thrown when one tries to read it. It comes from this https://github.com/apache/parquet-cpp/blob/master/src/parquet/arrow/record_reader.cc#L712.\nI attached two example input files and the outcome of merging.",
        "Issue Links": []
    },
    "PARQUET-2101": {
        "Key": "PARQUET-2101",
        "Summary": "Fix wrong descriptions about the default block size",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-avro,                                            parquet-mr,                                            parquet-protobuf",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "01/Nov/21 05:39",
        "Updated": "12/Dec/22 09:20",
        "Resolved": "02/Nov/21 09:44",
        "Description": "https://github.com/apache/parquet-mr/blob/master/parquet-avro/src/main/java/org/apache/parquet/avro/AvroParquetWriter.java#L90\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java#L240\nhttps://github.com/apache/parquet-mr/blob/master/parquet-protobuf/src/main/java/org/apache/parquet/proto/ProtoParquetWriter.java#L80\nThese javadocs say the default block size is 50 MB but it's actually 128MB.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2102": {
        "Key": "PARQUET-2102",
        "Summary": "Typo in ColumnIndexBase toString",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Ryan Rupp",
        "Reporter": "Ryan Rupp",
        "Created": "11/Nov/21 05:26",
        "Updated": "19/May/22 06:09",
        "Resolved": "11/Nov/21 08:13",
        "Description": "Trivial thing but noticed here since ColumnIndexBase.toString() was used in a wrapped exception message - \"boundary\" has a typo (boudary).",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2103": {
        "Key": "PARQUET-2103",
        "Summary": "crypto exception in print toPrettyJSON",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.0,                                            1.12.1,                                            1.12.2,                                            1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "16/Nov/21 07:59",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "26/Mar/23 05:49",
        "Description": "In debug mode, this code\u00a0\nif (LOG.isDebugEnabled()) {\n\u00a0 LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n}\ncalled in\u00a0org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata()\n\u00a0\nin encrypted files with plaintext footer\u00a0\ntriggers an exception:\n\u00a0\n{{Caused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [id]. Null File Decryptor \u00a0 \u00a0 }}\n\u00a0 \u00a0 at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:602) ~[parquet-hadoop-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at org.apache.parquet.hadoop.metadata.ColumnChunkMetaData.getEncodingStats(ColumnChunkMetaData.java:353) ~[parquet-hadoop-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n\u00a0 \u00a0 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n\u00a0 \u00a0 at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n\u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:689) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:755) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:728) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:755) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:728) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:755) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:480) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:319) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ObjectWriter$Prefetch.serialize(ObjectWriter.java:1516) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ObjectWriter._writeValueAndClose(ObjectWriter.java:1217) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at shaded.parquet.com.fasterxml.jackson.databind.ObjectWriter.writeValue(ObjectWriter.java:1059) ~[parquet-jackson-1.12.0jar:1.12.0]\n\u00a0 \u00a0 at org.apache.parquet.hadoop.metadata.ParquetMetadata.toJSON(ParquetMetadata.java:68) ~[parquet-hadoop-1.12.0jar:1.12.0]\n\u00a0 \u00a0 ... 23 more",
        "Issue Links": []
    },
    "PARQUET-2104": {
        "Key": "PARQUET-2104",
        "Summary": "parquet-cli broken in master",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Balaji K",
        "Created": "23/Nov/21 20:11",
        "Updated": "26/Apr/22 13:20",
        "Resolved": null,
        "Description": "Creating a Jira per this thread:\nhttps://lists.apache.org/thread/k233838g010lvbp81s99floqjmm7nnvs\n\nclone parquet-mr and build the repo locally\nrun parquet-cli without Hadoop (according to this ReadMe <https://github.com/apache/parquet-mr/tree/master/parquet-cli#running-without-hadoop> )\ntry a command that deserializes data such as cat or head\nobserve NoSuchMethodError being thrown\n\nError stack: ~/repos/parquet-mr/parquet-cli$ parquet cat ../../testdata/dictionaryEncodingSample.parquet WARNING: An illegal reflective access operation has occurred ...<trimmed some more WARNs>... Exception in thread \"main\" java.lang.NoSuchMethodError: 'org.apache.avro.Schema org.apache.parquet.avro.AvroSchemaConverter.convert(org.apache.parquet.schema.MessageType)' at org.apache.parquet.cli.util.Schemas.fromParquet(Schemas.java:89) at org.apache.parquet.cli.BaseCommand.getAvroSchema(BaseCommand.java:405) at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:66) at org.apache.parquet.cli.Main.run(Main.java:157) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.parquet.cli.Main.main(Main.java:187)",
        "Issue Links": []
    },
    "PARQUET-2105": {
        "Key": "PARQUET-2105",
        "Summary": "Refactor the test code of creating the test file",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "01/Dec/21 00:44",
        "Updated": "19/May/22 06:05",
        "Resolved": "14/Jan/22 21:32",
        "Description": "In the tests, there are many places that need to create a test parquet file with different settings. Currently, each test file just creates its own code. It would be better to have a test file builder to create that.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2106": {
        "Key": "PARQUET-2106",
        "Summary": "BinaryComparator should avoid doing ByteBuffer.wrap in the hot-path",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Alexey Kudinkin",
        "Reporter": "Alexey Kudinkin",
        "Created": "03/Dec/21 23:28",
        "Updated": "19/May/22 06:06",
        "Resolved": "09/Dec/21 09:31",
        "Description": "Background\nWhile writing out large Parquet tables using Spark, we've noticed that BinaryComparator is the source of substantial churn of extremely short-lived `HeapByteBuffer` objects \u2013 It's taking up to 16% of total amount of allocations in our benchmarks, putting substantial pressure on a Garbage Collector:\n\nprofile_48449_alloc_1638494450_sort_by.html\n\u00a0\nProposal\nWe're proposing to adjust lexicographical comparison (at least) to avoid doing any allocations, since this code lies on the hot-path of every Parquet write, therefore causing substantial churn amplification.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145",
            "/jira/browse/HUDI-2948"
        ]
    },
    "PARQUET-2107": {
        "Key": "PARQUET-2107",
        "Summary": "Travis failures",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "07/Dec/21 09:57",
        "Updated": "19/May/22 06:07",
        "Resolved": "08/Dec/21 09:07",
        "Description": "There are Travis failures since a while in our PRs. See e.g. https://app.travis-ci.com/github/apache/parquet-mr/jobs/550598285 or https://app.travis-ci.com/github/apache/parquet-mr/jobs/550598286",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2108": {
        "Key": "PARQUET-2108",
        "Summary": "Specification for RLEDictionary encoding is incorrect.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Balaji K",
        "Created": "10/Dec/21 20:53",
        "Updated": "11/Dec/21 00:37",
        "Resolved": null,
        "Description": "The spec for RLE Dictionary encoding says the \"length of the encoded-data\" is placed before the \"encoded-data\". Reproducing the first 3 lines here:\n```\nrle-bit-packed-hybrid: <length> <encoded-data>\nlength := length of the <encoded-data> in bytes stored as 4 bytes little endian (unsigned int32)\nencoded-data := <run>*\n```\nHowever, this is not true. Parquet-MR implementation does not encode the length in front of the data. It encodes bitWidth as 1 byte. See implementation.\nI'm proposing the spec be updated to state the above clearly.\nsee discussion here:\nhttps://lists.apache.org/thread/p45tpjd5r03qbswtpr7xfy072josnjxs",
        "Issue Links": []
    },
    "PARQUET-2109": {
        "Key": "PARQUET-2109",
        "Summary": "Parquet Cpp Reader Can Loop Forever If Page Values Overstated",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-7.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "17/Dec/21 00:24",
        "Updated": "14/Jan/22 07:51",
        "Resolved": "13/Jan/22 12:55",
        "Description": "If the page header states that there are more values than are actually present in the page, the Parquet CPP can loop forever.\u00a0 This is because HasNext() will return true but the actual ReadBatch() will have nothing to read and will not change reader state, causing an infinite loop. We first noticed the bug via ScanFileContents(), but this impacts any code that does not check to see if ReadBatch() consumed anything.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/11984"
        ]
    },
    "PARQUET-2110": {
        "Key": "PARQUET-2110",
        "Summary": "Fix Typos in LogicalTypes.md",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "jincongho",
        "Reporter": "jincongho",
        "Created": "18/Jan/22 14:43",
        "Updated": "21/Jan/22 15:07",
        "Resolved": "19/Jan/22 23:43",
        "Description": "interpertations -> interpretations\nregadless -> regardless\nunambigously -> unambiguously",
        "Issue Links": []
    },
    "PARQUET-2111": {
        "Key": "PARQUET-2111",
        "Summary": "Support limit push down and stop early for RecordReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Jackey Lee",
        "Created": "20/Jan/22 12:53",
        "Updated": "21/Jan/22 15:08",
        "Resolved": "21/Jan/22 03:43",
        "Description": "With limit push down, it can stop scanning parquet early, and reduce network and disk IO.",
        "Issue Links": []
    },
    "PARQUET-2112": {
        "Key": "PARQUET-2112",
        "Summary": "Fix typo in MessageColumnIO",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "22/Jan/22 15:11",
        "Updated": "19/May/22 06:04",
        "Resolved": "27/Jan/22 15:39",
        "Description": "Typo of the variable 'BitSet vistedIndexes'. Change it to 'visitedIndexes'",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2113": {
        "Key": "PARQUET-2113",
        "Summary": "Build failure with specified thrift and mvn command",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "yikaifei",
        "Created": "26/Jan/22 08:18",
        "Updated": "12/Apr/23 01:48",
        "Resolved": null,
        "Description": "Now, I git clone parquet-mr project with latest version and i build on branch master, i met the build failure with thrift version and mvn command specified as describe in README.md, env as follows:\n\u00a0\nyikaifei@***** parquet-mr % mvn -v\nApache Maven 3.8.3 (ff8e977a158738155dc465c6a97ffaf31982d739)\nMaven home: /Users/yikaifei/maven/apache-maven-3.8.3\nJava version: 1.8.0_192, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_192.jdk/Contents/Home/jre\nDefault locale: zh_CN, platform encoding: UTF-8\nOS name: \"mac os x\", version: \"10.16\", arch: \"x86_64\", family: \"mac\"\nyikaifei@***** parquet-mr % thrift -version\nThrift version 0.15.0\n\u00a0\nerrorMsg as follow:\n\nAny one help me for solving this issue? Thanks so much",
        "Issue Links": []
    },
    "PARQUET-2114": {
        "Key": "PARQUET-2114",
        "Summary": "Eliminate unnecessary warning when compile",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "yikaifei",
        "Created": "26/Jan/22 10:56",
        "Updated": "12/Apr/23 01:45",
        "Resolved": null,
        "Description": "Currently, there are some warning information which is unnecessary,\u00a0They are output due to shade\uff0cthis pr aims to eliminate them, warning information as follows:",
        "Issue Links": []
    },
    "PARQUET-2115": {
        "Key": "PARQUET-2115",
        "Summary": "Parquet Cpp Crash on Invalid Dictionary Bit Width",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "26/Jan/22 22:31",
        "Updated": "31/Jan/22 18:45",
        "Resolved": "31/Jan/22 10:18",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/73",
            "https://github.com/apache/arrow/pull/12274"
        ]
    },
    "PARQUET-2116": {
        "Key": "PARQUET-2116",
        "Summary": "Cell Level Encryption",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "27/Jan/22 15:48",
        "Updated": "13/Mar/22 03:03",
        "Resolved": null,
        "Description": "Cell level encryption can do finer-grained encryption than modular encryption(Parquet-1178) or file encryption. The idea is only some fields inside the column are encrypted based on a filter expression. For example, a table with column a, b, c.x, c.y, d, we can encrypt column a, c.x where d == 5 and c.y > 0.",
        "Issue Links": []
    },
    "PARQUET-2117": {
        "Key": "PARQUET-2117",
        "Summary": "Add rowPosition API in parquet record readers",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Prakhar Jain",
        "Created": "01/Feb/22 19:05",
        "Updated": "20/Jun/22 10:56",
        "Resolved": "08/Jun/22 18:16",
        "Description": "Currently the parquet-mr RecordReader/ParquetFileReader exposes API\u2019s to read parquet file in columnar fashion or record-by-record.\nIt will be great to extend them to also support rowPosition API which can tell the position of the current record in the parquet file.\nThe rowPosition can be used as a unique row identifier to mark a row. This can be useful to create an index (e.g. B+ tree) over a parquet file/parquet table (e.g.\u00a0 Spark/Hive).\nThere are multiple projects in the parquet eco-system which can benefit from such a functionality:\u00a0\n\nApache Iceberg needs this functionality. It has this implementation already as it relies on low level parquet APIs -\u00a0 Link1, Link2\nApache Spark can use this functionality - SPARK-37980",
        "Issue Links": [
            "/jira/browse/PARQUET-2145",
            "/jira/browse/PARQUET-2161",
            "/jira/browse/SPARK-37980"
        ]
    },
    "PARQUET-2118": {
        "Key": "PARQUET-2118",
        "Summary": "[C++] thift_internal.h assumes shared_ptr type in some cases",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "06/Feb/22 18:39",
        "Updated": "07/Feb/22 21:52",
        "Resolved": "07/Feb/22 17:23",
        "Description": "Thrift can still be built with boost shared_ptrs so we need to be pointer agnostic.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/12349"
        ]
    },
    "PARQUET-2119": {
        "Key": "PARQUET-2119",
        "Summary": "Parquet CPP DeltaBitPackDecoder Check Failure",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "07/Feb/22 23:40",
        "Updated": "09/Feb/22 22:52",
        "Resolved": "08/Feb/22 18:29",
        "Description": "DeltaBitPackDecoder uses num_values_ instead of total_value_count_ when computing batch size.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/75",
            "https://github.com/apache/arrow/pull/12365"
        ]
    },
    "PARQUET-2120": {
        "Key": "PARQUET-2120",
        "Summary": "parquet-cli dictionary command fails on pages without dictionary encoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Willi Raschkowski",
        "Created": "12/Feb/22 15:00",
        "Updated": "21/Jun/22 11:13",
        "Resolved": "21/Jun/22 11:13",
        "Description": "parquet-cli's dictionary\u00a0command fails\u00a0with an NPE if a page does not have dictionary encoding:\n\n\r\n$ parquet dictionary --column col a-b-c.snappy.parquet                \r\nUnknown error\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.parquet.column.page.DictionaryPage.getEncoding()\" because \"page\" is null\r\n\tat org.apache.parquet.cli.commands.ShowDictionaryCommand.run(ShowDictionaryCommand.java:78)\r\n\tat org.apache.parquet.cli.Main.run(Main.java:155)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.parquet.cli.Main.main(Main.java:185)\r\n\r\n$ parquet meta a-b-c.snappy.parquet      \r\n...\r\nRow group 0:  count: 1  46.00 B records  start: 4  total: 46 B\r\n--------------------------------------------------------------------------------\r\n     type      encodings count     avg size   nulls   min / max\r\ncol  BINARY    S   _     1         46.00 B    0       \"a\" / \"a\"\r\n\r\nRow group 1:  count: 200  0.34 B records  start: 50  total: 69 B\r\n--------------------------------------------------------------------------------\r\n     type      encodings count     avg size   nulls   min / max\r\ncol  BINARY    S _ R     200       0.34 B     0       \"b\" / \"c\"\r\n\n\n(Note the missing R / dictionary encoding on that first page.)\nSomeone familiar with Parquet might guess from the NPE that there's no dictionary encoding. But for files that mix pages with and without dictionary encoding (like above), the command will fail before getting to pages that actually have dictionaries.\nThe problem is that this line assumes readDictionaryPage always returns a page and doesn't handle when it does not, i.e. when it returns null.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2121": {
        "Key": "PARQUET-2121",
        "Summary": "Remove descriptions for the removed modules",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "14/Feb/22 03:28",
        "Updated": "26/Aug/22 04:16",
        "Resolved": "26/Aug/22 04:16",
        "Description": "PARQUET-2020 removed some deprecated modules, but the related descriptions still remain in some documents. They should be removed since their existence is misleading.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2122": {
        "Key": "PARQUET-2122",
        "Summary": "Adding Bloom filter to small Parquet file bloats in size X1700",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Ze'ev Maor",
        "Created": "14/Feb/22 16:19",
        "Updated": "10/May/22 04:13",
        "Resolved": null,
        "Description": "Converting a small, 14 rows/1 string column csv file to Parquet without bloom filter yields a 600B file, adding '.withBloomFilterEnabled(true)' to ParquetWriter then yields a 1049197B file.\nIt isn't clear what the extra space is used by.\nAttached csv and bloated Parquet files.",
        "Issue Links": []
    },
    "PARQUET-2123": {
        "Key": "PARQUET-2123",
        "Summary": "Invalid memory access in ScanFileContents",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "14/Feb/22 18:27",
        "Updated": "16/Feb/22 10:01",
        "Resolved": "15/Feb/22 17:19",
        "Description": "When a Parquet file has 0 columns, ScanFileContents will try to access the 0th element of a size 0 vector.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/12423"
        ]
    },
    "PARQUET-2124": {
        "Key": "PARQUET-2124",
        "Summary": "Bad DCHECK For Intermixed Dictionary Encoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "14/Feb/22 23:57",
        "Updated": "04/Mar/22 23:54",
        "Resolved": "15/Feb/22 17:45",
        "Description": "Parquet CPP has a DCHECK for a dictionary encoded page coming after a non-dictionary encoded page. This is bad because the DCHECK can be triggered by Parquet files that have a column that has a dictionary page, then a non-dictionary encoded page, then a page of dictionary encoded values(indices). Fuzzing found such a file. While this could be turned into an exception, I don't see anything in the Parquet specification that prohibits such an occurrence of pages.\nThis situation has brought up on the mailing list before(https://lists.apache.org/thread/3bzymmbxvmzj12km7cjz1150ndvy9bos) and it seems like this is valid but nobody is doing it.\nIn the PR that added this check(https://github.com/apache/parquet-cpp/pull/73) it was noted that the check is probably not needed.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/76",
            "https://github.com/apache/arrow/pull/12427"
        ]
    },
    "PARQUET-2125": {
        "Key": "PARQUET-2125",
        "Summary": "ParquetFileReader has a currentBlock information in a private field",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Tanuja Dubey",
        "Created": "15/Feb/22 03:59",
        "Updated": "22/Apr/22 22:05",
        "Resolved": null,
        "Description": "The currentBlock variable is a metric information which can be useful to know which block the current record is being read from. If this variable has a getter, it would be possible to skip over a certain blocks.",
        "Issue Links": []
    },
    "PARQUET-2126": {
        "Key": "PARQUET-2126",
        "Summary": "Thread safety bug in CodecFactory",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "James Turton",
        "Created": "17/Feb/22 12:11",
        "Updated": "10/Nov/22 07:42",
        "Resolved": null,
        "Description": "The code for returning Compressor objects to the caller goes to some lengths to achieve thread safety, including keeping Codec objects in an Apache Commons pool that has thread-safe borrow semantics.\u00a0 This is all undone by the BytesCompressor and BytesDecompressor Maps in org.apache.parquet.hadoop.CodecFactory which end up caching single compressor and decompressor instances due to code in CodecFactory@getCompressor and CodecFactory@getDecompressor.\u00a0 When the caller runs multiple threads, those threads end up sharing compressor and decompressor instances.\nFor compressors based on Xerial Snappy this bug has no effect because that library is itself thread safe.\u00a0 But when BuiltInGzipCompressor from Hadoop is selected for the CompressionCodecName.GZIP case, serious problems ensue.\u00a0 That class is not thread safe and sharing one instance of it between threads produces both silent data corruption and JVM crashes.\nTo fix this situation, parquet-mr should stop caching single compressor and decompressor instances.",
        "Issue Links": [
            "/jira/browse/DRILL-8139"
        ]
    },
    "PARQUET-2127": {
        "Key": "PARQUET-2127",
        "Summary": "Security risk in latest parquet-jackson-1.12.2.jar",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "phoebe chen",
        "Created": "17/Feb/22 20:30",
        "Updated": "12/Apr/23 01:34",
        "Resolved": "12/Apr/23 01:34",
        "Description": "Embed jackson-databind:2.11.4 has security risk of Possible DoS if using JDK serialization to serialize JsonNode (https://github.com/FasterXML/jackson-databind/issues/3328 ), upgrade to 2.13.1 can fix this.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2128": {
        "Key": "PARQUET-2128",
        "Summary": "Bump Thrift to 0.16.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Vinoo Ganesh",
        "Reporter": "Vinoo Ganesh",
        "Created": "20/Feb/22 20:59",
        "Updated": "14/Jun/23 12:24",
        "Resolved": "08/Mar/22 22:29",
        "Description": "Thrift 0.16.0 has been released https://github.com/apache/thrift/releases/tag/v0.16.0",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2129": {
        "Key": "PARQUET-2129",
        "Summary": "Add uncompressedSize to \"meta\" output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "None",
        "Assignee": "Vinoo Ganesh",
        "Reporter": "Vinoo Ganesh",
        "Created": "20/Feb/22 21:07",
        "Updated": "19/May/22 06:01",
        "Resolved": "08/Mar/22 22:30",
        "Description": "The `uncompressedSize` is currently not printed in the output of the parquet meta command. This PR adds the uncompressedSize in to the output.\u00a0\nThis was also reported by Deepak Gangwar.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2130": {
        "Key": "PARQUET-2130",
        "Summary": "Crash on non-standard map key name in debug",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "22/Feb/22 20:10",
        "Updated": "05/Mar/22 14:11",
        "Resolved": "04/Mar/22 21:48",
        "Description": "Reading a parquet file to an Arrow table with a non-standard map key field name in debug will trigger DCHECK. This is similar to ARROW-13735 but with Parquet files.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/77",
            "https://github.com/apache/arrow/pull/12488"
        ]
    },
    "PARQUET-2131": {
        "Key": "PARQUET-2131",
        "Summary": "Number values decoded DCHECKs should be exceptions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-8.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "22/Feb/22 22:16",
        "Updated": "05/Mar/22 19:31",
        "Resolved": "04/Mar/22 23:48",
        "Description": "As discussed on some other bugs, there are some parquet-cpp DCHECKs on the number of values decoded that really should be exceptions. When invalid Parquet files are read, it is possible for the decoders to return less values than expected and this should be signaled back to the user even in non-debug mode and it should not be a crash in debug mode. A fuzzer I have been running managed to create two examples that crashed parquet-cpp due to the DCHECKs. These don't currently crash the non-arrow cpp interfaces for reading parquet but we might consider making those methods also throw exceptions in these situations.",
        "Issue Links": [
            "https://github.com/apache/arrow-testing/pull/78",
            "https://github.com/apache/arrow/pull/12490"
        ]
    },
    "PARQUET-2132": {
        "Key": "PARQUET-2132",
        "Summary": "Support Quantile Compression q_compress column codec",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp,                                            parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Martin Loncaric",
        "Created": "28/Feb/22 21:19",
        "Updated": "28/Feb/22 21:21",
        "Resolved": null,
        "Description": "Quantile Compression (https://github.com/mwlon/quantile-compression) is a recent but stable compression algorithm for numerical sequences that averages 35%+ higher compression ratio than the next best codec (zstd), given the same compression time. It has fairly fast decompression speed, close to that of zstd. Compared to Parquet's built-in PFor-like integer compression algorithm, it achieves a much higher compression ratio at slower speed. Adding q_compress as a column codec for all numerical columns could substantially reduce the size of most Parquet files.\nq_compress is implemented in Rust, which has good interop with C++ and can run in JVM via JNI (e.g. https://github.com/pancake-db/pancake-scala-client).",
        "Issue Links": []
    },
    "PARQUET-2133": {
        "Key": "PARQUET-2133",
        "Summary": "Support Int8 and Int16 as basic type",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Jackey Lee",
        "Created": "01/Mar/22 04:03",
        "Updated": "08/Apr/22 17:40",
        "Resolved": null,
        "Description": "Int8 and Int16 are not supported as basic in previos version. Using 4 bytes to store int8 seems not a good idea, which means requiring more storage and read and write very slow. Besides, it is not friendly with regular computing format, such as velox, arrow, vector and so on.\nWith Int8 and Int16 supported, we can get less storage and better performance on reading and writing. As for forward compatible, we can use version in FileMetaData to choose how to read parquet data.",
        "Issue Links": []
    },
    "PARQUET-2134": {
        "Key": "PARQUET-2134",
        "Summary": "Incorrect type checking in HadoopStreams.wrap",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8.3,                                            1.10.1,                                            1.11.2,                                            1.12.2",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Todd Gao",
        "Created": "09/Mar/22 03:11",
        "Updated": "13/Apr/23 23:47",
        "Resolved": "26/Mar/23 06:12",
        "Description": "The method HadoopStreams.wrap wraps an FSDataInputStream to a SeekableInputStream. \nIt checks whether the underlying stream of the passed  FSDataInputStream implements ByteBufferReadable: if true, wraps the FSDataInputStream to H2SeekableInputStream; otherwise, wraps to H1SeekableInputStream.\nIn some cases, we may add another wrapper over FSDataInputStream. For example, \n\n\r\nclass CustomDataInputStream extends FSDataInputStream {\r\n    public CustomDataInputStream(FSDataInputStream original) {\r\n        super(original);\r\n    }\r\n}\r\n\n\nWhen we create an FSDataInputStream, whose underlying stream does not implements ByteBufferReadable, and then creates a CustomDataInputStream with it. If we use HadoopStreams.wrap to create a SeekableInputStream, we may get an error like \njava.lang.UnsupportedOperationException: Byte-buffer read unsupported by input stream\nWe can fix this by taking recursive checks over the underlying stream of FSDataInputStream.",
        "Issue Links": [
            "/jira/browse/PARQUET-2151",
            "/jira/browse/HADOOP-18336"
        ]
    },
    "PARQUET-2135": {
        "Key": "PARQUET-2135",
        "Summary": "Performance optimizations: Merged all LittleEndianDataInputStream functionality into ByteBufferInputStream",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Timothy Miller",
        "Created": "01/Apr/22 17:08",
        "Updated": "05/Apr/22 20:02",
        "Resolved": null,
        "Description": "This PR is all performance optimization. In benchmarking with Trino, we find query performance to improve from 5% to 15%, depending on the query, and that includes all the I/O time from S3.\nThe main modification is to merge all of LittleEndianDataInputStream functionality into ByteBufferInputStream, which yields the following benefits:\n\nElimination of extra layers of abstraction and method call overhead\nEnable the use of intrinsics for readInt, readLong, etc.\nAvailability of faster access methods like readFully and skipFully, without the need for helper functions\nReduces some object creation in the performance critical path\n\nThis also includes and enables performance optimizations to:\n\nByteBitPackingValuesReader\nPlainValuesReader\nRunLengthBitPackingHybridDecoder\n\nContext:\nI've been working on improving Parquet reading performance in Trino, mostly by profiling while running performance benchmarks and TPCDS queries. This PR is a subset of the changes I made that have more than doubled the performance of a lot of TPCDS queries (wall clock time, including the S3 access time). If you are kind enough to accept these changes, I have more I would like to contribute.",
        "Issue Links": []
    },
    "PARQUET-2136": {
        "Key": "PARQUET-2136",
        "Summary": "File writer construction with encryptor",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "04/Apr/22 10:44",
        "Updated": "28/Jul/22 08:16",
        "Resolved": "28/Jul/22 08:16",
        "Description": "Currently, a file writer object can be constructed with encryption properties. We need an additional constructor, that can accept an encryptor instead, in order to support lazy materialization of parquet file writers.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2137": {
        "Key": "PARQUET-2137",
        "Summary": "Show Dictionary command throws NPE when dictionary doen't exist",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "EdisonWang",
        "Created": "19/Apr/22 03:51",
        "Updated": "19/Apr/22 04:07",
        "Resolved": "19/Apr/22 04:07",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2138": {
        "Key": "PARQUET-2138",
        "Summary": "Add ShowBloomFilterCommand to parquet-cli",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "EdisonWang",
        "Created": "19/Apr/22 06:56",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "26/Mar/23 06:14",
        "Description": "Add ShowBloomFilterCommand to parquet-cli, which can check whether given values of a column match bloom filter",
        "Issue Links": []
    },
    "PARQUET-2139": {
        "Key": "PARQUET-2139",
        "Summary": "Bogus file offset for ColumnMetaData written to ColumnChunk metadata of single parquet files",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Timothy Miller",
        "Created": "20/Apr/22 19:24",
        "Updated": "20/Apr/22 20:37",
        "Resolved": null,
        "Description": "In an effort to understand the parquet format better, I've so far written my own Thrift parser, and upon examining the output, I noticed something peculiar.\nTo begin with, check out the definition for ColumnChunk here: https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift\nYou'll notice that if there's an element 2 in the struct, this is supposed to be a file offset to where a redundant copy of the ColumnMetaData.\nNext, have a look at the file called \"modified.parquet\" attached to https://issues.apache.org/jira/browse/PARQUET-2069. When I dump the metadata at the end of the file, I get this:\nStruct(FileMetaData):\n\u00a0 \u00a0 \u00a01: i32(version) = I32(1)\n\u00a0 \u00a0 \u00a02: List(SchemaElement schema):\n{{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ...\n\u00a0 \u00a0 \u00a03: i64(num_rows) = I64(1)\n\u00a0 \u00a0 \u00a04: List(RowGroup row_groups):\n\u00a0 \u00a0 \u00a0 \u00a0 1: Struct(RowGroup row_groups):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01: List(ColumnChunk columns):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1: Struct(ColumnChunk columns):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02: i64(file_offset) = I64(4)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03: Struct(ColumnMetaData meta_data):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1: Type(type) = I32(6) = BYTE_ARRAY\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2: List(Encoding encodings):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01: Encoding(encodings) = I32(0) = PLAIN\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02: Encoding(encodings) = I32(3) = RLE\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 3: List(string path_in_schema):\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01: string(path_in_schema) = Binary(\"destination_addresses\")\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02: string(path_in_schema) = Binary(\"array\")\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03: string(path_in_schema) = Binary(\"element\")\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 4: CompressionCodec(codec) = I32(0) = UNCOMPRESSED\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 5: i64(num_values) = I64(6)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 6: i64(total_uncompressed_size) = I64(197)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 7: i64(total_compressed_size) = I64(197)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 9: i64(data_page_offset) = I64(4)\n}}\nAs you can see, element 2 of the ColumnChunk indicates that there is another copy of the ColumnMetaData at offset 4 of the file. But then we see that element 9 of the ColumnMetaData shown above indicates that the data page offset is ALSO 4, where we should find a Thrift encoding of a PageHeader structure. Obviously, both structures can't be in the same place, and in fact a PageHeader is what is located there.\nBased on what I'm seeing here, I believe that element 2 of ColumnChunk should be omitted entirely in this scenario, so as to not falsely indicate that there would be another copy of the ColumnMetadata in this location in the file where indeed something else is present.\nIt may take me a while to locate the offending code, but I thought I'd go ahead and point this out before I set off to investigate.",
        "Issue Links": []
    },
    "PARQUET-2140": {
        "Key": "PARQUET-2140",
        "Summary": "parquet-cli unable to read UUID values",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Balaji K",
        "Created": "22/Apr/22 22:12",
        "Updated": "20/May/22 13:50",
        "Resolved": null,
        "Description": "I am finding that parquet-cli throws when trying to read UUID values.\u00a0\nAttached to this bug report is a parquet file with 2 columns, message encoded as byte-array and number encoded as fixed length byte array (UUID). This file was written by my .net implementation of parquet specification. The file has one row worth of data and is readable by parquet-cpp.\nSchema as read by parquet-cli:\nmessage root\n\n{ \u00a0\u00a0required binary Message (STRING); \u00a0\u00a0required fixed_len_byte_array(16) Number (UUID); }\n\nValues as read by parquet-cpp:\n\u2014 Values \u2014\nMessage \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |Number \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\nFirst record \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|215 48 212 219 218 57 169 67 166 116 7 79 44 227 50 17 |\n\u00a0\nHere is the exception stack from parquet-cli when trying to read uuid values:\n\n\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: required binary Number (STRING) != required fixed_len_byte_array(16) Number (UUID)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.incompatibleSchema(ColumnIOFactory.java:101)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:93)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.schema.PrimitiveType.accept(PrimitiveType.java:602)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visitChildren(ColumnIOFactory.java:83)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:57)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.schema.MessageType.accept(MessageType.java:55)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.io.ColumnIOFactory.getColumnIO(ColumnIOFactory.java:162)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:135)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\r\n \n\n\u00a0I debugged parquet-cli code and found that parquet-cli is trying to project the UUID as a string and later on that throws as these types are not compatible?\u00a0\n\u00a0\nSource code references:\nAt AvroReadSupport.java, line 97\n~~~~~~~~~~~~\n\u00a0\u00a0\u00a0 String requestedProjectionString = configuration.get(AVRO_REQUESTED_PROJECTION);\n\u00a0\u00a0\u00a0 if (requestedProjectionString != null)\n\n{ \u00a0\u00a0\u00a0\u00a0\u00a0 Schema avroRequestedProjection = new Schema.Parser().parse(requestedProjectionString); \u00a0\u00a0\u00a0\u00a0\u00a0 projection = new AvroSchemaConverter(configuration).convert(avroRequestedProjection); \u00a0\u00a0\u00a0 }\n\n~~~~~~~~~~~~\n\u00a0\nDebugger values for\nrequestedProjectionString=\n{\"type\":\"record\",\"name\":\"root\",\"fields\":[\n{\"name\":\"Message\",\"type\":\"string\"}\n\n,{\"name\":\"Number\",\"type\":{\"type\":\"string\",\"logicalType\":\"uuid\"}}]}\n[Note that `Number` now has a type of `string` and a logicalType of `uuid`]\n\u00a0\nAt ColumnIOFactory.java line 93\n~~~~~~~~~~~~\nincompatibleSchema(primitiveType, currentRequestedType);\n~~~~~~~~~~~~\nDebugger values for\u00a0\nprimitiveType = required fixed_len_byte_array(16) Number (UUID)\ncurrentRequestedType = required binary Number (STRING)\n\u00a0\nand this will throw.\n\u00a0\nIf I skip over the projection code in AvroReadSupport, parquet-cli is able to read my file.\nI am not sure if the bug is in parquet-cli or parquet-mr or in the library I used to encode this file. The fact that parquet-cpp is able to read it gives me some confidence to say that the problem is either in parquet-cli or parquet-mr.\nPlease point me in the right direction if I could verify this UUID roundtripping purely from parquet-mr itself in form of an unit-test. Happy to contribute tests or fix if needed.",
        "Issue Links": []
    },
    "PARQUET-2141": {
        "Key": "PARQUET-2141",
        "Summary": "Controlling memory utilization by ParquetReader",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rohan Garg",
        "Created": "23/Apr/22 17:15",
        "Updated": "23/Apr/22 17:15",
        "Resolved": null,
        "Description": "In Apache Druid, Parquet is one of the popular form of input source to ingest data into a druid cluster (https://druid.apache.org/docs/latest/development/extensions-core/parquet.html). We rely on the parquet-mr library to read the parquet files and then convert them into Druid native format row-for-row to ingest. A considerable amount of our usecases ingest the whole parquet files (ie all columns in a single shot) into the system.\nA challenge that we face is that the parquet reader loads up an entire row group into memory as part of its normal operation. Row groups can be quite large (like, 1GB large) and sometimes it creates a pressure on our reader JVM leading to OOMs. Further, in some other cases it ends up creating GC pressure on the JVM leading to a decrease in the throughput of the ingestion tasks.\nTo mitigate this problem, we are considering that would it be better to have an option to download the Parquet rowgroup/file first and memory-map it for reading? The code which buffers the rowgroup works on the ByteBuffer interface already (https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L1763), so it seems like it could compliment the MMappedByteBuffer implementation too. Such a thing would alleviate pressure off of our reader JVM there by heavily reducing the chances for OOMs.\nWe're very open to more ideas or already tried solutions around this problem.",
        "Issue Links": []
    },
    "PARQUET-2142": {
        "Key": "PARQUET-2142",
        "Summary": "parquet-cli without hadoop throws java.lang.NoSuchMethodError on any parquet file access command",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": "Kengo Seki",
        "Reporter": "Timothy Miller",
        "Created": "25/Apr/22 13:47",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "13/Oct/22 13:27",
        "Description": "I can't do even basic things with parquet-cli from 1.13.0-SNAPSHOT.\nSteps to reproduce:\n\ngit clone <parquet-mr repo> \r\ncd parquet-mr\r\nmvn clean install -DskipTests\r\ncd parquet-cli\r\nmvn clean install -DskipTests\r\nmvn dependency:copy-dependencies\r\njava -cp 'target/*:target/dependency/*' org.apache.parquet.cli.Main cat <parquet file>\n\nResults:\n\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.parquet.avro.AvroSchemaConverter.convert(Lorg/apache/parquet/schema/MessageType;)Lorg/apache/avro/Schema;\r\n\u00a0 \u00a0 at org.apache.parquet.cli.util.Schemas.fromParquet(Schemas.java:89)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand.getAvroSchema(BaseCommand.java:405)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:66)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.Main.run(Main.java:157)\r\n\u00a0 \u00a0 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.Main.main(Main.java:187)",
        "Issue Links": []
    },
    "PARQUET-2143": {
        "Key": "PARQUET-2143",
        "Summary": "parquet-cli with hadoop throws java.lang.RuntimeException on any parquet file access",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Timothy Miller",
        "Created": "25/Apr/22 14:08",
        "Updated": "27/May/22 16:18",
        "Resolved": null,
        "Description": "I can't do even basic things with parquet-cli from 1.13.0-SNAPSHOT.\n\ngit clone <parquet-mr repo>\u00a0\r\ncd parquet-mr\r\nmvn clean install -DskipTests\r\ncd parquet-cli\r\nmvn clean install -DskipTests\r\ncd target\r\nhadoop jar parquet-cli-1.13.0-SNAPSHOT-runtime.jar org.apache.parquet.cli.Main cat <parquet file>\n\nResults:\n\nUnknown error\r\njava.lang.RuntimeException: Failed on record 0\r\n\u00a0 \u00a0 at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:86)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.Main.run(Main.java:157)\r\n\u00a0 \u00a0 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.Main.main(Main.java:187)\r\n\u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498)\r\n\u00a0 \u00a0 at org.apache.hadoop.util.RunJar.run(RunJar.java:323)\r\n\u00a0 \u00a0 at org.apache.hadoop.util.RunJar.main(RunJar.java:236)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/home/theosib/guid.parquet\r\n\u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:264)\r\n\u00a0 \u00a0 at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:132)\r\n\u00a0 \u00a0 at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:136)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1$1.advance(BaseCommand.java:363)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1$1.<init>(BaseCommand.java:344)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1.iterator(BaseCommand.java:342)\r\n\u00a0 \u00a0 at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:73)\r\n\u00a0 \u00a0 ... 9 more\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: The requested schema is not compatible with the file schema. incompatible types: required binary Number (STRING) != required fixed_len_byte_array(16) Number (UUID)\r\n\u00a0 \u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.incompatibleSchema(ColumnIOFactory.java:101)\r\n\u00a0 \u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:93)\r\n\u00a0 \u00a0 at org.apache.parquet.schema.PrimitiveType.accept(PrimitiveType.java:602)\r\n\u00a0 \u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visitChildren(ColumnIOFactory.java:83)\r\n\u00a0 \u00a0 at org.apache.parquet.io.ColumnIOFactory$ColumnIOCreatorVisitor.visit(ColumnIOFactory.java:57)\r\n\u00a0 \u00a0 at org.apache.parquet.schema.MessageType.accept(MessageType.java:55)\r\n\u00a0 \u00a0 at org.apache.parquet.io.ColumnIOFactory.getColumnIO(ColumnIOFactory.java:162)\r\n\u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:140)\r\n\u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:230)\r\n\u00a0 \u00a0 ... 15 more",
        "Issue Links": []
    },
    "PARQUET-2144": {
        "Key": "PARQUET-2144",
        "Summary": "Fix ColumnIndexBuilder for notIn predicate",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Huaxin Gao",
        "Created": "28/Apr/22 06:32",
        "Updated": "21/Jun/22 11:08",
        "Resolved": "21/Jun/22 11:08",
        "Description": "Column Index is not built correctly for notIn predicate. Need to fix the bug.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2145": {
        "Key": "PARQUET-2145",
        "Summary": "Release 1.12.3",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gidon Gershinsky",
        "Created": "04/May/22 07:45",
        "Updated": "23/Jun/22 21:55",
        "Resolved": "21/Jun/22 11:08",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-2051",
            "/jira/browse/PARQUET-2107",
            "/jira/browse/PARQUET-2144",
            "/jira/browse/PARQUET-2148",
            "/jira/browse/PARQUET-2120",
            "/jira/browse/PARQUET-2101",
            "/jira/browse/PARQUET-2102",
            "/jira/browse/PARQUET-2117",
            "/jira/browse/PARQUET-2040",
            "/jira/browse/PARQUET-2105",
            "/jira/browse/PARQUET-2106",
            "/jira/browse/PARQUET-2112",
            "/jira/browse/PARQUET-2127",
            "/jira/browse/PARQUET-2136",
            "/jira/browse/PARQUET-2121",
            "/jira/browse/PARQUET-2128",
            "/jira/browse/PARQUET-2129",
            "/jira/browse/PARQUET-2076",
            "/jira/browse/PARQUET-2081"
        ]
    },
    "PARQUET-2146": {
        "Key": "PARQUET-2146",
        "Summary": "AvroParquetWriter  write to s3 bucket throws data intergrity exception",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sean",
        "Created": "13/May/22 19:20",
        "Updated": "13/May/22 19:20",
        "Resolved": null,
        "Description": "Hi, we are trying to use\u00a0org.apache.parquet.avro.AvroParquetWriter\nto write parquet file to s3 bucket. The file is successfully written to s3 bucket but\u00a0\nget an exception\ncom.amazonaws.SdkClientException: Unable to verify integrity of data upload.\nThe purpose is to resolve this exceptions while\u00a0\u00a0The s3 bucket is encrypted with SSE-KMS not SSE-S3.\u00a0\n\u00a0\nIt appears that the exceptions are thrown because of code blocks in the link below\nhttps://github.com/aws/aws-sdk-java/blob/fd409dee8ae23fb8953e0bb4dbde65536a7e0514/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L1876\nFrom amazon doc, the etag is not same as MD5 when s3 bucket is encrypted with SSE-KMS\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html\n\u00a0\nThe possible way is to pass MD5 in request header or set system.property to disable validation in\u00a0 skipMd5CheckStrategy.skipClientSideValidationPerPutResponse as indicated in link\nhttps://github.com/aws/aws-sdk-java/blob/99fe75a823d4b02f4e90fa0dda06a1558d5617a1/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/internal/SkipMd5CheckStrategy.java#L42\n\u00a0The issue is that I do not find a proper way to inject such configurations into AvroParquetWriter. Is this possible? If yes, can you help to show how to do it?\u00a0\n\u00a0\nThanks\n\u00a0\nSean",
        "Issue Links": []
    },
    "PARQUET-2147": {
        "Key": "PARQUET-2147",
        "Summary": "Can't run ParquetMR tests in IDEs",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.2,                                            1.13.0,                                            1.11.2,                                            1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-testing",
        "Assignee": null,
        "Reporter": "Timothy Miller",
        "Created": "13/May/22 20:36",
        "Updated": "13/May/22 20:36",
        "Resolved": null,
        "Description": "ParquetMR contains a suite of self-tests. When one of those self-tests fails, it would be nice to be able to pull up the test in an IDE like IntelliJ. Then we can use the debugger and track down what the problem is. Unfortunately, this is impossible due to some unfortunate design choices.\nIf I open a test source file in IntelliJ and tell the IDE to run it, the first errors I get is a bunch of errors in org.apache.parquet.VersionTest, which evidently cannot find a bunch of symbols. If I hack VersionTest to no longer depend on those symbols, the next thing that happens is this error:\n\n\r\n/home/theosib/packages/PARQUET-2069/parquet-mr/parquet-column/src/main/java/org/apache/parquet/io/MessageColumnIO.java:43:46\r\njava: cannot find symbol\r\n\u00a0 symbol: \u00a0 class IncrementallyUpdatedFilterPredicateBuilder\r\n\u00a0 location: package org.apache.parquet.filter2.recordlevel\n\nEvidently, IncrementallyUpdatedFilterPredicateBuilder doesn't exist. Instead, it is auto-generated during the maven build process, but the code is stashed away somewhere where IntelliJ can't find it, and this is the end of the road.\nIf we want more people to be able to contribute to ParquetMR, I think we should work towards making it easier to debug.",
        "Issue Links": []
    },
    "PARQUET-2148": {
        "Key": "PARQUET-2148",
        "Summary": "Enable uniform decryption with plaintext footer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12.3",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "16/May/22 07:31",
        "Updated": "21/Jun/22 11:09",
        "Resolved": "21/Jun/22 11:09",
        "Description": "Currently, uniform decryption is not enabled in the plaintext footer mode - for no good reason. Column metadata is available, we just need to decrypt and use it.",
        "Issue Links": [
            "/jira/browse/PARQUET-2145"
        ]
    },
    "PARQUET-2149": {
        "Key": "PARQUET-2149",
        "Summary": "Implement async IO for Parquet file reader",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Parth Chandra",
        "Created": "16/May/22 18:26",
        "Updated": "06/Jun/23 11:59",
        "Resolved": null,
        "Description": "ParquetFileReader's implementation has the following flow (simplified) -\u00a0\n\u00a0 \u00a0 \u00a0 - For every column -> Read from storage in 8MB blocks -> Read all uncompressed pages into output queue\u00a0\n\u00a0 \u00a0 \u00a0 - From output queues -> (downstream ) decompression + decoding\nThis flow is serialized, which means that downstream threads are blocked until the data has been read. Because a large part of the time spent is waiting for data from storage, threads are idle and CPU utilization is really low.\nThere is no reason why this cannot be made asynchronous and parallel. So\u00a0\nFor Column\u00a0i -> reading one chunk until end, from storage -> intermediate output queue -> read one uncompressed page until end -> output queue -> (downstream ) decompression + decoding\nNote that this can be made completely self contained in ParquetFileReader and downstream implementations like Iceberg and Spark will automatically be able to take advantage without code change as long as the ParquetFileReader apis are not changed.\u00a0\nIn past work with async io\u00a0 Drill - async page reader  , I have seen 2x-3x improvement in reading speed for Parquet files.",
        "Issue Links": []
    },
    "PARQUET-2150": {
        "Key": "PARQUET-2150",
        "Summary": "parquet-protobuf to compile on mac M1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-protobuf",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "23/May/22 17:34",
        "Updated": "19/Jul/22 19:08",
        "Resolved": "19/Jul/22 19:08",
        "Description": "parquet-protobuf module fails to compile on Mac M1 because the maven protoc plugin cannot find the native osx-aarch_64:3.16.1  binary.\nthe build needs to be tweaked to pick up the x86 binaries",
        "Issue Links": [
            "/jira/browse/PARQUET-2155",
            "/jira/browse/HADOOP-17939"
        ]
    },
    "PARQUET-2151": {
        "Key": "PARQUET-2151",
        "Summary": "Drop Hadoop 2 input stream reflection from parquet-hadoop",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "06/Jun/22 17:24",
        "Updated": "07/Jun/22 13:31",
        "Resolved": null,
        "Description": "Parquet uses reflection to load a hadoop2 input stream, falling back to a hadoop-1 compatible client if not found.\nAll hadoop 2.0.2+ releases work with H2SeekableInputStream, so the binding to H2SeekableInputStream reworked to avoid needing reflection. This would make it a lot easier to probe for/use the bytebuffer input, and line the code up for more recent hadoop releases.\nH1SeekableInputStream is still needed to handle streams without ByteBufferReadable.\nAt some poiint support for ByteBufferPositionedReadable is needed, because that is really what parquet wants. that's where reflection will be needed",
        "Issue Links": [
            "/jira/browse/PARQUET-2134"
        ]
    },
    "PARQUET-2152": {
        "Key": "PARQUET-2152",
        "Summary": "zstd compressor and decompressor use the same configuration",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Peidian Li",
        "Created": "07/Jun/22 06:16",
        "Updated": "07/Jun/22 06:16",
        "Resolved": null,
        "Description": "I use spark to rewrite the parquet files that are compressed by zstd. And the parquet version is\u00a0 1.12.2. I want to read the parquet files compressed by level 3 and compress them on another level. But the level can't be changed.\nAfter I check the source, I found the problem was the codec was cached, and the configuration will not be updated:\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java#L144\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java#L226\nI think the problem is important. I found it when I try to use a different level to compaction the files in the iceberg table. Asynchronous rewriting with a higher level can lead to higher compression ratio. This is important to save storage costs.",
        "Issue Links": []
    },
    "PARQUET-2153": {
        "Key": "PARQUET-2153",
        "Summary": "Cannot read schema from parquet file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Nils Broman",
        "Created": "07/Jun/22 14:07",
        "Updated": "14/Jun/22 11:08",
        "Resolved": null,
        "Description": "I'm trying to generate a Avro schema from a parquet file. I get the issue both when using https://github.com/benwatson528/intellij-avro-parquet-plugin in Intellij as well as when I'm using my own implementation of generating a schema. \nThe parquet file contains nested entries and arrays: {\"a\": a, \"b\": [\n{\"c\": c}\n]}\nI get the following error message: \n\n\r\norg.apache.avro.SchemaParseException: Can't redefine: element\r\n\u00a0 \u00a0 at org.apache.avro.Schema$Names.put(Schema.java:1547)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$NamedSchema.writeNameRef(Schema.java:810)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:972)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:1239)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:1000)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:984)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$ArraySchema.toJson(Schema.java:1134)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:1239)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:1000)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:984)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$UnionSchema.toJson(Schema.java:1239)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.fieldsToJson(Schema.java:1000)\r\n\u00a0 \u00a0 at org.apache.avro.Schema$RecordSchema.toJson(Schema.java:984)\r\n\u00a0 \u00a0 at org.apache.avro.Schema.toString(Schema.java:424)\r\n\u00a0 \u00a0 at org.apache.avro.Schema.toString(Schema.java:396)\r\n\u00a0 \u00a0 at uk.co.hadoopathome.intellij.viewer.fileformat.ParquetFileReader.getSchema(ParquetFileReader.java:65)\r\n\u00a0 \u00a0 at uk.co.hadoopathome.intellij.viewer.FileViewerToolWindow$2.doInBackground(FileViewerToolWindow.java:196)\r\n\u00a0 \u00a0 at uk.co.hadoopathome.intellij.viewer.FileViewerToolWindow$2.doInBackground(FileViewerToolWindow.java:184)\r\n\u00a0 \u00a0 at java.desktop/javax.swing.SwingWorker$1.call(SwingWorker.java:304)\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\u00a0 \u00a0 at java.desktop/javax.swing.SwingWorker.run(SwingWorker.java:343)\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:829) \n\nIt appears to be an issue similar to PARQUET-1441 or PARQUET-1409.\nOr could it possibly be something wrong in my parquet file?",
        "Issue Links": []
    },
    "PARQUET-2154": {
        "Key": "PARQUET-2154",
        "Summary": "ParquetFileReader should close its input stream when `filterRowGroups` throw Exception in constructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yang Jie",
        "Created": "08/Jun/22 08:12",
        "Updated": "12/Apr/23 01:33",
        "Resolved": "26/Mar/23 06:16",
        "Description": "public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\r\n  this.converter = new ParquetMetadataConverter(options);\r\n  this.file = file;\r\n  this.f = file.newStream();\r\n  this.options = options;\r\n  try {\r\n    this.footer = readFooter(file, options, f, converter);\r\n  } catch (Exception e) {\r\n    // In case that reading footer throws an exception in the constructor, the new stream\r\n    // should be closed. Otherwise, there's no way to close this outside.\r\n    f.close();\r\n    throw e;\r\n  }\r\n  this.fileMetaData = footer.getFileMetaData();\r\n  this.fileDecryptor = fileMetaData.getFileDecryptor(); // must be called before filterRowGroups!\r\n  if (null != fileDecryptor && fileDecryptor.plaintextFile()) {\r\n    this.fileDecryptor = null; // Plaintext file. No need in decryptor\r\n  }\r\n\r\n  this.blocks = filterRowGroups(footer.getBlocks());\r\n  this.blockIndexStores = listWithNulls(this.blocks.size());\r\n  this.blockRowRanges = listWithNulls(this.blocks.size());\r\n  for (ColumnDescriptor col : footer.getFileMetaData().getSchema().getColumns()) {\r\n    paths.put(ColumnPath.get(col.getPath()), col);\r\n  }\r\n  this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\r\n} \n\nDuring the construction of ParquetFileReader, if `filterRowGroups` method throws an exception, it will cause resource leak because when `filterRowGroups(footer.getBlocks())` throw an Exception, the open stream `this.f = file.newStream()`\u00a0looks unable to be closed.",
        "Issue Links": []
    },
    "PARQUET-2155": {
        "Key": "PARQUET-2155",
        "Summary": "Upgrade protobuf version to 3.17.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Chao Sun",
        "Reporter": "Chao Sun",
        "Created": "10/Jun/22 00:14",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:12",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-2150"
        ]
    },
    "PARQUET-2156": {
        "Key": "PARQUET-2156",
        "Summary": "Column bloom filter: Show bloom filters in tools",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "BingKun Pan",
        "Created": "10/Jun/22 11:00",
        "Updated": "05/Dec/22 01:27",
        "Resolved": null,
        "Description": "command result as follow:\nparquet-tools bloom-filter BloomFilter.snappy.parquet\nrow-group 0:\nbloom filter for column id:\nNONE\nbloom filter for column uuid:\nHash strategy: block\nAlgorithm: block\nCompression: uncompressed\nBitset size: 1048576",
        "Issue Links": []
    },
    "PARQUET-2157": {
        "Key": "PARQUET-2157",
        "Summary": "Add BloomFilter fpp config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Huaxin Gao",
        "Reporter": "Huaxin Gao",
        "Created": "12/Jun/22 23:46",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "30/Dec/22 05:54",
        "Description": "Currently parquet-mr hardcoded bloom filter fpp (false positive probability) to 0.01.  We should have a config to let user to specify fpp.",
        "Issue Links": []
    },
    "PARQUET-2158": {
        "Key": "PARQUET-2158",
        "Summary": "Upgrade Hadoop dependency to version 3.2.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "13/Jun/22 10:20",
        "Updated": "01/Aug/22 09:45",
        "Resolved": "01/Aug/22 09:44",
        "Description": "Parquet still builds against Hadoop 2.10. This is very out of date and does not work with java 11, let alone later releases.\nUpgrading the dependency to Hadoop 3.2.0 makes the release compatible with java 11, and lines up with active work on  HADOOP-18287,  Provide a shim library for modern FS APIs \nThis will significantly speed up access to columnar data, especially  in cloud stores.",
        "Issue Links": [
            "/jira/browse/HADOOP-18287",
            "/jira/browse/PARQUET-2165",
            "/jira/browse/HADOOP-12436"
        ]
    },
    "PARQUET-2159": {
        "Key": "PARQUET-2159",
        "Summary": "Parquet bit-packing de/encode optimization",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fang-Xie",
        "Reporter": "Fang-Xie",
        "Created": "15/Jun/22 14:18",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "26/Mar/23 05:46",
        "Description": "Current Spark use Parquet-mr as parquet reader/writer library, but the built-in bit-packing en/decode is not efficient enough.\u00a0\nOur optimization for Parquet bit-packing en/decode with jdk.incubator.vector in Open JDK18 brings prominent performance improvement.\nDue to Vector API is added to OpenJDK since 16, So this optimization request JDK16 or higher.\nBelow are our test results\nFunctional test is based on open-source parquet-mr Bit-pack decoding function:\u00a0public final void unpack8Values(final byte[] in, final int inPos, final int[] out, final int outPos)\u00a0__\ncompared with our implementation with vector API\u00a0public final void unpack8Values_vec(final byte[] in, final int inPos, final int[] out, final int outPos)\nWe tested 10 pairs (open source parquet bit unpacking vs ours optimized vectorized SIMD implementation) decode function with bit width={1,2,3,4,5,6,7,8,9,10}, below are test results:\n\nWe integrated our bit-packing decode implementation into parquet-mr, tested the parquet batch reader ability from Spark VectorizedParquetRecordReader which get parquet column data by the batch way. We construct parquet file with different row count and column count, the column data type is Int32, the maximum int value is 127 which satisfies bit pack encode with bit width=7,\u00a0\u00a0 the count of the row is from 10k to 100 million and the count of the column is from 1 to 4.",
        "Issue Links": []
    },
    "PARQUET-2160": {
        "Key": "PARQUET-2160",
        "Summary": "Close decompression stream to free off-heap memory in time",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Yujiang Zhong",
        "Created": "16/Jun/22 13:16",
        "Updated": "29/Jun/23 07:13",
        "Resolved": "09/Jan/23 23:21",
        "Description": "The decompressed stream in HeapBytesDecompressor$decompress now relies on the JVM GC to close. When reading parquet in zstd compressed format, sometimes I ran into OOM cause high off-heap usage. I think the reason is that the GC is not timely and causes off-heap memory fragmentation. I had to set\u00a0 lower MALLOC_TRIM_THRESHOLD_ to make glibc give back memory to system quickly. There is a [thread|https://apache-iceberg.slack.com/archives/C025PH0G1D4/p1650928750269869?thread_ts=1650927062.590789&cid=C025PH0G1D4] of this zstd parquet issus in Iceberg community slack: \u00a0some people had the same problem.\u00a0\nI think maybe we can use ByteArrayBytesInput as decompressed bytes input and close decompressed stream in time to solve this problem:\n\n\r\nInputStream is = codec.createInputStream(bytes.toInputStream(), decompressor);\r\ndecompressed = BytesInput.from(is, uncompressedSize); \n\n->\n\n\r\nInputStream is = codec.createInputStream(bytes.toInputStream(), decompressor);\r\ndecompressed = BytesInput.copy(BytesInput.from(is, uncompressedSize));\r\nis.close(); \n\nAfter I made this change to decompress, I found off-heap memory is significantly reduced (with same query on same data).",
        "Issue Links": []
    },
    "PARQUET-2161": {
        "Key": "PARQUET-2161",
        "Summary": "Row positions are computed incorrectly when range or offset metadata filter is used",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ala Luszczak",
        "Created": "20/Jun/22 10:55",
        "Updated": "12/Apr/23 01:33",
        "Resolved": "26/Mar/23 06:15",
        "Description": "The row indexes introduced in PARQUET-2117 are not computed correctly when\n(1) range or offset metadata filter is applied, and\n(2) the first row group was eliminated by the filter\nFor example, if a file has two row groups with 10 rows each, and we attempt to only read the 2nd row group, we are going to produce row indexes 0, 1, 2, ..., 9 instead of expected 10, 11, ..., 19.\nThis happens because functions `filterFileMetaDataByStart` (used here: https://github.com/apache/parquet-mr/blob/e06384455567c56d5906fc3a152ab00fd8dfdf33/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L1453) and `filterFileMetaDataByMidpoint` (used here: https://github.com/apache/parquet-mr/blob/e06384455567c56d5906fc3a152ab00fd8dfdf33/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java#L1460) modify their input `FileMetaData`. To address the issue we need to `generateRowGroupOffsets` before these filters are applied.",
        "Issue Links": [
            "/jira/browse/SPARK-39634",
            "/jira/browse/PARQUET-2117"
        ]
    },
    "PARQUET-2162": {
        "Key": "PARQUET-2162",
        "Summary": "Why the logic of ParquetFileWrite is diff from the parquet format",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Kun Liu",
        "Created": "25/Jun/22 06:18",
        "Updated": "27/Jun/22 10:40",
        "Resolved": null,
        "Description": "Hi parquet-mr teams,\nWhen I reading the parquet writer in [ParquetFileWriter](https://github.com/apache/parquet-mr/blob/e06384455567c56d5906fc3a152ab00fd8dfdf33/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java#L916),\u00a0 I find that there is no `column metadata` behind the each column chunk described in the [Parquet-Format] (https://github.com/apache/parquet-format#file-format) and [thrift definition](https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/src/main/thrift/parquet.thrift#L790)",
        "Issue Links": []
    },
    "PARQUET-2163": {
        "Key": "PARQUET-2163",
        "Summary": "Parquet C++ Float Runtime Error in Decimal Schema",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-9.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "William Butler",
        "Reporter": "William Butler",
        "Created": "28/Jun/22 22:09",
        "Updated": "07/Jul/22 13:41",
        "Resolved": "06/Jul/22 16:02",
        "Description": "The code that determines if a physical type is appropriate for a particular decimal logical type can overflow to infinity which then triggers a floating point error when converting to int32.\n\u00a0\nI don't know how large of decimals we want to support but there is a simple fix to support up to ~2^31 -1 digits.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/13456"
        ]
    },
    "PARQUET-2164": {
        "Key": "PARQUET-2164",
        "Summary": "CapacityByteArrayOutputStream overflow while writing causes negative row group sizes to be written",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Parth Chandra",
        "Created": "06/Jul/22 22:23",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "26/Mar/23 05:44",
        "Description": "It is possible, while writing a parquet file, to cause CapacityByteArrayOutputStream to overflow.\nThis is an extreme case but it has been observed in a real world data set.\nThe attached Spark program manages to reproduce the issue.\nShort summary of how this happens -\u00a0\n1. After many small records possibly including nulls, the dictionary page fills up and subsequent pages are written using plain encoding\n2. The estimate of when to perform the page size check is based on the number of values observed per page so far. Let's say this is about 100K\n3. A sequence of very large records shows up. Let's say each of these record is 200K.\u00a0\n4. After 11K of these records the size of the page has gone up beyond 2GB.\n5. CapacityByteArrayOutputStream is capable of holding more than 2GB of data but also it holds the size of the data in an int which overflows.\nThere are a couple of things to fix here -\n1. The check for page size should check both the number of values added as well as the buffered size of the data\n2. CapacityByteArrayOutputStream should throw an exception is the data size increases beyond 2GB (java.io.ByteArrayOutputStream does exactly that).",
        "Issue Links": [
            "/jira/browse/PARQUET-2052"
        ]
    },
    "PARQUET-2165": {
        "Key": "PARQUET-2165",
        "Summary": "remove deprecated PathGlobPattern and DeprecatedFieldProjectionFilter to compile on hadoop 3.2+",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-thrift",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "12/Jul/22 13:50",
        "Updated": "01/Aug/22 09:45",
        "Resolved": null,
        "Description": "remove the deprecated PathGlobPattern class and its uses from parquet-thrift\nThe return types from the hadoop  GlobPattern code changed in HADOOP-12436; in the class as is will not compile against hadoop 3.x\nParquet releases compiled against hadoop 2.x will not be able to instantiate these classes on a hadoop 3 release, because things will not link.\nNobody appears to have complained about the linkage problem to the extent of filing a JIRA.",
        "Issue Links": [
            "/jira/browse/PARQUET-2158"
        ]
    },
    "PARQUET-2166": {
        "Key": "PARQUET-2166",
        "Summary": "parquet writer runs into OOM during writing",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1,                                            1.12.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Ketki Bukkawar",
        "Created": "13/Jul/22 04:42",
        "Updated": "13/Jul/22 04:42",
        "Resolved": null,
        "Description": "Hi team,\nWe are getting OOM error on trying to writer data to the parquet file. Please check below stack trace:\n\nCaused by: java.lang.OutOfMemoryError: Direct buffer memory at java.base/java.nio.Bits.reserveMemory(Bits.java:175) at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118) at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317) at org.apache.parquet.hadoop.codec.SnappyCompressor.setInput(SnappyCompressor.java:97) at org.apache.parquet.hadoop.codec.NonBlockedCompressorStream.write(NonBlockedCompressorStream.java:48) at org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeToOutput(CapacityByteArrayOutputStream.java:227) at org.apache.parquet.bytes.CapacityByteArrayOutputStream.writeTo(CapacityByteArrayOutputStream.java:247) at org.apache.parquet.bytes.BytesInput$CapacityBAOSBytesInput.writeAllTo(BytesInput.java:405) at org.apache.parquet.bytes.BytesInput$SequenceBytesIn.writeAllTo(BytesInput.java:296) at org.apache.parquet.hadoop.CodecFactory$HeapBytesCompressor.compress(CodecFactory.java:164) at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:95) at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:147) at org.apache.parquet.column.impl.ColumnWriterV1.flush(ColumnWriterV1.java:235) at org.apache.parquet.column.impl.ColumnWriteStoreV1.flush(ColumnWriteStoreV1.java:122) at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:172) at org.apache.parquet.hadoop.InternalParquetRecordWriter.checkBlockSizeReached(InternalParquetRecordWriter.java:148) at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:130) at org.apache.parquet.hadoop.ParquetWriter.write(ParquetWriter.java:299) at com.fivetran.warehouses.common.parquet.AvroBasedParquetWriterAdapter.write(AvroBasedParquetWriterAdapter.java:39)\nWe believe that most of the memory is being consumed by slabs. From below warning we can see that a content column acquired 108 slabs:\n\n[content] optional binary content (UTF8) { r:0 d: RunLengthBitPackingHybrid 64 bytes data: FallbackValuesWriter{ data: initial: DictionaryValuesWriter\nUnknown macro: { data} \n data: fallback: PLAIN CapacityByteArrayOutputStream 108 slabs, 162,188,576 bytes data:} pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes total: 162,188,590/162,188,640 }\nCould you please help us resolve this issue?\nThanks",
        "Issue Links": []
    },
    "PARQUET-2167": {
        "Key": "PARQUET-2167",
        "Summary": "CLI show footer command fails if Parquet file contains date fields",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Bryan Keller",
        "Created": "23/Jul/22 03:59",
        "Updated": "12/Apr/23 01:33",
        "Resolved": "26/Mar/23 06:11",
        "Description": "The show footer command in the CLI fails with the following error if run against a file with date fields:\ncom.fasterxml.jackson.databind.exc.InvalidDefinitionException: Java 8 date/time type `java.time.ZoneOffset` not supported by default: add Module \"com.fasterxml.jackson.datatype:jackson-datatype-jsr310\" to enable handling (through reference chain: org.apache.parquet.hadoop.metadata.ParquetMetadata[\"blocks\"]>java.util.ArrayList[0]>org.apache.parquet.hadoop.metadata.BlockMetaData[\"columns\"]>java.util.ArrayList[2]>org.apache.parquet.hadoop.metadata.IntColumnChunkMetaData[\"statistics\"]>org.apache.parquet.column.statistics.IntStatistics[\"stringifier\"]>org.apache.parquet.schema.PrimitiveStringifier$5[\"formatter\"]->java.time.format.DateTimeFormatter[\"zone\"])",
        "Issue Links": []
    },
    "PARQUET-2168": {
        "Key": "PARQUET-2168",
        "Summary": "Potential bug in ParquetWriteProtocol",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Joy Bestourous",
        "Created": "29/Jul/22 18:58",
        "Updated": "09/Aug/22 14:00",
        "Resolved": null,
        "Description": "We found what we think is a bug in ParquetWriteProtocol, in which ParquetWriteProtocol will fail on instantiation of StructWriteProtocol if the StructType contains an empty child struct.\nSpecifically, for the ParquetWriteProtocol, if the thriftStruct contains an empty struct, logic in ThriftSchemaConvertVisitor drops the element, yielding a MessageType that has 1 fewer fields than the original schema. Subsequent logic in ParquetWriteProtocol.StructWriteProtocol tries to populate a `children` element by iterating through the thrift struct children and trying to get the element from the ColumnIO object\n\n\r\nGiven: ThriftStruct with 20 fields\r\nMessageType schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct)\r\n-> ThriftSchemaConvertVisotor.convert(StructType struct...)\r\n-> -> Visitor = new ThriftSchemaConvertVisitor(filter, true, keepOneOfEachUnion), state)\r\n-> -> ConvertedField = struct.accept(visitor)\r\n-> -> -> ThriftSchemaConvertVisotor.visit(struct, state)\r\n-> -> -> -> ConvertedField converted = child.getType().accept(this, childState)\r\n-> -> -> -> ThriftSchemaConvertVisotor.visit(struct, state) //here we\u2019re at the child struct\n\n\u00a0In here, we have both hasSentinelUnionColumns and hasNonSentinelUnionColumns defaulted as false and we look for any child elements, in which case, one of these is updated to true.\u00a0 Thus, when we come to this step, we fall into the Drop() case.\u00a0 \u00a0\u00a0\n\u00a0\n\n\r\n\u00a0 if (hasNonSentinelUnionColumns) {\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// user requested some of the fields of this struct, so we keep the struct\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return new Keep(state.path, new GroupType(state.repetition, state.name, convertedChildren));\r\n\u00a0\u00a0\u00a0\u00a0} else {\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// user requested none of the fields of this struct, so we drop it\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return new Drop(state.path);\r\n\u00a0\u00a0\u00a0\u00a0}\n\n\u00a0\nBecause this field is Dropped, our MessageType.fieldsList is 19\n\u00a0\n\n\r\nColumnIO = new ColumnIOFactory().getColumnIO(MessageType) // again yields a ColumnIO with only 19 fields\r\nTProtocol = new ParquetWriteProtocol(recordConsumer, columnIo, thriftStruct)\r\n-> MessageWriteProtocol = new MessageWriteProtocol(ColumnIO schema, StructType thriftType)\r\n-> -> new StructWriteProtocol(ColumnIO schema, StructType thriftType...)\r\n\r\nfor (i = 0 to thriftStruct.children.size) // which is 20\r\n schema.getChild(i) // Out of bounds error on index 19\n\nWe currently have a workaround for this but would like to get a fix if possible.",
        "Issue Links": []
    },
    "PARQUET-2169": {
        "Key": "PARQUET-2169",
        "Summary": "Upgrade Avro to version 1.11.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-avro",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "31/Jul/22 20:15",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2170": {
        "Key": "PARQUET-2170",
        "Summary": "Empty projection returns the wrong number of rows when column index is enabled",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Ivan Sadikov",
        "Created": "05/Aug/22 04:31",
        "Updated": "05/Aug/22 05:33",
        "Resolved": null,
        "Description": "Discovered in Spark, when returning an empty projection from a Parquet file with filter pushdown enabled (typically when doing filter + count), Parquet-Mr returns a wrong number of rows with column index enabled. When the column index feature is disabled, the result is correct.\n\u00a0\nThis happens due to the following:\n\nParquetFileReader::getFilteredRowCount() (https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L851) selects row ranges to calculate the row count when column index is enabled.\nIn ColumnIndexFilter (https://github.com/apache/parquet-mr/blob/0819356a9dafd2ca07c5eab68e2bffeddc3bd3d9/parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java#L80) we filter row ranges and pass the set of paths which in this case is empty.\nWhen evaluating the filter, if the column path is not in the set, we would return an empty list of rows (https://github.com/apache/parquet-mr/blob/0819356a9dafd2ca07c5eab68e2bffeddc3bd3d9/parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java#L178) which is always the case for an empty projection.\nThis results in the incorrect number of records reported by the library.\n\nI will provide the full repro later.",
        "Issue Links": [
            "/jira/browse/SPARK-39833"
        ]
    },
    "PARQUET-2171": {
        "Key": "PARQUET-2171",
        "Summary": "Implement vectored IO in parquet file format",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Mukund Thakur",
        "Created": "10/Aug/22 22:43",
        "Updated": "07/Jun/23 17:30",
        "Resolved": null,
        "Description": "We recently added a new feature called vectored IO in Hadoop for improving read performance for seek heavy readers. Spark Jobs and others which uses parquet will greatly benefit from this api. Details can be found here\u00a0\nhttps://github.com/apache/hadoop/commit/e1842b2a749d79cbdc15c524515b9eda64c339d5\nhttps://issues.apache.org/jira/browse/HADOOP-18103\nhttps://issues.apache.org/jira/browse/HADOOP-11867",
        "Issue Links": [
            "/jira/browse/PARQUET-2277"
        ]
    },
    "PARQUET-2172": {
        "Key": "PARQUET-2172",
        "Summary": "[C++] Make field return const NodePtr& instead of forcing copy of shared_ptr",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-10.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "12/Aug/22 06:08",
        "Updated": "13/Aug/22 04:31",
        "Resolved": "13/Aug/22 03:25",
        "Description": "This potentially removes some amount of tax from atomic increments/decrements.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/13865"
        ]
    },
    "PARQUET-2173": {
        "Key": "PARQUET-2173",
        "Summary": "Fix parquet build against hadoop 3.3.3+",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "16/Aug/22 19:34",
        "Updated": "02/Feb/23 11:32",
        "Resolved": "02/Feb/23 11:31",
        "Description": "parquet won't build against hadoop 3.3.3+ because it swapped out log4j 1.17 for reload4j, and this creates maven dependency problems in parquet cli\n\n\r\n[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (default) @ parquet-cli ---\r\n[WARNING] Used undeclared dependencies found:\r\n[WARNING]    ch.qos.reload4j:reload4j:jar:1.2.22:provided\r\n\r\n\n\nthe hadoop common dependencies need to exclude this jar and any changed slf4j ones.",
        "Issue Links": []
    },
    "PARQUET-2174": {
        "Key": "PARQUET-2174",
        "Summary": "Encrypting the entire table is impossible",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "ren shangtao",
        "Created": "18/Aug/22 11:30",
        "Updated": "18/Aug/22 11:30",
        "Resolved": null,
        "Description": "```java\npublic InternalColumnEncryptionSetup getColumnSetup(ColumnPath columnPath,\nboolean createIfNull, int ordinal) {\nInternalColumnEncryptionSetup internalColumnProperties = columnMap.get(columnPath);\nif (null != internalColumnProperties) {\nif (ordinal != internalColumnProperties.getOrdinal()) \n{\r\nthrow new ParquetCryptoRuntimeException(\"Column ordinal doesnt match \" + columnPath +\r\n\": \" + ordinal + \", \"+internalColumnProperties.getOrdinal());\r\n}\nreturn internalColumnProperties;\n}\n------ here is always true, then throw Exception\nif (!createIfNull) \n{\r\nthrow new ParquetCryptoRuntimeException(\"No encryption setup found for column \" + columnPath);\r\n}\nif (fileCryptoMetaDataCreated) \n{\r\nthrow new ParquetCryptoRuntimeException(\"Re-use: No encryption setup for column \" + columnPath);\r\n}\n\n```",
        "Issue Links": []
    },
    "PARQUET-2175": {
        "Key": "PARQUET-2175",
        "Summary": "Skip method skips levels and not rows for repeated fields",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "fatemah",
        "Created": "23/Aug/22 21:29",
        "Updated": "24/Aug/22 17:27",
        "Resolved": null,
        "Description": "The implementation of TypedColumnReader::Skip method with signature:\nvirtual int64_t Skip(int64_t num_levels_to_skip) = 0;\nwill skip levels for both repeated fields and non-repeated fields. We want to be able to skip rows for repeated fields, and skipping levels is not that useful.\nFor example, for the following rows:\nmessage M { repeated int32 b = 1 }\nrows: {}, {[10,10]}, {[20, 20, 20]}\nvalues = {10, 10, 20, 20, 20};\ndef_levels = {0, 1, 1, 1, 1, 1};\nrep_levels = {0, 0, 1, 0, 1, 1};\nWe want skip(2) to skip the first two rows, so that the next value that we read is 20. However, it will skip the first two levels, and the next value that we read is 10.",
        "Issue Links": []
    },
    "PARQUET-2176": {
        "Key": "PARQUET-2176",
        "Summary": "Parquet writers should allow for configurable index/statistics truncation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "patchwork01",
        "Created": "24/Aug/22 11:17",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:06",
        "Description": "ParquetWriter does not expose any way to set the properties for column index or statistics truncation.\nWith ParquetOutputFormat those can be set with parquet.columnindex.truncate.length and\u00a0parquet.statistics.truncate.length. These are not applied for ParquetWriter.\nThese properties are documented here: https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md",
        "Issue Links": []
    },
    "PARQUET-2177": {
        "Key": "PARQUET-2177",
        "Summary": "Fix parquet-cli not to fail showing descriptions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "26/Aug/22 05:04",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "26/Mar/23 06:02",
        "Description": "Currently, trying to show the descriptions of the 'prune' and 'masking' subcommands leads to NPE as follows.\n\n\r\n$ java -cp 'target/parquet-cli-1.13.0-SNAPSHOT.jar:target/dependency/*' org.apache.parquet.cli.Main help prune\r\nException in thread \"main\" java.lang.NullPointerException\r\n\tat com.beust.jcommander.JCommander$MainParameter.access$900(JCommander.java:64)\r\n\tat com.beust.jcommander.JCommander.getMainParameterDescription(JCommander.java:965)\r\n\tat org.apache.parquet.cli.Help.run(Help.java:65)\r\n\tat org.apache.parquet.cli.Main.run(Main.java:146)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.parquet.cli.Main.main(Main.java:189)\r\n\n\n\n\r\n$ java -cp 'target/parquet-cli-1.13.0-SNAPSHOT.jar:target/dependency/*' org.apache.parquet.cli.Main help masking\r\nException in thread \"main\" java.lang.NullPointerException\r\n\tat com.beust.jcommander.JCommander$MainParameter.access$900(JCommander.java:64)\r\n\tat com.beust.jcommander.JCommander.getMainParameterDescription(JCommander.java:965)\r\n\tat org.apache.parquet.cli.Help.run(Help.java:65)\r\n\tat org.apache.parquet.cli.Main.run(Main.java:146)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.parquet.cli.Main.main(Main.java:189)",
        "Issue Links": []
    },
    "PARQUET-2178": {
        "Key": "PARQUET-2178",
        "Summary": "ParquetReader constructed using builder fails to read encrypted files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Atul Mohan",
        "Created": "26/Aug/22 23:52",
        "Updated": "07/Sep/22 16:00",
        "Resolved": "07/Sep/22 16:00",
        "Description": "ParquetReader objects can be constructed using the builder as follows:\n\u00a0\n\n\r\nParquetReader<Group> builderReader = ParquetReader.builder(\r\nnew GroupReadSupport(),\r\nnew Path(\"path/to/c000.snappy.parquet\"))\r\n.withConf(conf)\r\n.build();\n\n\u00a0\n\u00a0\nThis parquetReader object cannot be used to read encrypted files as\u00a0\n\nbuilderReader.read()\n\n\u00a0fails with the following exception:\n\u00a0\n\u00a0\n\n\r\njava.lang.NullPointerException at org.apache.parquet.crypto.keytools.FileKeyUnwrapper.getKey(FileKeyUnwrapper.java:87) \u00a0\n\nIt seems like the reason is that the withConf method within the ParquetReader builder clears the optionsBuilder set earlier.\nMy proposal for a solution would be to un-deprecate the constructor:\u00a0\n\n\r\nParquetReader(Configuration conf, Path file, ReadSupport<T> readSupport)\n\n\u00a0so that applications can read encrypted parquet files using the ParquetReader.\nIf approved, I can do a PR to make this change.\nHere is a sample test showcasing the issue: https://gist.github.com/a2l007/3d813cc5e44c45100dda169dc6245ae4",
        "Issue Links": []
    },
    "PARQUET-2179": {
        "Key": "PARQUET-2179",
        "Summary": "Add a test for skipping repeated fields",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-10.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "30/Aug/22 18:33",
        "Updated": "18/Oct/22 18:50",
        "Resolved": "18/Oct/22 18:49",
        "Description": "The existing test only tests non-repeated fields. Adding a test for repeated fields to make it clear that it is skipping values and not records.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14364",
            "https://github.com/apache/arrow/pull/14366"
        ]
    },
    "PARQUET-2180": {
        "Key": "PARQUET-2180",
        "Summary": "make the default behavior for proto writing not-backwards compatible",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-protobuf",
        "Assignee": null,
        "Reporter": "J Y",
        "Created": "30/Aug/22 20:14",
        "Updated": "30/Aug/22 20:15",
        "Resolved": null,
        "Description": "https://issues.apache.org/jira/browse/PARQUET-968 introduced supporting maps and lists in a spec compliant way.\u00a0 however, to not break existing libraries, a flag was introduced and defaulted the write behavior to NOT use the specs compliant writes.\nit's been over 5 years, and people should be really off of it.\u00a0 so much so, that trying to use the new parquet-cli tool to read parquet files generated by flink doesn't work b/c it's hard coded to never allow the old style.\u00a0 the deprecated parquet-tools reads these files fine b/c it's the older style.\ni started coding up a workaround in flink-parquet and parquet-cli, but stopped.\u00a0 we really should just move on at this point, imho.\u00a0 protobufs often have repeated primitives and maps, so it's more pressing to get proper specs compliant support for it now.\u00a0 we should keep the flag around and let people override it back to being backwards compatible though.\ni have the code written and can submit a PR if you'd like.\ni'm not an expert in parquet though, so i'm unclear as to the deep downstream ramifications of this change, so i would love to get feedback in this area.",
        "Issue Links": []
    },
    "PARQUET-2181": {
        "Key": "PARQUET-2181",
        "Summary": "parquet-cli fails at supporting parquet-protobuf generated files",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cli",
        "Assignee": null,
        "Reporter": "J Y",
        "Created": "31/Aug/22 05:37",
        "Updated": "07/Sep/22 19:36",
        "Resolved": null,
        "Description": "i generated a parquet file using a protobuf with this proto definition:\n\n\r\nmessage IndexPath {\r\n\u00a0 // Index of item in path.\r\n\u00a0 repeated int32 index = 1;\r\n}\r\n\r\nmessage SomeEvent {\r\n\u00a0 // truncated/obfuscated wrapper\r\n  optional IndexPath client_position = 1;\r\n}\r\n\n\nthis gets translated to the following parquet schema using the new compliant schema for lists:\n\n\r\nmessage SomeEvent {\r\n\u00a0 optional group client_position = 1 {\r\n\u00a0 \u00a0 optional group index (LIST) = 1 {\r\n\u00a0 \u00a0 \u00a0 repeated group list {\r\n\u00a0 \u00a0 \u00a0 \u00a0 required int32 element;\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n\u00a0 }\r\n}\r\n\n\nthis causes parquet-cli cat to barf on a file containing these events:\njava.lang.RuntimeException: Failed on record 0\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:86)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.Main.run(Main.java:157)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.Main.main(Main.java:187)\nCaused by: java.lang.ClassCastException: required int32 element is not a group\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.schema.Type.asGroupType(Type.java:248)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:284)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:228)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.access$100(AvroRecordConverter.java:74)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter$ElementConverter.<init>(AvroRecordConverter.java:539)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter$AvroCollectionConverter.<init>(AvroRecordConverter.java:489)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:293)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:137)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.newConverter(AvroRecordConverter.java:284)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:137)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:91)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:142)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:190)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:166)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1$1.advance(BaseCommand.java:363)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1$1.<init>(BaseCommand.java:344)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.BaseCommand$1.iterator(BaseCommand.java:342)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.parquet.cli.commands.CatCommand.run(CatCommand.java:73)\n\u00a0 \u00a0 \u00a0 \u00a0 ... 3 more\nusing the old parquet-tools binary to cat this file works fine.",
        "Issue Links": []
    },
    "PARQUET-2182": {
        "Key": "PARQUET-2182",
        "Summary": "Handle unknown logical types",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gabor Szadovszky",
        "Created": "31/Aug/22 05:47",
        "Updated": "07/Sep/22 11:44",
        "Resolved": null,
        "Description": "New logical types introduced in parquet-format shall be properly handled in parquet-mr releases that are not aware of this new type. In this case we shall read the data as if only the primitive type would be defined (without a logical type) with one exception: We shall not use min/max based statistics (including column indexes) since we don't know the proper ordering of that type.",
        "Issue Links": []
    },
    "PARQUET-2183": {
        "Key": "PARQUET-2183",
        "Summary": "Fix statistics issue of Column Encryptor",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Xinli Shang",
        "Reporter": "Xinli Shang",
        "Created": "03/Sep/22 05:34",
        "Updated": "03/Sep/22 05:36",
        "Resolved": null,
        "Description": "There is an issue that missing column statistics if that column is re-encrypted.",
        "Issue Links": []
    },
    "PARQUET-2184": {
        "Key": "PARQUET-2184",
        "Summary": "Improve SnappyCompressor buffer expansion performance",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Andrew Baranec",
        "Created": "04/Sep/22 04:25",
        "Updated": "05/Dec/22 14:43",
        "Resolved": null,
        "Description": "The existing implementation of SnappyCompressor will only allocate enough bytes for the buffer passed into setInput().\u00a0 This leads to suboptimal performance when there are patterns of writes that cause repeated buffer expansions.\u00a0 In the worst case it must copy the entire buffer for every single invocation of setInput()\nInstead of allocating a buffer of size current + write length,\u00a0 there should be an expansion strategy that reduces the amount of copying required.",
        "Issue Links": []
    },
    "PARQUET-2185": {
        "Key": "PARQUET-2185",
        "Summary": "ParquetReader constructed using builder fails to read encrypted files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Atul Mohan",
        "Created": "07/Sep/22 16:01",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "26/Mar/23 06:08",
        "Description": "ParquetReader objects can be constructed using the builder as follows:\n\u00a0\n\n\r\nParquetReader<Group> builderReader = ParquetReader.builder(new GroupReadSupport(),new Path(\"path/to/c000.snappy.parquet\"))\r\n.withConf(conf)\r\n.build();\r\n\n\nThis parquetReader object cannot be used to read encrypted files as\u00a0\n\nbuilderReader.read()\n\n\u00a0fails with the following exception:\n\u00a0\n\n\r\njava.lang.NullPointerException at org.apache.parquet.crypto.keytools.FileKeyUnwrapper.getKey(FileKeyUnwrapper.java:87) \u00a0\n\nIt seems like the reason is that the withConf method within the ParquetReader builder clears the optionsBuilder set earlier.\nHere is a sample test showcasing the issue: https://gist.github.com/a2l007/3d813cc5e44c45100dda169dc6245ae4",
        "Issue Links": []
    },
    "PARQUET-2186": {
        "Key": "PARQUET-2186",
        "Summary": "[Java] parquet-mr fails compiling",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "12/Sep/22 14:09",
        "Updated": "12/Sep/22 14:09",
        "Resolved": null,
        "Description": "This is on git master:\n\n\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary for Apache Parquet MR 1.13.0-SNAPSHOT:\r\n[INFO] \r\n[INFO] Apache Parquet MR .................................. FAILURE [  0.958 s]\r\n[INFO] Apache Parquet Format Structures ................... SKIPPED\r\n[INFO] Apache Parquet Generator ........................... SKIPPED\r\n[INFO] Apache Parquet Common .............................. SKIPPED\r\n[INFO] Apache Parquet Encodings ........................... SKIPPED\r\n[INFO] Apache Parquet Column .............................. SKIPPED\r\n[INFO] Apache Parquet Arrow ............................... SKIPPED\r\n[INFO] Apache Parquet Jackson ............................. SKIPPED\r\n[INFO] Apache Parquet Hadoop .............................. SKIPPED\r\n[INFO] Apache Parquet Avro ................................ SKIPPED\r\n[INFO] Apache Parquet Benchmarks .......................... SKIPPED\r\n[INFO] Apache Parquet Command-line ........................ SKIPPED\r\n[INFO] Apache Parquet Pig ................................. SKIPPED\r\n[INFO] Apache Parquet Pig Bundle .......................... SKIPPED\r\n[INFO] Apache Parquet Protobuf ............................ SKIPPED\r\n[INFO] Apache Parquet Scala ............................... SKIPPED\r\n[INFO] Apache Parquet Thrift .............................. SKIPPED\r\n[INFO] Apache Parquet Hadoop Bundle ....................... SKIPPED\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  1.148 s\r\n[INFO] Finished at: 2022-09-12T16:06:24+02:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (default) on project parquet: Too many files with unapproved license: 1 See RAT report in: /home/antoine/parquet/mr/target/rat.txt -> [Help 1]\r\n[ERROR] \r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\r\n\n\nThis is the \"RAT\" (sic) report:\n\n\r\n*****************************************************\r\n\r\nFiles with unapproved licenses:\r\n\r\n  cli.sh\r\n\r\n*****************************************************\r\n\r\n*****************************************************\r\n  Files with Apache License headers will be marked AL\r\n  Binary files (which do not require any license headers) will be marked B\r\n  Compressed archives will be marked A\r\n  Notices, licenses etc. will be marked N\r\n  N     NOTICE\r\n  AL    .travis.yml\r\n  AL    CHANGES.md\r\n  N     LICENSE\r\n  AL    dev/ci-before_install.sh\r\n  AL    dev/prepare-release.sh\r\n  AL    dev/finalize-release\r\n  AL    dev/ci-before_install-master.sh\r\n  AL    dev/merge_parquet_pr.py\r\n  AL    dev/COMMITTERS.md\r\n  AL    dev/source-release.sh\r\n  AL    dev/README.md\r\n  N     src/license.txt\r\n  AL    .editorconfig\r\n  AL    changelog.sh\r\n  AL    .github/workflows/test.yml\r\n  B     doc/dremel_paper/schema.png\r\n  B     doc/dremel_paper/dremel_example.png\r\n  AL    pom.xml\r\n !????? cli.sh\r\n  AL    PoweredBy.md\r\n  AL    README.md\r\n \r\n*****************************************************\r\n\n\nThis is because I have a script file \"cli.sh\" at the base of the git checkout.\nThe \"RAT\" report shouldn't fail because of unrelated files that are not in the git repository...",
        "Issue Links": []
    },
    "PARQUET-2187": {
        "Key": "PARQUET-2187",
        "Summary": "Add Parquet file containing a boolean column with RLE encoding to paquet",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-testing",
        "Assignee": "Nishanth",
        "Reporter": "Nishanth",
        "Created": "13/Sep/22 17:14",
        "Updated": "29/Sep/22 07:51",
        "Resolved": "29/Sep/22 07:50",
        "Description": "Precursor to https://issues.apache.org/jira/browse/ARROW-17450 .\u00a0\nAdd a test file in parquet-testing containing a boolean column with RLE encoding.\u00a0\nThe test files will be used by parquet implementation to validate the encoding can be read",
        "Issue Links": [
            "https://github.com/apache/parquet-testing/pull/26",
            "https://github.com/apache/parquet-testing/pull/27"
        ]
    },
    "PARQUET-2188": {
        "Key": "PARQUET-2188",
        "Summary": "Add SkipRecords API to RecordReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "13/Sep/22 23:45",
        "Updated": "01/Nov/22 02:01",
        "Resolved": "31/Oct/22 17:41",
        "Description": "The RecordReader is missing an API to skip records. There is a Skip method in the ColumnReader, but that skips based on the number of values/levels and not records. For repeated fields, this SkipRecords API will detect the record boundaries and correctly skip the right number of values for the requested number of records.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14142"
        ]
    },
    "PARQUET-2189": {
        "Key": "PARQUET-2189",
        "Summary": "byte array has better performance than  ByteBuffer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jiangjiguang0719",
        "Created": "14/Sep/22 02:53",
        "Updated": "14/Sep/22 03:05",
        "Resolved": null,
        "Description": "Currently the The abstract class BytePacker has the following method\n@Deprecated\npublic void unpack8Values(final byte[] input, final int inPos, final int[] output, final int outPos) \n{\r\n\u00a0 \u00a0 unpack8Values(ByteBuffer.wrap(input), inPos, output, outPos);\r\n\r\n}\n\nI don\u2019t know why to use ByteBuffer wrap byte[], ByteBuffer has poor performance.\n\n\u00a0\n\nI suggest using \u00a0\n\npublic abstract void unpack8Values(final byte[]input, final int inPos, final int[] output, final int outPos);\n\nto replace\n\n@Deprecated\npublic void unpack8Values(final byte[] input, final int inPos, final int[] output, final int outPos) {\n\u00a0 \u00a0 unpack8Values(ByteBuffer.wrap(input), inPos, output, outPos);\n\n}\n\n\u00a0\n\u00a0\nTested by me the byte array api has better performance than ByteBuffer api,\u00a0\nMy test result is:\n[Unpack8ValuesByteArray spent time] 80 ms\n[Unpack8ValuesByteBuffer spent time] 133 ms\n\u00a0\nMy test code is:\npackage org.apache.parquet.column.values.bitpacking;\nimport java.nio.ByteBuffer;\npublic class ByteBufferTest {\n\u00a0 private static final BytePacker bytePacker = Packer.LITTLE_ENDIAN.newBytePacker(7);\n\u00a0 private static final int COUNT = 100000;\n\u00a0 public static void main(String[] args) {\n\u00a0 \u00a0 byte \u00a0[] in \u00a0= new byte[1008];\n\u00a0 \u00a0 int [] out = new int[1152];\n\u00a0 \u00a0 int [] out1 = new int[1152];\n\u00a0 \u00a0 int [] out2 = new int[1152];\n\u00a0 \u00a0 int res = 0;\n\u00a0 \u00a0 for(int i = 0; i < in.length; i++) \n{\r\n\u00a0 \u00a0 \u00a0 in[i] = (byte) i;\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesBytes(in, out, i % out.length);\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 res = 0;\n\u00a0 \u00a0 long t1 = System.currentTimeMillis();\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesBytes(in, out1, i % out.length);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 long t2 = System.currentTimeMillis();\n\u00a0 \u00a0 System.out.println(\"[Unpack8ValuesByteArray spent time] \" + (t2-t1) + \" ms\");\n\u00a0 \u00a0 ByteBuffer byteBuffer = ByteBuffer.wrap(in);\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesByteBuffer(byteBuffer, out, i % out.length);\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 res = 0;\n\u00a0 \u00a0 long t3 = System.currentTimeMillis();\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesByteBuffer(byteBuffer, out2, i % out.length);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 long t4 = System.currentTimeMillis();\n\u00a0 \u00a0 System.out.println(\"[Unpack8ValuesByteBuffer spent time] \" + (t4-t3) + \" ms\");\n\u00a0 \u00a0 for (int i=0; i<out1.length; i++) {\n\u00a0 \u00a0 \u00a0 if(out1[i] != out2[i]) \n{\r\n\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"diff: \" + out1[i] + \" \" + out2[i]);\r\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 }\n\u00a0 }\n\u00a0 private static int unpack8ValuesBytes(byte [] in, int [] out, int ctr) {\n\u00a0 \u00a0 for(int i = 0, j = 0; i < in.length; i+=7, j+=8) \n{\r\n\u00a0 \u00a0 \u00a0 bytePacker.unpack8Values(in, i, out, j);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return out[ctr];\n\u00a0 }\n\u00a0 private static int unpack8ValuesByteBuffer(ByteBuffer in, int [] out, int ctr) {\n\u00a0 \u00a0 for(int i = 0, j = 0; i < in.capacity(); i+=7, j+=8) {\n\u00a0 \u00a0 \u00a0 bytePacker.unpack8Values(in, i, out, j);\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return out[ctr];\n\u00a0 }\n}",
        "Issue Links": []
    },
    "PARQUET-2190": {
        "Key": "PARQUET-2190",
        "Summary": "byte array has better performance than  ByteBuffer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jiangjiguang0719",
        "Created": "14/Sep/22 03:01",
        "Updated": "06/Mar/23 02:56",
        "Resolved": null,
        "Description": "The BytePacker should add the following method\npublic abstract void unpack8Values(final byte[] input, final int inPos, final int[] output, final int outPos);\nto replace method\u00a0\u00a0\n@Deprecated\npublic void unpack8Values(final byte[] input, final int inPos, final int[] output, final int outPos) \n{\r\n\u00a0 \u00a0 unpack8Values(ByteBuffer.wrap(input), inPos, output, outPos);\r\n}\n\n\u00a0\nTested by me byte array has better performance than \u00a0ByteBuffer,\nThe test result is:\n[Unpack8ValuesByteArray spent time] 80 ms\n[Unpack8ValuesByteBuffer spent time] 133 ms\n\u00a0\nThe test code is:\npackage org.apache.parquet.column.values.bitpacking;\nimport java.nio.ByteBuffer;\npublic class ByteBufferTest {\n\u00a0 private static final BytePacker bytePacker = Packer.LITTLE_ENDIAN.newBytePacker(7);\n\u00a0 private static final int COUNT = 100000;\n\u00a0 public static void main(String[] args) {\n\u00a0 \u00a0 byte \u00a0[] in \u00a0= new byte[1008];\n\u00a0 \u00a0 int [] out = new int[1152];\n\u00a0 \u00a0 int [] out1 = new int[1152];\n\u00a0 \u00a0 int [] out2 = new int[1152];\n\u00a0 \u00a0 int res = 0;\n\u00a0 \u00a0 for(int i = 0; i < in.length; i++) \n{\r\n\u00a0 \u00a0 \u00a0 in[i] = (byte) i;\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesBytes(in, out, i % out.length);\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 res = 0;\n\u00a0 \u00a0 long t1 = System.currentTimeMillis();\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesBytes(in, out1, i % out.length);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 long t2 = System.currentTimeMillis();\n\u00a0 \u00a0 System.out.println(\"[Unpack8ValuesByteArray spent time] \" + (t2-t1) + \" ms\");\n\u00a0 \u00a0 ByteBuffer byteBuffer = ByteBuffer.wrap(in);\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesByteBuffer(byteBuffer, out, i % out.length);\r\n\u00a0 \u00a0 }\n\n\u00a0 \u00a0 res = 0;\n\u00a0 \u00a0 long t3 = System.currentTimeMillis();\n\u00a0 \u00a0 for(int i = 0; i < COUNT; i++) \n{\r\n\u00a0 \u00a0 \u00a0 res += unpack8ValuesByteBuffer(byteBuffer, out2, i % out.length);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 long t4 = System.currentTimeMillis();\n\u00a0 \u00a0 System.out.println(\"[Unpack8ValuesByteBuffer spent time] \" + (t4-t3) + \" ms\");\n\u00a0 \u00a0 for (int i=0; i<out1.length; i++) {\n\u00a0 \u00a0 \u00a0 if(out1[i] != out2[i]) \n{\r\n\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"diff: \" + out1[i] + \" \" + out2[i]);\r\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 }\n\u00a0 }\n\u00a0 private static int unpack8ValuesBytes(byte [] in, int [] out, int ctr) {\n\u00a0 \u00a0 for(int i = 0, j = 0; i < in.length; i+=7, j+=8) \n{\r\n\u00a0 \u00a0 \u00a0 bytePacker.unpack8Values(in, i, out, j);\r\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return out[ctr];\n\u00a0 }\n\u00a0 private static int unpack8ValuesByteBuffer(ByteBuffer in, int [] out, int ctr) {\n\u00a0 \u00a0 for(int i = 0, j = 0; i < in.capacity(); i+=7, j+=8) {\n\u00a0 \u00a0 \u00a0 bytePacker.unpack8Values(in, i, out, j);\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return out[ctr];\n\u00a0 }\n}",
        "Issue Links": []
    },
    "PARQUET-2191": {
        "Key": "PARQUET-2191",
        "Summary": "Upgrade Scala to 2.12.17",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "16/Sep/22 04:51",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:10",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2192": {
        "Key": "PARQUET-2192",
        "Summary": "Add Java 17 build test to GitHub action",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-testing",
        "Assignee": null,
        "Reporter": "Yuming Wang",
        "Created": "17/Sep/22 06:50",
        "Updated": "12/Apr/23 01:33",
        "Resolved": "26/Mar/23 06:09",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2193": {
        "Key": "PARQUET-2193",
        "Summary": "Encrypting only one field in nested field prevents reading of other fields in nested field without keys",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Vignesh Nageswaran",
        "Created": "20/Sep/22 13:00",
        "Updated": "10/May/23 09:20",
        "Resolved": null,
        "Description": "Hi Team,\nWhile exploring parquet encryption, it is found that, if a field in nested column is encrypted , and If I want to read this parquet directory from other applications which does not have encryption keys to decrypt it, I cannot read the remaining fields of the nested column without keys.\u00a0\nExample\u00a0\n`\n\n\r\ncase class nestedItem(ic: Int = 0, sic : Double, pc: Int = 0)\r\ncase class SquareItem(int_column: Int, square_int_column : Double, partitionCol: Int, nestedCol :nestedItem)\r\n`\n\nIn the case class `SquareItem` , `nestedCol` field is nested field and I want to encrypt a field `ic` within it.\u00a0\n\u00a0\nI also want the footer to be non encrypted , so that I can use the encrypted parquet file by legacy applications.\u00a0\n\u00a0\nEncryption is successful, however, when I query the parquet file using spark 3.3.0 without having any configuration for parquet encryption set up , I cannot non encrypted fields of `nestedCol` `sic`. I was expecting that only `nestedCol` `ic` field will not be querable.\n\u00a0\n\u00a0\nReproducer.\u00a0\nSpark 3.3.0 Using Spark-shell\u00a0\nDownloaded the file parquet-hadoop-1.12.0-tests.jar and added it to spark-jars folder\nCode to create encrypted data. # \u00a0\n\u00a0\n\n\r\nsc.hadoopConfiguration.set(\"parquet.crypto.factory.class\" ,\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\")\r\n\r\nsc.hadoopConfiguration.set(\"parquet.encryption.kms.client.class\" ,\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\")\r\n\r\nsc.hadoopConfiguration.set(\"parquet.encryption.key.list\",\"key1a: BAECAwQFBgcICQoLDA0ODw==, key2a: BAECAAECAAECAAECAAECAA==, keyz: BAECAAECAAECAAECAAECAA==\")\r\n\r\nsc.hadoopConfiguration.set(\"parquet.encryption.key.material.store.internally\",\"false\")\r\n\r\nval encryptedParquetPath = \"/tmp/par_enc_footer_non_encrypted\"\r\nvalpartitionCol = 1\r\ncase class nestedItem(ic: Int = 0, sic : Double, pc: Int = 0)\r\ncase class SquareItem(int_column: Int, square_int_column : Double, partitionCol: Int, nestedCol :nestedItem)\r\nval dataRange = (1 to 100).toList\r\nval squares = sc.parallelize(dataRange.map(i => new SquareItem(i, scala.math.pow(i,2), partitionCol,nestedItem(i,i))))\r\nsquares.toDS().show()\r\nsquares.toDS().write.partitionBy(\"partitionCol\").mode(\"overwrite\").option(\"parquet.encryption.column.keys\", \"key1a:square_int_column,nestedCol.ic;\").option(\"parquet.encryption.plaintext.footer\",true).option(\"parquet.encryption.footer.key\", \"keyz\").parquet(encryptedParquetPath)\r\n\n\nCode to read the data trying to access non encrypted nested field by opening a new spark-shell\n\u00a0\n\n\r\nval encryptedParquetPath = \"/tmp/par_enc_footer_non_encrypted\"\r\nspark.sqlContext.read.parquet(encryptedParquetPath).createOrReplaceTempView(\"test\")\r\nspark.sql(\"select nestedCol.sic from test\").show()\n\nAs you can see that nestedCol.sic is not encrypted , I was expecting the results, but\nI get the below error\n\u00a0\n\n\r\nCaused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: [square_int_column]. Null File Decryptor\r\n\u00a0 at org.apache.parquet.hadoop.metadata.EncryptedColumnChunkMetaData.decryptIfNeeded(ColumnChunkMetaData.java:602)\r\n\u00a0 at org.apache.parquet.hadoop.metadata.ColumnChunkMetaData.getEncodings(ColumnChunkMetaData.java:348)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetRecordReader.checkDeltaByteArrayProblem(ParquetRecordReader.java:191)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:177)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:375)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\u00a0 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\u00a0 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\u00a0 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\u00a0 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\u00a0 at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\u00a0 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\u00a0 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\u00a0 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\u00a0 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\u00a0 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\u00a0 at java.base/java.lang.Thread.run(Thread.java:833)",
        "Issue Links": [
            "/jira/browse/PARQUET-2297"
        ]
    },
    "PARQUET-2194": {
        "Key": "PARQUET-2194",
        "Summary": "parquet.encryption.plaintext.footer parameter being true, code expects parquet.encryption.footer.key",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Vignesh Nageswaran",
        "Created": "20/Sep/22 13:08",
        "Updated": "29/Sep/22 07:45",
        "Resolved": null,
        "Description": "Hi Team,\nI want my footer in parquet file to be non encrypted. so I set the parquet.encryption.plaintext.footer to be true, but when I tried to run my code, parquet-mr is expecting __ value __ for the __ property parquet.encryption.footer.key \u00a0** \u00a0\nReproducer\nSpark 3.3.0\u00a0\nDownload the [file|https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.12.0/parquet-hadoop-1.12.0-tests.jar ] and place it in spark - jar directory\u00a0\nusing spark-shell\n\n\r\nsc.hadoopConfiguration.set(\"parquet.crypto.factory.class\" ,\"org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory\") sc.hadoopConfiguration.set(\"parquet.encryption.kms.client.class\" ,\"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\") sc.hadoopConfiguration.set(\"parquet.encryption.key.list\",\"key1a: BAECAwQFBgcICQoLDA0ODw==, key2a: BAECAAECAAECAAECAAECAA==, keyz: BAECAAECAAECAAECAAECAA==\") sc.hadoopConfiguration.set(\"parquet.encryption.key.material.store.internally\",\"false\") \r\nval encryptedParquetPath = \"/tmp/par_enc_footer_non_encrypted\" \r\nval partitionCol = 1 \r\ncase class nestedItem(ic: Int = 0, sic : Double, pc: Int = 0) \r\ncase class SquareItem(int_column: Int, square_int_column : Double, partitionCol: Int, nestedCol :nestedItem) \r\nval dataRange = (1 to 100).toList \r\nval squares = sc.parallelize(dataRange.map(i => new SquareItem(i, scala.math.pow(i,2), partitionCol,nestedItem(i,i)))) \r\nsquares.toDS().show() squares.toDS().write.partitionBy(\"partitionCol\").mode(\"overwrite\").option(\"parquet.encryption.column.keys\", \"key1a:square_int_column,nestedCol.ic;\").option(\"parquet.encryption.plaintext.footer\",true).parquet(encryptedParquetPath)\n\nI get the below error, my expectation is if I set properties for my footer to be plain text, why do we need keys for footer.\n\u00a0\n\n\r\n\u00a0\r\nCaused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: Undefined footer key\r\n\u00a0 at org.apache.parquet.crypto.keytools.PropertiesDrivenCryptoFactory.getFileEncryptionProperties(PropertiesDrivenCryptoFactory.java:88)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetOutputFormat.createEncryptionProperties(ParquetOutputFormat.java:554)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:478)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\r\n\u00a0 at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:155)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:298)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:365)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\r\n\u00a0 at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\r\n\u00a0 at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\r\n\u00a0 ... 9 more",
        "Issue Links": []
    },
    "PARQUET-2195": {
        "Key": "PARQUET-2195",
        "Summary": "Add scan command to parquet-cli",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "23/Sep/22 06:09",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:02",
        "Description": "parquet-cli has cat and head commands to print the records but it does not have the capability to scan (w/o printing) all records to check if the file is corrupted.",
        "Issue Links": []
    },
    "PARQUET-2196": {
        "Key": "PARQUET-2196",
        "Summary": "Support LZ4_RAW codec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "27/Sep/22 08:47",
        "Updated": "25/Apr/23 08:12",
        "Resolved": "16/Dec/22 05:05",
        "Description": "There is a long history about the LZ4 interoperability of parquet files between parquet-mr and parquet-cpp (which is now in the Apache Arrow). Attached links are the evidence. In short, a new LZ4_RAW codec type has been introduced since parquet format v2.9.0. However, only parquet-cpp supports LZ4_RAW. The parquet-mr library still uses the old Hadoop-provided LZ4 codec and cannot read parquet files with LZ4_RAW.",
        "Issue Links": [
            "/jira/browse/SPARK-43273",
            "/jira/browse/PARQUET-1974",
            "/jira/browse/ARROW-9177",
            "/jira/browse/PARQUET-1878",
            "/jira/browse/PARQUET-1996"
        ]
    },
    "PARQUET-2197": {
        "Key": "PARQUET-2197",
        "Summary": "Document uniform encryption",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "28/Sep/22 15:01",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:07",
        "Description": "Document the hadoop parameter for uniform encryption",
        "Issue Links": []
    },
    "PARQUET-2198": {
        "Key": "PARQUET-2198",
        "Summary": "Vulnerabilities in jackson-databind",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0141ukasz Dziedziul",
        "Created": "06/Oct/22 09:36",
        "Updated": "12/Apr/23 07:32",
        "Resolved": "26/Mar/23 06:01",
        "Description": "Update jackson-databind to mitigate CVEs:\n\nCVE-2022-42003\u00a0-\u00a0https://nvd.nist.gov/vuln/detail/CVE-2022-42003\nCVE-2022-42004\u00a0-\u00a0https://nvd.nist.gov/vuln/detail/CVE-2022-42004 (fixed in \u00a02.13.4)",
        "Issue Links": []
    },
    "PARQUET-2199": {
        "Key": "PARQUET-2199",
        "Summary": "checkBlockSizeReached zero record size perf issue",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Huicheng Song",
        "Created": "06/Oct/22 19:17",
        "Updated": "06/Oct/22 19:59",
        "Resolved": null,
        "Description": "Parquet checks Block size after writing records to decide when it shall flush. This is relatively expensive, so it estimates the next check based on record size, record count etc.\nFor small records (less than 1byte after compression), the average record size is 0 after integer division. This caused overflow when calculating the next record count for block size check, resulting block size being checked for every record.",
        "Issue Links": []
    },
    "PARQUET-2200": {
        "Key": "PARQUET-2200",
        "Summary": "Add SkipValues() to decoder, Refactor TypedColumnReader::Skip to use it.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-cpp",
        "Assignee": null,
        "Reporter": "fatemah",
        "Created": "06/Oct/22 20:08",
        "Updated": "06/Oct/22 20:08",
        "Resolved": null,
        "Description": "The proposed SkipValues will read and throw away values. We can then refactor TypedColumnReader and RecordReader (https://issues.apache.org/jira/browse/PARQUET-2188) Skip methods to use it.",
        "Issue Links": []
    },
    "PARQUET-2201": {
        "Key": "PARQUET-2201",
        "Summary": "Add Stress test for RecordReader SkipRecords",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-12.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "06/Oct/22 22:06",
        "Updated": "11/May/23 01:17",
        "Resolved": "24/Feb/23 04:58",
        "Description": "Adding a stress test that will call a random sequence of ReadRecords and SkipRecords.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14879"
        ]
    },
    "PARQUET-2202": {
        "Key": "PARQUET-2202",
        "Summary": "Redundant String allocation on the hot path in CapacityByteArrayOutputStream.setByte",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Andrei Pangin",
        "Created": "09/Oct/22 22:57",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "26/Mar/23 05:42",
        "Description": "Profiling of a Spark application revealed a performance issue in production:\nCapacityByteArrayOutputStream.setByte consumed 2.2% of total CPU time and made up 4.6% of total allocations. However, in normal case, this method should allocate nothing at all.\nHere is an excerpt from async-profiler report.\nCPU profile:\n\nAllocation profile:\n\nThe reason is a checkArgument() call with an unconditionally constructed dynamic String:\nhttps://github.com/apache/parquet-mr/blob/62b774cd0f0c60cfbe540bbfa60bee15929af5d4/parquet-common/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java#L303\nThe suggested fix is to move String construction under the condition:\n\n\r\nif (index >= bytesUsed) {\r\n  throw new IllegalArgumentException(\"Index: \" + index +\r\n      \" is >= the current size of: \" + bytesUsed);\r\n}",
        "Issue Links": []
    },
    "PARQUET-2203": {
        "Key": "PARQUET-2203",
        "Summary": "Make ParquetReadOptions and HadoopReadOptions extendable",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chao Sun",
        "Created": "24/Oct/22 17:04",
        "Updated": "24/Oct/22 17:04",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2204": {
        "Key": "PARQUET-2204",
        "Summary": "TypedColumnReaderImpl::Skip should reuse scratch space",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "25/Oct/22 23:04",
        "Updated": "08/Dec/22 15:42",
        "Resolved": "08/Dec/22 15:42",
        "Description": "TypedColumnReaderImpl::Skip allocates scratch space on every call. The scratch space is used to read rep/def levels and values and throw them away. The memory allocation slows down the skip based on microbenchmarks. The scratch space can be allocated once and re-used.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14509"
        ]
    },
    "PARQUET-2205": {
        "Key": "PARQUET-2205",
        "Summary": "Accelerate CI by commonizing thrift build and leveraging cache",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kengo Seki",
        "Reporter": "Kengo Seki",
        "Created": "26/Oct/22 05:28",
        "Updated": "26/Oct/22 06:34",
        "Resolved": null,
        "Description": "Building thrift compiler takes several minutes and occupies approximately one out of four or five in the total CI duration.\nIt can be improved by optimizing workflow and leveraging cache so as to avoid building thrift many times.",
        "Issue Links": []
    },
    "PARQUET-2206": {
        "Key": "PARQUET-2206",
        "Summary": "Microbenchmark for ColumnReadaer ReadBatch and Skip",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "26/Oct/22 18:13",
        "Updated": "16/Nov/22 00:23",
        "Resolved": "15/Nov/22 16:19",
        "Description": "Adding a micro benchmark for column reader ReadBatch and Skip. Later, I will add benchmarks for RecordReader's ReadRecords and SkipRecords.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14523"
        ]
    },
    "PARQUET-2207": {
        "Key": "PARQUET-2207",
        "Summary": "support saving meta and data seperately",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "lei yu",
        "Created": "27/Oct/22 01:40",
        "Updated": "27/Oct/22 01:40",
        "Resolved": null,
        "Description": "I often needs to create tens of milliions of small dataframes and save them into parquet files. all these dataframes have the same column and index information. and normally they have the same number of rows(around 300).\u00a0\u00a0\nas the data is quite small, the parquet meta information is relatively large and it's quite a big waste of disk space, as the same meta information is repeated tens of millions of times.\nconcating them into one big parquet file can save disk space, but it's not friendly for parallel processing of each small dataframe.\u00a0\n\u00a0\nif I can save one copy of the meta information into one file, and the rest parquet files contains only the data. then the disk space can be saved, and still good for parallel processing.\nseems to me this is possible by design, but I couldn't find any API supporting this.",
        "Issue Links": []
    },
    "PARQUET-2208": {
        "Key": "PARQUET-2208",
        "Summary": "Add details to nested column encryption config doc and exception text",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "31/Oct/22 07:27",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 06:00",
        "Description": "Parquet columnar encryption requires an explicit full path for each column to be encrypted. If a partial path is configured, the thrown exception is not informative enough, doesn't help much in correcting the parameters.\nThe goal is to make the exception print something like:\nCaused by: org.apache.parquet.crypto.ParquetCryptoRuntimeException: Encrypted column [rider] not in file schema column list: [foo] , [rider.list.element.foo] , [rider.list.element.bar] , [ts] , [uuid]",
        "Issue Links": []
    },
    "PARQUET-2209": {
        "Key": "PARQUET-2209",
        "Summary": "[C++] Optimize skip for the case that number of values to skip equals page size",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "31/Oct/22 16:20",
        "Updated": "02/Nov/22 10:53",
        "Resolved": "02/Nov/22 10:53",
        "Description": "Optimize skip for the case that the number of values to skip equals page size. Right now, we end up reading to the end of the page and throwing away the rep/defs and values that we have read, which is unnecessary.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14545"
        ]
    },
    "PARQUET-2210": {
        "Key": "PARQUET-2210",
        "Summary": "[C++] Skip pages based on header metadata using a callback",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "31/Oct/22 16:40",
        "Updated": "14/Jan/23 10:35",
        "Resolved": "12/Jan/23 23:11",
        "Description": "Currently, we do not expose the page header metadata and they cannot be used for skipping pages. I propose exposing the metadata through a callback that would allow the caller to decide if they want to read or skip the page based on the metadata. The signature of the callback would be the following: std::function<bool(const format::PageHeader&)> skip_page_callback)",
        "Issue Links": [
            "/jira/browse/ARROW-13998",
            "https://github.com/apache/arrow/pull/14603"
        ]
    },
    "PARQUET-2211": {
        "Key": "PARQUET-2211",
        "Summary": "[C++] Print ColumnMetaData.encoding_stats field",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-11.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "01/Nov/22 06:00",
        "Updated": "06/Nov/22 17:12",
        "Resolved": "06/Nov/22 14:56",
        "Description": "The ParquetFilePrinter of parquet-cpp prints column chunk encodings solely from ColumnMetaData.encodings field. As ColumnMetaData.encoding_stats has been introduced long ago and it is a better source of obtain encodings, the printer should be aware of it.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/14556"
        ]
    },
    "PARQUET-2212": {
        "Key": "PARQUET-2212",
        "Summary": "Add ByteBuffer api for decryptors to allow direct memory to be decrypted",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Parth Chandra",
        "Reporter": "Parth Chandra",
        "Created": "09/Nov/22 19:21",
        "Updated": "23/May/23 15:55",
        "Resolved": "23/May/23 15:55",
        "Description": "The decrypt API in BlockCipher.Decryptor currently only provides an api that takes in a byte array\n\n\r\nbyte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD);\n\nA parquet reader that uses the DirectByteBufferAllocator has to incur the cost of copying the data into a byte array (and sometimes back to a DirectByteBuffer) to decrypt data.\nThis proposes adding a new API that accepts ByteBuffer as input and avoids the data copy.\n\n\r\nByteBuffer decrypt(ByteBuffer from, byte[] AAD);\n\nThe decryption in ColumnChunkPageReadStore can also be updated to use the ByteBuffer based api if the buffer is a DirectByteBuffer. If the buffer is a HeapByteBuffer, then we can continue to use the byte array API since that does not incur a copy when the underlying byte array is accessed.\nAlso, some investigation has shown that decryption with ByteBuffers is not able to use hardware acceleration in JVM's before JDK17. In those cases, the overall decryption speed is faster with byte arrays even after incurring the overhead of making a copy.\u00a0\nThe proposal, then, is to enable the use of the ByteBuffer api for DirectByteBuffers only, and only if the JDK is JDK17 or higher or the user explicitly configures it.",
        "Issue Links": []
    },
    "PARQUET-2213": {
        "Key": "PARQUET-2213",
        "Summary": "Add an alternative InputFile.newStream that allow an input range",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chao Sun",
        "Created": "10/Nov/22 18:06",
        "Updated": "16/Nov/22 17:26",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2214": {
        "Key": "PARQUET-2214",
        "Summary": "Support re-encryption in ColumnEncryptor",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kai Jiang",
        "Reporter": "Kai Jiang",
        "Created": "17/Nov/22 04:44",
        "Updated": "17/Nov/22 04:44",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2215": {
        "Key": "PARQUET-2215",
        "Summary": "Document how DELTA_BINARY_PACKED handles overflow for deltas",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Rok Mihevc",
        "Created": "22/Nov/22 19:18",
        "Updated": "25/Nov/22 08:47",
        "Resolved": null,
        "Description": "Current docs do not explicitly state how overflow is handled.\nSee discussion for more details.",
        "Issue Links": []
    },
    "PARQUET-2216": {
        "Key": "PARQUET-2216",
        "Summary": "Parquet writer classes don't close underlying output stream in case of errors.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Andrei Lopukhov",
        "Created": "25/Nov/22 13:45",
        "Updated": "02/Dec/22 11:18",
        "Resolved": null,
        "Description": "org.apache.parquet.io.OutputFile interface does not implement Closeable.\nIn my opinion it implies that created streams are fully managed by parquet-mr classes.\nUnfortunately opened stream will not be closed in case of IO or other failure.\nThere are two places I can find for this problem:\n\nDuring writer creation (org.apache.parquet.hadoop.ParquetWriter.Builder#build()) - created stream should be closed if writer creation fails.\nDuring writer close(org.apache.parquet.hadoop.ParquetWriter#close) - underlying stream should be closed regardless of any faced failures.\nAlthough I didn't examine ParquetReaded that much.",
        "Issue Links": []
    },
    "PARQUET-2217": {
        "Key": "PARQUET-2217",
        "Summary": "Support gorilla encoding for float numbers",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Frank Dai",
        "Created": "04/Dec/22 22:44",
        "Updated": "04/Dec/22 22:44",
        "Resolved": null,
        "Description": "Gorilla is a de facto encoding algorithm for float numbers, it has been used by many time series database such as InfluxDB, TimescaleDB for a while.\nFor now Parquet only supports BYTE_STREAM_SPLIT encoding for float numbers, it would be nice to add gorilla encoding.",
        "Issue Links": []
    },
    "PARQUET-2218": {
        "Key": "PARQUET-2218",
        "Summary": "[Format] Clarify CRC computation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "13/Dec/22 14:38",
        "Updated": "03/Jan/23 14:28",
        "Resolved": "03/Jan/23 14:28",
        "Description": "The format spec on CRC checksumming felt ambiguous when trying to implement it in Parquet C++, so we should make the wording clearer.\n(see discussion on https://github.com/apache/parquet-format/pull/126#issuecomment-1348081137 and below)",
        "Issue Links": [
            "/jira/browse/PARQUET-1539"
        ]
    },
    "PARQUET-2219": {
        "Key": "PARQUET-2219",
        "Summary": "ParquetFileReader throws a runtime exception when a file contains only headers and now row data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.1",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "chris stockton",
        "Created": "16/Dec/22 04:48",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "16/Mar/23 09:27",
        "Description": "Google BigQuery has an option to export table data to Parquet-formatted files, but some of these files are written with header data only.\u00a0 When this happens and these files are opened with the ParquetFileReader, an exception is thrown:\nRuntimeException(\"Illegal row group of 0 rows\");\nIt seems like the ParquetFileReader should not throw an exception when it encounters such a file.\nhttps://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java#L949",
        "Issue Links": []
    },
    "PARQUET-2220": {
        "Key": "PARQUET-2220",
        "Summary": "Parquet Filter predicate storing nested string causing OOM's",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Abhishek Jain",
        "Created": "29/Dec/22 12:51",
        "Updated": "31/Dec/22 13:09",
        "Resolved": null,
        "Description": "Each Instance of ColumnFilterPredicate stores the filter values in toString variable eagerly. Which is not useful\n\n\r\nstatic abstract class ColumnFilterPredicate<T extends Comparable<T>> implements FilterPredicate, Serializable  {\r\n  private final Column<T> column;\r\n  private final T value;\r\n  private final String toString; \r\n\r\n\r\nprotected ColumnFilterPredicate(Column<T> column, T value) {\r\n  this.column = Objects.requireNonNull(column, \"column cannot be null\");\r\n\r\n  // Eq and NotEq allow value to be null, Lt, Gt, LtEq, GtEq however do not, so they guard against\r\n  // null in their own constructors.\r\n  this.value = value;\r\n\r\n  String name = getClass().getSimpleName().toLowerCase(Locale.ENGLISH);\r\n  this.toString = name + \"(\" + column.getColumnPath().toDotString() + \", \" + value + \")\";\r\n}\n\n\u00a0\n\u00a0\nIf your filter predicate is too long/nested this can take a lot of memory while creating Filter.\nWe have seen in our productions this can go upto 4gbs of space while opening multiple parquet readers\nSame thing is replicated in BinaryLogicalFilterPredicate. Where toString is eagerly calculated and stored in string and lot of duplication is happening while making And/or filter.\nI did not find use case of storing it so eagerly",
        "Issue Links": []
    },
    "PARQUET-2221": {
        "Key": "PARQUET-2221",
        "Summary": "[Format] Encoding spec incorrect for dictionary fallback",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "03/Jan/23 14:23",
        "Updated": "04/Jan/23 02:46",
        "Resolved": null,
        "Description": "The spec for DICTIONARY_ENCODING states that:\nIf the dictionary grows too big, whether in size or number of distinct values, the encoding will fall back to the plain encoding. \nhttps://github.com/apache/parquet-format/blob/master/Encodings.md#dictionary-encoding-plain_dictionary--2-and-rle_dictionary--8\nHowever, the parquet-mr implementation was deliberately changed to a different fallback mechanism in https://issues.apache.org/jira/browse/PARQUET-52\nI'm assuming the parquet-mr implementation is authoritative here. But then the spec is incorrect and should be fixed to reflect expected behavior.",
        "Issue Links": []
    },
    "PARQUET-2222": {
        "Key": "PARQUET-2222",
        "Summary": "[Format] RLE encoding spec incorrect for v2 data pages",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Xuwei Fu",
        "Reporter": "Antoine Pitrou",
        "Created": "04/Jan/23 16:02",
        "Updated": "15/Jun/23 15:38",
        "Resolved": null,
        "Description": "The spec (https://github.com/apache/parquet-format/blob/master/Encodings.md#run-length-encoding--bit-packing-hybrid-rle--3) has this:\n\n\r\nrle-bit-packed-hybrid: <length> <encoded-data>\r\nlength := length of the <encoded-data> in bytes stored as 4 bytes little endian (unsigned int32)\r\n\n\nBut the length is actually prepended only in v1 data pages, not in v2 data pages.",
        "Issue Links": []
    },
    "PARQUET-2223": {
        "Key": "PARQUET-2223",
        "Summary": "Parquet Data Masking for Column Encryption",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jiashen Zhang",
        "Created": "04/Jan/23 21:40",
        "Updated": "28/Jun/23 18:23",
        "Resolved": null,
        "Description": "Background\nWhat is Data Masking?\nData masking is a technique used to protect sensitive data by replacing it with modified or obscured values. The purpose of data masking is to ensure that sensitive information, such as Personally Identifiable Information (PII), remains hidden from unauthorized users while allowing authorized users to perform their tasks.\nHere are a few key points about data masking:\n\nProtection of Sensitive Data: Data masking helps to safeguard sensitive data, such as Social Security numbers, credit card numbers, names, addresses, and other personally identifiable information. By applying masking techniques, the original values are replaced with fictional or transformed data that retains the format and structure but removes any identifiable information.\nControlled Access: Data masking enables controlled access to sensitive data. Authorized users, typically with appropriate permissions, can access the unmasked or original data, while unauthorized users or users without the necessary permissions will only see the masked data.\nVarious Masking Techniques: There are different masking techniques available, depending on the specific data privacy requirements and use cases. Some commonly used techniques include:\n\t\nNullification: Replacing original data with NULL values.\nRandomization: Replacing sensitive data with randomly generated values.\nSubstitution: Replacing sensitive data with fictional but realistic values.\nHashing: Transforming sensitive data into irreversible hashed values.\nRedaction: Removing or masking specific parts of sensitive data while retaining other non-sensitive information.\n\n\n\n\nCompliance and Data Privacy: Data masking is often employed to comply with data protection regulations and maintain data privacy. By masking sensitive data, we can reduce the risk of data breaches and unauthorized access while still allowing legitimate users to perform their tasks.\nMaintaining Data Consistency: Data masking techniques aim to maintain data consistency and integrity by ensuring that masked data retains the original data's format, structure, and relationships. This allows applications and processes that rely on the data to continue functioning correctly.\n\nWhy do we need it?\nData masking serves several important purposes and provides numerous benefits. Here are some reasons why we need data masking:\n\nData Privacy and Compliance: Data masking helps us comply with data privacy regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA). These regulations require us to protect sensitive data and ensure that it is only accessible to authorized individuals. Data masking enables us to comply with these regulations by de-identifying sensitive data.\nMinimize Data Exposure: By masking sensitive data, we can reduce the risk of data breaches and unauthorized access. If a security breach occurs, the exposed data will be meaningless to unauthorized users due to the masking. This helps protect individuals' privacy and prevents misuse of sensitive information.\nSecure Testing and Development Environments: Data masking is particularly useful in creating secure testing and development environments. By masking sensitive data, we can use realistic but fictional data for testing, analysis, and development activities without exposing real personal or sensitive information.\nEnhanced Data Sharing: Data masking allows us to share data with external parties, such as partners or third-party vendors, while protecting sensitive information. Masked data can be shared with confidence, as the original sensitive values are replaced with transformed or fictional data.\nEmployee Privacy: Data masking helps protect employee privacy by obfuscating sensitive employee information, such as social security numbers or salary details, in databases or HR systems. This safeguards employees' personal data from unauthorized access or internal misuse.\nInsider Threat Mitigation: Data masking reduces the risk posed by insider threats, where authorized individuals intentionally or accidentally misuse or expose sensitive data. By masking data, even individuals with access to the data will only see masked or fictional values, limiting the potential damage caused by internal security breaches.\nFlexibility and Granularity: Data masking techniques offer flexibility and granularity in selecting the level of masking required for different types of data. We can determine the appropriate masking technique based on the sensitivity of the data and the specific use case.\n\nOverall, data masking is essential for protecting sensitive data, maintaining compliance with regulations, mitigating data breach risks, and enabling secure data sharing and testing environments. It plays a crucial role in ensuring data privacy and maintaining the trust of individuals whose data is being processed.",
        "Issue Links": []
    },
    "PARQUET-2224": {
        "Key": "PARQUET-2224",
        "Summary": "Publish SBOM artifacts",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "05/Jan/23 17:44",
        "Updated": "12/Apr/23 01:30",
        "Resolved": "26/Mar/23 05:59",
        "Description": null,
        "Issue Links": [
            "/jira/browse/AVRO-3700",
            "/jira/browse/FLINK-30578",
            "/jira/browse/SPARK-41893",
            "/jira/browse/ORC-1342"
        ]
    },
    "PARQUET-2225": {
        "Key": "PARQUET-2225",
        "Summary": "[C++] Allow reading dense with RecordReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-12.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "10/Jan/23 17:36",
        "Updated": "11/May/23 01:18",
        "Resolved": "03/Mar/23 19:41",
        "Description": "Currently ReadRecords reads spaced by default. Some readers may need to read the values dense, and reading spaced is less efficient than reading dense. We need an option for reading dense.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/17877"
        ]
    },
    "PARQUET-2226": {
        "Key": "PARQUET-2226",
        "Summary": "Support merge Bloom Filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "miracle",
        "Reporter": "Mars",
        "Created": "12/Jan/23 11:37",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "16/Jan/23 09:59",
        "Description": "We need to collect Parquet's bloom filter of multiple files, and then synthesize a more comprehensive bloom filter for common use.\u00a0\nGuava supports similar api operations\nhttps://guava.dev/releases/31.0.1-jre/api/docs/src-html/com/google/common/hash/BloomFilter.html#line.252",
        "Issue Links": []
    },
    "PARQUET-2227": {
        "Key": "PARQUET-2075 Unified Rewriter Tool",
        "Summary": "Refactor different file rewriters to use single implementation",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "14/Jan/23 15:05",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "30/Jan/23 01:33",
        "Description": "A new ParquetRewriter is implemented to support all logics in the ColumnPruner, CompressionConverter, ColumnMasker, and ColumnEncrypter. And refactor all the old rewriters to use ParquetRewriter under the hood.",
        "Issue Links": []
    },
    "PARQUET-2228": {
        "Key": "PARQUET-2075 Unified Rewriter Tool",
        "Summary": "ParquetRewriter supports more than one input file",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "14/Jan/23 15:07",
        "Updated": "12/Apr/23 01:28",
        "Resolved": "21/Feb/23 10:43",
        "Description": "ParquetRewriter currently supports only one input file. The scope of this task is to support multiple input files and the rewriter merges them into a single one w/o some rewrite options specified.",
        "Issue Links": []
    },
    "PARQUET-2229": {
        "Key": "PARQUET-2075 Unified Rewriter Tool",
        "Summary": "ParquetRewriter supports masking and encrypting the same column",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "14/Jan/23 15:08",
        "Updated": "12/Apr/23 01:28",
        "Resolved": "11/Feb/23 15:25",
        "Description": "ParquetRewriter does not yet support  masking and encrypting the same column. The scope of this task is to enable it.",
        "Issue Links": []
    },
    "PARQUET-2230": {
        "Key": "PARQUET-2075 Unified Rewriter Tool",
        "Summary": "Add a new rewrite command powered by ParquetRewriter",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-cli",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "14/Jan/23 15:11",
        "Updated": "12/Apr/23 01:28",
        "Resolved": "16/Mar/23 09:26",
        "Description": "parquet-cli has several commands for rewriting files but missing a consolidated one to provide the full features of ParquetRewriter.",
        "Issue Links": []
    },
    "PARQUET-2231": {
        "Key": "PARQUET-2231",
        "Summary": "[Format] Encoding spec incorrect for DELTA_BYTE_ARRAY",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Antoine Pitrou",
        "Reporter": "Antoine Pitrou",
        "Created": "16/Jan/23 10:10",
        "Updated": "19/Jan/23 22:21",
        "Resolved": "19/Jan/23 22:21",
        "Description": "The spec says that DELTA_BYTE_ARRAY is only supported for BYTE_ARRAY, but in parquet-mr it has been allowed for FIXED_LEN_BYTE_ARRAY as well since 2015.",
        "Issue Links": [
            "/jira/browse/PARQUET-152"
        ]
    },
    "PARQUET-2232": {
        "Key": "PARQUET-2232",
        "Summary": "Add an api to ColumnChunkMetaData to indicate if the column chunk uses a bloom filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-12.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Jinpeng Zhou",
        "Reporter": "Jinpeng Zhou",
        "Created": "18/Jan/23 05:14",
        "Updated": "11/May/23 01:18",
        "Resolved": "20/Jan/23 21:39",
        "Description": "Although bloom filter is not fully supported in parquet-cpp for now, it can be useful to provide an api that tells if a column chunk is using bloom filters. This would lead to better understanding of file characteristics.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/33736"
        ]
    },
    "PARQUET-2233": {
        "Key": "PARQUET-2233",
        "Summary": "Parquet Travis CI jobs to be turned off February 15th",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Xinli Shang",
        "Created": "24/Jan/23 20:17",
        "Updated": "25/Jan/23 18:25",
        "Resolved": null,
        "Description": "Greetings Parquet PMC,\nInfrastructure has reached out to you regarding the Travis CI Open Source policy changes, and the resulting need for Apache projects to migrate away from using Travis.\nSo far, we have received no response from your PMC.\nOn February 15th, we will begin the final phase of this migration, turning off Travis builds in order to bring our Travis usage down to 0.\nWe have found the following repositories mention or make use of .travis.yml files:\n\nparquet-mr.git\nparquet-cpp.git\n\nYou must immediately move to migrate your builds from Travis. If you do not, you will soon be unable to do builds that now rely on Travis.\nMany projects have moved to using GitHub Actions, and migrating to GHA is quite straightforward. Other projects use Jenkins providing ARM support, with nodes using the arm label\nIf you are unsure how to proceed, I would be happy to explain your next steps.\nPlease at least respond to acknowledge the need to migrate away from Travis, and to tell us your current plans.\nThank you!",
        "Issue Links": []
    },
    "PARQUET-2234": {
        "Key": "PARQUET-2159 Parquet bit-packing de/encode optimization",
        "Summary": "Parquet java vector decode optimization for Big Endian",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jiangjiguang0719",
        "Created": "29/Jan/23 02:33",
        "Updated": "29/Jan/23 02:35",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2235": {
        "Key": "PARQUET-2159 Parquet bit-packing de/encode optimization",
        "Summary": "Parquet java vector decode optimization Long for Big Endian",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jiangjiguang0719",
        "Created": "29/Jan/23 02:36",
        "Updated": "29/Jan/23 02:36",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2236": {
        "Key": "PARQUET-2159 Parquet bit-packing de/encode optimization",
        "Summary": "Parquet java vector decode optimization Long for Little Endian",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jiangjiguang0719",
        "Created": "29/Jan/23 02:37",
        "Updated": "29/Jan/23 02:37",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2237": {
        "Key": "PARQUET-2237",
        "Summary": "Improve performance when filters in RowGroupFilter can match exactly",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Mars",
        "Reporter": "Mars",
        "Created": "04/Feb/23 09:08",
        "Updated": "22/Apr/23 17:36",
        "Resolved": null,
        "Description": "If we can accurately judge by the minMax status, we don\u2019t need to load the dictionary from filesystem and compare one by one anymore.\nSimilarly , Bloomfilter needs to load from filesystem, it may costs time and memory. If we can exactly determine the existence/nonexistence of the value from minMax or dictionary filters , then we can avoid using Bloomfilter to Improve performance.\nFor example,\n\nread data greater than\u00a0x1\u00a0in the block, if minMax in status is all greater than\u00a0x1, then we don't need to read dictionary and compare one by one.\nIf we already have page dictionaries and have compared one by one, we don't need to read BloomFilter and compare.",
        "Issue Links": []
    },
    "PARQUET-2238": {
        "Key": "PARQUET-2238",
        "Summary": "Spec and parquet-mr disagree on DELTA_BYTE_ARRAY encoding",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": null,
        "Reporter": "Jan Finis",
        "Created": "04/Feb/23 11:30",
        "Updated": "04/Feb/23 11:33",
        "Resolved": null,
        "Description": "The spec in parquet-format specifies that DELTA_BYTE_ARRAY is only supported for the physical type BYTE_ARRAY. Yet, parquet-mr also uses it to encode FIXED_LEN_BYTE_ARRAY.\nSo, I guess the spec should be updated to include FIXED_LEN_BYTE_ARRAY in the supported types of DELTA_BYTE_ARRAY encoding, or the code should be changed to no longer write this encoding for FIXED_LEN_BYTE_ARRAY.\nI guess changing the spec is more prudent, given that\u00a0\na) the encoding can make sense for FIXED_LEN_BYTE_ARRAY\nand\nb) there might already be countless files written with this encoding / type combination.",
        "Issue Links": []
    },
    "PARQUET-2239": {
        "Key": "PARQUET-2239",
        "Summary": "Replace log4j1 with reload4j",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Akshat Mathur",
        "Created": "06/Feb/23 10:50",
        "Updated": "07/Feb/23 09:00",
        "Resolved": null,
        "Description": "Due to multiple CVE in log4j1, replace log4j dependency with reload4j.\nMore about reload4j: https://reload4j.qos.ch/",
        "Issue Links": []
    },
    "PARQUET-2240": {
        "Key": "PARQUET-2240",
        "Summary": "DateTimeFormatter is used in static context, but not thread safe",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Shani Elharrar",
        "Created": "07/Feb/23 09:05",
        "Updated": "17/Feb/23 08:50",
        "Resolved": null,
        "Description": "DateTimeFormatter is used in static context but not thread safe, a formatter instance is created in PrimitiveStringifer.DateStringifier, and DateStringifier is created in static final DATE_STRINGIFIER, TIMESTAMP_MILLIS_STRINGIFIER, TIMESTAMP_MICROS_STRINGIFIER, TIMESTAMP_NANOS_STRINGIFIER, TIMESTAMP_MILLIS_UTC_STRINGIFIER, TIMESTAMP_MICROS_UTC_STRINGIFIER, and TIMESTAMP_NANOS_UTC_STRINGIFIER.\nThis causes exceptions like the following to be thrown from parquet-code:\njava.lang.ArrayIndexOutOfBoundsException: Index 633 out of bounds for length 13\nstacktrace:\n\u00a0 \u00a0 at java.base/sun.util.calendar.BaseCalendar.getCalendarDateFromFixedDate(BaseCalendar.java:457)\n\u00a0 \u00a0 at java.base/java.util.GregorianCalendar.computeFields(GregorianCalendar.java:2358)\n\u00a0 \u00a0 at java.base/java.util.GregorianCalendar.computeFields(GregorianCalendar.java:2273)\n\u00a0 \u00a0 at java.base/java.util.Calendar.setTimeInMillis(Calendar.java:1827)\n\u00a0 \u00a0 at java.base/java.util.Calendar.setTime(Calendar.java:1793)\n\u00a0 \u00a0 at java.base/java.text.SimpleDateFormat.format(SimpleDateFormat.java:978)\n\u00a0 \u00a0 at java.base/java.text.SimpleDateFormat.format(SimpleDateFormat.java:971)\n\u00a0 \u00a0 at java.base/java.text.DateFormat.format(DateFormat.java:339)\n\u00a0 \u00a0 at java.base/java.text.Format.format(Format.java:159)\n\u00a0 \u00a0 at org.apache.parquet.schema.PrimitiveStringifier$DateStringifier.toFormattedString(PrimitiveStringifier.java:265)\n\u00a0 \u00a0 at org.apache.parquet.schema.PrimitiveStringifier$DateStringifier.stringify(PrimitiveStringifier.java:256)\n\u00a0 \u00a0 at org.apache.parquet.column.statistics.IntStatistics.stringify(IntStatistics.java:92)\n\u00a0 \u00a0 at org.apache.parquet.column.statistics.IntStatistics.stringify(IntStatistics.java:25)\n\u00a0 \u00a0 at org.apache.parquet.column.statistics.Statistics.minAsString(Statistics.java:423)\n\u00a0 \u00a0 (... unrelated code)\nA simple solution would be to change those from static to non static values.\nI can create a PR if the solution is ok by the maintainers of the library.",
        "Issue Links": []
    },
    "PARQUET-2241": {
        "Key": "PARQUET-2241",
        "Summary": "ByteStreamSplitDecoder broken in presence of nulls",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "format-2.8.0",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-format,                                            parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Xuwei Fu",
        "Created": "09/Feb/23 03:49",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "21/Feb/23 15:27",
        "Description": "This problem is shown in this issue: https://github.com/apache/arrow/issues/15173\nLet me talk about it briefly:\n\nEncoder doesn't write \"num_values\" on Page payload for BYTE_STREAM_SPLIT, but using \"num_values\" as stride in BYTE_STREAM_SPLIT\nWhen decoding, for DATA_PAGE_V2, it can now the num_values and num_nulls in the page, however, in DATA_PAGE_V1, without statistics, we should read def-levels and rep-levels to get the real num-of-values. And without the num-of-values, we aren't able to decode BYTE_STREAM_SPLIT correctly\n\u00a0\nThe bug-reproducing code is in the issue.",
        "Issue Links": []
    },
    "PARQUET-2242": {
        "Key": "PARQUET-2242",
        "Summary": "record count for  row group size check configurable",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "xjlem",
        "Created": "09/Feb/23 12:42",
        "Updated": "14/Feb/23 03:30",
        "Resolved": null,
        "Description": "org.apache.parquet.hadoop.InternalParquetRecordWriter#checkBlockSizeReached\n\n\r\n\u00a0private void checkBlockSizeReached() throws IOException {\r\n\u00a0 \u00a0 if (recordCount >= recordCountForNextMemCheck) { // checking the memory size is relatively expensive, so let's not do it for every record.\r\n\u00a0 \u00a0 \u00a0 long memSize = columnStore.getBufferedSize();\r\n\u00a0 \u00a0 \u00a0 long recordSize = memSize / recordCount;\r\n\u00a0 \u00a0 \u00a0 // flush the row group if it is within ~2 records of the limit\r\n\u00a0 \u00a0 \u00a0 // it is much better to be slightly under size than to be over at all\r\n\u00a0 \u00a0 \u00a0 if (memSize > (nextRowGroupSize - 2 * recordSize)) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOG.info(\"mem size {} > {}: flushing {} records to disk.\", memSize, nextRowGroupSize, recordCount);\r\n\u00a0 \u00a0 \u00a0 \u00a0 flushRowGroupToStore();\r\n\u00a0 \u00a0 \u00a0 \u00a0 initStore();\r\n\u00a0 \u00a0 \u00a0 \u00a0 recordCountForNextMemCheck = min(max(MINIMUM_RECORD_COUNT_FOR_CHECK, recordCount / 2), MAXIMUM_RECORD_COUNT_FOR_CHECK);\r\n\u00a0 \u00a0 \u00a0 \u00a0 this.lastRowGroupEndPos = parquetFileWriter.getPos();\r\n\u00a0 \u00a0 \u00a0 } else {\r\n\u00a0 \u00a0 \u00a0 \u00a0 recordCountForNextMemCheck = min(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max(MINIMUM_RECORD_COUNT_FOR_CHECK, (recordCount + (long)(nextRowGroupSize / ((float)recordSize))) / 2), // will check halfway\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 recordCount + MAXIMUM_RECORD_COUNT_FOR_CHECK // will not look more than max records ahead\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 );\r\n\u00a0 \u00a0 \u00a0 \u00a0 LOG.debug(\"Checked mem at {} will check again at: {}\", recordCount, recordCountForNextMemCheck);\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 }\r\n\u00a0 } \n\nin this code\uff0cif the block size is small ,for example 8M,and the first 100 lines record size is small and\u00a0 after 100 lines the record size is big\uff0cit will cause big row group\uff0cin our real scene\uff0cit will more than 64M. So i think the size for block check can configurable.",
        "Issue Links": []
    },
    "PARQUET-2243": {
        "Key": "PARQUET-2243",
        "Summary": "Support zstd-jni in DirectCodecFactory",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "15/Feb/23 07:21",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "23/Feb/23 06:14",
        "Description": "During switching to zstd-jni (from the Hadoop native zstd codec) we missed to add proper implementations for DirectCodecFactory. Currently, NPE occurs in case of the DirectCodecFactory is used while reading/writing.",
        "Issue Links": []
    },
    "PARQUET-2244": {
        "Key": "PARQUET-2244",
        "Summary": "Dictionary filter may skip row-groups incorrectly when evaluating notIn",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.2",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "Yujiang Zhong",
        "Reporter": "Yujiang Zhong",
        "Created": "15/Feb/23 10:00",
        "Updated": "12/Apr/23 01:32",
        "Resolved": "15/Feb/23 10:59",
        "Description": "Dictionary filter may skip row-groups incorrectly when evaluating `notIn` on optional columns with null values. Here is an example:\nSay there is a optional column `c1` with all pages dict encoded, `c1` has and only has two distinct values: ['foo', null], \u00a0and the predicate is\u00a0 `c1 not in ('foo', 'bar')`.\u00a0\nNow dictionary filter may skip this row-group that is actually should not be skipped, because there are nulls in the column.\n\u00a0\nThis is a bug similar to #1510.",
        "Issue Links": []
    },
    "PARQUET-2245": {
        "Key": "PARQUET-2245",
        "Summary": "Improve dictionary filter evaluating notEq",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yujiang Zhong",
        "Created": "15/Feb/23 12:21",
        "Updated": "16/Feb/23 05:50",
        "Resolved": null,
        "Description": "When evaluating `notEq`, if the column may contain nulls and the `notEq` value is non-null, the row-group must not be skipped. In such scenario reading dictionary and compare values is not necessary.",
        "Issue Links": []
    },
    "PARQUET-2246": {
        "Key": "PARQUET-2246",
        "Summary": "Add short circuit logic to column index filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Yujiang Zhong",
        "Reporter": "Yujiang Zhong",
        "Created": "15/Feb/23 13:00",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "23/Feb/23 13:36",
        "Description": "ColumnIndexFilter can be optimized by adding short-circuit logic to `AND` and `OR` operations. It's not necessary to evaluating the right node in some cases:\n\nIf the left result row ranges of `AND` is empty\nIf the left result row ranges of `OR` is full range of the row-group",
        "Issue Links": []
    },
    "PARQUET-2247": {
        "Key": "PARQUET-2247",
        "Summary": "Fail-fast if CapacityByteArrayOutputStream write overflow",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "parquet-mr",
        "Assignee": "dzcxzl",
        "Reporter": "dzcxzl",
        "Created": "16/Feb/23 13:45",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "22/Feb/23 08:17",
        "Description": "The bytesUsed of CapacityByteArrayOutputStream may overflow when writing some large byte data, resulting in parquet file write corruption.",
        "Issue Links": []
    },
    "PARQUET-2248": {
        "Key": "PARQUET-2075 Unified Rewriter Tool",
        "Summary": "ParquetRewriter supports merging files by record",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "16/Feb/23 15:29",
        "Updated": "16/Feb/23 15:29",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2249": {
        "Key": "PARQUET-2249",
        "Summary": "Parquet spec (parquet.thrift) is inconsistent w.r.t. ColumnIndex + NaNs",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Jan Finis",
        "Created": "19/Feb/23 12:09",
        "Updated": "4 days ago 07:22",
        "Resolved": null,
        "Description": "Currently, the specification of ColumnIndex in parquet.thrift is inconsistent, leading to cases where it is impossible to create a parquet file that is conforming to the spec.\nThe problem is with double/float columns if a page contains only NaN values. The spec mentions that NaN values should not be included in min/max bounds, so a page consisting of only NaN values has no defined min/max bound. To quote the spec:\n\n\u00a0 \u00a0* \u00a0 \u00a0 When writing statistics the following rules should be followed:\r\n\u00a0 \u00a0* \u00a0 \u00a0 - NaNs should not be written to min or max statistics fields.\n\nHowever, the comments in the ColumnIndex on the null_pages member states the following:\n\nstruct ColumnIndex {\r\n\u00a0 /**\r\n\u00a0 \u00a0* A list of Boolean values to determine the validity of the corresponding\r\n\u00a0 \u00a0* min and max values. If true, a page contains only null values, and writers\r\n\u00a0 \u00a0* have to set the corresponding entries in min_values and max_values to\r\n\u00a0 \u00a0* byte[0], so that all lists have the same length. If false, the\r\n\u00a0 \u00a0* corresponding entries in min_values and max_values must be valid.\r\n\u00a0 \u00a0*/\r\n\u00a0 1: required list<bool> null_pages\n\nFor a page with only NaNs, we now have a problem. The page definitly does not only contain null values, so null_pages should be false for this page. However, in this case the spec requires valid min/max values in min_values and max_values for this page. As the only value in the page is NaN, the only valid min/max value we could enter here is NaN, but as mentioned before, NaNs should never be written to min/max values.\nThus, no writer can currently create a parquet file that conforms to this specification as soon as there is a only-NaN column and column indexes are to be written.\nI see three possible solutions:\n1. A page consisting only of NaNs (or a mixture of NaNs and nulls) has it's null_pages entry set to true.\n2. A page consisting of only NaNs (or a mixture of NaNs and nulls) has byte[0] as min/max, even though the null_pages entry is set to false.\n3. A page consisting of only NaNs (or a mixture of NaNs and nulls) does have NaN as min & max in the column index.\nNone of the solutions is perfect. But I guess solution 3. is the best of them. It gives us valid min/max bounds, makes null_pages compatible with this, and gives us a way to determine only-Nan pages (min=max=NaN).\nAs a general note: I would say that it is a shortcoming that Parquet doesn't track NaN counts. E.g., Iceberg does track NaN counts and therefore doesn't have this inconsistency. In a future version, NaN counts could be introduced, but that doesn't help for backward compatibility, so we do need a solution for now.\nAny of the solutions is better than the current situation where engines writing such a page cannot write a conforming parquet file and will randomly pick any of the solutions.\nThus, my suggestion would be to update parquet.thrift to use solution 3. I.e., rewrite the comments saying that NaNs shouldn't be included in min/max bounds by adding a clause stating that \"if a page contains only NaNs or a mixture of NaNs and NULLs, then NaN should be written as min & max\".",
        "Issue Links": []
    },
    "PARQUET-2250": {
        "Key": "PARQUET-2250",
        "Summary": "Expose column descriptor through RecordReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-12.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "fatemah",
        "Reporter": "fatemah",
        "Created": "23/Feb/23 21:07",
        "Updated": "11/May/23 01:18",
        "Resolved": "03/Mar/23 00:43",
        "Description": "Currently, the RecordReader does not expose the underlying column descriptor. This would be useful in some scenarios.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/34318"
        ]
    },
    "PARQUET-2251": {
        "Key": "PARQUET-2251",
        "Summary": "Avoid generating Bloomfilter when all pages of a column are encoded by dictionary",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mars",
        "Created": "24/Feb/23 03:25",
        "Updated": "12/Apr/23 01:31",
        "Resolved": "02/Mar/23 02:11",
        "Description": "In parquet pageV1, even all pages of a column are encoded by dictionary, it will still generate BloomFilter. Actually it is unnecessary to generate BloomFilter and it cost time and occupy storage.\nParquet pageV2 doesn't generate BloomFilter if all pages of a column are encoded by dictionary,",
        "Issue Links": []
    },
    "PARQUET-2252": {
        "Key": "PARQUET-2252",
        "Summary": "Make some methods public to allow external projects to implement page skipping",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Yujiang Zhong",
        "Reporter": "Yujiang Zhong",
        "Created": "01/Mar/23 11:31",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "07/Mar/23 01:23",
        "Description": "Iceberg hopes to implement the column index filter based on Iceberg's own expressions, we would like to be able to use some of the methods in Parquet repo, for example: methods in `RowRanges` and `IndexIterator`, however these are currently not public. Currently we can only rely on reflection to use them.",
        "Issue Links": []
    },
    "PARQUET-2253": {
        "Key": "PARQUET-2253",
        "Summary": "Postpone dictionary encoding decision for starting null pages.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "02/Mar/23 11:38",
        "Updated": "02/Mar/23 11:38",
        "Resolved": null,
        "Description": "Discussion from the dev mailing list: Fallback Encoding for Very Sparse or Sorted Datasets-Apache Mail Archives\nIf beginning values are all nulls, the dictionary encoding is disabled when first data page is created. This loses the advantage of dictionary encoding if following values are good fit.",
        "Issue Links": []
    },
    "PARQUET-2254": {
        "Key": "PARQUET-2254",
        "Summary": "Build a BloomFilter with a more precise size",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Mars",
        "Reporter": "Mars",
        "Created": "07/Mar/23 07:10",
        "Updated": "24/May/23 01:33",
        "Resolved": "24/May/23 01:33",
        "Description": "Why are the changes needed?\nNow the usage of bloom filter is to specify the NDV(number of distinct values) or max bytes, and then build BloomFilter. In general scenarios, it is actually not sure how much the distinct value is.\nIf BloomFilter can be automatically generated according to the data, the file size can be reduced and the reading efficiency can also be improved.\nWhat changes were proposed in this pull request?\n`AdaptiveBlockSplitBloomFilter` contains multiple `BlockSplitBloomFilter` as candidates and inserts values in\n\u00a0the candidates at the same time. Finally we will choose the smallest candidate to write out.\nDoes this PR introduce any user-facing change?\nadd new configuration:\n`parquet.bloom.filter.adaptive.enabled` : default false, Whether to enable writing adaptive bloom filter. \u00a0\nIf it is true, the bloom filter will be generated with the optimal bit size according to the number of real data distinct values. If it is false, it will not take effect.\nNote that the maximum bytes of the bloom filter will not exceed `parquet.bloom.filter.max.bytes` configuration (if it is\u00a0\nset too small, the generated bloom filter will not be efficient).\n`parquet.bloom.filter.candidates.number`: default 5, the number of candidate bloom filters written at the same time. \u00a0\nWhen `parquet.bloom.filter.adaptive.enabled` is true, multiple candidate bloom filters will be inserted\u00a0\nat the same time, finally a bloom filter with the optimal bit size will be selected and written to the file.",
        "Issue Links": []
    },
    "PARQUET-2255": {
        "Key": "PARQUET-2255",
        "Summary": "BloomFilter and float point is ambiguous",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.9.0",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Xuwei Fu",
        "Created": "13/Mar/23 14:25",
        "Updated": "13/Mar/23 16:35",
        "Resolved": null,
        "Description": "Currently, our Parquet can use BloomFilter for any physical types. However, when BloomFilter apply on float:\n\nWhat does +0 -0 means? Are they equal?\nShould qNaN sNaN written in BloomFilter? Are they equal?",
        "Issue Links": []
    },
    "PARQUET-2256": {
        "Key": "PARQUET-2256",
        "Summary": "Adding Compression for BloomFilter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "format-2.9.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Xuwei Fu",
        "Reporter": "Xuwei Fu",
        "Created": "13/Mar/23 14:30",
        "Updated": "05/May/23 07:35",
        "Resolved": null,
        "Description": "In Current Parquet implementions, if BloomFilter doesn't set the ndv, most implementions will guess the 1M as the ndv. And use it for fpp. So, if fpp is 0.01, the BloomFilter size may grows to 2M for each column, which is really huge. Should we support compression for BloomFilter, like:\n\u00a0\n```\n\u00a0/**\n\nThe compression used in the Bloom filter.\n **/\nstruct Uncompressed {}\nunion BloomFilterCompression \n{\r\n  1: Uncompressed UNCOMPRESSED;\r\n+2: CompressionCodec COMPRESSION;\r\n}\n\n```",
        "Issue Links": []
    },
    "PARQUET-2257": {
        "Key": "PARQUET-2257",
        "Summary": "[Format] Add bloom_filter_length to ColumnMetaData",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "13/Mar/23 15:08",
        "Updated": "15/Apr/23 13:10",
        "Resolved": "15/Apr/23 13:10",
        "Description": "The specs only has added bloom_filter_offset to locate the bloom filter. The reader cannot load the bloom filter in a single shot until it parses the bloom filter header to get the total size.\nThis issue proposes to add an optional bloom_filter_length field to track the size of bloom filter to facilitate I/O scheduling.",
        "Issue Links": []
    },
    "PARQUET-2258": {
        "Key": "PARQUET-2258",
        "Summary": "Storing toString fields in FilterPredicate instances can lead to memory pressure",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "L\u00e1szl\u00f3 Bodor",
        "Reporter": "L\u00e1szl\u00f3 Bodor",
        "Created": "16/Mar/23 11:54",
        "Updated": "12/Apr/23 01:29",
        "Resolved": "17/Mar/23 07:25",
        "Description": "It happens with Hive (HiveServer2), a certain amount of predicate instances can make HiveServer2 OOM. According to the heapdump and background information, the predicates must have been simplified a bit, but still, storing toString in the objects looks very weird.",
        "Issue Links": []
    },
    "PARQUET-2259": {
        "Key": "PARQUET-2259",
        "Summary": "[Site] Update parquet site",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-site",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "16/Mar/23 15:40",
        "Updated": "17/Mar/23 15:35",
        "Resolved": null,
        "Description": "The parquet-site has not been updated for a long time while the parquet-format has added new changes. We need to update the site to match those changes.",
        "Issue Links": [
            "https://github.com/apache/parquet-site/pull/31"
        ]
    },
    "PARQUET-2260": {
        "Key": "PARQUET-2260",
        "Summary": "Bloom filter bytes size shouldn't be larger than maxBytes size in the configuration",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Mars",
        "Reporter": "Mars",
        "Created": "19/Mar/23 09:58",
        "Updated": "12/Apr/23 01:50",
        "Resolved": "12/Apr/23 01:50",
        "Description": "Before this PR: If\u00a0parquet.bloom.filter.max.bytes\u00a0configuration is not a power of 2 value, the size of the bloom filter generated will exceed this value. For example, if set\u00a0parquet.bloom.filter.max.bytes\u00a0as 1024 * 1024+1= 1048577 , the bytes size of bloom filter generated will be 1024 * 1024 * 2 = 2097152. This does not match the definition of the parameter\nAfter this PR: If\u00a0parquet.bloom.filter.max.bytes\u00a0configuration is not a power of 2, and the size of\u00a0parquet.bloom.filter.max.bytes\u00a0is the final bloom filter bytes size.",
        "Issue Links": []
    },
    "PARQUET-2261": {
        "Key": "PARQUET-2261",
        "Summary": "[Format] Add statistics that reflect decoded size to metadata",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": "Micah Kornfield",
        "Reporter": "Micah Kornfield",
        "Created": "26/Mar/23 02:29",
        "Updated": "11/May/23 16:23",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2262": {
        "Key": "PARQUET-2262",
        "Summary": "Fix local build failure from maven-surefire-plugin due to missing surefire.argLine",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "27/Mar/23 01:56",
        "Updated": "12/Apr/23 01:49",
        "Resolved": "12/Apr/23 01:49",
        "Description": "The issue can be reproduced simply by running mvn test on the laptop locally.\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary for Apache Parquet MR 1.13.0-SNAPSHOT:\n[INFO]\n[INFO] Apache Parquet MR .................................. SUCCESS [ 1.056 s]\n[INFO] Apache Parquet Format Structures ................... FAILURE [ 2.009 s]\n[INFO] Apache Parquet Generator ........................... SKIPPED\n[INFO] Apache Parquet Common .............................. SKIPPED\n[INFO] Apache Parquet Encodings ........................... SKIPPED\n[INFO] Apache Parquet Column .............................. SKIPPED\n[INFO] Apache Parquet Arrow ............................... SKIPPED\n[INFO] Apache Parquet Jackson ............................. SKIPPED\n[INFO] Apache Parquet Hadoop .............................. SKIPPED\n[INFO] Apache Parquet Avro ................................ SKIPPED\n[INFO] Apache Parquet Benchmarks .......................... SKIPPED\n[INFO] Apache Parquet Command-line ........................ SKIPPED\n[INFO] Apache Parquet Pig ................................. SKIPPED\n[INFO] Apache Parquet Pig Bundle .......................... SKIPPED\n[INFO] Apache Parquet Protobuf ............................ SKIPPED\n[INFO] Apache Parquet Scala ............................... SKIPPED\n[INFO] Apache Parquet Thrift .............................. SKIPPED\n[INFO] Apache Parquet Hadoop Bundle ....................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 3.162 s\n[INFO] Finished at: 2023-03-27T09:52:19+08:00\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.0:test (default-test) on project parquet-format-structures: There are test failures.\n[ERROR]\n[ERROR] Please refer to /Users/gangwu/Projects/parquet-mr/parquet-format-structures/target/surefire-reports for the individual test results.\n[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.\n[ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\n[ERROR] Command was /bin/sh -c cd /Users/gangwu/Projects/parquet-mr/parquet-format-structures && /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jre/bin/java '${surefire.argLine}' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -jar /Users/gangwu/Projects/parquet-mr/parquet-format-structures/target/surefire/surefirebooter4762879200927950684.jar /Users/gangwu/Projects/parquet-mr/parquet-format-structures/target/surefire 2023-03-27T09-52-19_523-jvmRun1 surefire1144200075397565188tmp surefire_08731611064868391570tmp\n[ERROR] Error occurred in starting fork, check output in log\n[ERROR] Process Exit Code: 1\n[ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\n[ERROR] Command was /bin/sh -c cd /Users/gangwu/Projects/parquet-mr/parquet-format-structures && /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home/jre/bin/java '${surefire.argLine}' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -jar /Users/gangwu/Projects/parquet-mr/parquet-format-structures/target/surefire/surefirebooter4762879200927950684.jar /Users/gangwu/Projects/parquet-mr/parquet-format-structures/target/surefire 2023-03-27T09-52-19_523-jvmRun1 surefire1144200075397565188tmp surefire_08731611064868391570tmp\n[ERROR] Error occurred in starting fork, check output in log\n[ERROR] Process Exit Code: 1\n[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:671)\n[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:533)\n[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:278)\n[ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:244)\n[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1194)\n[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1022)\n[ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:868)\n[ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)\n[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)\n[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)\n[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)\n[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)\n[ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)\n[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)\n[ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)\n[ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)\n[ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)\n[ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:294)\n[ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)\n[ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)\n[ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:960)\n[ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:293)\n[ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:196)\n[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[ERROR] at java.lang.reflect.Method.invoke(Method.java:498)\n[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)\n[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)\n[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)\n[ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)\n[ERROR]\n[ERROR] -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR] mvn <args> -rf :parquet-format-structures",
        "Issue Links": []
    },
    "PARQUET-2263": {
        "Key": "PARQUET-2263",
        "Summary": "Upgrade maven-shade-plugin to 3.4.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Yuming Wang",
        "Reporter": "Yuming Wang",
        "Created": "04/Apr/23 06:02",
        "Updated": "13/Apr/23 19:13",
        "Resolved": "13/Apr/23 19:13",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2264": {
        "Key": "PARQUET-2264",
        "Summary": "Update specification to allow DecimalType scale == precision",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Devin Smith",
        "Created": "05/Apr/23 17:12",
        "Updated": "19/Apr/23 01:34",
        "Resolved": null,
        "Description": "The majority of implementations I've used allow for scale == precision.\nSee https://github.com/apache/arrow-rs/pull/1607 for further motivation.",
        "Issue Links": []
    },
    "PARQUET-2265": {
        "Key": "PARQUET-2265",
        "Summary": "AvroParquetWriter should default to data supplier model from Configuration",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "None",
        "Assignee": "Claire McGinty",
        "Reporter": "Claire McGinty",
        "Created": "10/Apr/23 14:46",
        "Updated": "06/May/23 02:13",
        "Resolved": "14/Apr/23 03:27",
        "Description": "I recently ran into a bug where the AvroDataSupplier I specified in my Configuration wasn't respected when creating an AvroParquetWriter:\n\u00a0\n```\nConfiguration configuration = new Configuration();\nconfiguration.put(AvroWriteSupport.AVRO_DATA_SUPPLIER, myCustomDataSupplier)\nAvroParquetWriter<MyAvroRecord> writer =\n\u00a0 AvroParquetWriter.<MyAvroRecord>builder(...)\n\u00a0 \u00a0 .withSchema(...)\n\u00a0 \u00a0 .withConf(configuration)\n\u00a0 \u00a0 .build();\n```\nIn this instance, the writer's attached AvroWriteSupport uses a SpecificData model, rather than the value of `myCustomDataSupplier.get()`. This is due to AvroParquetWriter defaulting to SpecificData model[0] if it's not supplied in the AvroParquetWriter.Builder.\nI see that AvroParquetWriter.Builder has a `.withDataModel` method, but IMO this creates confusion/redundancy, since I end up supplying the data model twice; also, I can't create any abstractions around this (i.e. a `createWriterForConfiguration(Configuration conf)` type of method) without having to use reflection to invoke a dataModel for the value of `conf.getClass(AvroWriteSupport.AVRO_DATA_SUPPLIER)`.\nI think it would be simplest if AvroWriteSupport just defaulted to `model = null` and let AvroWriteSupport initialize it based on the Configuration[1]. What do you think? That seems to be what AvroParquetReader is currently doing[2].\n\u00a0\n[0]https://github.com/apache/parquet-mr/blob/59e9f78b8b3a30073db202eb6432071ff71df0ec/parquet-avro/src/main/java/org/apache/parquet/avro/AvroParquetWriter.java#L163\n[1]https://github.com/apache/parquet-mr/blob/59e9f78b8b3a30073db202eb6432071ff71df0ec/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java#L134\u00a0\n[2]https://github.com/apache/parquet-mr/blob/9a1fbc4ee3f63284a675eeac6c62e96ffc973575/parquet-avro/src/main/java/org/apache/parquet/avro/AvroParquetReader.java#L133",
        "Issue Links": []
    },
    "PARQUET-2266": {
        "Key": "PARQUET-2266",
        "Summary": "Fix support for files without ColumnIndexes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12.3",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "None",
        "Assignee": "Richard Kerr",
        "Reporter": "Richard Kerr",
        "Created": "13/Apr/23 02:10",
        "Updated": "16/Apr/23 11:41",
        "Resolved": "14/Apr/23 02:19",
        "Description": "Fix for failure when writing ColumnChunks that do not have a ColumnIndex populated.\nThis is introduced by PARQUET-2081",
        "Issue Links": []
    },
    "PARQUET-2267": {
        "Key": "PARQUET-2267",
        "Summary": "Add dependabot to update dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 13:13",
        "Updated": "14/Apr/23 08:11",
        "Resolved": "14/Apr/23 08:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2268": {
        "Key": "PARQUET-2268",
        "Summary": "Bump Thrift to 0.18.1",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 14:00",
        "Updated": "14/Apr/23 09:08",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2269": {
        "Key": "PARQUET-2269",
        "Summary": "Update version in README.md",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 14:13",
        "Updated": "14/Apr/23 06:52",
        "Resolved": "14/Apr/23 01:58",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2270": {
        "Key": "PARQUET-2270",
        "Summary": "Bump Thrift to 0.18.1",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 14:16",
        "Updated": "14/Jun/23 13:36",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2271": {
        "Key": "PARQUET-2271",
        "Summary": "Bump Parquet POM to 29",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 14:19",
        "Updated": "19/Apr/23 01:33",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2272": {
        "Key": "PARQUET-2272",
        "Summary": "Bump protobuf-java from 3.17.3 to 3.19.6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-protobuf",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 19:20",
        "Updated": "13/Apr/23 19:20",
        "Resolved": "13/Apr/23 19:20",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2273": {
        "Key": "PARQUET-2273",
        "Summary": "Remove Travis from the repository",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 19:21",
        "Updated": "14/Apr/23 02:36",
        "Resolved": "14/Apr/23 02:36",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2274": {
        "Key": "PARQUET-2274",
        "Summary": "Remove Yetus",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "13/Apr/23 19:49",
        "Updated": "23/May/23 14:11",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2275": {
        "Key": "PARQUET-2275",
        "Summary": "Upgrade `cyclonedx-maven-plugin` to 2.7.6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.1",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Dongjoon Hyun",
        "Reporter": "Dongjoon Hyun",
        "Created": "13/Apr/23 20:07",
        "Updated": "14/Apr/23 02:41",
        "Resolved": "13/Apr/23 20:48",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2276": {
        "Key": "PARQUET-2276",
        "Summary": "ParquetReader reads do not work with Hadoop version 2.8.5",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Atul Mohan",
        "Created": "13/Apr/23 23:34",
        "Updated": "16/May/23 19:31",
        "Resolved": "16/May/23 19:31",
        "Description": "ParquetReader.read() fails with the following exception on parquet-mr version 1.13.0 when using hadoop version 2.8.5:\n\n\r\n java.lang.NoSuchMethodError: 'boolean org.apache.hadoop.fs.FSDataInputStream.hasCapability(java.lang.String)' \r\n\r\n\r\nat org.apache.parquet.hadoop.util.HadoopStreams.isWrappedStreamByteBufferReadable(HadoopStreams.java:74) \r\n\r\n\r\nat org.apache.parquet.hadoop.util.HadoopStreams.wrap(HadoopStreams.java:49) \r\n\r\n\r\nat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:69) \r\n\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:787) \r\n\r\n\r\nat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657) \r\n\r\n\r\nat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:162) \r\n\r\n\r\norg.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\r\n\n\n\u00a0\n\u00a0\n\u00a0\nFrom an initial investigation, it looks like HadoopStreams has started using FSDataInputStream.hasCapability but FSDataInputStream does not have the hasCapability API in hadoop 2.8.x.",
        "Issue Links": []
    },
    "PARQUET-2277": {
        "Key": "PARQUET-2277",
        "Summary": "Bump hadoop.version from 3.2.3 to 3.3.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "14/Apr/23 09:09",
        "Updated": "14/Apr/23 10:37",
        "Resolved": "14/Apr/23 09:10",
        "Description": null,
        "Issue Links": [
            "/jira/browse/PARQUET-2171"
        ]
    },
    "PARQUET-2278": {
        "Key": "PARQUET-2278",
        "Summary": "Bump re2j from 1.1 to 1.7",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "14/Apr/23 09:10",
        "Updated": "14/Apr/23 09:10",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2279": {
        "Key": "PARQUET-2279",
        "Summary": "Bump slf4j.version from 1.7.22 to 1.7.33",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "14/Apr/23 09:11",
        "Updated": "14/Apr/23 09:11",
        "Resolved": "14/Apr/23 09:11",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2280": {
        "Key": "PARQUET-2280",
        "Summary": "Bump h2 from 2.1.210 to 2.1.214",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "14/Apr/23 09:12",
        "Updated": "14/Apr/23 09:12",
        "Resolved": "14/Apr/23 09:12",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2281": {
        "Key": "PARQUET-2281",
        "Summary": "Bump thrift to 0.18.1",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-thrift",
        "Assignee": null,
        "Reporter": "Liu Jiayu",
        "Created": "14/Apr/23 12:43",
        "Updated": "02/May/23 01:29",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2282": {
        "Key": "PARQUET-2282",
        "Summary": "Dont initialize HadoopCodec",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "17/Apr/23 16:16",
        "Updated": "19/Apr/23 09:10",
        "Resolved": "18/Apr/23 07:40",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2283": {
        "Key": "PARQUET-2283",
        "Summary": "Remove Hadoop HiddenFileFilter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "17/Apr/23 18:34",
        "Updated": "19/Apr/23 09:10",
        "Resolved": "18/Apr/23 07:40",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2284": {
        "Key": "PARQUET-2284",
        "Summary": "Bump junit from 4.10 to 4.13.1",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "18/Apr/23 20:42",
        "Updated": "18/Apr/23 20:42",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2285": {
        "Key": "PARQUET-2285",
        "Summary": "Add dependabot",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "18/Apr/23 20:43",
        "Updated": "19/Apr/23 01:34",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2286": {
        "Key": "PARQUET-2286",
        "Summary": "Bump apache-rat-plugin from 0.12 to 0.15",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 08:34",
        "Updated": "19/Apr/23 08:35",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2287": {
        "Key": "PARQUET-2287",
        "Summary": "Bump maven-shade-plugin from 2.2 to 3.4.1",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 08:35",
        "Updated": "19/Apr/23 08:35",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2288": {
        "Key": "PARQUET-2288",
        "Summary": "Bump exec-maven-plugin from 1.2.1 to 3.1.0",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-format",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 08:36",
        "Updated": "19/Apr/23 08:36",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2289": {
        "Key": "PARQUET-2289",
        "Summary": "Avoid using hasCapability",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 08:59",
        "Updated": "19/Apr/23 12:56",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2290": {
        "Key": "PARQUET-2290",
        "Summary": "Add CI for Hadoop 2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 11:48",
        "Updated": "26/Apr/23 16:52",
        "Resolved": "23/Apr/23 18:58",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2291": {
        "Key": "PARQUET-2291",
        "Summary": "Remove lingering japicmp exclusions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "19/Apr/23 20:47",
        "Updated": "23/Apr/23 18:57",
        "Resolved": "23/Apr/23 18:57",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2292": {
        "Key": "PARQUET-2292",
        "Summary": "Improve default SpecificRecord model selection for Avro{Write,Read}Support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-avro",
        "Assignee": "Claire McGinty",
        "Reporter": "Claire McGinty",
        "Created": "21/Apr/23 19:31",
        "Updated": "06/May/23 02:55",
        "Resolved": "06/May/23 02:13",
        "Description": "AvroWriteSupport/AvroReadSupport can improve the precision of their default `model` selection. Currently they default to new SpecificDataSupplier().get()[0]. This means that SpecificRecord classes that contain logical types will fail out-of-the-box unless a specific DATA_SUPPLIER is configured that contains logical type conversions.\nI think we can improve this and make logical types work by default by defaulting to the value of the `MODEL$` field that every SpecificRecordBase implementation contains, which already contains all the logical conversions for that Avro type. It would require reflection, but that's what the Avro library is already doing to fetch models for Specific types[1].\n\u00a0\n[0] https://github.com/apache/parquet-mr/blob/d38044f5395494e1543581a4b763f624305d3022/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java#L403-L407\n[1] https://github.com/apache/avro/blob/release-1.11.1/lang/java/avro/src/main/java/org/apache/avro/specific/SpecificData.java#L76-L86",
        "Issue Links": []
    },
    "PARQUET-2293": {
        "Key": "PARQUET-2293",
        "Summary": "Bump guava from 27.0.1-jre to 31.1-jre",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "23/Apr/23 08:27",
        "Updated": "23/May/23 14:04",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2294": {
        "Key": "PARQUET-2294",
        "Summary": "Bump fastutil from 8.4.2 to 8.5.12",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "23/Apr/23 08:27",
        "Updated": "26/Apr/23 16:43",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2295": {
        "Key": "PARQUET-2295",
        "Summary": "Bump truth-proto-extension from 1.0 to 1.1.3",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "01/May/23 06:58",
        "Updated": "09/May/23 07:18",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2296": {
        "Key": "PARQUET-2296",
        "Summary": "Bump easymock from 3.4 to 5.1.0",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Fokko Driesprong",
        "Reporter": "Fokko Driesprong",
        "Created": "01/May/23 06:59",
        "Updated": "09/May/23 07:18",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2297": {
        "Key": "PARQUET-2297",
        "Summary": "Encrypted files should not be checked for delta encoding problem",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0,                                            1.13.1",
        "Component/s": "parquet-mr",
        "Assignee": "Gidon Gershinsky",
        "Reporter": "Gidon Gershinsky",
        "Created": "04/May/23 06:12",
        "Updated": "16/May/23 19:31",
        "Resolved": "16/May/23 19:31",
        "Description": "Delta encoding problem (https://issues.apache.org/jira/browse/PARQUET-246) was fixed in writers since parquet-mr-1.8. This fix also added a `checkDeltaByteArrayProblem` method in readers, that runs over all columns and checks for this problem in older files.\u00a0\nThis now triggers an unrelated exception when reading encrypted files, in the following situation: trying to read an unencrypted column, without having keys for encrypted columns (see https://issues.apache.org/jira/browse/PARQUET-2193). This happens in Spark, with nested columns (files with regular columns are ok).\nPossible solution: don't call the `checkDeltaByteArrayProblem` method for encrypted files - because these files can be written only with parquet-mr-1.12 and newer, where the delta encoding problem is already fixed.",
        "Issue Links": [
            "/jira/browse/PARQUET-2193"
        ]
    },
    "PARQUET-2298": {
        "Key": "PARQUET-2298",
        "Summary": "RecordReaderImplementation throw ArrayIndexOutOfBoundsException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10.1,                                            1.12.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Hui An",
        "Created": "10/May/23 09:19",
        "Updated": "11/May/23 01:37",
        "Resolved": null,
        "Description": "Writing data by spark 3.1(parquet 1.10.1), sometimes it could get the error when reading it.\n\n\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 141009 in block 1 in file hdfs://test/test_table/part-01423-58cbf8bc-5b0d-4d53-8279-2c8e3d293cc0.c000.zstd.parquet\r\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\r\nat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\nat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\r\nat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\nat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\nat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\nat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\r\nat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\nat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\nat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\r\nat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\nat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\nat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\nat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1509)\r\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 254\r\nat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:398)\r\nat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)\r\n... 19 more\r\n\n\nThis table is a wide table(contains 160 columns), and it looks when it try to get a column definition level, which should be 0, but got 254 instead.",
        "Issue Links": []
    },
    "PARQUET-2299": {
        "Key": "PARQUET-2299",
        "Summary": "Use `true` instead of `1` as default value for `is_compressed bool` field",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": "Matthijs Brobbel",
        "Reporter": "Matthijs Brobbel",
        "Created": "11/May/23 17:53",
        "Updated": "12/May/23 06:22",
        "Resolved": "12/May/23 06:21",
        "Description": "I noticed that the default value for the optional boolean `is_compressed` field of the `DataPageHeaderV2` struct has a default value of `1`. According to the Thrift docs a boolean value is either `true` or `false`.\u00a0\nThis currently works because the Apache Thrift compiler internally handles bools as ints:\n\nhttps://github.com/apache/thrift/blob/3880a09565a9a1dad028b3679746eafac268c819/compiler/cpp/src/thrift/thriftl.ll#L208-L209\nhttps://github.com/apache/thrift/blob/3880a09565a9a1dad028b3679746eafac268c819/compiler/cpp/src/thrift/main.cc#L748\n\nIt may however not work with other Thrift compilers that are more strict about this.\nBased on the docs and a test (https://github.com/apache/thrift/blob/3880a09565a9a1dad028b3679746eafac268c819/test/ThriftTest.thrift#L406) in the Thrift repository it seems that using `true` here is the correct way of defining a default for an optional bool field.",
        "Issue Links": []
    },
    "PARQUET-2300": {
        "Key": "PARQUET-2300",
        "Summary": "Update jackson-core 2.13.4 to a version without CVE PRISMA-2023-0067",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gianluca Vagnoni",
        "Created": "22/May/23 09:29",
        "Updated": "26/May/23 04:52",
        "Resolved": null,
        "Description": "The library \"parquet-jackson\" version 1.13.0 and 1.13.1 contains the vulnerability PRISMA-2023-0067 (https://github.com/FasterXML/jackson-core/pull/827) (https://github.com/IBM/ibm-cos-sdk-java/issues/58)\nPlease upgrade the shaded library to jackson-core version 2.15.0 to fix it.",
        "Issue Links": []
    },
    "PARQUET-2301": {
        "Key": "PARQUET-2301",
        "Summary": "Add missing argument in ParquetRewriter logging",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ethan Guo",
        "Created": "25/May/23 00:14",
        "Updated": "04/Jun/23 14:51",
        "Resolved": "25/May/23 16:20",
        "Description": "The logging statement misses one argument\n\n\r\nLOG.info(\"Start rewriting {} input files {} to {}\", inputFiles.size(), outPath);",
        "Issue Links": []
    },
    "PARQUET-2302": {
        "Key": "PARQUET-2302",
        "Summary": "Bump joda-time from 2.9.7 to 2.12.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "29/May/23 02:25",
        "Updated": "29/May/23 02:26",
        "Resolved": "29/May/23 02:26",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2303": {
        "Key": "PARQUET-2303",
        "Summary": "Bump cyclonedx-maven-plugin from 2.7.6 to 2.7.9",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "29/May/23 02:26",
        "Updated": "29/May/23 02:27",
        "Resolved": "29/May/23 02:27",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2304": {
        "Key": "PARQUET-2304",
        "Summary": "Bump buildnumber-maven-plugin from 1.1 to 3.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "29/May/23 02:27",
        "Updated": "29/May/23 02:28",
        "Resolved": "29/May/23 02:28",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2305": {
        "Key": "PARQUET-2305",
        "Summary": "Allow Parquet to Proto conversion even though Target Schema has less fields",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-protobuf",
        "Assignee": "Sanjay Sharma",
        "Reporter": "Sanjay Sharma",
        "Created": "02/Jun/23 13:19",
        "Updated": "14/Jun/23 04:01",
        "Resolved": "14/Jun/23 04:01",
        "Description": "If Parquet has any field which has been removed from the schema and Parquet to Proto conversion happens, it errors out due to Unknown fields. There could be some scenarios that we want to still convert PARQUET into the target proto schema object which has lesser fields.\nIf specified \"ignoreUnknownFields\" as an argument, this should allow the conversion which ignore fields it can't convert and not error out.\nSimilar functionality exist in\u00a0https://github.com/protocolbuffers/protobuf/blob/main/java/util/src/main/java/com/google/protobuf/util/JsonFormat.java\u00a0with field \"ignoringUnknownFields\"",
        "Issue Links": []
    },
    "PARQUET-2306": {
        "Key": "PARQUET-2306",
        "Summary": "Parquet statistics stats_null_count is wrong for large json string colums",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rotem Levi",
        "Created": "04/Jun/23 13:30",
        "Updated": "04/Jun/23 15:45",
        "Resolved": null,
        "Description": "stats_null_count always None when one of the columns values is about 46977 characters length",
        "Issue Links": []
    },
    "PARQUET-2307": {
        "Key": "PARQUET-2307",
        "Summary": "Bump zero-allocation-hashing from 0.9 to 0.16",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gang Wu",
        "Created": "05/Jun/23 06:52",
        "Updated": "05/Jun/23 06:54",
        "Resolved": "05/Jun/23 06:54",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2308": {
        "Key": "PARQUET-2308",
        "Summary": "Bump powermock.version from 2.0.2 to 2.0.9",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gang Wu",
        "Created": "07/Jun/23 02:10",
        "Updated": "07/Jun/23 02:22",
        "Resolved": "07/Jun/23 02:22",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2309": {
        "Key": "PARQUET-2309",
        "Summary": "Bump site-maven-plugin from 0.8 to 0.12",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gang Wu",
        "Created": "12/Jun/23 05:29",
        "Updated": "12/Jun/23 05:30",
        "Resolved": "12/Jun/23 05:30",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2310": {
        "Key": "PARQUET-2310",
        "Summary": "[Doc] Add implementation status / matrix",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Antoine Pitrou",
        "Created": "15/Jun/23 14:26",
        "Updated": "21/Jun/23 17:01",
        "Resolved": null,
        "Description": "In Apache Arrow we have a documentation page listed the feature status for various implementations of Arrow: https://arrow.apache.org/docs/status.html\nIt could be nice to have a similar page for the main Parquet implementations (at least Java, C++, Rust).\nThe main downside is that it needs to be kept up to date.",
        "Issue Links": [
            "https://github.com/apache/parquet-site/pull/34"
        ]
    },
    "PARQUET-2311": {
        "Key": "PARQUET-2311",
        "Summary": "Incomptable with latest spark version",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.14.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "ronan doolan",
        "Created": "15/Jun/23 15:28",
        "Updated": "15/Jun/23 15:46",
        "Resolved": null,
        "Description": "Im getting the following errors when I try use version 1.14.0 of parquet with the 3.5.0 of spark\n\u00a0\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:661: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:665: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:163: value options is not a member of org.apache.spark.sql.catalyst.plans.logical.TableSpecBase\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:165: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:166: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:167: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:167: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:175: value options is not a member of org.apache.spark.sql.catalyst.plans.logical.TableSpecBase\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:180: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:181: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:182: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala:182: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:258: value multipartIdentifier is not a member of org.apache.spark.sql.catalyst.parser.SqlBaseParser.UseNamespaceContext\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:333: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:334: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:495: value multipartIdentifier is not a member of org.apache.spark.sql.catalyst.parser.SqlBaseParser.CreateViewContext\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:510: value multipartIdentifier is not a member of org.apache.spark.sql.catalyst.parser.SqlBaseParser.CreateViewContext\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:555: value multipartIdentifier is not a member of org.apache.spark.sql.catalyst.parser.SqlBaseParser.CreateFunctionContext\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:594: value multipartIdentifier is not a member of org.apache.spark.sql.catalyst.parser.SqlBaseParser.DropFunctionContext\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala:818: not enough arguments for method unsupportedLocalFileSchemeError: (ctx: org.apache.spark.sql.catalyst.parser.SqlBaseParser.InsertOverwriteDirContext, actualSchema: String)Throwable.\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala:1040: not enough arguments for method cannotOverwritePathBeingReadFromError: (path: String)Throwable.\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala:546: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala:154: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala:166: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala:276: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala:280: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FallBackFileSourceV2.scala:37: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala:379: value mismatchedInsertedDataColumnNumberError is not a member of object org.apache.spark.sql.errors.QueryCompilationErrors\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala:412: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala:493: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala:515: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.InsertIntoStatement(table: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,partitionSpec: Map[String,Option[String]],userSpecifiedCols: Seq[String],query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,overwrite: Boolean,ifPartitionNotExists: Boolean,byName: Boolean)\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:185: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:192: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:195: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:213: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:216: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:228: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:238: type mismatch;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala:313: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.ReplaceData(table: org.apache.spark.sql.catalyst.analysis.NamedRelation,condition: org.apache.spark.sql.catalyst.expressions.Expression,query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,originalTable: org.apache.spark.sql.catalyst.analysis.NamedRelation,groupFilterCondition: Option[org.apache.spark.sql.catalyst.expressions.Expression],write: Option[org.apache.spark.sql.connector.write.Write])\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/GroupBasedRowLevelOperationScanPlanning.scala:42: not enough patterns for object GroupBasedRowLevelOperation offering (org.apache.spark.sql.catalyst.plans.logical.ReplaceData, org.apache.spark.sql.catalyst.expressions.Expression, Option[org.apache.spark.sql.catalyst.expressions.Expression], org.apache.spark.sql.catalyst.plans.logical.LogicalPlan): expected 4, found 3\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala:76: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.ReplaceData(table: org.apache.spark.sql.catalyst.analysis.NamedRelation,condition: org.apache.spark.sql.catalyst.expressions.Expression,query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,originalTable: org.apache.spark.sql.catalyst.analysis.NamedRelation,groupFilterCondition: Option[org.apache.spark.sql.catalyst.expressions.Expression],write: Option[org.apache.spark.sql.connector.write.Write])\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala:90: value tableNotSupportTimeTravelError is not a member of object org.apache.spark.sql.errors.QueryCompilationErrors\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala:93: value tableNotSupportTimeTravelError is not a member of object org.apache.spark.sql.errors.QueryCompilationErrors\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala:98: wrong number of arguments for pattern org.apache.spark.sql.catalyst.plans.logical.ReplaceData(table: org.apache.spark.sql.catalyst.analysis.NamedRelation,condition: org.apache.spark.sql.catalyst.expressions.Expression,query: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,originalTable: org.apache.spark.sql.catalyst.analysis.NamedRelation,groupFilterCondition: Option[org.apache.spark.sql.catalyst.expressions.Expression],write: Option[org.apache.spark.sql.connector.write.Write])\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/RowLevelOperationRuntimeGroupFiltering.scala:47: not enough patterns for object GroupBasedRowLevelOperation offering (org.apache.spark.sql.catalyst.plans.logical.ReplaceData, org.apache.spark.sql.catalyst.expressions.Expression, Option[org.apache.spark.sql.catalyst.expressions.Expression], org.apache.spark.sql.catalyst.plans.logical.LogicalPlan): expected 4, found 3\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/RowLevelOperationRuntimeGroupFiltering.scala:48: constructor cannot be instantiated to expected type;\n[ERROR] [Error] spark/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala:176: value jsonValue is not a member of org.apache.spark.sql.Row\n[ERROR] 47 errors found\n[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.0:compile (scala-compile-first) on project spark-sql_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.0:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit value: 255) -> [Help 1]\n[ERROR]\u00a0\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\u00a0\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException\n\u00a0\nAny help would be appreciated",
        "Issue Links": []
    },
    "PARQUET-2312": {
        "Key": "PARQUET-2312",
        "Summary": "Bump snappy-java from 1.1.8.3 to 1.1.10.1 in /parquet-hadoop",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": null,
        "Reporter": "Gang Wu",
        "Created": "16/Jun/23 09:05",
        "Updated": "16/Jun/23 09:06",
        "Resolved": "16/Jun/23 09:06",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2313": {
        "Key": "PARQUET-2313",
        "Summary": "Bump actions/setup-java from 1 to 3",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "format-2.10.0",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Gang Wu",
        "Created": "16/Jun/23 09:08",
        "Updated": "16/Jun/23 09:11",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2314": {
        "Key": "PARQUET-2314",
        "Summary": "Bump jackson.version from 2.15.0 to 2.15.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "None",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "19/Jun/23 05:32",
        "Updated": "19/Jun/23 05:33",
        "Resolved": "19/Jun/23 05:33",
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2315": {
        "Key": "PARQUET-2315",
        "Summary": "Local timestamps fields for Avro schema are not supported",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.14.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-avro",
        "Assignee": null,
        "Reporter": "Stefan Ziegler",
        "Created": "19/Jun/23 13:11",
        "Updated": "22/Jun/23 15:52",
        "Resolved": null,
        "Description": "Avro schema supports the creation of local timestampe fields:\n\n\r\nLocalTimestampMillis datetimeType = LogicalTypes.localTimestampMillis();\r\nfield = new Schema.Field(attrDesc.getAttributeName(), Schema.createUnion(datetimeType.addToSchema(Schema.create(Schema.Type.LONG), Schema.create(Schema.Type.NULL)), doc, null);\r\n\n\nBut since there is no support in the AvroSchemaConverter.convertLogicalType method, these fields end up as BIGINT (as reported in e.g. Apache Drill).",
        "Issue Links": []
    },
    "PARQUET-2316": {
        "Key": "PARQUET-2316",
        "Summary": "Allow partial prebuffer in parquet FileReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "cpp-13.0.0",
        "Component/s": "parquet-cpp",
        "Assignee": "Jinpeng Zhou",
        "Reporter": "Jinpeng Zhou",
        "Created": "20/Jun/23 21:49",
        "Updated": "2 days ago 22:48",
        "Resolved": "06/Jul/23 14:19",
        "Description": "The current FileReader can only work in\u00a0 one of the two modes, coalescing (when Prebuffer is called) and non-coalescing (when Prefufer is not called), due to the if statement here\nSince Prebuffer is basically caching all specified column chunks, it would raise concerns on memory usage for systems with tight memory budget. In such scenarios, one may want to Prebuffer some small chunks while being able to read the rest chunks using\u00a0 BufferedInputStream.",
        "Issue Links": [
            "https://github.com/apache/arrow/pull/36192"
        ]
    },
    "PARQUET-2317": {
        "Key": "PARQUET-2317",
        "Summary": "parquet-format and parquet-format-structures defines Util with inconsitent methods provided",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12.0,                                            1.13.0",
        "Fix Version/s": "None",
        "Component/s": "parquet-format",
        "Assignee": null,
        "Reporter": "Joey Pereira",
        "Created": "23/Jun/23 23:39",
        "Updated": "26/Jun/23 05:13",
        "Resolved": null,
        "Description": "I have been running into a bug due to parquet-format and parquet-format-structures both defining the org.apache.parquet.format.Util class but doing so inconsistently.\nExamples of this are several methods which include a BlockCipher parameter that are defined from parquet-format-structures but not parquet-format. While invoking code that happens to use these, such as org.apache.parquet.hadoop.ParquetFileReader.readFooter, the code will fail if the parquet-format happens to be loaded first on the classpath.\nHere is an example stack trace for a Scala Spark application.\n\n\r\nCaused by: java.lang.NoSuchMethodError: 'org.apache.parquet.format.FileMetaData org.apache.parquet.format.Util.readFileMetaData(java.io.InputStream, org.apache.parquet.format.BlockCipher$Decryptor, byte[])'\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter$3.visit(ParquetMetadataConverter.java:1441) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter$3.visit(ParquetMetadataConverter.java:1438) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter$NoFilter.accept(ParquetMetadataConverter.java:1173) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:1438) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:591) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:536) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:530) ~[parquet_hadoop.jar:1.13.1]\r\nat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:478) ~[parquet_hadoop.jar:1.13.1]\r\n... (my application code invoking the above)\r\n\n\nBecause of issues external to Parquet that I have yet to figure out (a complex Spark and dependency setup), my classpaths are not deterministically ordered and I am unable to pin the parquet-format-structures ahead hence why I'm chiming in about this.\nEven if that weren't the case, this is a fairly prickly edge to run into as both modules define overlapping classes. Util is not the only class that appears to be defined by both, just what I have been focusing on due to this bug.\nIt appears these methods were introduced in at least 1.12: https://github.com/apache/parquet-mr/commit/65b95fb72be8f5a8a193a6f7bc4560fdcd742fc7#diff-852341c99dcae06c8fa2b764bcf3d9e6860e40442d0ab1cf5b935df80a9cacb7",
        "Issue Links": []
    },
    "PARQUET-2318": {
        "Key": "PARQUET-2318",
        "Summary": "Implement a tool to list page headers",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-cli",
        "Assignee": "Gabor Szadovszky",
        "Reporter": "Gabor Szadovszky",
        "Created": "27/Jun/23 13:22",
        "Updated": "30/Jun/23 08:40",
        "Resolved": "30/Jun/23 08:32",
        "Description": "Needs a tool which lists the page headers in a Parquet file for debugging purposes.",
        "Issue Links": []
    },
    "PARQUET-2319": {
        "Key": "PARQUET-2319",
        "Summary": "Upgrade Avro to version 1.11.2",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13.0",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-avro",
        "Assignee": "Isma\u00ebl Mej\u00eda",
        "Reporter": "Isma\u00ebl Mej\u00eda",
        "Created": "03/Jul/23 13:37",
        "Updated": "05/Jul/23 02:16",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "PARQUET-2320": {
        "Key": "PARQUET-2320",
        "Summary": "Bump jackson-databind from 2.14.2 to 2.15.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14.0",
        "Component/s": "parquet-mr",
        "Assignee": "Gang Wu",
        "Reporter": "Gang Wu",
        "Created": "05/Jul/23 02:44",
        "Updated": "05/Jul/23 02:45",
        "Resolved": "05/Jul/23 02:45",
        "Description": null,
        "Issue Links": []
    }
}