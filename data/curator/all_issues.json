{
    "CURATOR-1": {
        "Key": "CURATOR-1",
        "Summary": "Release initial org.apache.curator version",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Apache",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "22/Mar/13 22:00",
        "Updated": "09/May/13 22:50",
        "Resolved": "09/May/13 22:50",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-2": {
        "Key": "CURATOR-2",
        "Summary": "OSGi-friend distribution",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.2.0-incubating",
        "Component/s": "General",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Luca Burgazzoli",
        "Created": "26/Mar/13 07:21",
        "Updated": "06/Sep/16 19:20",
        "Resolved": "31/Jul/13 19:49",
        "Description": "The current curator jars (netfix/github) are not OSGi-enabled, would be nice to have the apache distributions OSGi-aware.",
        "Issue Links": []
    },
    "CURATOR-3": {
        "Key": "CURATOR-3",
        "Summary": "LeaderLatch race condition causing extra nodes to be added in Zookeeper Edit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Mar/13 22:28",
        "Updated": "26/Jun/22 13:43",
        "Resolved": "26/Jun/22 13:43",
        "Description": "From https://github.com/Netflix/curator/issues/265\nLooks like there's a race condition in LeaderLatch. If LeaderLatch.close() is called at the right time while the latch's watch handler is running, the latch will place another node in Zookeeper after the latch is closed.\nBasically how it happens is this:\n1) I have two processes contesting a LeaderLatch, ProcessA and ProcessB. ProcessA is leader.\n2) ProcessA loses leadership somehow (it releases, its connection goes down, etc.)\n3) This causes ProcessB's watch to get called, check the state is still STARTED, and if so the LeaderLatch will re-evaluate if it is leader.\n4) While the watch handler is running, close() is called on the LeaderLatch on ProcessB. This sets the LeaderLatch state to CLOSED, removes the znode from ZK and closes off the LeaderLatch.\n5) The watch handler has already checked that the state is STARTED, so it does a getChildren() on the latch path, and finds the latch's znode is missing. It goes ahead and calls reset(), which places a new znode in Zookeeper.\nResult: The LeaderLatch is closed, but there is still a node in Zookeeper that isn't associated with any LeaderLatch and won't go away until the session goes down. Subsequent LeaderLatches at this path can never get leadership while that session is up.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/209",
            "https://github.com/apache/curator/pull/210"
        ]
    },
    "CURATOR-4": {
        "Key": "CURATOR-4",
        "Summary": "POST_INITIALIZED_EVENT race conditions / optimizations",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Mar/13 22:29",
        "Updated": "25/Sep/13 03:33",
        "Resolved": null,
        "Description": "From https://github.com/Netflix/curator/pull/261\nWe've been running a data structure modeled after PathChildrenCache for a while now on a path with ~2500 child nodes and using a async loading strategy very similar to the new POST_INITIALIZED_EVENT startup mode. I noticed a couple subtle race conditions that we've encountered in our own code - thought I'd share them back.\ntrun@5229c8c If a node is removed during startup (after getChildren() but before getDataAndStat()) the INITIALIZED_EVENT will never fire. Handling NONODE events fixes this.\ntrun@fb94530 Though highly unlikely, I think it's possible for the initialSet to appear to be fully initialized before all the getDataAndStat() calls have even been issued? Constructing the initialSet before issuing any getDataAndStat() calls eliminates this possibility.\ntrun@e4ddc6c Each call to maybeOfferInitializedEvent() loops over the entire initialSet, and since it's called after each updateInitialSet() this can get pretty expensive ( O(n2) ) with thousands of children. There doesn't seem to be much value in keeping the entire initialSet around so removing each node after it's loaded simplifies this check a great deal.\nAlso one simple bugfix (which may not even be necessary any more)...\ntrun@3385adb initialSet is keyed on node, not fullPath so this call was just a NOP before.",
        "Issue Links": []
    },
    "CURATOR-5": {
        "Key": "CURATOR-5",
        "Summary": "Make JsonInstanceSerializer#deserialize leverage ServiceInstanceBuilder",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Mar/13 22:31",
        "Updated": "20/Jan/15 18:19",
        "Resolved": null,
        "Description": "From https://github.com/Netflix/curator/pull/255\nProposing a way to change JsonInstanceSerializer leverage ServiceInstance's builder pattern directly.\nSee discussion and request to generate a patch here: https://groups.google.com/d/topic/curator-users/Ai9RsVoUdgU/discussion\n===\nIt's possible a lot of the lower-level JSON deserialization code in JsonInstanceSerializer#deserialize() could be passed off to Jackson if ServiceInstanceBuilder's setters also followed the bean patterns (e.g. setId() versus id()). If ServiceInstanceBuilder followed this pattern, Jackson's standard bean deserialization could, I believe, deserialize directly into the ServiceInstanceBuilder and JsonInstanceSerializer#deserialize() could use that higher-level pattern to eliminate a fair chunk of code here.\n===\nSo, why don't we make ServiceInstanceBuilder follow the bean API? Can you update this pull to reflect that? I suggest leaving the old builder-style methods and marking them as deprecated. This way it won't break existing users.",
        "Issue Links": [
            "/jira/browse/OOZIE-1485"
        ]
    },
    "CURATOR-6": {
        "Key": "CURATOR-6",
        "Summary": "JavaDoc in com.netflix.curator.framework.state.ConnectionState isn't clear",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Mar/13 22:32",
        "Updated": "30/Jul/14 20:10",
        "Resolved": "30/Jul/14 20:05",
        "Description": "From https://github.com/Netflix/curator/issues/253\nThe JavaDoc for ConnectionState.RECONNECTED mentions only SUSPENDED and read-only however RECONNECTED can occur after LOST. The JavaDoc for this enum should be updated.",
        "Issue Links": []
    },
    "CURATOR-7": {
        "Key": "CURATOR-7",
        "Summary": "Session ids not preserved if EnsembleProvider has changed the ensemble",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "26/Mar/13 23:41",
        "Updated": "10/May/23 12:46",
        "Resolved": "10/May/23 12:46",
        "Description": "See https://github.com/Netflix/curator/issues/266\nInterProcessMutex, LeaderLatch, etc use an ephemeral node. If ZooKeeper gives a Disconnected event, the native client reconnects with the same session id, and the ephemeral node is preserved. If the ensemble changes at any point before a Disconnect, Curator's ConnectionState#checkState() calls handleNewConnectionString(), which constructs a new native ZK client, discarding the previous session id, and losing all locks.\nThis can be expensive.\nCan ConnectionState be made to preserve the session id, or be more conservative about discarding the entire native client on a Disconnected event?\nClarifications:\nAn ensemble change (e.g. adding a new node to a cluster) does not mean that all session ids are now invalid. The next network glitch should not break locks.\nThere is no assumption in this description that the reconfiguration and the glitch/disconnection were related - they need not be.",
        "Issue Links": [
            "/jira/browse/CURATOR-551",
            "/jira/browse/ZOOKEEPER-1683",
            "/jira/browse/ZOOKEEPER-1680",
            "/jira/browse/ZOOKEEPER-1684",
            "/jira/browse/CURATOR-8"
        ]
    },
    "CURATOR-8": {
        "Key": "CURATOR-8",
        "Summary": "Possible mishandling of connection timeouts on large/shooty clusters",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "29/Mar/13 23:11",
        "Updated": "25/Sep/13 03:29",
        "Resolved": "25/Sep/13 03:29",
        "Description": "See CURATOR-7 for background.\nConnectionState.checkTimeouts resets the connection if the session timeout has expired, whether or not we ever had a session. If one has a large, shooty cluster with a short session timeout and a long connection timeout, this might reset the host list pointer to zero every sessionTimeoutMs, never letting it connect to a valid, available host towards the end of the list. On the other hand, if it is an assertion that sessionTimeout > connectionTimeout always, then the min() call is not required and the ifelse should read \"else if (elapsed > connectionTimeoutMs)\" instead of just \"else\".",
        "Issue Links": [
            "/jira/browse/CURATOR-7"
        ]
    },
    "CURATOR-9": {
        "Key": "CURATOR-9",
        "Summary": "DefaultTracerDriver should call log.isTraceEnabled()",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "29/Mar/13 23:26",
        "Updated": "25/Sep/13 03:32",
        "Resolved": "25/Sep/13 03:32",
        "Description": "The cost of string concatenation may be incurred frequently at trace level; it's worth checking if trace is enabled before incurring that cost.",
        "Issue Links": []
    },
    "CURATOR-10": {
        "Key": "CURATOR-10",
        "Summary": "NPE from JMX infrastructure in curator-test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.1.0-incubating",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "02/Apr/13 19:40",
        "Updated": "10/Jun/13 22:38",
        "Resolved": "10/Jun/13 22:37",
        "Description": "QuorumPeer.java contains this:\nfinally \n{ MBeanRegistry.getInstance().unregisterAll(); }\n\nSince MBeanRegistry is static, this unregisters all MBeans from ALL QuorumPeers in a TestingCluster.\nThis causes:\n[ERROR] NIOServerCnxnFactory - Thread Thread[QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:41733,5,main] died <java.lang.AssertionError>java.lang.AssertionError\n        at org.apache.zookeeper.jmx.MBeanRegistry.register(MBeanRegistry.java:89)\n        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:771)\n        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:912)\nand similar when connecting a new client to a server after killing another in a test suite.\nLooking for a workaround, but bringing this to your attention early.\nNote: Code against ZK 3.5.x library, although that unregisterAll() call seems to predate that by a long way, so this is presumably valid throughout?",
        "Issue Links": []
    },
    "CURATOR-11": {
        "Key": "CURATOR-11",
        "Summary": "QuorumPeerBuilder does not account for dynamic config in 3.5.x",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "awaiting-response",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "Shevek",
        "Created": "02/Apr/13 21:17",
        "Updated": "06/Sep/13 05:29",
        "Resolved": null,
        "Description": "Curator fails to load ZK clusters without this patch because server cannot find its own id in the config file. See patch.",
        "Issue Links": []
    },
    "CURATOR-12": {
        "Key": "CURATOR-12",
        "Summary": "java.io.IOException: ZooKeeperServer not running after server.kill()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Shevek",
        "Created": "02/Apr/13 21:29",
        "Updated": "25/Sep/13 03:24",
        "Resolved": "25/Sep/13 03:24",
        "Description": "[DEBUG] NIOServerCnxn - IOException stack trace <java.io.IOException: ZooKeeperServer not running>java.io.IOException: ZooKeeperServer not running\n        at org.apache.zookeeper.server.NIOServerCnxn.readLength(NIOServerCnxn.java:936)\n        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:328)\n        at org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:530)\n        at org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:152)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nPreceding election seemed successful.\nZK 3.5.0. Tracking my work.",
        "Issue Links": []
    },
    "CURATOR-13": {
        "Key": "CURATOR-13",
        "Summary": "KillSession does not work if ZooKeeper does not allow sessionId in constructor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Shevek",
        "Created": "02/Apr/13 21:32",
        "Updated": "25/Sep/13 03:25",
        "Resolved": "25/Sep/13 03:25",
        "Description": "https://issues.apache.org/jira/browse/ZOOKEEPER-1680\nThe ticket linked above suggests that KillSession is broken, as it's impossible to construct a duplicate session.\nBackground: KillSession works by creating a duplicate session and closing it. If a duplicate session cannot be created, it won't work.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-1680"
        ]
    },
    "CURATOR-14": {
        "Key": "CURATOR-14",
        "Summary": "Memory leak in Curator watches",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Brandon Beck",
        "Created": "08/Apr/13 22:44",
        "Updated": "20/Apr/15 08:58",
        "Resolved": null,
        "Description": "The JVM runs out of memory if you repetitively create a PathChildrenCache, start it then immediately stop it.  It appears that the memory is taken up by a watch that isn't ever cleaned up.  Curator attempts to do some pooling of watches, but doesn't seem to use the path in the pooling.",
        "Issue Links": []
    },
    "CURATOR-15": {
        "Key": "CURATOR-15",
        "Summary": "LeaderSelector may (undetectably) fail to elect",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Shevek",
        "Created": "13/Apr/13 00:35",
        "Updated": "23/May/23 02:57",
        "Resolved": "23/May/23 02:57",
        "Description": "In LeaderSelector, if mutex.acquire() throws an Exception, for example because CuratorFramework.getZooKeeper() threw a previously-enqueued background exception, then that failure will propagate out of doWork and doWorkLoop, and kill the background submission onto the executor service.\nThis means that a leaderselector which was start()ed will NEVER elect, and this situation is NOT DETECTABLE externally, since that exception happens on a private executorservice thread and is not client visible. It's impossible to look at a LeaderSelector and decide whether it is still \"viable\".\nThis can leave a machine/process \"hung\" and not automatically recoverable within curator.\nEither isQueued() needs to be exposed, which means that a leader is either elected or queued; or the finally{} block which calls clearIsQueued() needs also to set state to CLOSED or FAILED, so that we can query this failure externally.",
        "Issue Links": [
            "/jira/browse/CURATOR-16",
            "/jira/browse/CURATOR-16",
            "/jira/browse/CURATOR-104",
            "/jira/browse/ZOOKEEPER-1683"
        ]
    },
    "CURATOR-16": {
        "Key": "CURATOR-16",
        "Summary": "LeaderSelector does not (auto)requeue on session expired",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.1.0-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Julio Lopez",
        "Created": "18/Apr/13 20:47",
        "Updated": "23/May/23 02:57",
        "Resolved": "10/Jun/13 14:59",
        "Description": "If mutex. acquire() throws a KeeperException.SessionExpiredException in LeaderSelector.doWork(), the exception is not handled in LeaderSelector.doWorkLoop(), causing the loop to terminate even when autoRequeue is true.",
        "Issue Links": [
            "/jira/browse/CURATOR-15",
            "/jira/browse/CURATOR-15"
        ]
    },
    "CURATOR-17": {
        "Key": "CURATOR-17",
        "Summary": "PathChildrenCache closes it's executor always",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.0.1-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Eric Tschetter",
        "Created": "22/Apr/13 20:43",
        "Updated": "11/May/13 02:13",
        "Resolved": "11/May/13 02:12",
        "Description": "PathChildrenCache.close() calls shutdownNow() on its executor, always.\nI generally reuse executors and have been passing them in on the constructor.  I have to wrap my executors in something that ignores the shutdownNow() call in order to work around this.  It's not the end of the world, but it is a little annoying.",
        "Issue Links": []
    },
    "CURATOR-18": {
        "Key": "CURATOR-18",
        "Summary": "Create/delete race in LeaderLatch",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.0.1-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Oleg Shaldybin",
        "Created": "23/Apr/13 00:03",
        "Updated": "10/May/13 00:01",
        "Resolved": "10/May/13 00:01",
        "Description": "There is a race between creating a node and deleting it. If LeaderLatch is closed while the node creation is in flight, the created node won't get deleted, and latch node will stay in ZK forever.",
        "Issue Links": []
    },
    "CURATOR-19": {
        "Key": "CURATOR-19",
        "Summary": "InterProcessSemaphoreV2.getParticipantNodes throws NoNodeException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Andrew Chen",
        "Created": "24/Apr/13 01:39",
        "Updated": "25/Sep/13 03:49",
        "Resolved": "25/Sep/13 03:49",
        "Description": "Calls to getParticipantNodes() always throws an exception:\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /c4/my-path/leases/lease-\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1586)\n\tat com.netflix.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:213)\n\tat com.netflix.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:202)\n\tat com.netflix.curator.RetryLoop.callWithRetry(RetryLoop.java:106)\n\tat com.netflix.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:198)\n\tat com.netflix.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:190)\n\tat com.netflix.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:37)\n\tat com.netflix.curator.framework.recipes.locks.InterProcessSemaphoreV2.getParticipantNodes(InterProcessSemaphoreV2.java:161)",
        "Issue Links": []
    },
    "CURATOR-20": {
        "Key": "CURATOR-20",
        "Summary": "Update website with mailing list information",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ioannis Canellos",
        "Created": "24/Apr/13 09:34",
        "Updated": "24/Apr/13 18:23",
        "Resolved": "24/Apr/13 18:23",
        "Description": "It would be nice if the site contained information about the mailing lists and how to subscribe.\nThe usual ones for incubator projects:\ndev-subscribe@<name>.incubator.apache.org\nuser-subscribe@<name>.incubator.apache.org\ndo not seem to be active and there is no obvious way to subscribe.",
        "Issue Links": []
    },
    "CURATOR-21": {
        "Key": "CURATOR-21",
        "Summary": "PathChildrenCache consumes an entire thread all the time no matter what",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.0.1-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Eric Tschetter",
        "Created": "24/Apr/13 19:30",
        "Updated": "06/May/13 18:57",
        "Resolved": "06/May/13 18:57",
        "Description": "PathChildrenCache currently takes an Executor, but only to fire off a Runnable that does a blocking while loop waiting for work.  This means that you must have one thread per PathChildrenCache, which is not that great.\nPathChildrenCache should just use the Executor's work queuing mechanism to enqueue work items instead of maintaining its own work queue mechanism.",
        "Issue Links": []
    },
    "CURATOR-22": {
        "Key": "CURATOR-22",
        "Summary": "Add a listener to LeaderLatch",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.0.1-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Eric Tschetter",
        "Created": "24/Apr/13 21:26",
        "Updated": "09/May/13 23:49",
        "Resolved": "09/May/13 23:48",
        "Description": "From the mailing list:\nHi guys,\nI'm looking at using Leader Latch and I noticed it only has polling mechanisms for figuring out if you are the leader. I was wondering if there is a reason to not have a callback mechanism that you can register with two methods:\nstartBeingMaster();\nstopBeingMaster();\nBasically, any time setLeadership() is called with a value other than what it was, it would cause either startBeingMaster() to be called or stopBeingMaster() to be called.\nI can effect this using an extra thread and the polling mechanisms that are there, but am wondering if I'm the only one that would prefer to interact with it in this way or if others would also prefer to have access to this type of API?\n(Btw, for anyone wondering, I'm currently swapping out an old zookeeper client library for Curator and that's why I have all of these random questions about APIs and stuff)\nJordan Zimmerman jordan@jordanzimmerman.com via googlegroups.com\n1:48 PM (2 hours ago)\nto curator-users\nLook at LeaderSelector. It is a leader recipe that has a callback mechanism.\nJordan Zimmerman\nEric Tschetter cheddar@metamarkets.com\n2:27 PM (2 hours ago)\nto curator-users\nOh, that's the Leader Election recipe.\nI don't really like that abstraction either because it makes me worry about too much:\n1) Losing connection, etc.\n2) \"Relinquishing\" leadership\nI just want to either be the leader or not be the leader and have that choice be completely external. That is, I want something to tell me that I became the leader or that I lost leadership. I assume that I can always give up my leadership by just closing the thing down.",
        "Issue Links": []
    },
    "CURATOR-23": {
        "Key": "CURATOR-23",
        "Summary": "Calling markLost in ZookeeperClient does not propagate event to ConnectionStateListeners",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ioannis Canellos",
        "Created": "28/Apr/13 12:50",
        "Updated": "28/Apr/13 17:49",
        "Resolved": "28/Apr/13 14:01",
        "Description": "I would expect that a calling curatorFramework.getZookeeperClient().markLost() would propagate a LOST event and then a RECONNECTED (once the client gets reconnected) event to the registered ConnectionStateListeners, which doesn't seem to happen.",
        "Issue Links": []
    },
    "CURATOR-24": {
        "Key": "CURATOR-24",
        "Summary": "The current method of managing hung ZK handles needs improvement",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.0.1-incubating",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "10/May/13 00:06",
        "Updated": "10/May/13 00:19",
        "Resolved": "10/May/13 00:19",
        "Description": "In v1.3.0, a \"major change\" was added whereby \"when the Curator state changes to LOST, a flag will be set\nso that the next time Curator needs to get the ZooKeeper instance, the current instance will be closed and a new ZooKeeper instance will be allocated (as if the session had expired).\"\nThis has turned out not to be optimum. Instead, if the session timeout elapses before a SysConnected is received, treat it as a failed session and dispose and reallocate the ZooKeeper handle. This has be shown to be superior internally at Netflix.",
        "Issue Links": []
    },
    "CURATOR-25": {
        "Key": "CURATOR-25",
        "Summary": "Create a PersistentEphemeralNode recipe",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1.0-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "13/May/13 05:15",
        "Updated": "12/Jan/16 16:11",
        "Resolved": "10/Jun/13 15:51",
        "Description": "Per https://github.com/Netflix/curator/pull/201 and similar requests, create a PersistentEphemeralNode recipe",
        "Issue Links": [
            "/jira/browse/CURATOR-287"
        ]
    },
    "CURATOR-26": {
        "Key": "CURATOR-26",
        "Summary": "Make the tests more resliant",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "13/May/13 06:09",
        "Updated": "06/Dec/18 19:51",
        "Resolved": "25/Sep/13 03:38",
        "Description": "Some of the tests are flakey and fail intermittently. This task will remain open as a container for fixing tests.",
        "Issue Links": [
            "/jira/browse/CURATOR-490"
        ]
    },
    "CURATOR-27": {
        "Key": "CURATOR-27",
        "Summary": "PathChildrenCache creates but doesn't shut down Executors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Ioannis Canellos",
        "Created": "22/May/13 13:52",
        "Updated": "03/Nov/13 21:12",
        "Resolved": "06/Oct/13 14:54",
        "Description": "After the introduction of the CloseableExecutorService (see CURATOR-17) the PathChildrenCache may create Executor services that it doesn't close.\nIdeally, the CloseableExecutorService should accept a parameter that specifies if the executor needs to be shutdown on close. Recipes that create their own ExecutorService should set this parameter to true. When an executor not directly managed by the recipe is used, the flag should be just false, so that the the executor doesn't always close (satisfy CURATOR-17).",
        "Issue Links": [
            "/jira/browse/CURATOR-39",
            "/jira/browse/CURATOR-66"
        ]
    },
    "CURATOR-28": {
        "Key": "CURATOR-28",
        "Summary": "Add Expiry Time To InterProcessLocks",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "3l3ph4n1n3",
        "Created": "28/May/13 02:26",
        "Updated": "25/Sep/13 03:37",
        "Resolved": null,
        "Description": "If a client takes a distributed lock and fails without breaking its zookeeper connection (e.g. the main application thread deadlocks) then that lock will never be released (at least without manual intervention, e.g. killing the process that has it). When a client's acquiring a lock I'd like to be able to specify a time after which the lock is automatically released. If the client currently holds the lock it should be able to extend this time period as many times as it likes. A write-up for what I'm describing for redis is here: https://chris-lamb.co.uk/posts/distributing-locking-python-and-redis .\nI can see a couple of ways of going about this - the lock lifetime could be stored in the node's date (and so clients could check if the node had expired by adding the lifetime to the node's ctime or mtime). However, comparing the client's current time with the expiry time in the node is probably not the right thing to do as the client's clock may be out of sync with the other clients (or the zookeeper nodes). It'd be nice if zookeeper could automatically delete nodes (i.e. release the lock) after a certain amount of time - i.e. make it the zookeeper cluster's decision when the lock is expired, not the client's decision. However, I'm not sure exactly how to do this...\nThanks,",
        "Issue Links": []
    },
    "CURATOR-29": {
        "Key": "CURATOR-29",
        "Summary": "getPath needs improved exception cases",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "John Vines",
        "Created": "29/May/13 20:33",
        "Updated": "01/Aug/14 19:02",
        "Resolved": "29/May/13 20:37",
        "Description": "https://github.com/Netflix/curator/issues/13\nKnown issue, before was dismissed, but I think there is room for improvement. A generic exception provides very little recourse for good error handling without trudging through the code to determine real error cases.",
        "Issue Links": [
            "/jira/browse/CURATOR-135"
        ]
    },
    "CURATOR-30": {
        "Key": "CURATOR-30",
        "Summary": "Recursive delete",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Ioannis Canellos",
        "Reporter": "John Vines",
        "Created": "30/May/13 23:22",
        "Updated": "05/Oct/13 18:33",
        "Resolved": "05/Oct/13 18:33",
        "Description": "Currently there is the ability to recursive create parent znodes when you create a node. However, there is no ability to recursively delete a hierarchy. Zookeeper already provides this in their ZKUtil.java package, but it seems like a very curator-ish thing to perform as well. There is the potential difficulty involved with the guarantee() functionality, but it should be workable.",
        "Issue Links": []
    },
    "CURATOR-31": {
        "Key": "CURATOR-31",
        "Summary": "Incorrect CuratorEventType set in SetACLBuilderImpl's performBackgroundOperation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Yip Ng",
        "Created": "07/Jun/13 02:35",
        "Updated": "25/Sep/13 03:23",
        "Resolved": "25/Sep/13 03:23",
        "Description": "The SetACLBuilderImpl is using CuratorEventType.SET_DATA instead of CuratorEventType.SET_ACL in its performBackgroundOperation method.  Also, it appears that SET_ACL is not defined in CuratorEventType.",
        "Issue Links": []
    },
    "CURATOR-32": {
        "Key": "CURATOR-32",
        "Summary": "Improve javadocs around create/setData",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "John Vines",
        "Created": "10/Jun/13 18:17",
        "Updated": "10/May/23 14:52",
        "Resolved": "10/May/23 14:52",
        "Description": "Currently the javadocs for create and setData do nothing to explain the behavior if nodes already exist. This would be enormously helpful for users new to Curator.",
        "Issue Links": []
    },
    "CURATOR-33": {
        "Key": "CURATOR-33",
        "Summary": "Recursive Node Cache",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "John Vines",
        "Created": "10/Jun/13 19:29",
        "Updated": "02/Sep/14 22:22",
        "Resolved": "08/Aug/14 02:10",
        "Description": "Currently the PathChildrenCache will trigger listen events for all children at the given node. However, it would be useful to have a cache that would trigger listen events for the entire hierarchy below the given node.",
        "Issue Links": [
            "/jira/browse/CURATOR-137",
            "/jira/browse/CURATOR-143"
        ]
    },
    "CURATOR-34": {
        "Key": "CURATOR-34",
        "Summary": "CuratorFramework.create().creatingParentsIfNeeded().forPath() throws NodeExistsException if node exists",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.0.0-incubating,                                            2.0.1-incubating",
        "Fix Version/s": "TBD",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Nandor Kracser",
        "Created": "17/Jun/13 10:24",
        "Updated": "01/Jan/17 09:53",
        "Resolved": null,
        "Description": "I think that NodeExistsException should be ignored in CreateBuilderImpl. In the try and in the NoNodeException catch block as well.\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /examples/something\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:626)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:610)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:606)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:429)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:409)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:317)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:253)",
        "Issue Links": []
    },
    "CURATOR-35": {
        "Key": "CURATOR-35",
        "Summary": "Add callback to CuratorFramework.sync()",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "2.1.0-incubating",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Narayanan Arunachalam",
        "Created": "18/Jun/13 20:00",
        "Updated": "18/Jun/13 21:12",
        "Resolved": "18/Jun/13 21:12",
        "Description": "Add support for callback to sync() method like any other ZK operations.",
        "Issue Links": []
    },
    "CURATOR-36": {
        "Key": "CURATOR-36",
        "Summary": "Bad session, infinite connection loop from Curator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": "Eric Tschetter",
        "Reporter": "Eric Tschetter",
        "Created": "18/Jun/13 20:09",
        "Updated": "18/Dec/15 14:37",
        "Resolved": "25/Sep/13 03:28",
        "Description": "On the ZK clients that I am running Curator on, we sometimes see reconnect loops like the following.  These are infinite and happen until the process is restarted.\n2013-06-18 19:57:28,660 INFO [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - State change: RECONNECTED\n2013-06-18 19:57:28,660 WARN [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - ConnectionStateManager queue full - dropping events to make room\n2013-06-18 19:57:28,786 INFO [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - State change: SUSPENDED\n2013-06-18 19:57:28,786 WARN [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - ConnectionStateManager queue full - dropping events to make room\n2013-06-18 19:57:29,048 INFO [main-SendThread(ip-10:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server ip-10/10.:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)\n2013-06-18 19:57:29,049 INFO [main-SendThread(ip-10:2181)] org.apache.zookeeper.ClientCnxn - Socket connection established to ip-10/10.:2181, initiating session\n2013-06-18 19:57:29,160 WARN [main-SendThread(ip-10:2181)] org.apache.zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2013-06-18 19:57:29,160 INFO [main-SendThread(ip-10:2181)] org.apache.zookeeper.ClientCnxn - Session establishment complete on server ip-10/10.:2181, sessionid = 0x63f5865925e0010, negotiated timeout = 30000\n2013-06-18 19:57:29,177 INFO [main-EventThread] org.apache.curator.framework.state.ConnectionStateManager - State change: RECONNECTED\nLooking on the ZK side, it looks like\n2013-06-18 20:07:31,215 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1580] - Established session 0x63f5865925e0010 with negotiated timeout 30000 for client /10.:56263\n2013-06-18 20:07:31,324 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@639] - Exception causing close of session 0x63f5865925e0010 due to java.io.IOException: Len error 6736057\n2013-06-18 20:07:31,325 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /10.:56263 which had sessionid 0x63f5865925e0010\nSo, there appears to be some issue with trying to recover the session.  I don't know exactly what is causing that issue recovering the session, but it would be awesome if Curator were able to notice that it's failing at getting its session back and just try to make a brand new connection.\nIt appears like this might be doable in reaction to the ConnectionStateManager queue filling up?",
        "Issue Links": []
    },
    "CURATOR-37": {
        "Key": "CURATOR-37",
        "Summary": "ADD tickTime AND maxClientCnxns SUPPORT TO CURATOR TESTS",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Chevaris",
        "Created": "22/Jun/13 18:56",
        "Updated": "06/Sep/13 15:45",
        "Resolved": "04/Sep/13 04:18",
        "Description": "Adding support to set the tickTime and maxClientCnxns when using Curator Tests tool to instantiate Zookeeper servers",
        "Issue Links": []
    },
    "CURATOR-38": {
        "Key": "CURATOR-38",
        "Summary": "Service Discovery does not have a way to mark an instance as down temporarily",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "2.2.0-incubating",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "23/Jun/13 18:41",
        "Updated": "11/Jul/13 17:44",
        "Resolved": "11/Jul/13 17:44",
        "Description": "Users of Service Discovery need a mechanism to mark an instance as down/inaccessible for a short period of time. This can be done manually currently, but an integrated solution would be nice.",
        "Issue Links": []
    },
    "CURATOR-39": {
        "Key": "CURATOR-39",
        "Summary": "ServiceProvider thread is not closed during ServiceDiscovery close",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.0",
        "Component/s": "None",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Colin Morelli",
        "Created": "24/Jun/13 11:48",
        "Updated": "07/Jun/16 21:54",
        "Resolved": "07/Oct/13 08:05",
        "Description": "I'm using 2.0.1-incubating, using the following to build a ServiceProvider:\nserviceProvider = serviceDiscovery.serviceProviderBuilder()\n                    .serviceName(name)\n                    .providerStrategy(new RoundRobinStrategy<DiscoverableService>())\n                    .build();\nserviceProvider.start();\nI have a name => ServiceProvider map that stores all my service providers. During shutdown, I call:\n        for (ServiceProvider serviceProvider : serviceProviderMap.values()) \n{\n            serviceProvider.close();\n        }\n\n        serviceDiscovery.close();\nWhich is redundant, it seems, because serviceDiscovery itself appears to shutdown the providers. However, I still end up with a dangling \"ServiceProvider-0\" thread that keeps Tomcat open.\nIf I start the container up, without making a service call (so I don't allocate any ServiceProviders), Tomcat shuts down cleanly. As soon as a ServiceProvider is allocated (by making a service call) Tomcat hangs during shutdown, and informs me of the remaining thread. I will try to provide more information as it becomes available to me.",
        "Issue Links": [
            "/jira/browse/CURATOR-27"
        ]
    },
    "CURATOR-40": {
        "Key": "CURATOR-40",
        "Summary": "Curator client cannot connect after one zookeeper host shuts down on EC2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Andy Grove",
        "Created": "25/Jun/13 20:58",
        "Updated": "22/Jul/13 20:30",
        "Resolved": "22/Jul/13 20:24",
        "Description": "We use DNS names on Amazon EC2 to specify Zookeeper host names. If one of the ZK hosts shuts down or loses network connectivity we can no longer connect via Curator, even though the other ZK hosts are still running and have quorum. The issue is specific to an UnknownHostException being thrown on DNS resolution when calling the start() method on CuratorZookeeperClient. The workaround is for us to use IP addresses rather than DNS names, but this isn't really workable on EC2 since IP addresses change when servers restart so we use Elastic IPs to ensure that the ZK hosts have fixed IP addresses.\nI have attached a unit test which demonstrates the issue and provides more detail in the comments.",
        "Issue Links": []
    },
    "CURATOR-41": {
        "Key": "CURATOR-41",
        "Summary": "Curator masks/loses exception in LockInternals",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "Recipes",
        "Assignee": "Kezhu Wang",
        "Reporter": "Shevek",
        "Created": "25/Jun/13 22:10",
        "Updated": "17/Jun/23 13:50",
        "Resolved": "17/Jun/23 13:50",
        "Description": "private boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception\n        try \n{\n        ...\n        }\n        catch ( Exception e )\n        {\n            doDelete = true;\n            throw e;\n        }\n        finally\n        {\n            if ( doDelete )\n            {\n                deleteOurPath(ourPath);\n            }\n        }\nSay ... throws an exception. catch sets doDelete = true. Then deleteOurPath throws an exception. We never find out what the original exception was that caused the mutex to fail.\nIn JDK8, I think we get the extra Throwable#getSuppressed() call, which will return us this extra exception, but we aren't on JDK8 yet.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/466"
        ]
    },
    "CURATOR-42": {
        "Key": "CURATOR-42",
        "Summary": "Background guaranteed delete considers NoNode to be a failed delete, and retries it",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "25/Jun/13 23:15",
        "Updated": "29/Jul/14 23:08",
        "Resolved": "29/Jul/14 22:17",
        "Description": "In delete(), NoNode should be a special case which succeeds instantly, and does not consider the delete failed.",
        "Issue Links": []
    },
    "CURATOR-43": {
        "Key": "CURATOR-43",
        "Summary": "Acquire gets stuck in InterProcessMutex if there are sub-nodes in lock path",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jay Zarfoss",
        "Reporter": "Phil Mikhailov",
        "Created": "26/Jun/13 15:33",
        "Updated": "20/Jul/13 15:01",
        "Resolved": "19/Jul/13 18:15",
        "Description": "If lock path has sub-node, this sub-node is also treated as try to acquire this lock.\nI created small test method in TestInterProcessMutex that reproduces this issue:\n@Test\npublic void testWithSubNode() throws Exception {\n    final CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString(), new RetryOneTime(1));\n    try \n{\n        client.start();\n        final InterProcessMutex lock = new InterProcessMutex(client, LOCK_PATH);\n        final InterProcessMutex lock0 = new InterProcessMutex(client, LOCK_PATH + \"/0\");\n        boolean acquired0 = lock0.acquire(5, TimeUnit.SECONDS);\n        Assert.assertTrue(acquired0);\n        lock0.release();\n        boolean acquired = lock.acquire(5, TimeUnit.SECONDS);\n        Assert.assertTrue(acquired);\n        lock.release();\n    }\n finally \n{\n        client.close();\n    }\n}\nI also patched our curator build in LockInternals with this temporary patch before you found better solution.\nstatic final String             PROTECTED_PREFIX = \"c\";\npublic static List<String> getSortedChildren(CuratorFramework client, String basePath, final String lockName, final LockInternalsSorter sorter) throws Exception\n{\n    List<String> children = Lists.newArrayList(Collections2.filter(client.getChildren().forPath(basePath),\n            new Predicate<String>()\n            {\n                @Override\n                public boolean apply(String s)\n                {\n                    return s.startsWith(PROTECTED_PREFIX);\n                }\n            }));\n    return getSortedChildren(lockName, sorter, children);\n}",
        "Issue Links": []
    },
    "CURATOR-44": {
        "Key": "CURATOR-44",
        "Summary": "LeaderSelector does not assign leader randomly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "2.2.0-incubating",
        "Component/s": "Documentation,                                            Recipes",
        "Assignee": "Jay Zarfoss",
        "Reporter": "Anders Wallgren",
        "Created": "26/Jun/13 23:50",
        "Updated": "19/Jul/13 23:09",
        "Resolved": "19/Jul/13 23:09",
        "Description": "The javadoc for LeaderSelector says leadership is assigned randomly but this does not appear to be the case.\nA fair mutex is used so it appears that leaders are selected in the order that they attempt to acquire the mutex.\nThis probably wouldn't be much more than a documentation issue except that it tends to concentrate leaders on one node in the cluster when each node participates in more than one election (and assuming the node starts the LeaderSelectors right around the time the node comes up, which is the case in our usage).",
        "Issue Links": []
    },
    "CURATOR-45": {
        "Key": "CURATOR-45",
        "Summary": "LeaderSelector threw exception, but still created ephemeral node, breaking everything",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shevek",
        "Created": "27/Jun/13 19:22",
        "Updated": "18/Oct/13 13:43",
        "Resolved": "25/Sep/13 16:10",
        "Description": "ZooKeeper hiccupped, and then this happened:\n    2013-06-19 02:23:35,561 DEBUG [LeaderSelector-1] com.netflix.curator.RetryLoop.takeException (RetryLoop.java:184) - Retry-able exception received\n    org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /[REMOVED]/election/_c_1ccdb2b9-7f9a-4570-9555-201c91ec2dcb-lock-\n            at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.5.0.jar:3.5.0--1]\n            at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.5.0.jar:3.5.0--1]\n            at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:876) ~[zookeeper-3.5.0.jar:3.5.0--1]\n            at com.netflix.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:625) ~[curator-framework-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.imps.CreateBuilderImpl$10.call(CreateBuilderImpl.java:609) ~[curator-framework-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.RetryLoop.callWithRetry(RetryLoop.java:106) [curator-client-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:605) [curator-framework-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:428) [curator-framework-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:41) [curator-framework-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:218) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:218) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:74) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:314) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:373) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:46) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at com.netflix.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:195) [curator-recipes-1.3.5-SNAPSHOT.jar:?]\n            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) [?:1.6.0_27]\n            at java.util.concurrent.FutureTask.run(FutureTask.java:166) [?:1.6.0_27]\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146) [?:1.6.0_27]\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.6.0_27]\n            at java.lang.Thread.run(Thread.java:679) [?:1.6.0_27]\nHowever, the ephemeral node got created, and this hung leader election for this path.\nI'm investigating to work out where to put an extra guaranteed-delete. I see the case in LockInternals, which sometimes triggers to do this cleanup, but it didn't trigger in this case.\nYou must really love our bugs by now.",
        "Issue Links": [
            "/jira/browse/CURATOR-52"
        ]
    },
    "CURATOR-46": {
        "Key": "CURATOR-46",
        "Summary": "ZKPaths.makePath should return the valid path when the child is \"/\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "2.2.0-incubating",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Jae Hyeon Bae",
        "Created": "09/Jul/13 22:33",
        "Updated": "10/Jul/13 05:54",
        "Resolved": "10/Jul/13 05:54",
        "Description": "ZKPaths.makePath() has a bug when parent is \"abc\" and child is '/', it should return \"/abc\" but it's returning \"/abc/\" which is invalid.",
        "Issue Links": []
    },
    "CURATOR-47": {
        "Key": "CURATOR-47",
        "Summary": "Two leaders in the leader election recipe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Germ\u00e1n Blanco",
        "Created": "12/Jul/13 11:52",
        "Updated": "17/Jul/13 18:02",
        "Resolved": "17/Jul/13 18:02",
        "Description": "When using the Leader Election recipe and doing a rolling restart of the zookeeper servers in the cluster, it seems that at some points there are two participants that see themselves as leaders.",
        "Issue Links": []
    },
    "CURATOR-48": {
        "Key": "CURATOR-48",
        "Summary": "InterProcessSemaphoreMutex should use InterProcessSemaphoreV2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Jul/13 23:23",
        "Updated": "06/Sep/13 15:45",
        "Resolved": "06/Sep/13 06:07",
        "Description": "InterProcessSemaphoreMutex currently uses InterProcessSemaphore which is deprecated. It should instead use InterProcessSemaphoreV2.",
        "Issue Links": []
    },
    "CURATOR-49": {
        "Key": "CURATOR-49",
        "Summary": "LeaderSelector has no response when delete the leader node by other clients",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Ding Jinqiang",
        "Created": "29/Jul/13 05:18",
        "Updated": "30/Jul/13 06:15",
        "Resolved": "30/Jul/13 06:15",
        "Description": "If I setup a normal LeaderSelection with two participants, and as expected one of them becomes the leader. But when I delete the leader node manually by other clients, the other leaderSelector becomes the leader normally. But the leaderSelector, which is the previous leader, does not receive any notification in the LeaderSelectorListener. \nWell, my question is, is this normal, or I just missed something? \nSample code may like this:\n\t\tRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 1);\n\t\tclient = CuratorFrameworkFactory.newClient(Config.getInstance().getServerConnectString(),\n\t\t\t\tretryPolicy);\n\t\tclient.start();\n \t\tLeaderSelectorListener listener = new LeaderSelectorListener() {\n\t\t\t@Override\n\t\t\tpublic void takeLeadership(CuratorFramework client) throws Exception \n{\n\t\t\t\tlogger.info(\"takeLeadership, thread id:\"+Thread.currentThread().getId());\n \t\t\t\tThread.sleep(Long.MAX_VALUE);\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void stateChanged(CuratorFramework client, ConnectionState newState) \n{\n\t\t\t\tlogger.info(\"Zookeeper connection stateChanged, new state is \" + newState);\n\t\t\t}\n\t\t};\n\t\tselector = new LeaderSelector(client, Config.getInstance().getEclectPath());\n\t\tselector.autoRequeue();\n\t\tselector.start();",
        "Issue Links": []
    },
    "CURATOR-50": {
        "Key": "CURATOR-50",
        "Summary": "CreateBuilderImpl fails on forPath if connectString features a chroot that does not already exist.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Sean-Michael Lewis",
        "Created": "20/Aug/13 04:50",
        "Updated": "10/May/23 14:16",
        "Resolved": "10/May/23 14:16",
        "Description": "When a CuratorFramework is initialized with a connectString featuring a chroot that does not already exist in the ensemble, CreateBuilderImpl will fail to create new nodes even if createParentsIfNeeded is true.\nFor example, the following code will result in a org.apache.zookeeper.KeeperException$NoNodeException.\n\nCuratorFramework client = CuratorFramework.builder().retryPolicy(myPolicy).connectString(\"myServer1:2181,myServer2:2181/chroot).build();\nclient.create().createParentsIfNeeded().forPath(\"test\");\n\n\nThis can be worked around by using a namespace in lieu of the chroot or by calling \n\nclient.create().forPath(\"/\")\n\n before attempting to create any other nodes. \nWhile using namespaces is likely the best practice, the framework does initialize with the chroot connectString. There are also reasons why one might want to use both chroot connectStrings as well as namespaces (application environments for the former, application for the latter).\nMy proposed fix is to alter \n\nZkPaths.mkdirs\n\n to not skip \"/\" when it walks the tree. In cases where no chroot configured or a chroot that already exists, the node will be found and skipped. Otherwise, it will be created.",
        "Issue Links": [
            "/jira/browse/CURATOR-357"
        ]
    },
    "CURATOR-51": {
        "Key": "CURATOR-51",
        "Summary": "LeaderSelector with custom Executor does not guarantee unique leadership",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Henrik Nordvik",
        "Created": "29/Aug/13 12:54",
        "Updated": "07/Sep/13 02:41",
        "Resolved": "07/Sep/13 02:41",
        "Description": "When providing a custom executor to the leader selector it creates a thread that waits for leadership.\nWhen it has leadership, it submits a runnable to the custom executor and then release leadership. \nIf the executor is is e.g. a ThreadPoolExecutor, the takeLeadership method is executed asynchrounously, after releasing leadership.\nIs this a bug or am I missing the point of using my own executor?\nI have attached an example which shows multiple leaders being elected at the same time.\n(The example uses curator 1.3.3, but I think it is the same in 2.x)",
        "Issue Links": []
    },
    "CURATOR-52": {
        "Key": "CURATOR-52",
        "Summary": "Retry issues with background operations",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Michael Morello",
        "Created": "03/Sep/13 13:45",
        "Updated": "01/Jun/23 15:15",
        "Resolved": "24/Sep/13 01:54",
        "Description": "Retry-able errors (CONNECTIONLOSS, OPERATIONTIMEOUT, SESSIONMOVED, SESSIONEXPIRED) are not propagated to callback when the retry policy gave up if the operation is done in background.\nAdditionally, all background operations do not retry connection issues if the zookeeper connection is not currently connected. This is a major oversight.",
        "Issue Links": [
            "/jira/browse/CURATOR-45",
            "/jira/browse/CURATOR-72",
            "/jira/browse/CURATOR-673"
        ]
    },
    "CURATOR-53": {
        "Key": "CURATOR-53",
        "Summary": "PersistentEphemeralNode does not work when ZK server are not available when the node is started",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating,                                            2.3.0",
        "Fix Version/s": "2.4.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Chevaris",
        "Created": "06/Sep/13 07:43",
        "Updated": "24/Dec/13 23:02",
        "Resolved": "10/Nov/13 02:58",
        "Description": "This sequence is not working:\n// ZK servers are not started here\nExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 1, 2000);\nCuratorFramework zkClient = CuratorFrameworkFactory.newClient(\"127.0.0.1:30101\", \n4500,\n4500,\nretryPolicy);\nzkClient.start();\nThread.sleep(2000);\nPersistentEphemeralNode pen = new PersistentEphemeralNode(zkClient, Mode.EPHEMERAL, \"/abc/pen\", \"hello\".getBytes());\nThread.sleep(2000);\n// Start ZK servers here\nThread.sleep(2000);\n// \"/abc/pen\" ephemeral node is not created\nIt is expected that once the CuratorFrameowrk is connected the ephemeral node is created\nI attach a program that shows the failure",
        "Issue Links": []
    },
    "CURATOR-54": {
        "Key": "CURATOR-54",
        "Summary": "No reliable way to cancel leadership in LeaderSelector when connection fails due to edge cases",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "10/Sep/13 21:54",
        "Updated": "11/Sep/13 15:59",
        "Resolved": "10/Sep/13 23:00",
        "Description": "From a post to user@\n\"Hi,\nI'm fairly new with Zookeeper and Curator. I want to achieve a simple leader election process.\nBut, I ran into trouble implementing the interruption behavior. I could not find a reliable way to stop the leader (withdraw from leadership).\nI think even the schoolbook example that Curator brings is flawed.\nIn leader.ExampleClient:\n @Override\n    public void stateChanged(CuratorFramework client, ConnectionState newState)\n    {\n        // you MUST handle connection state changes. This WILL happen in production code.\n        if ( (newState == ConnectionState.LOST) || (newState == ConnectionState.SUSPENDED) )\n        {\n            if ( ourThread != null )\n            {\n                ourThread.interrupt();\n            }\n        }\n    }\nSo in case of lost leadership, the ourThread thread is interrupted. However, ourThread is set in the 2nd line of the takeLeadership() method. Until then, it is null.\nWhat happens if the connection is lost immediately after it is established, and ourThread stays null? Won't it be the case that the thread will go on, thinking that it is the leader, despite it being supposed to withdraw?\nThanks,\nArie\"",
        "Issue Links": []
    },
    "CURATOR-55": {
        "Key": "CURATOR-55",
        "Summary": "EnsurePath should have a getPath() function",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "John Vines",
        "Created": "16/Sep/13 18:54",
        "Updated": "17/Sep/13 15:35",
        "Resolved": "17/Sep/13 15:35",
        "Description": "EnsurePath provides no mechanism to extract the path out of the object. This means if you're using it, you need to maintain the mapping of path and EP in order to check and then utilize the path appropriately. This seems a bit convoluted since the path is right there in the EP object.",
        "Issue Links": []
    },
    "CURATOR-56": {
        "Key": "CURATOR-56",
        "Summary": "DistributedQueue znode name grows on re-queue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Adarsh Bhat",
        "Created": "20/Sep/13 00:50",
        "Updated": "21/Aug/15 18:33",
        "Resolved": "23/Sep/13 01:29",
        "Description": "Create a DistributedQueue with a lockPath. If QueueConsumer.consumeMessage() throws an exception, the message gets re-queued. Every time this happens, the name of the queue item znode grows in length. After many iterations, node names get very large and the server stops accepting new client connections.\nDuring a re-queue, the new znode created uses the old znode path as the base, and the SEQUENTIAL flag is set. Zookeeper appends a sequence number to the old path (which already had a sequence number), leading to a longer name during each re-queue.",
        "Issue Links": [
            "/jira/browse/CURATOR-231"
        ]
    },
    "CURATOR-57": {
        "Key": "CURATOR-57",
        "Summary": "Event order lost with PathChildrenCache",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "24/Sep/13 17:46",
        "Updated": "26/Sep/13 15:06",
        "Resolved": "26/Sep/13 15:06",
        "Description": "I am attaching a simple test to demonstrate the problem:\nSingle threaded 'for' loop that creates children 0 to N, the events are not received in order.",
        "Issue Links": []
    },
    "CURATOR-58": {
        "Key": "CURATOR-58",
        "Summary": "ACLs support for ZKPaths.mkdirs()",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.1-incubating",
        "Fix Version/s": "2.4.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "24/Sep/13 21:28",
        "Updated": "12/Jan/16 15:12",
        "Resolved": "08/Nov/13 16:41",
        "Description": "ZKPaths always creates parent node using the OPEN_ACL_UNSAFE ACL:\nline 168:\nzookeeper.create(subPath, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\nAs a result:\n\nEnsurePath.ensure(..)\nCreateBuilderImpl.backgroundCreateParentsThenNode\nCreateBuilderImpl.pathInForeground\nwill never use the ACLProvider and will always create unprotected nodes.\n\nThe workaround is not to use those methods and create the parents manually, node by node.",
        "Issue Links": [
            "/jira/browse/OOZIE-1491",
            "/jira/browse/CURATOR-244"
        ]
    },
    "CURATOR-59": {
        "Key": "CURATOR-59",
        "Summary": "ConnectionState.CONNECTED can get set incorrectly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Sep/13 15:47",
        "Updated": "26/Sep/13 17:56",
        "Resolved": "26/Sep/13 17:56",
        "Description": "If the client is never able to connect to a server, it will incorrectly set ConnectionState.CONNECTED. \nExample:\n\n    @Test\n    public void testNeverConnected() throws Exception\n    {\n        PersistentEphemeralNode persistentEphemeralNode = null;\n        // use a connection string to a non-existent server\n        CuratorFramework client = CuratorFrameworkFactory.newClient(\"localhost:1111\", 100, 100, new RetryOneTime(1));\n        try\n        {\n            final BlockingQueue<ConnectionState> queue = Queues.newLinkedBlockingQueue();\n            ConnectionStateListener listener = new ConnectionStateListener()\n            {\n                @Override\n                public void stateChanged(CuratorFramework client, ConnectionState state)\n                {\n                    queue.add(state);\n                }\n            };\n            client.getConnectionStateListenable().addListener(listener);\n            client.start();\n\n            // use a recipe that continuously retries\n            persistentEphemeralNode = new PersistentEphemeralNode(client, PersistentEphemeralNode.Mode.EPHEMERAL, \"/abc/pen\", \"hello\".getBytes());\n            persistentEphemeralNode.start();\n\n            Assert.assertEquals(queue.take(), ConnectionState.LOST);\n        }\n        finally\n        {\n            Closeables.closeQuietly(persistentEphemeralNode);\n            Closeables.closeQuietly(client);\n        }\n    }",
        "Issue Links": []
    },
    "CURATOR-60": {
        "Key": "CURATOR-60",
        "Summary": "Upgrade to maven-surefure-plugin 2.16",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "TBD",
        "Component/s": "Tests",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Ioannis Canellos",
        "Created": "04/Oct/13 18:57",
        "Updated": "04/Oct/13 21:47",
        "Resolved": "04/Oct/13 19:37",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-61": {
        "Key": "CURATOR-61",
        "Summary": "Use a new surefire process per test class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.0",
        "Component/s": "Tests",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Ioannis Canellos",
        "Created": "04/Oct/13 19:22",
        "Updated": "04/Oct/13 21:46",
        "Resolved": "04/Oct/13 19:36",
        "Description": "Currently, we are using a single surefire process per test suite. \nWith this setup we only see the test results only when all tests of the module have finished.\nIn many cases, it is useful to get the results right after the test class is complete.\nWhy? Because, some modules take a really long time to run and some times it hard to tell if particular has hung.\nIf we use fork mode \"always\" / forkCount=1 reuseForks=false each test class will use its own surefire process and we will get the results on screen for each test class.",
        "Issue Links": []
    },
    "CURATOR-62": {
        "Key": "CURATOR-62",
        "Summary": "Leader Election Deadlock",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Duncan Jones",
        "Created": "07/Oct/13 15:53",
        "Updated": "14/May/15 20:53",
        "Resolved": null,
        "Description": "I've noticed that it is possible for a leader election to deadlock if a thread is interrupted while it is trying to acquire the mutex for the election.\nI've created a forced example of this here: https://github.com/dfjones/curator/commit/544220b1e6b51c2718a7d3511a74962ff1c5ff48\nYou can see deadlock by using my modified code and running the LeaderSelectorExample. Some leaders may execute, but on my system I eventually see deadlock. Note that I only see deadlock when running against a remote zk server rather than the embedded test server. I'm using Zookeeper 3.4.5 on Mac OS X 10.8.4.\nFrom what I can tell by inspecting the ZK state/watching in the debugger, the thread that is interrupted is able to successfully create the lock object in ZK. However, due to the interrupt an exception is generated and LockInternals#internalLockLoop never runs. Later, in LeaderSelector#doWork when mutex.release() is called this fails at the for lockData.\nOnce this occurs, the lock object in ZK is the oldest and will cause deadlock.",
        "Issue Links": []
    },
    "CURATOR-63": {
        "Key": "CURATOR-63",
        "Summary": "ServiceCacheImpl doesn't check if service name is null",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Nandor Kracser",
        "Created": "08/Oct/13 18:05",
        "Updated": "08/Oct/13 19:46",
        "Resolved": "08/Oct/13 19:46",
        "Description": "If not giving a service name with ServiceCacheBuilder.name() the ServiceCache throws a weird Exception later on, because it wants to fetch data from the service group node instead of the service instance node:\nException in thread \"main\" java.io.EOFException: No content to map to Object due to end of input\n\tat org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:2766)\n\tat org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2709)\n\tat org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1964)\n\tat org.apache.curator.x.discovery.details.JsonInstanceSerializer.deserialize(JsonInstanceSerializer.java:50)\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:178)\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.start(ServiceCacheImpl.java:81)",
        "Issue Links": []
    },
    "CURATOR-64": {
        "Key": "CURATOR-64",
        "Summary": "Retry logic appears to delay reconnect after session expiry",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Shaun Senecal",
        "Created": "09/Oct/13 04:19",
        "Updated": "10/Oct/13 05:41",
        "Resolved": "10/Oct/13 05:16",
        "Description": "If a watch is triggered immediately before a session expiry, and the watch attempts to fetch data from ZK (using Curator), its possible that the reconnect behaviour is delayed until the retry gives up\nIt currently looks something like this:\n1. watch A is triggered, begins processing\n2. session is expired (watch A hasnt completed execution yet)\n3. watch A attempts to fetch data from ZK (say: curator.getData()...)\n4. the getData() will retry until the policy tells it to give up (could be several minutes)\n5. finally curator will reconnect to ZK\nI would expect something more like this:\n1. watch A is triggered, begins processing\n2. session is expired (watch A hasnt completed execution yet)\n3. watch A attempts to fetch data from ZK (say: curator.getData()...)\n4. the first getData() fails because of session expiry (should be nearly instantly)\n5. curator reconnects to ZK\n6. a second attempt to call getData() is made via the RetryPolicy\n7. watch A completes processing\nWe are using the BoundedExponentialBackoffRetry, so we end up waiting for quite a while after session expiry, leaving our services dead in the water for much longer than is necessary.\nThis occurs with curator v1.3.3 and ZK 3.4.5",
        "Issue Links": []
    },
    "CURATOR-65": {
        "Key": "CURATOR-65",
        "Summary": "Update various dependencies",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "12/Oct/13 06:46",
        "Updated": "02/Mar/14 05:07",
        "Resolved": "02/Mar/14 05:07",
        "Description": "Curator uses common third party libs (e.g. Guava). Update to latest versions of these.",
        "Issue Links": [
            "/jira/browse/CURATOR-69"
        ]
    },
    "CURATOR-66": {
        "Key": "CURATOR-66",
        "Summary": "PathChildrenCache leaks a thread on close().",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Shawn Smith",
        "Created": "17/Oct/13 18:36",
        "Updated": "17/Oct/13 19:37",
        "Resolved": "17/Oct/13 19:37",
        "Description": "The PathChildrenCache does not shutdown its ExecutorService in close().  As a result, using any of the PathChildrenCache constructors that create a single threaded executor (which is all of them but the one that takes an ExecutorService explicitly) will leak a thread when PathChildrenCache.close() is called.\nHere's a program that reproduces the error.  Currently it prints \"# Total PathChildrenCache threads: 100\" to indicate that 100 threads were leaked.\n\nimport org.apache.curator.ensemble.fixed.FixedEnsembleProvider;\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.framework.recipes.cache.PathChildrenCache;\nimport org.apache.curator.retry.RetryOneTime;\n\nimport java.util.Map;\n\npublic class ThreadLeakTest {\n    public static final int NUM_ITERATIONS = 100;\n\n    public static void main(String[] args) throws Exception {\n        CuratorFramework curator = CuratorFrameworkFactory.builder()\n                .ensembleProvider(new FixedEnsembleProvider(\"localhost:2181\"))\n                .retryPolicy(new RetryOneTime(100))\n                .build();\n        curator.start();\n\n        for (int i = 0; i < NUM_ITERATIONS; i++) {\n            PathChildrenCache cache = new PathChildrenCache(curator, \"/\", true);\n            cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n            Thread.sleep(10);\n            cache.close();\n        }\n\n        curator.close();\n\n        // Wait for things to settle\n        Thread.sleep(1000);\n\n        int threadCount = 0;\n        Map<Thread,StackTraceElement[]> allThreads = Thread.getAllStackTraces();\n        for (Thread thread : allThreads.keySet()) {\n            if (thread.getName().startsWith(\"PathChildrenCache\")) {\n                System.out.printf(\"Thread still alive: %s%n\", thread.getName());\n                threadCount++;\n            }\n        }\n        System.out.printf(\"# Total PathChildrenCache threads: %d%n\", threadCount);\n    }\n}\n\n\nWorkaround: use the PathChildrenCache constructor that takes an ExecutorService and call shutdown() yourself.  Note that closing the CuratorFramework ZK connection does not clean up the leaked threads.",
        "Issue Links": [
            "/jira/browse/CURATOR-27"
        ]
    },
    "CURATOR-67": {
        "Key": "CURATOR-67",
        "Summary": "Issue with default JSONInstanceSerializer for discovery service builder",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Dan Diodati",
        "Created": "17/Oct/13 21:00",
        "Updated": "05/Aug/14 04:11",
        "Resolved": "05/Aug/14 03:45",
        "Description": "There is a problem with the ServiceDiscoveryBuilder.java not letting me provide a custom InstanceSerializer.\nThis build creates a new instance of the JsonInstanceSerailzer in the main builder method before I get a chance to provide my own instance in the serializer method.\nIn my case it ends up giving me a incompatible class error due to the fact that I have a legacy system which is using an older version of Jackson library( ~1.5) which is not binary compatible with the jackson version used by ServiceDiscovery (~1.9).\nSo I tried to provide my own serializer but the default instance is always being created.\nLook at https://git-wip-us.apache.org/repos/asf?p=incubator-curator.git;a=blob;f=curator-x-discovery/src/main/java/org/apache/curator/x/discovery/ServiceDiscoveryBuilder.java;h=ab62004e72d138e1195e01ce4d3e2f1a7d4825a6;hb=HEAD\n     /**\n  34      * Return a new builder. The builder will be defaulted with a \n{@link JsonInstanceSerializer}.\n  35      *\n  36      * @param payloadClass the class of the payload of your service instance (you can use {@link Void}\n  37      * if your instances don't need a payload)\n  38      * @return new builder\n  39      */\n  40     public static<T> ServiceDiscoveryBuilder<T>     builder(Class<T> payloadClass)\n  41     {\n  42         return new ServiceDiscoveryBuilder<T>(payloadClass).serializer(new JsonInstanceSerializer<T>(payloadClass));\n  43     }\n\n\nSo to fix this can we change this to :\n     /**\n  34      * Return a new builder. The builder will be defaulted with a {@link JsonInstanceSerializer}\n.\n  35      *\n  36      * @param payloadClass the class of the payload of your service instance (you can use \n{@link Void}\n  37      * if your instances don't need a payload)\n  38      * @return new builder\n  39      */\n  40     public static<T> ServiceDiscoveryBuilder<T>     builder(Class<T> payloadClass)\n  41     \n{\n  42         return new ServiceDiscoveryBuilder<T>(payloadClass);\n  43     }\n\nThen in the build method from:\n  45     /**\n  46      * Build a new service discovery with the currently set values\n  47      *\n  48      * @return new service discovery\n  49      */\n  50     public ServiceDiscovery<T>      build()\n  51     \n{\n  52         return new ServiceDiscoveryImpl<T>(client, basePath, serializer, thisInstance);\n  53     }\n\nTo something like:\n  44 \n  45     /**\n  46      * Build a new service discovery with the currently set values\n  47      *\n  48      * @return new service discovery\n  49      */\n  50     public ServiceDiscovery<T>      build()\n  51     {\n\t\tIf (serializer == null) \n{\n                   serializer = new JsonInstanceSerializer<T>(payloadClass);  // NOTE Need to add payloadClass as a private data member too\n                }\n  52         return new ServiceDiscoveryImpl<T>(client, basePath, serializer, thisInstance);\n  53     }",
        "Issue Links": []
    },
    "CURATOR-68": {
        "Key": "CURATOR-68",
        "Summary": "Namespace is not stripped off events generated via a NamspaceFacade",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Cam McKenzie",
        "Created": "21/Oct/13 00:42",
        "Updated": "21/Oct/13 13:41",
        "Resolved": "21/Oct/13 13:41",
        "Description": "When a namespace is defined via the CuratorFrameworkFactory.Builder methods, paths for events generated by the CuratorFramework have the namespace stripped off them.\nWhen using a NamespaceFacade, the paths for generated event paths include the namespace.\nThis appears to be due to the implementation of the Watcher() interface in the constructor of the CuratorFrameworkImpl class. It attempts to remove the namespace from events before passing them to CuratorListener instances. In the case where the namespace is defined by the builder (and is thus part of the base CuratorFramework instance) this works fine. In the case where a NamespaceFacade is being used then the namespace of the base CuratorFramework instance is used instead of the namespace of the NamespaceFacade).\nI will attach a simple test case to reproduce",
        "Issue Links": []
    },
    "CURATOR-69": {
        "Key": "CURATOR-69",
        "Summary": "update guava to 15,the version 14 can't deploy in glassfish",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "yuedaxia76",
        "Created": "30/Oct/13 05:25",
        "Updated": "16/Feb/14 15:54",
        "Resolved": "30/Oct/13 14:30",
        "Description": "can't be deployed   Curator  in glassfish v4,becase guava.\nthis bug:\nhttps://code.google.com/p/guava-libraries/issues/detail?id=1433\nupdate guava version 15",
        "Issue Links": [
            "/jira/browse/CURATOR-65"
        ]
    },
    "CURATOR-70": {
        "Key": "CURATOR-70",
        "Summary": "Old Netflix Github Links",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "2.3.0",
        "Component/s": "Documentation",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Vito Laurenza",
        "Created": "30/Oct/13 18:56",
        "Updated": "01/Nov/13 02:40",
        "Resolved": "01/Nov/13 02:40",
        "Description": "Documentation still contains old Netflix Github links.\nEg:\nhttp://curator.incubator.apache.org/curator-x-discovery-server/index.html#JSON_specs",
        "Issue Links": []
    },
    "CURATOR-71": {
        "Key": "CURATOR-71",
        "Summary": "Error message in StandardLockInternalsDriver for retryable lock acquire error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1.0-incubating",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Vegard B. Havdal",
        "Created": "01/Nov/13 12:48",
        "Updated": "05/Aug/14 05:09",
        "Resolved": "05/Aug/14 04:35",
        "Description": "We occasionally get the error message \"Sequential path not found...\" from StandardLockInternalDriver, but the subsequent NoNode exception is caught and treated as normal in some cases in LockInternals. Ie. bogus error message in our log.\nConsider removing the error log msg from StandardLockInternalDriver, not log and throw.",
        "Issue Links": []
    },
    "CURATOR-72": {
        "Key": "CURATOR-72",
        "Summary": "Background operations don't wait for connection timeout",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Evaristo Camarero",
        "Created": "09/Nov/13 22:57",
        "Updated": "27/May/14 17:49",
        "Resolved": "05/Feb/14 18:42",
        "Description": "Background operations don't wait for the configured connection timeout before failing. Attached test shows the problem.",
        "Issue Links": [
            "/jira/browse/CURATOR-52"
        ]
    },
    "CURATOR-73": {
        "Key": "CURATOR-73",
        "Summary": "No reliable way to restart leadership in LeaderSelector when connection fails due to edge cases",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Henrik Nordvik",
        "Created": "17/Nov/13 22:17",
        "Updated": "23/Feb/14 10:08",
        "Resolved": "23/Feb/14 10:08",
        "Description": "This is related to CURATOR-54, and possibly also CURATOR-62.\nIf a LeaderSelector-thread is cancelled (e.g. because of lost connection to zookeeper), there is no way of restarting it. \nFirst it jumps out of the doWork-loop, because the interrupt flag is set.\nThe isQueued flag is not reset when this happens, so requeue() does nothing, even though the thread has been parked.\nI'm using curator 2.3.0 with the new ListenerAdapter-way of handling stateChange().",
        "Issue Links": []
    },
    "CURATOR-74": {
        "Key": "CURATOR-74",
        "Summary": "Documentation on InterProcessMutex is misleading",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.2",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Swarnim Kulkarni",
        "Created": "20/Nov/13 23:22",
        "Updated": "03/Apr/14 04:22",
        "Resolved": "03/Apr/14 04:22",
        "Description": "Documentation on the two acquire methods is slightly misleading as it does not make clear what the behavior should be if a thread owns a lock and another thread attempts to get the lock. The doc should be fixed.",
        "Issue Links": []
    },
    "CURATOR-75": {
        "Key": "CURATOR-75",
        "Summary": "InterProcessSemaphoreV2 does not register changes to SharedCountReader while blocking",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Joshua Bandur",
        "Created": "21/Nov/13 00:42",
        "Updated": "03/Feb/14 21:15",
        "Resolved": "03/Feb/14 21:15",
        "Description": "When using InterProcessSemaphoreV2 with a SharedCountReader:\nIf the call to acquire() is currently blocking because no leases are available, and you increase the value that the SharedCountReader is tracking, acquire() does not notice the change, and continues to block until timeout. The expected behavior would be for it to notice the new value right away, completing the call successfully if there is now a sufficient number of leases available, or continuing to block if there still aren't enough.\nI suspect the problem is in the SharedCountListener:\n            count.addListener\n                (\n                    new SharedCountListener()\n                    {\n                        @Override\n                        public void countHasChanged(SharedCountReader sharedCount, int newCount) throws Exception\n                        {\n                            InterProcessSemaphoreV2.this.maxLeases = newCount;\n                        }\n                        ...\n                    }\n                );\nThis event handler should probably call InterProcessSemaphoreV2.this.notifyAll() to wake any threads blocked in internalAcquire1Lease().",
        "Issue Links": []
    },
    "CURATOR-76": {
        "Key": "CURATOR-76",
        "Summary": "Adding leader selection ChildReaper recipe",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4.2",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jae Hyeon Bae",
        "Created": "26/Nov/13 18:14",
        "Updated": "02/Apr/14 19:00",
        "Resolved": "02/Apr/14 18:18",
        "Description": "We are having serious data corruption issue when we are rolling restart of zookeeper servers due to one application which is using ChildReaper recipe. I am not sure its root cause but my theory is, when the multiple instances are running ChildReaper recipe, they would conflict each other among checking exist and deleting paths. This conflict can cause data corruption. We observed all servers died due to corrupted data and we had to manually copy log/snapshot data and restart them.",
        "Issue Links": []
    },
    "CURATOR-77": {
        "Key": "CURATOR-77",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sebb",
        "Created": "27/Nov/13 02:08",
        "Updated": "15/Dec/13 22:54",
        "Resolved": "15/Dec/13 22:32",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "CURATOR-78": {
        "Key": "CURATOR-78",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sebb",
        "Created": "27/Nov/13 02:10",
        "Updated": "03/Dec/13 18:00",
        "Resolved": "03/Dec/13 15:06",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "CURATOR-79": {
        "Key": "CURATOR-79",
        "Summary": "InterProcessMutex doesn't clean up after interrupt",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.0.0-incubating,                                            2.1.0-incubating,                                            2.2.0-incubating,                                            2.3.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Orcun Simsek",
        "Created": "27/Nov/13 03:26",
        "Updated": "20/Aug/14 21:26",
        "Resolved": "20/Aug/14 21:26",
        "Description": "InterProcessMutex can deadlock if a thread is interrupted during acquire().  Specifically, CreateBuilderImpl.pathInForeground submits a create request to ZooKeeper, and an InterruptedException is thrown after the node is created in ZK but before ZK.create returns. ZK.create propagates a non-KeeperException, so Curator assumes the create has failed, but does not retry, and the node is now orphaned. At some point in the future, the node becomes the next in the acquisition sequence, but is not reclaimed as the ZK session has not expired.\n<stack trace attached in comments below>\nCurator should catch the InterruptedException and other non-KeeperExceptions, and delete the created node before propagating these exceptions.\n(as originally discussed on https://groups.google.com/forum/#!topic/curator-users/9ii5of8SbdQ)",
        "Issue Links": []
    },
    "CURATOR-80": {
        "Key": "CURATOR-80",
        "Summary": "DistributedDoubleBarrier should remove slef before throw exception",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Arien Chen",
        "Created": "03/Jan/14 17:49",
        "Updated": "22/Jul/15 22:08",
        "Resolved": null,
        "Description": "while enter(), leave() throw exception and checkDeleteOurPath() is not called.\nthe node is still exists\nit will confuse other barrier's enter() , treat it is a valid member.\nsuggest to add a method release() to remove the node manually\nor \nremove the node before throw exception in enter(), leave()",
        "Issue Links": []
    },
    "CURATOR-81": {
        "Key": "CURATOR-81",
        "Summary": "Service Discovery InstanceCleanup cleans up DYNAMIC registration",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Trott",
        "Created": "07/Jan/14 18:29",
        "Updated": "07/Feb/14 23:20",
        "Resolved": "07/Jan/14 19:44",
        "Description": "Line #110 of InstanceCleanup reads:\n                if ( instance.getServiceType() != ServiceType.PERMANENT )\nIt should be:\n                if ( instance.getServiceType() == ServiceType.STATIC )\nOtherwise the cleanup code processes dynamic registrations such as it's own registration.\nSince dynamic registrations are created with EPHEMERAL nodes (ServiceDiscoveryImpl#175) there is no need to cleanup dynamic registrations anyway.",
        "Issue Links": []
    },
    "CURATOR-82": {
        "Key": "CURATOR-82",
        "Summary": "Possible Test Race Condition in org.apache.curator.framework.recipes.locks.TestInterProcessMutex",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Timothy St. Clair",
        "Created": "09/Jan/14 15:20",
        "Updated": "09/Jan/14 15:20",
        "Resolved": null,
        "Description": "Periodically we see a race failure, it's not consistent: \nFull ref: https://bugzilla.redhat.com/show_bug.cgi?id=1049903\n[snip]\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning TestSuite\nTests run: 144, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1,089.234 sec <<< FAILURE! - in TestSuite\ntestReentrant2Threads(org.apache.curator.framework.recipes.locks.TestInterProcessMutex)  Time elapsed: 10.292 sec  <<< FAILURE!\njava.lang.AssertionError: expected [false] but found [true]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertFalse(Assert.java:63)\n\tat org.testng.Assert.assertFalse(Assert.java:73)\n\tat org.apache.curator.framework.recipes.locks.TestInterProcessMutexBase.testReentrant2Threads(TestInterProcessMutexBase.java:308)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:84)\n\tat org.testng.internal.Invoker.invokeMethod(Invoker.java:714)\n\tat org.testng.internal.Invoker.invokeTestMethod(Invoker.java:901)\n\tat org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1231)\n\tat org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)\n\tat org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)\n\tat org.testng.TestRunner.privateRun(TestRunner.java:767)\n\tat org.testng.TestRunner.run(TestRunner.java:617)\n\tat org.testng.SuiteRunner.runTest(SuiteRunner.java:334)\n\tat org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)\n\tat org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)\n\tat org.testng.SuiteRunner.run(SuiteRunner.java:240)\n\tat org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)\n\tat org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86)\n\tat org.testng.TestNG.runSuitesSequentially(TestNG.java:1224)\n\tat org.testng.TestNG.runSuitesLocally(TestNG.java:1149)\n\tat org.testng.TestNG.run(TestNG.java:1057)\n\tat org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:91)\n\tat org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:204)\n\tat org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:107)\n\tat org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:113)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nResults :\nFailed tests: \n  TestInterProcessMutex>TestInterProcessMutexBase.testReentrant2Threads:308 expected [false] but found [true]\n[end-snip]",
        "Issue Links": []
    },
    "CURATOR-83": {
        "Key": "CURATOR-83",
        "Summary": "LeaderLatch.close() doesn't fire LeaderLatchListener.notLeader()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Craig Muchinsky",
        "Created": "13/Jan/14 18:38",
        "Updated": "06/Mar/14 10:52",
        "Resolved": "06/Mar/14 10:52",
        "Description": "When closing a LeaderLatch, the order of cleanup is such that LeaderLatchListeners are not told they are no longer the leader. Within the finally block of the close() method the listener container is being cleared prior to the call to setLeadership(false). I believe simply reversing these calls will fix the problem.",
        "Issue Links": [
            "/jira/browse/CURATOR-92"
        ]
    },
    "CURATOR-84": {
        "Key": "CURATOR-84",
        "Summary": "More flexibility for InterProcessMutex extensions",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Cam McKenzie",
        "Reporter": "Jozef Vilcek",
        "Created": "22/Jan/14 13:12",
        "Updated": "26/Aug/14 17:59",
        "Resolved": "26/Aug/14 17:59",
        "Description": "I have a need for a durable InterProcessMutex. Main reason for this are processes with critical sections, where I can not afford to loose a lock due to session expiration. In such case, others might acquire a lock and kick in while the previous process is still running but e.g. experiencing connection issues. To kill this temporally detached process in favor of others would be too costly.\nTo achieve such behavior, I need lock nodes to be created in PERSISTENT mode. This is not possible to do easily with currently implementation of locks due to few internal scoped classes and methods. I would like to change this.",
        "Issue Links": []
    },
    "CURATOR-85": {
        "Key": "CURATOR-85",
        "Summary": "curator-client ConnectionState uses deprecated Closeables method",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.0-incubating,                                            2.3.0",
        "Fix Version/s": "2.4.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Dave Cohrs",
        "Created": "23/Jan/14 22:41",
        "Updated": "03/Feb/14 18:57",
        "Resolved": "03/Feb/14 18:57",
        "Description": "Tried to upgrade to guava-16 and started getting these errors from Curator on exit:\nException in thread \"Thread-1\" java.lang.NoSuchMethodError: com.google.common.io.Closeables.closeQuietly(Ljava/io/Closeable;)V\n\tat org.apache.curator.ConnectionState.close(ConnectionState.java:109)\n\tat org.apache.curator.CuratorZookeeperClient.close(CuratorZookeeperClient.java:196)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.close(CuratorFrameworkImpl.java:284)\n\tat com.virident.fmc.service.AbstractService.closeCF(AbstractService.java:199)\n\tat com.virident.fmc.service.AbstractService.shutdown(AbstractService.java:215)\n\tat com.virident.fmc.service.AbstractService.access$000(AbstractService.java:31)\n\tat com.virident.fmc.service.AbstractService$1.run(AbstractService.java:98)\ntail: /var/lib/vgccluster/manager.out: file truncated\nInvestigation shows this code:\n    public void close() throws IOException\n    {\n        log.debug(\"Closing\");\n        Closeables.closeQuietly(ensembleProvider);\nThis closeQuietly() method is deprecated as of guava 14, and it gone in guava 16.  The 2-parameter closeQuietly() needs to be called, eg:\n        Closeables.closeQuietly(ensembleProvider, true);\nCurator master source still seems to have this bug.  The 2-parameter closeQuietly is already be available in guava-14 and the 1-parameter version is marked decorated as deprecated in that release which should have generated deprecation warnings.\nA visual inspection of the various curator projects shows that this deprecated API is used in many places.  They all should be corrected to use the 2-parameter Closeables.closeQuietly().",
        "Issue Links": []
    },
    "CURATOR-86": {
        "Key": "CURATOR-86",
        "Summary": "ServiceProvider would benefit from a getAllInstances() method",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.4.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Feb/14 19:20",
        "Updated": "06/Feb/14 19:30",
        "Resolved": "06/Feb/14 19:30",
        "Description": "The ServiceProvider would benefit from a getAllInstances() method. The infrastructure for this method is already there so this is really a trivial addition.",
        "Issue Links": []
    },
    "CURATOR-87": {
        "Key": "CURATOR-87",
        "Summary": "new LeaderLatch \"jitters\" after network outage",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.2.0-incubating",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Oliver Dain",
        "Created": "14/Feb/14 00:33",
        "Updated": "24/May/14 15:38",
        "Resolved": "24/May/14 15:38",
        "Description": "I have a LeaderLatch that has become the leader. Then all of ZooKeeper becomes unreachable (due to network issues or something). I do know that I could maintain the same LeaderLatch instance and when ZK becomes reachable again it would re-negotiate leadership. However, for my particular use case this doesn't work and I have to release the LeaderLatch. Later, when ZK is available again I allocate a new LeaderLatch instance and call start() and on it. The bug is that this when await() is called on the new latch it immediately calls the isLeader callback and then almost immediately after the await() call returns, notLeader gets called.\nThe following unit test reproduces the problem:\n @Test\n    public void leaderLatchJitters() throws Exception {\n        TestingServer server = new TestingServer();\n        CuratorFramework zkClient = CuratorFrameworkFactory.newClient(server.getConnectString(),\n                new ExponentialBackoffRetry(1000, 3));\n        zkClient.start();\n        LeaderLatch leaderLatch = new LeaderLatch(zkClient, \"/path/to/lock\");\n        final AtomicInteger numIsLeader = new AtomicInteger(0);\n        final AtomicInteger numNotLeader = new AtomicInteger(0);\n        LeaderLatchListener lll = new LeaderLatchListener() {\n            @Override\n            public void isLeader() \n{\n                log.debug(\"isLeader called\");\n                numIsLeader.incrementAndGet();\n            }\n\n            @Override\n            public void notLeader() \n{\n                log.debug(\"notLeader called\");\n                numNotLeader.incrementAndGet();\n            }\n        };\n        leaderLatch.addListener(lll, MoreExecutors.sameThreadExecutor());\n        leaderLatch.start();\n        leaderLatch.await();\n        assertTrue(leaderLatch.hasLeadership());\n        assertEquals(1, numIsLeader.get());\n        assertEquals(0, numNotLeader.get());\n        // Shut down the server, wait for us to lose the lock, then restart\n        File zkTmpDir = server.getTempDirectory();\n        int zkServerPort = server.getPort();\n        server.stop();\n        while (leaderLatch.hasLeadership()) \n{\n            log.debug(\"Waiting for curator to notice it's not the leader\");\n            Thread.sleep(100);\n        }\n        log.debug(\"Curator has noticed that it is no longer the leader\");\n        assertEquals(1, numNotLeader.get());\n        assertEquals(1, numIsLeader.get());\n        leaderLatch.close();\n        // Restart ZooKeeper\n        server = new TestingServer(zkServerPort, zkTmpDir);\n        leaderLatch = new LeaderLatch(zkClient, \"/path/to/lock\");\n        leaderLatch.addListener(lll, MoreExecutors.sameThreadExecutor());\n        log.debug(\"Calling leaderLatch.start()\");\n        leaderLatch.start();\n        log.debug(\"Trying to regain leadership\");\n        leaderLatch.await();\n        log.debug(\"We have regained leadership\");\n        // Wait so we have time to observe the \"jitter\"\n        Thread.sleep(100);\n        assertTrue(leaderLatch.hasLeadership());\n        // Bug here. numIsLeader == 3\n        assertEquals(2, numIsLeader.get());\n        // Bug here too, numNotLeader == 2\n        assertEquals(1, numNotLeader.get());\n        log.debug(\"calling leaderLatch.close\");\n        leaderLatch.close();\n}\nThe output from this is:\nRunning com.threeci.commons.zkrecipes.TransactionalLockTest\n0    [main-EventThread] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - isLeader called\n104  [ConnectionStateManager-0] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - notLeader called\n132  [main] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - Curator has noticed that it is no longer the leader\n171  [main] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - Calling leaderLatch.start()\n172  [main] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - Trying to regain leadership\n1882 [main-EventThread] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - isLeader called\n1883 [main] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - We have regained leadership\n1883 [main-EventThread] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - notLeader called\n1885 [main-EventThread] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - isLeader called\n2084 [ConnectionStateManager-0] DEBUG com.threeci.commons.zkrecipes.TransactionalLockTest  - notLeader called\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.632 sec <<< FAILURE!\njava.lang.AssertionError: expected:<2> but was:<3>",
        "Issue Links": []
    },
    "CURATOR-88": {
        "Key": "CURATOR-88",
        "Summary": "RPC Interface for Curator",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.6.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "15/Feb/14 16:11",
        "Updated": "07/Jun/14 23:47",
        "Resolved": "07/Jun/14 18:19",
        "Description": "An RPC proxy for Curator would be beneficial for languages with incomplete or poor ZooKeeper client libraries and, possibly, other unknown situations.",
        "Issue Links": []
    },
    "CURATOR-89": {
        "Key": "CURATOR-89",
        "Summary": "ChildReaper only checks for children once",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.4.1",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Kesler",
        "Created": "20/Feb/14 19:33",
        "Updated": "25/Feb/14 23:32",
        "Resolved": "25/Feb/14 22:39",
        "Description": "I've created a child reaper for a particular path underneath which have a bunch of lock paths that I want cleaned up periodically.  The problem is that I'm seeing ChildReaper.doWork only get called once.  Thus newly added children nodes never get picked up by the child reaper and passed to the actual reaper.\nThe problem appears to be the fact that an InternalFutureTask is being submitted to the ScheduledExecutorService through the ClosableScheduledExecutorService that ChildReaper uses.  Putting a breakpoint on java's FutureTask.run and FutureTask.runAndReset, it looks like what happens is that when the InternalFutureTask gets submitted to the ScheduledExecutorService, another task gets created.  THAT task wraps the InternalFutureTask that the ClosableScheduledExecutorService submitted to the real executor and correctly gets executed repeatedly via its runAndReset method.  However when the outermost task executes it calls .run on the wrapped InternalFutureTask.  The first time the InternalFutureTask itself is executed, the state of it's Sync field gets set to RAN (2).  Then every future invocation of run on the InternaFutureTask is ignored because the task has already run.\nThe Reaper itself doesn't seem to have a problem because it's manually rescheduling the task after every invocation rather than using scheduleWithFixedInterval.\nI don't know if it makes a difference, but I'm using the default scheduled executor for ChildReaper.  I'm using java 6 and tried both curator 2.1.0 and 2.4.0",
        "Issue Links": []
    },
    "CURATOR-90": {
        "Key": "CURATOR-90",
        "Summary": "Reduce the verbosity of connection error log messages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.5.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Julio Lopez",
        "Created": "25/Feb/14 01:19",
        "Updated": "14/May/14 14:53",
        "Resolved": "14/May/14 14:43",
        "Description": "On connection error, such as a network disconnection from the ZK cluster,  CuratorFrameworkImpl does excessive logging.  While a system property, namely PROPERTY_DONT_LOG_CONNECTION_ISSUES = \"curator-dont-log-connection-problems\", can be set to avoid filling out the logs with connection error messages, this is an all or nothing setting.\nIt is desirable to allow for less verbose logging that can be more easily controlled and filtered using the logging framework of choice.",
        "Issue Links": []
    },
    "CURATOR-91": {
        "Key": "CURATOR-91",
        "Summary": "DefaultTraceDriver doesn't log the name of the trace",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.4.1",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Matt Brown",
        "Created": "26/Feb/14 22:07",
        "Updated": "27/Feb/14 00:49",
        "Resolved": "27/Feb/14 00:49",
        "Description": "DefaultTraceDriver.addTrace() only prints out the elapsed time of the trace, but not the name passed to the method.",
        "Issue Links": []
    },
    "CURATOR-92": {
        "Key": "CURATOR-92",
        "Summary": "Add CloseMode support to LeaderLatch (Pull Request)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.4.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Trott",
        "Created": "06/Mar/14 01:53",
        "Updated": "06/Mar/14 10:59",
        "Resolved": "06/Mar/14 10:59",
        "Description": "https://github.com/apache/curator/pull/1\nPatch submission to add a CloseMode to leader latch in order to notify the callback if the Latch is explictily closed (forcing the leader to become a \"dunce\").",
        "Issue Links": [
            "/jira/browse/CURATOR-83"
        ]
    },
    "CURATOR-93": {
        "Key": "CURATOR-93",
        "Summary": "Have TransactionCreateBuilder implement Compressible",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.6.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "07/Mar/14 19:55",
        "Updated": "20/Jun/14 14:09",
        "Resolved": "20/Jun/14 14:09",
        "Description": "It would be nice to support compression for operations done in a transaction.",
        "Issue Links": []
    },
    "CURATOR-94": {
        "Key": "CURATOR-94",
        "Summary": "PERMANENT registration should not be added to the services map.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "David Trott",
        "Created": "08/Mar/14 11:11",
        "Updated": "25/Aug/15 08:20",
        "Resolved": null,
        "Description": "In ServiceDiscoveryImpl the registerService(...) method should be:\n    {\n        if (service.getServiceType() == ServiceType.STATIC)\n        {\n            services.put(service.getId(), service);\n        }\n        internalRegisterService(service);\n    }\nThis prevents two side effects:\n+ PERMANENT registration are not deleted when the ServiceDiscoveryImpl class is closed.\n+ PERMANENT registration are not re-registered (potentially with old data) after a connection loss event.\nThe first case is a problem, since shutting down the registration app cleanly deletes all PERMANENT registrations.\nAdditionally since PERMANENT registrations do not use ephemeral nodes the re-registration functionality is not needed.",
        "Issue Links": []
    },
    "CURATOR-95": {
        "Key": "CURATOR-95",
        "Summary": "TestQueueSharder test fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4.2",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "08/Mar/14 18:31",
        "Updated": "30/Mar/14 18:30",
        "Resolved": "30/Mar/14 18:30",
        "Description": "When running mvn clean package on a clean clone of master (commit 40fbe06) there is a failure in TestQueueSharder:\n\njunit.framework.AssertionFailedError: null\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.assertTrue(Assert.java:20)\n\tat junit.framework.Assert.assertTrue(Assert.java:27)\n\tat org.apache.curator.framework.recipes.queue.TestQueueSharder.testSharderWatchSync(TestQueueSharder.java:122)",
        "Issue Links": []
    },
    "CURATOR-96": {
        "Key": "CURATOR-96",
        "Summary": "Upgrade to ZooKeeper 3.4.6 and other Dependency Updates",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "2.5.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "13/Mar/14 19:06",
        "Updated": "23/May/14 15:12",
        "Resolved": "14/May/14 15:12",
        "Description": "ZooKeeper 3.4.6 has been released. Upgrade and test, etc. Also, update various other dependency versions.",
        "Issue Links": []
    },
    "CURATOR-97": {
        "Key": "CURATOR-97",
        "Summary": "Adding back ConnectionStateListener to PersistentEphemeralNode recipe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4.2",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jae Hyeon Bae",
        "Created": "18/Mar/14 17:09",
        "Updated": "22/Mar/14 13:36",
        "Resolved": "22/Mar/14 13:36",
        "Description": "I observed PersistentEphemeralNode didn't reinstate its node on serious zookeeper outage. It's not easy to reproduce but my theory is, PersistentEphemeralNode's watcher can be lost on zk outage and then PEN cannot resume its work. So, it would be safe to add ConnectionStateListener back to PEN recipe and if it's overhead, option for using ConnectionStateListener would be great.",
        "Issue Links": []
    },
    "CURATOR-98": {
        "Key": "CURATOR-98",
        "Summary": "Avoid erroneous 'Could not cancel' warning produced by cache tasks",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "2.4.2",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Joe Littlejohn",
        "Created": "19/Mar/14 15:49",
        "Updated": "19/Mar/14 18:36",
        "Resolved": "19/Mar/14 18:36",
        "Description": "Pull request:\nhttps://github.com/apache/curator/pull/3\nWhen closing providers, we regularly see an erroneous warning related to a failure to cancel cache tasks.\nThis change applies some more defensive checks before attempting to cancel tasks, ensuring that the cancel operation is only attempted if the future has not completed.",
        "Issue Links": []
    },
    "CURATOR-99": {
        "Key": "CURATOR-99",
        "Summary": "Java 8 refactoring",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.3.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "28/Mar/14 15:51",
        "Updated": "09/Feb/17 18:36",
        "Resolved": "09/Feb/17 18:36",
        "Description": "Refactor Curator where possible to take advantage of java 8 features",
        "Issue Links": [
            "/jira/browse/CURATOR-148",
            "/jira/browse/CURATOR-135",
            "https://github.com/apache/curator/pull/183",
            "https://github.com/apache/curator/pull/189"
        ]
    },
    "CURATOR-100": {
        "Key": "CURATOR-100",
        "Summary": "The close() method of CuratorFramework does not take effect when using EnsurePath",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "2.4.2",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "zhangshuxing",
        "Created": "02/Apr/14 13:48",
        "Updated": "03/Apr/14 21:17",
        "Resolved": "03/Apr/14 21:17",
        "Description": "In my application,I use just one instance of CuratorFramework and expose it as a bean,so other bean can refer to  it .I find when the close method of CuratorFramework is invoked,and then EnsurePath still try to get a new instance of ZooKeeper.I wonder whether it is a problem.",
        "Issue Links": []
    },
    "CURATOR-101": {
        "Key": "CURATOR-101",
        "Summary": "ServiceDiscoveryImpl.close() fails if the service was never started",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Steve Loughran",
        "Created": "10/Apr/14 13:17",
        "Updated": "19/Apr/15 17:36",
        "Resolved": "19/Apr/15 17:36",
        "Description": "the close() operator of ServiceDiscoveryImpl raises an IllegalStateException if you attempt to call it -this is a runtime exception and somewhat drastic on an operation that should be robust against odd states.",
        "Issue Links": []
    },
    "CURATOR-102": {
        "Key": "CURATOR-102",
        "Summary": "org.apache.curator.framework.recipes.cache.ChildData should have public constructor",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "2.4.2",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ted Pearson",
        "Created": "10/Apr/14 19:01",
        "Updated": "13/Apr/14 14:24",
        "Resolved": "13/Apr/14 14:24",
        "Description": "See https://github.com/apache/curator/pull/4\nWhen unit testing an application making use of PathChildrenCache, I wanted to make some PathChildrenCacheEvents to pass to my PathChildrenCacheListener-implementing class. However I found that PathChildrenCacheEvent contains ChildData, which has only a no-modifier constructor, meaning my unit test would have to be in the org.apache.curator.framework.recipies.cache package in order to perform these tests.",
        "Issue Links": []
    },
    "CURATOR-103": {
        "Key": "CURATOR-103",
        "Summary": "NPE in PathChildrenCache",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.4.2",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jae Hyeon Bae",
        "Created": "12/Apr/14 18:51",
        "Updated": "12/Apr/14 20:55",
        "Resolved": "12/Apr/14 20:55",
        "Description": "On ConnectionLoss, PathChildrenCache cannot recover the connection with the following NPE\nERROR 2014-04-09 17:48:15,231 [DaemonThreadFactory-2-thread-2] org.apache.curator.framework.imps.CuratorFrameworkImpl: Background retry gave up\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:766)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:749)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:56)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl$3.call(CuratorFrameworkImpl.java:244)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nINFO  2014-04-09 17:48:15,276 [ServerInventoryView-0-EventThread] org.apache.curator.framework.state.ConnectionStateManager: State change: RECONNECTED\nINFO  2014-04-09 17:48:15,382 [ServerInventoryView-0-EventThread] org.apache.curator.framework.state.ConnectionStateManager: State change: SUSPENDED\nERROR 2014-04-09 17:48:15,748 [DaemonThreadFactory-2-thread-2] org.apache.curator.framework.imps.CuratorFrameworkImpl: Background exception was not retry-able or retry gave up\njava.lang.NullPointerException\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)\n        at com.google.common.collect.Lists$TransformingSequentialList.<init>(Lists.java:527)\n        at com.google.common.collect.Lists.transform(Lists.java:510)\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.processChildren(PathChildrenCache.java:635)\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.access$200(PathChildrenCache.java:68)\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache$4.processResult(PathChildrenCache.java:476)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:686)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:659)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:783)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:749)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:56)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl$3.call(CuratorFrameworkImpl.java:244)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)",
        "Issue Links": []
    },
    "CURATOR-104": {
        "Key": "CURATOR-104",
        "Summary": "LeaderSelector issue after losing ZooKeeper leader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "2.4.2",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Greg Moulliet",
        "Created": "18/Apr/14 20:14",
        "Updated": "23/May/23 02:57",
        "Resolved": "21/Apr/14 19:53",
        "Description": "LeaderSelectors are not re-attempting leadership after a ZooKeeper leader is stopped and a client with leadership is stopped.\nI have a client process running on 2 servers.  Each process is using LeaderSelectors for the same set of leaderPaths.  \nThe scenario:\n1 - Both clients running, with one client being the leader of each path (2 children are under each leaderPath)\n2 - Stop the ZooKeeper leader\n3 - All clients temporarily lose leadership (0 children are under each leaderPath)\n4 - Leadership is regained by the same clients that had leadership in step 1 (1 child is under each leaderPath)\n5 - Stop a client with leadership\n6 - No other clients pick up leadership of the leaderPaths from step 5 (0 children are under each leaderPath)\nSometimes, a client will pick up one of the leaderPaths, but not more than one.\nI\u2019m using Curator 2.4.1 and ZooKeeper 3.4.5.  \nI originally saw the issue with Curator 2.3.0, and was hoping it was the same as https://issues.apache.org/jira/browse/CURATOR-73.",
        "Issue Links": [
            "/jira/browse/CURATOR-15"
        ]
    },
    "CURATOR-105": {
        "Key": "CURATOR-105",
        "Summary": "Memory leak when using PathChildrenCache",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1,                                            2.4.2",
        "Fix Version/s": "2.5.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Joe Littlejohn",
        "Created": "29/Apr/14 16:38",
        "Updated": "23/May/14 15:08",
        "Resolved": "23/May/14 15:08",
        "Description": "I've observed a memory leak in our production system using Curator service discovery.\nIf you run the attached test case and watch the process with jvisualvm you'll see that the heap grows and grows as the test is running. Taking a heap dump will reveal thousands of ServiceInstance and ServiceCacheImpl instances that are retained even though the provider is closed after each usage. The references appear to be traced back to the PathChildrenCache. This appears to be a leak that shouldn't occur if the provider is correctly closed each time.\nThere is also a heap dump available here:\nhttps://dl.dropboxusercontent.com/u/10909453/heapdump-curator-leak.hprof.tar.gz",
        "Issue Links": []
    },
    "CURATOR-106": {
        "Key": "CURATOR-106",
        "Summary": "Issuing a guaranteed delete can cause stack overflow if ZK is not reachable",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4.2",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Jasdeep Hundal",
        "Created": "08/May/14 22:36",
        "Updated": "30/May/17 19:45",
        "Resolved": null,
        "Description": "For guaranteed deletes (eg. lock releases) that fail, the FailedDeleteManager issues another guaranteed delete here:\nhttps://github.com/apache/curator/blob/master/curator-framework/src/main/java/org/apache/curator/framework/imps/FailedDeleteManager.java#L35\nIn an environment where ZK has the potential to be down for an extended period of time, this has the potential to recurse until there is a stack overflow (particularly if the application is using multiple locks.)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/215"
        ]
    },
    "CURATOR-107": {
        "Key": "CURATOR-107",
        "Summary": "Regarding large number of watch count",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.1,                                            2.4.2",
        "Fix Version/s": "2.5.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Hyun Sik Kang",
        "Created": "15/May/14 04:40",
        "Updated": "30/May/14 08:18",
        "Resolved": "23/May/14 12:55",
        "Description": "It's about issue that zookeeper's watch count is being increased.\nI wrote issue on zookeeper mailing list but I couldn't get cool answer yet.\nPlease share any opinion with us if you have an idea..\nhttp://zookeeper-user.578899.n2.nabble.com/Regarding-large-number-of-watch-count-td7579811.html\nRefer to this article.\nI've used following code for getting and releasing lock\ntry {\n lock.acquire(...);\n} finally {\n lock.release();\n}\nPlease check if I misunderstood how to use curator properly.\nThank you.",
        "Issue Links": []
    },
    "CURATOR-108": {
        "Key": "CURATOR-108",
        "Summary": "Wrong usage of Arrays.equals in DistributedAtomicValue for values equality test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.2",
        "Fix Version/s": "2.5.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Alex Lopashev",
        "Created": "22/May/14 14:32",
        "Updated": "14/Oct/14 17:22",
        "Resolved": "23/May/14 15:52",
        "Description": "Without setting value ZooKeeper DistributedAtomicValue instance returns byte[0]{} as its value, but in #compareAndSet method expectedValue is always non-zero-sized byte array, so in fresh new DistributedAtomicValue with no data in ZooKeeper node test Arrays.equals(byte[4]\n{0,0,0,0}\n, byte[0]{}) for integer and Arrays.equals(byte[8]\n{0,0,0,0,0,0,0,0}\n, byte[0]{}) will always fail. Solution is to not only check with Arrays.equals, but also if one of byte arrays is empty then check if second one has only zero elements.",
        "Issue Links": []
    },
    "CURATOR-109": {
        "Key": "CURATOR-109",
        "Summary": "Threads that are sleeping should get interrupted when closing the client.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Client",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Ioannis Canellos",
        "Created": "27/May/14 15:12",
        "Updated": "13/Jan/16 20:19",
        "Resolved": "13/Jan/16 20:19",
        "Description": null,
        "Issue Links": [
            "/jira/browse/CURATOR-208"
        ]
    },
    "CURATOR-110": {
        "Key": "CURATOR-110",
        "Summary": "LeaderLatch does not complete if it is started without a connection to ZooKeeper",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.6.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Cam McKenzie",
        "Created": "01/Jun/14 22:12",
        "Updated": "17/Jun/14 23:03",
        "Resolved": "17/Jun/14 23:03",
        "Description": "Given the following conditions:\n1.) No connection is available to ZK\n2.) A LeaderLatch is created and started\n3.) All retries for the leader latch creating its ephemeral zNode have been exhausted.\nAt this point the LeaderLatch will not begin functioning correctly when a connection is established. This is due to it ignoring 'CONNECTED' connection state events (it only handles RECONNECTED events).\nThe fix should simply be a case of making the state handling for CONNECTED and RECONNECTED the same.",
        "Issue Links": []
    },
    "CURATOR-111": {
        "Key": "CURATOR-111",
        "Summary": "CuratorFramework.Builder should allow adding multiple auths",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Karthik Kambatla",
        "Created": "04/Jun/14 03:58",
        "Updated": "13/Jan/15 12:49",
        "Resolved": "12/Jan/15 21:04",
        "Description": "Currently, one can add a single authentication scheme/bytes when building CuratorFramework. It would be handy to add multiple.",
        "Issue Links": [
            "/jira/browse/YARN-2716"
        ]
    },
    "CURATOR-112": {
        "Key": "CURATOR-112",
        "Summary": "ExhibitorEnsembleProvider / ExhibitorRestClient dont support basic auth",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Scott Clasen",
        "Created": "04/Jun/14 04:24",
        "Updated": "10/May/23 15:04",
        "Resolved": "10/May/23 15:04",
        "Description": "Doesnt seem possible to use the exhibitor ensemble provider when you are running exhibitor with basic auth enabled, without a funky impl of ExhibitorRestClient that takes the user/pass in its constructor and the rest of the connection info in getRaw(...)",
        "Issue Links": []
    },
    "CURATOR-113": {
        "Key": "CURATOR-113",
        "Summary": "Please create a DOAP file for your TLP",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sebb",
        "Created": "04/Jun/14 21:11",
        "Updated": "04/Jun/14 21:29",
        "Resolved": "04/Jun/14 21:29",
        "Description": "Please can you set up a DOAP for your project and get it added to files.xml?\nPlease see http://projects.apache.org/create.html\nOnce you have created the DOAP and committed it to your source code repository, please submit it for inclusion in the Apache projects listing as per:\nhttp://projects.apache.org/create.html#submit\nRemember, if you ever move or rename the doap file in future, please\nensure that files.xml is updated to point to the new location.\nThanks!",
        "Issue Links": []
    },
    "CURATOR-114": {
        "Key": "CURATOR-114",
        "Summary": "TestingServer should expose restart() method.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.6.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Cam McKenzie",
        "Created": "17/Jun/14 00:45",
        "Updated": "18/Jun/14 16:59",
        "Resolved": "18/Jun/14 16:59",
        "Description": "Currently restarting the TestingServer involves:\nserver.stop()\nserver = new TestingServer(server.getPort(), server.getTempDirectory());\nThe underlying Zookeeper testing server that is contained within TestingServer supports a restart() method. It would be cleaner to expose this on the TestingServer interface, so a restart would simply involve:\nserver.restart()",
        "Issue Links": []
    },
    "CURATOR-115": {
        "Key": "CURATOR-115",
        "Summary": "delayeddistributedqueue failed to sort children",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.2",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "kimi",
        "Created": "17/Jun/14 11:57",
        "Updated": "10/May/23 15:01",
        "Resolved": "10/May/23 15:01",
        "Description": "i have a delayeddistributed queues to store some jobs, on startup, sometimes this issue come up:\nrecipes.queue.DistributedDelayQueue$1-QueueBuilder-5 [ERROR] Exception caught in background handler\njava.lang.IllegalArgumentException: Comparison method violates its general contract!\n        at java.util.TimSort.mergeLo(TimSort.java:747)\n        at java.util.TimSort.mergeAt(TimSort.java:483)\n        at java.util.TimSort.mergeCollapse(TimSort.java:408)\n        at java.util.TimSort.sort(TimSort.java:214)\n        at java.util.TimSort.sort(TimSort.java:173)\n        at java.util.Arrays.sort(Arrays.java:659)\n        at java.util.Collections.sort(Collections.java:217)\n        at org.apache.curator.framework.recipes.queue.DistributedDelayQueue$1.sortChildren(DistributedDelayQueue.java:89)\n        at org.apache.curator.framework.recipes.queue.DistributedQueue.runLoop(DistributedQueue.java:551)\n        at org.apache.curator.framework.recipes.queue.DistributedQueue.access$000(DistributedQueue.java:65)\n        at org.apache.curator.framework.recipes.queue.DistributedQueue$1.call(DistributedQueue.java:196)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)",
        "Issue Links": [
            "/jira/browse/CURATOR-116",
            "/jira/browse/CURATOR-116",
            "/jira/browse/CURATOR-116"
        ]
    },
    "CURATOR-116": {
        "Key": "CURATOR-116",
        "Summary": "Ordering of delayed distributed queue is inconsistent",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.6.0",
        "Component/s": "Recipes",
        "Assignee": "Cam McKenzie",
        "Reporter": "Cam McKenzie",
        "Created": "24/Jun/14 05:36",
        "Updated": "10/May/23 15:01",
        "Resolved": "08/Jul/14 01:35",
        "Description": "The ordering in which elements in the delayed distributed queue are processed is inconsistent. Only elements that have reached their expiry time are processed, but their actual order is not deterministic due to the logic used for sorting. The current time is used during the sorting process, so if an element becomes ready for processing half way through the sort, the order is not deterministic.\nThe current time should be determined at the start of the sort process, and this point in time used for all comparisons.\nI believe this is the root cause for CURATOR-115 also, but I cannot reproduce CURATOR-115, so I have raised this as a separate defect.",
        "Issue Links": [
            "/jira/browse/CURATOR-115",
            "/jira/browse/CURATOR-115",
            "/jira/browse/CURATOR-115"
        ]
    },
    "CURATOR-117": {
        "Key": "CURATOR-117",
        "Summary": "TestingServer to detect failures of ZooKeeperMainFace to start",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Steve Loughran",
        "Created": "08/Jul/14 15:04",
        "Updated": "20/Apr/15 10:09",
        "Resolved": "20/Apr/15 10:09",
        "Description": "TestingServer blocks until the ZK server is started. But if it doesn't start for some reason, the block lasts until the test run times out -which then fails the test with little in terms of meaningful messages.",
        "Issue Links": []
    },
    "CURATOR-118": {
        "Key": "CURATOR-118",
        "Summary": "Streamline release process by adding configuration to the pom",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Apache",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "09/Jul/14 15:50",
        "Updated": "21/Jul/14 01:47",
        "Resolved": "21/Jul/14 01:47",
        "Description": "We can streamline the release process by adding some useful configuration parameters to the maven release plugin, like auto-versioning submodules, and automatically templating the versions.",
        "Issue Links": []
    },
    "CURATOR-119": {
        "Key": "CURATOR-119",
        "Summary": "Remove explicit year from Appendix in LICENSE files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Apache",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "09/Jul/14 16:03",
        "Updated": "30/Jul/14 02:06",
        "Resolved": "30/Jul/14 01:12",
        "Description": "The LICENSE files in the project should match http://www.apache.org/licenses/LICENSE-2.0.txt exactly.\n\nmdrob@mdrob-W530:~/workspace/curator$ diff LICENSE LICENSE-2.0.txt \n190c190\n<    Copyright 2013 The Apache Software Foundation\n---\n>    Copyright [yyyy] [name of copyright owner]\n\n\nAnd there's a bunch of them.\n\nmdrob@mdrob-W530:~/workspace/curator$ grep -r 2013 .\n./curator-x-discovery/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-x-discovery/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-test/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-test/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./curator-examples/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-examples/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./curator-client/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-client/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./curator-framework/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-framework/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./curator-recipes/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-recipes/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n./curator-x-discovery-server/LICENSE:   Copyright 2013 The Apache Software Foundation\n./curator-x-discovery-server/NOTICE:Copyright 2013-2014 The Apache Software Foundation\n\n\nAlso, I think we might only need a LICENSE file in the root directory, but I'm not sure.",
        "Issue Links": []
    },
    "CURATOR-120": {
        "Key": "CURATOR-120",
        "Summary": "NodeCache and PathChildrenCache should not perform writes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Scott Blum",
        "Created": "09/Jul/14 21:21",
        "Updated": "20/Aug/14 22:16",
        "Resolved": "20/Aug/14 15:48",
        "Description": "NodeCache and PathChildrenCache should act as read-only views of data.  Unnecessarily, both of them forcibly create the node being watched.",
        "Issue Links": [
            "/jira/browse/CURATOR-143"
        ]
    },
    "CURATOR-121": {
        "Key": "CURATOR-121",
        "Summary": "PathChildrenCache is throwing an InterruptedException when being closed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0,                                            2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Cam McKenzie",
        "Reporter": "Corey J. Nolet",
        "Created": "16/Jul/14 03:18",
        "Updated": "21/Jul/14 00:43",
        "Resolved": "21/Jul/14 00:43",
        "Description": "I have a server which is using the PathChildrenCache with the LeaderSelector so that I can keep track of changes in leadership for a particular znode. Upon closing the PathChildrenCache, I intermittently receive an InterruptedException like the following:\n\njava.lang.InterruptedException\n\nat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1038)\n\nat java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)\n\nat java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282)\n\nat org.apache.curator.CuratorZookeeperClient.internalBlockUntilConnectedOrTimedOut(CuratorZookeeperClient.java:324)\n\nat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:105)\n\nat org.apache.curator.utils.EnsurePath$InitialHelper.ensure(EnsurePath.java:140)\n\nat org.apache.curator.utils.EnsurePath.ensure(EnsurePath.java:99)\n\nat org.apache.curator.framework.recipes.cache.PathChildrenCache.refresh(PathChildrenCache.java:481)\n\nat org.apache.curator.framework.recipes.cache.RefreshOperation.invoke(RefreshOperation.java:35)\n\nat org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)\n\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\nat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\nat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\nat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\nat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\nat java.lang.Thread.run(Thread.java:724)\n\n\n\nIt doesn't happen all the time but it does happen about 2-3 times in a test suite that's creating and stopping the server about 60 times total.\nrandgalt mentioned on the mailing list that this may be a bug in the framework.",
        "Issue Links": []
    },
    "CURATOR-122": {
        "Key": "CURATOR-122",
        "Summary": "ConnectionStateListener advertises READ_ONLY instead of CONNECTED the first time around",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "18/Jul/14 23:10",
        "Updated": "23/Jul/14 00:11",
        "Resolved": "22/Jul/14 22:48",
        "Description": "When there is only 1 out of 3 zookeeper started, the initial state of the listener is CONNECTED but should be READ_ONLY.",
        "Issue Links": []
    },
    "CURATOR-123": {
        "Key": "CURATOR-123",
        "Summary": "ConnectionStateListener advertises READ_ONLY instead of CONNECTED the first time around",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "18/Jul/14 23:26",
        "Updated": "23/Jul/14 17:04",
        "Resolved": "23/Jul/14 00:39",
        "Description": "see Test.java attached.\nWhen there is only 1 out of 3 zookeeper started, the initial state of the listener is CONNECTED but should be READ_ONLY.\nNOTE: this issue got confused with CURATOR-122 and is being reworded to match CURATOR-122.",
        "Issue Links": []
    },
    "CURATOR-124": {
        "Key": "CURATOR-124",
        "Summary": "PathChildrenCache StartMode documentation improvement",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Documentation,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Patrick Peralta",
        "Created": "22/Jul/14 17:17",
        "Updated": "23/Jul/14 22:34",
        "Resolved": "23/Jul/14 22:34",
        "Description": "When using PathChildrenCache, the documented behavior for StartMode.NORMAL is as follows:\n\ncache will not be primed. i.e. it will start empty and you will receive events for all nodes added, etc.\n\nHowever my observation is that the cache is primed. This was confirmed by Jordan in an email thread on the mailing list on June 23 2014:\n\nYes, POST_INITIALIZED_EVENT is the same as NORMAL except you get the INITIALIZED event. You can see this in TestPathChildrenCache.testChildrenInitialized(). Please send a PR with doc updates if you think they can be improved.\n\nWhen I looked into TestPathChildrenCache.testChildrenInitialized(), it appears to be testing the POST_INITIALIZED_EVENT behavior. I will add a new test testChildrenInitializedNormal to assert the cache initialization behavior describe above for mode NORMAL. Furthermore I will modify the doc to indicate this behavior.\nThis issue will be updated with a link to the PR when it is ready.",
        "Issue Links": []
    },
    "CURATOR-125": {
        "Key": "CURATOR-125",
        "Summary": "ConnectionStateListener is confused by READ_ONLY state",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "23/Jul/14 17:23",
        "Updated": "28/Jul/14 19:50",
        "Resolved": null,
        "Description": "To reproduce:\n\nhave a 3 nodes ZK ensemble with readonlymode.enabled\nshut down 2 of the 3 ZK servers ( we start in read only mode)\n\nThen create a piece of code (see Test.java attached):\n\na curator client (keep the timeout reasonably short)\na NodeCache listener on '/'\na separate ZooKeeper client\n\n-> the connection goes into READ_ONLY/ConnectedReadOnly as expected\n\nstart another ZooKeeper\n\n-> the connection goes into SUSPENDED/Disconnected, then CONNECTED/SyncConnected, fine.\n\nstop one of the 2 ZooKeeper alive\n\n-> the connection goes:\nZOOKEEPER STATE: Disconnected\nCURATOR STATE: SUSPENDED\nZOOKEEPER STATE: ConnectedReadOnly\nCURATOR STATE: READ_ONLY\nCURATOR STATE: SUSPENDED\nCURATOR STATE: READ_ONLY\nCURATOR STATE: SUSPENDED\nCURATOR STATE: READ_ONLY\nSo it's flaky. Sometimes it doesn't switch back and forth, sometimes twice only, sometimes a lot more.\nDepending on the timeout on the client, it might take more time to appear.\nI attached a sample code that would reproduce it in 20-30 seconds.\nNote that the problem may ultimately be on the ZooKeeper side but at that point I just don't know.",
        "Issue Links": []
    },
    "CURATOR-126": {
        "Key": "CURATOR-126",
        "Summary": "IllegalStateException in performBackgroundOperation during close",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Scott Blum",
        "Created": "24/Jul/14 10:50",
        "Updated": "29/Jul/14 12:57",
        "Resolved": "29/Jul/14 00:16",
        "Description": "[CuratorFramework-0] ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl  - Background exception was not retry-able or retry gave up\njava.lang.IllegalStateException: Client is not started\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:176)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:113)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:807)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:793)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.access$400(CuratorFrameworkImpl.java:57)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:275)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:744)\n\n\nI see this sometimes during test runs; I believe this happens because CuratorZookeeperClient.started gets set to false during shutdown, but the backgroundOperation loop can still be running since shutting down the backgroundOperation loop is inherently racy.",
        "Issue Links": []
    },
    "CURATOR-127": {
        "Key": "CURATOR-127",
        "Summary": "Fix trivial warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "25/Jul/14 18:20",
        "Updated": "25/Jul/14 23:05",
        "Resolved": "25/Jul/14 23:05",
        "Description": "There some trivial warnings in the code that might lead to relevant warnings going unnoticed. We should either fix or suppress them as best as we can.",
        "Issue Links": []
    },
    "CURATOR-128": {
        "Key": "CURATOR-128",
        "Summary": "Enhance ZKPaths to be more lenient",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Scott Blum",
        "Created": "29/Jul/14 00:41",
        "Updated": "30/Jul/14 20:05",
        "Resolved": "30/Jul/14 17:24",
        "Description": "Add the following test to TestNamespaceFacade:\n\n    @Test\n    public void     testRootAccess() throws Exception\n    {\n        CuratorFramework    client = CuratorFrameworkFactory.newClient(server.getConnectString(), new RetryOneTime(1));\n        try\n        {\n            client.start();\n\n            client.create().forPath(\"/one\");\n            Assert.assertNotNull(client.getZookeeperClient().getZooKeeper().exists(\"/one\", false));\n\n            Assert.assertNotNull(client.checkExists().forPath(\"/\"));\n            try\n            {\n                client.checkExists().forPath(\"\");\n                Assert.fail(\"IllegalArgumentException expected\");\n            }\n            catch ( IllegalArgumentException expected )\n            {\n            }\n\n            Assert.assertNotNull(client.usingNamespace(\"one\").checkExists().forPath(\"\"));\n            try\n            {\n                client.usingNamespace(\"one\").checkExists().forPath(\"/\");\n                Assert.fail(\"IllegalArgumentException expected\");\n            }\n            catch ( IllegalArgumentException expected )\n            {\n            }\n        }\n        finally\n        {\n            CloseableUtils.closeQuietly(client);\n        }\n    }\n\n\nThis tests PASSES, which means that there's no canonical way to refer to the root node.  If the client is not namespaced, \"/\" works and \"\" does not work.  If the client is namespaced, \"\" works and \"/\" does not.\nIn either case, I think ZKPaths.makePath mishandles certain cases.\nIf you append \"/foo\" and \"/\" the result is \"/foo/\" which is an invalid path.\nOn the other hand, if you append \"\" and \"bar\" the result is \"//bar\" which is also invalid.\nWhat's the right behavior here?  Does the root node / root of a namespace always need to be referred to as \"/\" or is empty string an acceptable alias?",
        "Issue Links": []
    },
    "CURATOR-129": {
        "Key": "CURATOR-129",
        "Summary": "Two of PathChildrenCache's constructors need better doc",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Documentation,                                            Recipes",
        "Assignee": "Mike Drob",
        "Reporter": "Jordan Zimmerman",
        "Created": "31/Jul/14 18:34",
        "Updated": "11/Aug/15 17:14",
        "Resolved": "11/Aug/15 17:14",
        "Description": "The two constructors for PathChildrenCache that take executors as arguments need to document that the executors must be single threaded. If a pool executor is passed in, PathChildrenCache will not work correctly.",
        "Issue Links": []
    },
    "CURATOR-130": {
        "Key": "CURATOR-130",
        "Summary": "Some examples may throw NPE on end of input",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "31/Jul/14 20:07",
        "Updated": "31/Jul/14 21:12",
        "Resolved": "31/Jul/14 20:13",
        "Description": "Some of the examples use in.readLine().trim() without checking the results of the readLine() call. This can cause an NPE since it may return null on end of input, e.g. EOF or Ctrl-D.",
        "Issue Links": []
    },
    "CURATOR-131": {
        "Key": "CURATOR-131",
        "Summary": "Should not modify collection while iterating over it",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "31/Jul/14 21:23",
        "Updated": "11/Aug/14 06:33",
        "Resolved": "11/Aug/14 06:33",
        "Description": "A couple of places in the code attempt to remove elements from a map while iterating over it, but without using the iterator. This can lead to undefined behaviour including skipping elements, repeating elements, or ConcurrentModificationException thrown.\nStatic analysis tools can help spot these locations.",
        "Issue Links": []
    },
    "CURATOR-132": {
        "Key": "CURATOR-132",
        "Summary": "setACL doesn't work with namespaced curator",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0,                                            2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": "Cam McKenzie",
        "Reporter": "John Vines",
        "Created": "31/Jul/14 22:47",
        "Updated": "20/Aug/14 21:25",
        "Resolved": "20/Aug/14 21:24",
        "Description": "Experienced this on 2.3.0, doesn't seem to be resolved in 2.6.0\nAttempting to setACL with a namespaced curator fails to recognize the namespace. It seems that setACL doesn't properly pass on the Namespaced facade. The following test code is an exemplar.\n\npublic class BreakCurator {\n  public static void main(String args[]) throws Exception {\n    TestingServer ts = new TestingServer();\n\n    CuratorFramework curator = CuratorFrameworkFactory.builder().connectString(ts.getConnectString()).retryPolicy(new ExponentialBackoffRetry(1000, 5)).build();\n    curator.start();\n    curator.getZookeeperClient().blockUntilConnectedOrTimedOut();\n\n    curator.create().creatingParentsIfNeeded().forPath(\"/parent/child\", \"A string\".getBytes());\n    CuratorFramework curator2 = curator.usingNamespace(\"parent\");\n\n    try {\n      System.out.println(new String(curator2.getData().forPath(\"child\")));\n      curator.setACL().withACL(Collections.singletonList(new ACL(ZooDefs.Perms.WRITE, ZooDefs.Ids.ANYONE_ID_UNSAFE))).forPath(\"/parent/child\");\n// This should attempt to setACL on /parent/child, but instead fails because /child isn't present. Using \"child\" causes a failure because the path doesn't start with a slash\n      curator2.setACL().withACL(Collections.singletonList(new ACL(ZooDefs.Perms.DELETE, ZooDefs.Ids.ANYONE_ID_UNSAFE))).forPath(\"/child\");\n      System.out.println(curator2.getACL().forPath(\"/child\"));\n    } catch (Exception e) {\n      e.printStackTrace();\n    }\n\n    ts.close();\n  }\n}",
        "Issue Links": []
    },
    "CURATOR-133": {
        "Key": "CURATOR-133",
        "Summary": "NodeCache doesn't properly respect namespaced curators",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.3.0,                                            2.6.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "John Vines",
        "Created": "31/Jul/14 23:06",
        "Updated": "01/Aug/14 15:08",
        "Resolved": "01/Aug/14 14:48",
        "Description": "When you construct a NodeCache from a Namespaced CuratorFramework, it will create an EnsurePath to validate the path for it. However, it will attach the listener in internalRebuild() to the raw path, which hasn't been adjusted for the namespace.",
        "Issue Links": []
    },
    "CURATOR-134": {
        "Key": "CURATOR-134",
        "Summary": "Curator sends a connection LOST event before sessionTimeout",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "01/Aug/14 18:27",
        "Updated": "08/Sep/15 18:24",
        "Resolved": "08/Sep/15 18:24",
        "Description": "Created a Curator client with:\n\nconnection timeout: 10 seconds\nsession timeout: 30 seconds\nretry policy: RetryNTimes(3, 10000)\n\nA scenario where the ensemble is lost produces the the curator client to send a LOST event in less than the expected 30 seconds:\nFri Aug 01 11:17:19 PDT 2014 - CURATOR STATE: SUSPENDED\nFri Aug 01 11:17:29 PDT 2014 - CURATOR STATE: LOST\nThe client code is attached, this is the complete output:\nFri Aug 01 11:16:53 PDT 2014 - CURATOR STATE: CONNECTED\nFri Aug 01 11:16:54 PDT 2014 - Creating ZK client...\nFri Aug 01 11:16:54 PDT 2014 - ZK client created...\nFri Aug 01 11:16:54 PDT 2014 - ZOOKEEPER STATE: SyncConnected\nFri Aug 01 11:16:58 PDT 2014 - ZOOKEEPER STATE: Disconnected\nFri Aug 01 11:16:58 PDT 2014 - CURATOR STATE: SUSPENDED\nFri Aug 01 11:17:16 PDT 2014 - CURATOR STATE: RECONNECTED\nFri Aug 01 11:17:17 PDT 2014 - ZOOKEEPER STATE: SyncConnected\nFri Aug 01 11:17:19 PDT 2014 - ZOOKEEPER STATE: Disconnected\nFri Aug 01 11:17:19 PDT 2014 - CURATOR STATE: SUSPENDED\nFri Aug 01 11:17:29 PDT 2014 - CURATOR STATE: LOST\nI think that the LOST event is actually 30 seconds away from the very first SUSPENDED event, whereas is should be 30 seconds away from the last one.\nTo reproduce it, I started only 2 ZK servers in a 3 nodes ensembles, then I stopped one of them (-> 1st SUSPENDED), waited for 10-20 seconds, then started it and stopped it again.",
        "Issue Links": [
            "/jira/browse/CURATOR-247"
        ]
    },
    "CURATOR-135": {
        "Key": "CURATOR-135",
        "Summary": "API Cleanup: Avoid throwing Exception",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "01/Aug/14 18:57",
        "Updated": "07/Jan/17 16:42",
        "Resolved": "07/Jan/17 16:41",
        "Description": "It would be nice if the API did not throw Exception in so many places and threw the more specific subclasses instead. We should scrub the API to improve the method signatures, possibly creating sub-tasks to do so on a module-by-module (or more granular) basis.",
        "Issue Links": [
            "/jira/browse/CURATOR-99",
            "/jira/browse/CURATOR-29"
        ]
    },
    "CURATOR-136": {
        "Key": "CURATOR-136",
        "Summary": "Invalid LeaderLatch path never errors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "John Vines",
        "Created": "07/Aug/14 21:58",
        "Updated": "06/Nov/14 21:06",
        "Resolved": "20/Aug/14 21:14",
        "Description": "I was messing with the LeaderLatch and I noticed some ill behavior for the pathing of it. Given the following code-\n\n  public static void main(String args[]) throws Exception {\n    TestingServer ts = new TestingServer();\n\n    CuratorFramework curator = CuratorFrameworkFactory.builder().connectString(ts.getConnectString()).retryPolicy(new ExponentialBackoffRetry(1000, 5)).build();\n    curator.start();\n    curator.getZookeeperClient().blockUntilConnectedOrTimedOut();\n    curator.create().creatingParentsIfNeeded().forPath(\"/parent\", \"A string\".getBytes());\n    \n    try {\n      LeaderLatch ll = new LeaderLatch(curator, \"parent\", \"myNode\");\n      ll.start();\n      ll.await();\n      System.out.println(ll.hasLeadership());\n      ll.close();\n    } catch (Exception e) {\n      e.printStackTrace();\n    }\n\n    ts.close();\n  }\n\n\nthe system will just hang forever. Inspecting the TestingServer with zkcli shows entries showing up though.\nIf I switch out the path in the LeaderLatch constructor with \"/parent\", the same path is created AND the program runs fine.\nLeaderLatch should either accept the path all around or fail outright, not this half state that makes things difficult for the user.",
        "Issue Links": []
    },
    "CURATOR-137": {
        "Key": "CURATOR-137",
        "Summary": "BaseTestTreeCache does not compile using Java 6",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "08/Aug/14 13:10",
        "Updated": "08/Aug/14 20:27",
        "Resolved": "08/Aug/14 20:27",
        "Description": "Seeing reported failures on Jenkins for compilation error.\n\n [ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] <https://builds.apache.org/job/Curator/org.apache.curator$curator-recipes/ws/src/test/java/org/apache/curator/framework/recipes/cache/BaseTestTreeCache.java>:[121,30] cannot find symbol\nsymbol  : method addSuppressed(java.lang.Throwable)\nlocation: class java.lang.AssertionError\n[INFO] 1 error\n[INFO] -------------------------------------------------------------",
        "Issue Links": [
            "/jira/browse/CURATOR-33"
        ]
    },
    "CURATOR-138": {
        "Key": "CURATOR-138",
        "Summary": "Unify event listening for NodeCache, PathChildrenCache, TreeCache",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Scott Blum",
        "Created": "09/Aug/14 02:02",
        "Updated": "20/Aug/14 22:15",
        "Resolved": "20/Aug/14 20:39",
        "Description": null,
        "Issue Links": [
            "/jira/browse/CURATOR-143"
        ]
    },
    "CURATOR-139": {
        "Key": "CURATOR-139",
        "Summary": "Add slf4j logging implementation to test scope",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "11/Aug/14 20:27",
        "Updated": "20/Aug/14 04:18",
        "Resolved": "20/Aug/14 04:18",
        "Description": "When attempting to debug unit tests, it can be very difficult because the no-op logger is used as the default slf4j implementation. We should add a test scoped log4j (or logback, doesn't really matter) dependency so that test output is saved.\nReasons against are that it potentially slows down the tests because of extra IO or that it uses up a non-trivial amount of disk space. If either of these is the case, I feel that they should be addressed separately. Logging can be specifically configured for the cases where we have known issues.",
        "Issue Links": []
    },
    "CURATOR-140": {
        "Key": "CURATOR-140",
        "Summary": "Support the initial data for the node in InterProcessReadWriteLock",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.1",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "seoeun",
        "Created": "13/Aug/14 05:04",
        "Updated": "04/Nov/14 00:12",
        "Resolved": "04/Nov/14 00:12",
        "Description": "Currently, locks(writeLock, readLock) in InterProcessReadWriteLock use the InetAddress for a initial data for the node, It is a little difficult to debug the lock owner as it shows only IP. If the initial data for lock node can be set, participantNodes contains the given information(hostname, id). So, it is easy  to debug participantNodes.\n\nwriteLock = InterProcessReadWriteLock.writeLock();\nCollection<String> nodes = writeLock.getParticipantNodes();\nfor (String node: nodes) {\n    String nodeData = new String(client.getData().forPath(node));\n}\n\n\nnodeData should be the information which is given in argument.",
        "Issue Links": []
    },
    "CURATOR-141": {
        "Key": "CURATOR-141",
        "Summary": "Avoid confusing error logging when PathChildrenCache and curator client are closed.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Karel Vervaeke",
        "Created": "14/Aug/14 11:43",
        "Updated": "20/Aug/14 06:38",
        "Resolved": "20/Aug/14 06:38",
        "Description": "See this message on the list:\nhttp://mail-archives.apache.org/mod_mbox/curator-user/201408.mbox/%3CCAA5C_ptEzXbBRrmq5HmBsSmjyzbn6vD1E8tXMixufySbyvRNgw%40mail.gmail.com%3E\nPathChildrenCache instances should not make calls to the curator client after they have been closed.",
        "Issue Links": []
    },
    "CURATOR-142": {
        "Key": "CURATOR-142",
        "Summary": "Make surefire forkCount configurable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "20/Aug/14 18:02",
        "Updated": "20/Aug/14 21:37",
        "Resolved": "20/Aug/14 21:36",
        "Description": "When running unit tests, it would be nice if the parallelism was configurable. Running with default settings takes about ~35 minutes on my laptop and I would like to reduce that.\nTried it at 3, and the runtime was ~15 minutes. Tried it at 8 and got test failures. We can leave the default at 1 for now, to make sure Jenkins doesn't see bad results, but eventually it would be good to increase that as well to benefit new contributors.",
        "Issue Links": []
    },
    "CURATOR-143": {
        "Key": "CURATOR-143",
        "Summary": "Introduce a max depth setting for TreeCache",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Mike Drob",
        "Created": "20/Aug/14 22:14",
        "Updated": "04/Nov/14 00:11",
        "Resolved": "04/Nov/14 00:11",
        "Description": "Users should be able to specify a max depth on TreeCache. As part of this new feature, NodeCache and PathChildrenCache should both be deprecated with documentation referring to TreeCache with depth 0 and 1, respectively.",
        "Issue Links": [
            "/jira/browse/CURATOR-138",
            "/jira/browse/CURATOR-33",
            "/jira/browse/CURATOR-120"
        ]
    },
    "CURATOR-144": {
        "Key": "CURATOR-144",
        "Summary": "TreeCache should use a builder for advanced options",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Scott Blum",
        "Created": "22/Aug/14 20:00",
        "Updated": "05/Nov/14 17:22",
        "Resolved": "05/Nov/14 17:22",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-145": {
        "Key": "CURATOR-145",
        "Summary": "TreeCache should implement maxDepth",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Scott Blum",
        "Created": "22/Aug/14 20:33",
        "Updated": "12/Jan/16 18:14",
        "Resolved": "12/May/15 18:04",
        "Description": null,
        "Issue Links": [
            "/jira/browse/CURATOR-288"
        ]
    },
    "CURATOR-146": {
        "Key": "CURATOR-146",
        "Summary": "discovery: registration of wrong/old service instance on RECONNECT",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3.0,                                            2.4.0,                                            2.4.1,                                            2.4.2,                                            2.5.0,                                            2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Kamen Petroff",
        "Created": "11/Sep/14 13:34",
        "Updated": "17/May/23 11:23",
        "Resolved": "17/May/23 11:23",
        "Description": "ServiceDiscoveryImpl.registerService() remembers service registrations in a map. However updateService() does not update the map. \nThis causes a wrong registration in the case of a RECONNECT - e.g. reRegisterServices() called by the ConnectionStateListener registers the old ServiceInstance instead of the updated one.",
        "Issue Links": [
            "/jira/browse/CURATOR-164"
        ]
    },
    "CURATOR-147": {
        "Key": "CURATOR-147",
        "Summary": "Avoid internal use of deprecated APIs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "11/Sep/14 23:01",
        "Updated": "15/Sep/14 07:03",
        "Resolved": "15/Sep/14 07:03",
        "Description": "We should not use our own deprecated APIs unless there is an explicit reason to do so.",
        "Issue Links": []
    },
    "CURATOR-148": {
        "Key": "CURATOR-148",
        "Summary": "Order of modifiers on framework commands matters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "12/Sep/14 02:43",
        "Updated": "24/Jan/17 02:26",
        "Resolved": "24/Jan/17 02:26",
        "Description": "I only tested this with the CreateBuilder, but visual inspection makes me think that it is present on other operations as well.\nIf I want to create a ZK node, using a bunch of the features that curator provides, I have to do it in a specific order. (Arguments removed for clarity)\n\nclient.create()\n    .compressed()\n    .withMode()\n    .withACL()\n    .inBackground()\n    .forPath();\n\n\nIf I unknowingly call inBackground() first, then the only methods available to me are the forPath() variants. Similarly, if I call create().withMode(), then there is longer a way for me to call compressed() on that object.\nEven more concerning is that it is impossible to call compressed() and withProtection() on the same chain, regardless of order.\nSince each of the fluent-style methods already returns this, it might make sense to modify each method as implemented on CreateBuilder to have a declared return type of CreateBuilder, taking advantage of covariant return types.\nAnother option, with similar results, would be to remove many of the intermediate interfaces from CreateBuilder like ACLCreateModeBackgroundPathAndBytesable<String> and declare it to be: \n\ninterface CreateBuilder extends Pathable<String>,\n    Backgroundable<CreateBuilder>,\n    Compressible<CreateBuilder>,\n    ACLable<CreateBuilder>,\n    CreateModable<CreateBuilder>,\n    ....\n\n\nThis option is very verbose, however.\nA disadvantage of both of these options is that it allows users to call methods multiple times. In some cases, like inBackground(), it won't matter. In other cases, like withACLs() we'd have to make a decision on taking an intersection or \"last call wins\" approach. That might even differ per call, so we'd have to have careful documentation on each.\nAnother option is to simply document the behavior (probably on the create() method) and hope that users will see it. Maybe the best solution is to document in a minor release line, and then make breaking changes in a major version?",
        "Issue Links": [
            "/jira/browse/CURATOR-99",
            "https://github.com/apache/curator/pull/63"
        ]
    },
    "CURATOR-149": {
        "Key": "CURATOR-149",
        "Summary": "ChildData's path check prevents the initialization of PathChildrenCache class",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Nandor Kracser",
        "Created": "18/Sep/14 18:29",
        "Updated": "14/Oct/14 03:11",
        "Resolved": "14/Oct/14 03:11",
        "Description": "PathChildrenCache has a static field:\n    private static final ChildData NULL_CHILD_DATA = new ChildData(null, null, null);\nBut this can't be created because of:\nCaused by: java.lang.IllegalArgumentException: Path cannot be null\n    at org.apache.curator.utils.PathUtils.validatePath(PathUtils.java:48)\n    at org.apache.curator.framework.recipes.cache.ChildData.<init>(ChildData.java:34)\n    at org.apache.curator.framework.recipes.cache.PathChildrenCache.<clinit>(PathChildrenCache.java:90)\n    ... 31 more",
        "Issue Links": []
    },
    "CURATOR-150": {
        "Key": "CURATOR-150",
        "Summary": "InterProcessMutex revocation is complicated and can only be done from the original client",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jill Renee Singer",
        "Created": "02/Oct/14 15:03",
        "Updated": "02/Oct/14 15:03",
        "Resolved": null,
        "Description": "i want a lock that I can revoke from anywhere.\nthat is easy to use.",
        "Issue Links": []
    },
    "CURATOR-151": {
        "Key": "CURATOR-151",
        "Summary": "SharedValue has limited utility but can be improved",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Oct/14 17:08",
        "Updated": "31/Oct/14 04:48",
        "Resolved": "02/Oct/14 18:05",
        "Description": "Currently, SharedValue has limited utility as the internally managed version is always used for trySetValue. A good improvement would be a) add an API to get the current value AND current version and b) add an alternate trySetValue that takes a new value AND an expected version.",
        "Issue Links": []
    },
    "CURATOR-152": {
        "Key": "CURATOR-152",
        "Summary": "NamespaceFacade does not provide access to underyling client",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Whitney Sorenson",
        "Created": "06/Oct/14 10:47",
        "Updated": "06/Oct/14 10:47",
        "Resolved": null,
        "Description": "NamespaceFacade throws UnsupportedOperationException on close() and start().\nThis is impractical for the case where NamespaceFacade is being used as the only CuratorFramework available to an application. Clients are left to either:\n\nMake a separate binding for underlying curator framework\nUse reflection to access underlying client\nReimplement Namespace functionality in their own code\n\nNone of these are ideal or necessary. The NamespaceFacade should allow close to be called on the underlying client or should allow access to the underlying client for this type of use case. Since it previously threw an exception, proxying the close call to the underlying client would not break existing users.",
        "Issue Links": []
    },
    "CURATOR-153": {
        "Key": "CURATOR-153",
        "Summary": "PathChildrenCache occasionally cannot reconnect to ZK",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.2,                                            2.5.0,                                            2.6.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Fangjin Yang",
        "Created": "09/Oct/14 18:08",
        "Updated": "07/Jun/15 18:09",
        "Resolved": "07/Jun/15 18:09",
        "Description": "We use Curator as part of the Druid open source project (druid.io). We've had issues where if ZK is brought down and back up, numerous nodes cannot reconnect. The issue is very difficult to reproduce locally but we've seen it often in production. The issue appears to be in PathChildrenCache. There is a longer description here:\nhttps://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/druid-development/54avmEvLN3E/orZ1taF8hFsJ",
        "Issue Links": []
    },
    "CURATOR-154": {
        "Key": "CURATOR-154",
        "Summary": "PersistentEphemeralNode May Fail to Apply Updated Data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": "Cam McKenzie",
        "Reporter": "Matt Briggs",
        "Created": "20/Oct/14 15:55",
        "Updated": "21/Apr/15 23:10",
        "Resolved": "21/Apr/15 23:10",
        "Description": "Steps to Reproduce:\n1. Establish an ephemeral znode using PersistentEphemeralNode via a test app.\n2. Invoke PersistentEphemeralNode.setData with an updated data value that is different than the data originall provided to the PersistentEphemeralNode constructor.\n3. Manually force a disconnect from ZooKeeper.  One way to achieve this is to set a breakpoint in SetDataBuilderImpl.performBackgroundOperation before setData is physically launched, take down the ZooKeeper server, then resume the test app.\n4. Restart the ZooKeeper server before the test app's session has expired.\n5. Observe that PersistentEphemeralNode will attempt to create the znode again on reconnect, however it will encounter a NODEEXISTS error and leave whatever data was there intact.\n6. Ultimately it appears like the updated data captured by the original setData call will not be published to the znode unless the znode is deleted (e.g. by a session expiration).\nThe hope was that the updated data would be applied at reconnect time.\nStepping back, I'm also wondering if PersistentEphemeralNode should be more aggressive in the setting of data.  setData for instance appears to giveup (aside from normal Curator framework retry attempts) if it encounters an error, whereas createNode's callback will continue to issue createNode calls on errors (other than NODEEXISTS).  Also, it appears that if I supply the PersistentEphemeralNode constructor with initial data and the createNode call encounters an existing znode, then my initial data won't be applied.",
        "Issue Links": []
    },
    "CURATOR-155": {
        "Key": "CURATOR-155",
        "Summary": "PersistentEphemeralNode doesn't update data with sequence or protected mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Andrew Kondratovich",
        "Created": "24/Oct/14 11:22",
        "Updated": "24/Oct/14 15:47",
        "Resolved": "24/Oct/14 15:47",
        "Description": "PersistentEphemeralNode#setData(byte[] data) uses basePath to update node's data. When you use Sequence mode, resulting path is not equal basePath.",
        "Issue Links": []
    },
    "CURATOR-156": {
        "Key": "CURATOR-156",
        "Summary": "A typo error in \"PathCacheExample\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Dapeng Sun",
        "Created": "28/Oct/14 07:33",
        "Updated": "28/Oct/14 21:14",
        "Resolved": "28/Oct/14 21:13",
        "Description": "PathCacheExample.java\n         System.out.println(\"An example of using PathChildrenCache. This example is driven by entering commands at the prompt:\\n\");\n         System.out.println(\"set <name> <value>: Adds or updates a node with the given name\");\n         System.out.println(\"remove <name>: Deletes the node with the given name\");\n-        System.out.println(\"delete: List the nodes/values in the cache\");\n+        System.out.println(\"list: List the nodes/values in the cache\");\n         System.out.println(\"quit: Quit the example\");\n         System.out.println();\n     }",
        "Issue Links": []
    },
    "CURATOR-157": {
        "Key": "CURATOR-157",
        "Summary": "Avoid stack traces closing PathChildrenCache followed by closing CuratorFramework",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Bruno Dumon",
        "Created": "30/Oct/14 10:49",
        "Updated": "06/Sep/22 13:26",
        "Resolved": null,
        "Description": "When closing PathChildrenCache, and immediately afterwards closing CuratorFramework, some ERROR-level stack traces are logged.\nThis was previously reported on the mailing list: http://curator.markmail.org/thread/bmfr62ekx5p2vv7f\nThe cause is that the BackgroundCallback defined in PathChildrenCache.refresh() will, when triggered, perform some more ZooKeeper operations.\nThus one can get in sequences such as:\n\noperation with BackgroundCallback is submitted\nprocessResult of the BackgroundCallback is called\nPathChildrenCache is closed\nCuratorFramework is closed\nprocessResult, which is running on another thread, comes to the point it does operations on ZooKeeper, which fail because ZooKeeper is closed.\n\nThere is no real impact on the application, it is just for log-esthetical reasons that I'd like to avoid it.\nIn the more common case, the processResult will receive an IllegalStateException, which could be easily catched and ignored in PathChildrenCache if the PathChildrenCache is closed:\n\n14/10/30 11:24:51 ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl: Background exception was not retry-able or retry gave up\njava.lang.IllegalStateException: instance must be started before calling this method\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:360)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.getDataAndStat(PathChildrenCache.java:545)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.processChildren(PathChildrenCache.java:668)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.access$200(PathChildrenCache.java:68)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache$4.processResult(PathChildrenCache.java:490)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:715)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:502)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\n\nBut sometimes it also fails with other async operations deeper down:\n\n14/10/30 11:24:51 ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl: Background exception was not retry-able or retry gave up\njava.lang.IllegalStateException: Client is not started\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:113)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:474)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.performBackgroundOperation(GetDataBuilderImpl.java:263)\n\tat org.apache.curator.framework.imps.OperationAndData.callPerformBackgroundOperation(OperationAndData.java:65)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:789)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:487)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:275)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.getDataAndStat(PathChildrenCache.java:545)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.processChildren(PathChildrenCache.java:668)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.access$200(PathChildrenCache.java:68)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache$4.processResult(PathChildrenCache.java:490)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:715)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:502)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\n\nTherefore I have created a patch where PathChildrenCache.close() will wait until the possibly running BackgroundCallback is finished.\nI will also attach a small class that illustrates the problem.",
        "Issue Links": []
    },
    "CURATOR-158": {
        "Key": "CURATOR-158",
        "Summary": "curator-x-rpc/pom.xml is missing a license header",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Apache",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "31/Oct/14 14:43",
        "Updated": "15/Jan/15 18:21",
        "Resolved": "15/Jan/15 18:21",
        "Description": "The curator-x-rpc/pom.xml file is missing a license header, should very likely be Apache 2.0",
        "Issue Links": []
    },
    "CURATOR-159": {
        "Key": "CURATOR-159",
        "Summary": "Parent Task for ZooKeeper 3.5.0 support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "01/Nov/14 16:28",
        "Updated": "09/Oct/15 14:22",
        "Resolved": "09/Oct/15 14:22",
        "Description": "This is a parent task to hold sub-tasks related to ZooKeeper 3.5.0 support",
        "Issue Links": []
    },
    "CURATOR-160": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support Dynamic Reconfig",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "None",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Jordan Zimmerman",
        "Created": "01/Nov/14 16:32",
        "Updated": "12/May/15 13:54",
        "Resolved": "12/May/15 13:54",
        "Description": "ZooKeeper 3.5.0 added a new feature \"Dynamic Reconfiguration\". Curator needs to support this.",
        "Issue Links": []
    },
    "CURATOR-161": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support Watcher Removal",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "Jordan Zimmerman",
        "Created": "01/Nov/14 16:32",
        "Updated": "25/Aug/15 20:14",
        "Resolved": "24/Aug/15 16:27",
        "Description": "ZooKeeper 3.5.0 added a new feature \"Watcher Removal\". Curator needs to support this. Further, all Curator recipe uses of watchers should clear their watchers on close, etc. as appropriate.",
        "Issue Links": [
            "/jira/browse/CURATOR-217"
        ]
    },
    "CURATOR-162": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support Local Sessions",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "01/Nov/14 16:33",
        "Updated": "06/May/15 04:54",
        "Resolved": "06/May/15 04:54",
        "Description": "ZooKeeper 3.5.0 added support for Local Session. Determine if Curator needs to add support for this.",
        "Issue Links": []
    },
    "CURATOR-163": {
        "Key": "CURATOR-163",
        "Summary": "Discovery doesn't remove services without instances in Zookeeper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Pawe\u0142 Szymczyk",
        "Created": "04/Nov/14 09:50",
        "Updated": "12/Jan/15 19:27",
        "Resolved": "12/Jan/15 19:27",
        "Description": "After few months using discovery my zookeeper is flooded by many not used service nodes without instances. \nI try to remove these nodes manualy but i have many ServiceCache instances in memory that doesn't work without this path.\nExample flow:\n1. Register service in discovery\n2. Unregister\n3. Remove node without instances from Zookeeper\n4. Try to register again - node with instance is present in Zookeeper but ServiceCache returns empty list of instances.",
        "Issue Links": []
    },
    "CURATOR-164": {
        "Key": "CURATOR-164",
        "Summary": "curator-x-discovery: unregisterService is not guaranteed to remove the service, due to reconnectListener concurrency issue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.8.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Rasmus Berg Palm",
        "Created": "11/Nov/14 09:48",
        "Updated": "17/May/23 11:23",
        "Resolved": "27/Apr/15 22:10",
        "Description": "In ServiceDiscoveryImpl:\nWhen unregistering a service, the reconnect listener might fire while deleting the path.\nThis can cause a condition where the delete finishes successfully, the service is removed from services, and then the reRegisterServices completes successfully and the service is added back in ZK and in services, end result being that the service was not removed, even though unregisterService did not throw any exceptions. \nEssentially the use of the internal 'services' cache makes for a nightmare of concurrency issues. I put this as critical as the library it's really not usable IMO.",
        "Issue Links": [
            "/jira/browse/CURATOR-146"
        ]
    },
    "CURATOR-165": {
        "Key": "CURATOR-165",
        "Summary": "LeaderLatch.checkLeadership(LeaderLatch.java:478) will throw a exception, then no leader will be elected.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "zhaogang",
        "Created": "12/Nov/14 08:16",
        "Updated": "21/Apr/15 01:18",
        "Resolved": "21/Apr/15 01:18",
        "Description": "We use curator for leader election within our 2 clients, sometimes none of the 2 clients was a leader.\nWe checked the log and found this caused by a exception which was being throwed in method LeaderLatch.checkLeadership(), after throwing this exception the curator won't do the leader election. Here is error log:\nERROR org.apache.curator.framework.imps.CuratorFrameworkImpl - Background exception was not retry-able or retry gave up\nINFO   | jvm 1    | 2014/11/12 00:36:52 | java.lang.IllegalArgumentException: Path must start with / character\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.utils.PathUtils.validatePath(PathUtils.java:53)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.utils.ZKPaths.getNodeFromPath(ZKPaths.java:56)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:478)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.recipes.leader.LeaderLatch.access$500(LeaderLatch.java:60)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.recipes.leader.LeaderLatch$6.processResult(LeaderLatch.java:536)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:730)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:516)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590)\nINFO   | jvm 1    | 2014/11/12 00:36:52 | \tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
        "Issue Links": [
            "/jira/browse/CURATOR-168"
        ]
    },
    "CURATOR-166": {
        "Key": "CURATOR-166",
        "Summary": "Make ZKPaths accept more than one child",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.1",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ricardo Ferreira",
        "Created": "14/Nov/14 23:47",
        "Updated": "12/Jan/15 21:04",
        "Resolved": "12/Jan/15 20:50",
        "Description": "ZKPaths currently only accepts one parent and one child nodes. It would be useful to be able to create paths with more depth.",
        "Issue Links": []
    },
    "CURATOR-167": {
        "Key": "CURATOR-167",
        "Summary": "All registered watchers should be cleared on close",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Client",
        "Assignee": "Cam McKenzie",
        "Reporter": "Craig McNally",
        "Created": "18/Nov/14 16:12",
        "Updated": "26/Aug/15 17:58",
        "Resolved": "26/Aug/15 05:34",
        "Description": "There's a memory leak in NodeCache.  I was able to reliably reproduce the problem using a very simple test that performs the following:\n1) Creates a CuratorFramework instance and starts it.\n2) in a loop:  Creates a NodeCache and starts it, then closes it.\nEventually you get a java.lang.OutOfMemoryError:  Java heap space.  This happens regardless of the heap size, though it happens much faster with a small heap.\nUpon furher investigation w/ a profiler, I can see that each NodeCache is being referenced by the NamespaceWatcherMap.\nHere's the test code:\nNodeCacheLeakTest.java\nimport java.util.Date;\n\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.framework.recipes.cache.NodeCache;\nimport org.apache.curator.retry.ExponentialBackoffRetry;\n\npublic class NodeCacheLeakTest {\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tString zkConnect;\n\t\tif (args.length > 0 && args[0] != null)\n\t\t\tzkConnect = args[0];\n\t\telse\n\t\t\tzkConnect = \"localhost:2181/test\";\n\n\t\tCuratorFramework curator = CuratorFrameworkFactory.newClient(zkConnect,\n\t\t\t\tnew ExponentialBackoffRetry(500, 10));\n\t\tcurator.start();\n\n\t\tint count = 0;\n\t\twhile (true) {\n\t\t\tString nodePath = \"/foo/node-\" + (count);\n\n\t\t\tNodeCache cache = new NodeCache(curator, nodePath);\n\t\t\tcache.start(true);\n\t\t\tcache.close();\n\n\t\t\tcount++;\n\n\t\t\tif (count % 1000 == 0)\n\t\t\t\tSystem.out.println(new Date() + \" Started and Closed \" + count\n\t\t\t\t\t\t+ \" NodeCache instances\");\n\t\t}\n\t}\n}\n\n\nAnd here's the output/OOM Error when using (-Xms10m -Xmx10m -XX:+UseG1GC):\n\nlog4j:WARN No appenders could be found for logger (org.apache.curator.framework.imps.CuratorFrameworkImpl).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nTue Nov 18 15:59:12 GMT+00:00 2014 Started and Closed 1000 NodeCache instances\nTue Nov 18 15:59:16 GMT+00:00 2014 Started and Closed 2000 NodeCache instances\nTue Nov 18 15:59:20 GMT+00:00 2014 Started and Closed 3000 NodeCache instances\nTue Nov 18 15:59:23 GMT+00:00 2014 Started and Closed 4000 NodeCache instances\nTue Nov 18 15:59:27 GMT+00:00 2014 Started and Closed 5000 NodeCache instances\nTue Nov 18 15:59:31 GMT+00:00 2014 Started and Closed 6000 NodeCache instances\nTue Nov 18 15:59:36 GMT+00:00 2014 Started and Closed 7000 NodeCache instances\nTue Nov 18 15:59:40 GMT+00:00 2014 Started and Closed 8000 NodeCache instances\nTue Nov 18 15:59:45 GMT+00:00 2014 Started and Closed 9000 NodeCache instances\nTue Nov 18 15:59:49 GMT+00:00 2014 Started and Closed 10000 NodeCache instances\nTue Nov 18 16:00:01 GMT+00:00 2014 Started and Closed 11000 NodeCache instances\njava.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:105)\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:88)\n\tat java.io.PrintStream.<init>(PrintStream.java:112)\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\n\tat org.apache.jute.CsvOutputArchive.<init>(CsvOutputArchive.java:57)\n\tat org.apache.zookeeper.proto.RequestHeader.toString(RequestHeader.java:62)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:308)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:815)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\njava.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:105)\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:88)\n\tat java.io.PrintStream.<init>(PrintStream.java:112)\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\n\tat org.apache.jute.CsvOutputArchive.<init>(CsvOutputArchive.java:57)\n\tat org.apache.zookeeper.proto.RequestHeader.toString(RequestHeader.java:62)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:308)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:815)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\njava.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:105)\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:88)\n\tat java.io.PrintStream.<init>(PrintStream.java:112)\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\n\tat org.apache.jute.CsvOutputArchive.<init>(CsvOutputArchive.java:57)\n\tat org.apache.zookeeper.proto.RequestHeader.toString(RequestHeader.java:62)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:308)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:815)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\njava.lang.OutOfMemoryError: Java heap space\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:105)\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:88)\n\tat java.io.PrintStream.<init>(PrintStream.java:112)\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\n\tat org.apache.jute.CsvOutputArchive.<init>(CsvOutputArchive.java:57)\n\tat org.apache.zookeeper.proto.RequestHeader.toString(RequestHeader.java:62)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:308)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:815)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\n\nException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"main\"\n\n\nAdditional Notes:\n\nWhen I don't specify the G1 collector I still get OutOfMemoryErrors, only the cause is \"GC overhead limit exceeded\".  \n\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:105)\n\tat java.io.BufferedWriter.<init>(BufferedWriter.java:88)\n\tat java.io.PrintStream.<init>(PrintStream.java:112)\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\n\tat org.apache.jute.CsvOutputArchive.<init>(CsvOutputArchive.java:57)\n\tat org.apache.zookeeper.proto.GetDataRequest.toString(GetDataRequest.java:62)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$Packet.toString(ClientCnxn.java:310)\n\tat java.lang.String.valueOf(String.java:2854)\n\tat java.lang.StringBuilder.append(StringBuilder.java:128)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:815)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:94)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:355)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)\n\nThis has been around for a while as I first noticed it with v1.3.3.  I upgraded to v2.6.0 and re-ran the test, but as you can see this still appears to be an issue.\nI ran this test on a x86_64 Ubuntu 10.043 system and observed the same behavior",
        "Issue Links": [
            "/jira/browse/CURATOR-217"
        ]
    },
    "CURATOR-168": {
        "Key": "CURATOR-168",
        "Summary": "Background protected mode incorrectly sets Event name to just ZNode name. Should be full path.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "zhaogang",
        "Created": "27/Nov/14 09:19",
        "Updated": "26/Nov/19 14:42",
        "Resolved": "21/Apr/15 01:17",
        "Description": "We use LeaderLatch for leader election within our several zookeeper clients,but sometimes our leaderLatch stopped to work due to a error \"Background exception was not retry-able or retry gave up java.lang.IllegalArgumentException: Path must start with / characte\".\nTo found the reason we tracked the execution of program in class LeaderLatch with debug mode,we found the code \"setNode(event.getName());\"  in line 487 of LeaderLatch.class in which \"event.getName()\" code sometimes didn't return the full path of the znode but only return the last part of the full path,like \"_c_b1097329-4a77-493b-806c-5c888067eeab-latch-0000004068\", but the full path should be \"/api1/leaders/_c_b1097329-4a77-493b-806c-5c888067eeab-latch-0000004068\".\nSo  the when event.getName()==\"_c_b1097329-4a77-493b-806c-5c888067eeab-latch-0000004068\", the program will fail to go through the codes below:\n1.LeaderLatch.checkLeadership()  line 526\n2.ZKPaths.getNodeFromPath()      line 61\n3.PathUtils.validatePath(path)        line 46\nAnd then the LeaderLatch stopped to work.",
        "Issue Links": [
            "/jira/browse/CURATOR-165",
            "/jira/browse/SPARK-20884"
        ]
    },
    "CURATOR-169": {
        "Key": "CURATOR-169",
        "Summary": "CuratorFramework with a set namespace causes blocking in PathChildrenCache constructor",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Dmitry Konstantinov",
        "Created": "04/Dec/14 22:14",
        "Updated": "04/Dec/14 22:42",
        "Resolved": null,
        "Description": "org.apache.curator.framework.CuratorFramework#newNamespaceAwareEnsurePath is used in PathChildrenCache, so if during a PathChildrenCache initialisation where is no established connection to Zookeeper we are blocking in PathChildrenCache constructor. The situation is valid for case with an specified namespace for CuratorFramework.",
        "Issue Links": []
    },
    "CURATOR-170": {
        "Key": "CURATOR-170",
        "Summary": "ChildReaper would benefit by being able to manage multiple paths",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "09/Dec/14 19:29",
        "Updated": "30/Dec/14 19:24",
        "Resolved": "30/Dec/14 19:24",
        "Description": "ChildReaper currently only allows one path. This means that if you want to reap multiple paths you have to create multiple child reapers. This ends up creating multiple executors. It would better if ChildReaper could manage multiple paths.",
        "Issue Links": []
    },
    "CURATOR-171": {
        "Key": "CURATOR-171",
        "Summary": "LeaderLatch and LeaderSelector should support Revokable behavior",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.1,                                            2.11.1",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "John Vines",
        "Created": "09/Dec/14 21:30",
        "Updated": "22/Oct/18 18:25",
        "Resolved": null,
        "Description": "Many Curator lock recipes support revoking. The Leader recipes should support this also as they use locking internally. See http://curator.apache.org/curator-recipes/shared-reentrant-lock.html - Revoking",
        "Issue Links": [
            "https://github.com/apache/curator/pull/195"
        ]
    },
    "CURATOR-172": {
        "Key": "CURATOR-172",
        "Summary": "Deadlock when performing background operation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.4.2",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Tom Byrne",
        "Created": "15/Dec/14 20:19",
        "Updated": "09/Feb/23 18:46",
        "Resolved": "26/Oct/16 17:35",
        "Description": "Had a box get into a state where our ZK connections were all deadlocked, waiting on an object monitor. jstack shows that our background thread that was creating a node was waiting on a lock that was held by the CuratorFramework thread, who was waiting on an object monitor that looks like it couldn't be completed until our other write was finished (packet.finish would never return true.) \nWe have seen this happen twice, but don't notice it until afterwards, and don't have enough logging to know what's triggering it (possible ZK connections going away?) \nRest of the box is fine, network connections are not flapping, main IO threads continue to accept and process connections, until we get backed up waiting for ZK. \nHere are the two stack traces:\n\"ZooChangeWatcher-BackgroundReader--2-1-SendThread()\" daemon prio=10 tid=0x00007fcf64108000 nid=0x88d waiting for monitor entry [0x00007fcbf5d16000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:177)\n\nwaiting to lock <0x00000000d526bcc8> (a org.apache.curator.ConnectionState)\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:763)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:470)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInBackground(CreateBuilderImpl.java:648)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:427)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat org.apache.curator.framework.recipes.nodes.PersistentEphemeralNode.createNode(PersistentEphemeralNode.java:340)\n\tat org.apache.curator.framework.recipes.nodes.PersistentEphemeralNode.access$000(PersistentEphemeralNode.java:52)\n\tat org.apache.curator.framework.recipes.nodes.PersistentEphemeralNode$4.processResult(PersistentEphemeralNode.java:224)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:686)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:659)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:479)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.sendBackgroundResponse(CreateBuilderImpl.java:526)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.access$600(CreateBuilderImpl.java:44)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$6.processResult(CreateBuilderImpl.java:485)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:602)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.queuePacket(ClientCnxn.java:475)\nlocked <0x00000000fa8e16f8> (a java.util.concurrent.LinkedBlockingQueue)\n\tat org.apache.zookeeper.ClientCnxn.finishPacket(ClientCnxn.java:627)\n\tat org.apache.zookeeper.ClientCnxn.conLossPacket(ClientCnxn.java:645)\n\tat org.apache.zookeeper.ClientCnxn.access$2400(ClientCnxn.java:85)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1160)\nlocked <0x00000000fa8e1380> (a java.util.LinkedList)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1109)\n\n\"CuratorFramework-0\" daemon prio=10 tid=0x00007fd02cb57800 nid=0x4425 in Object.wait() [0x00007fcfc507e000]\n   java.lang.Thread.State: WAITING (on object monitor)\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:503)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1309)\n\nlocked <0x00000000fa8e6750> (a org.apache.zookeeper.ClientCnxn$Packet)\n\tat org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1281)\n\tat org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:677)\nlocked <0x00000000fa8e0948> (a org.apache.zookeeper.ZooKeeper)\n\tat org.apache.curator.HandleHolder.internalClose(HandleHolder.java:139)\n\tat org.apache.curator.HandleHolder.closeAndReset(HandleHolder.java:77)\n\tat org.apache.curator.ConnectionState.reset(ConnectionState.java:218)\nlocked <0x00000000d526bcc8> (a org.apache.curator.ConnectionState)\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:194)\nlocked <0x00000000d526bcc8> (a org.apache.curator.ConnectionState)\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:763)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:749)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.access$300(CuratorFrameworkImpl.java:56)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl$3.call(CuratorFrameworkImpl.java:244)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n\tat java.lang.Thread.run(Thread.java:722)\n\nHelp me Obi-Wan Kenobi, you're my only hope.",
        "Issue Links": [
            "/jira/browse/CURATOR-194"
        ]
    },
    "CURATOR-173": {
        "Key": "CURATOR-173",
        "Summary": "InterProcessSemaphoreV2 nodes not reapable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.8.0",
        "Component/s": "None",
        "Assignee": "Scott Blum",
        "Reporter": "David Kesler",
        "Created": "22/Dec/14 19:30",
        "Updated": "19/Apr/15 18:22",
        "Resolved": "19/Apr/15 18:22",
        "Description": "The curator documentation recommends using a reaper or childreaper to clean up stale lock nodes.  This worked for InterProcessSemaphore locks.  However lock paths that are created by InterProcessSemaphoreV2 cannot be reaped.  The V2 recipe creates two subnodes beneath the lock node, 'locks' and 'leases', which are never cleaned up by the recipe.  This ensures that the lock node itself will never be empty and thus never reaped.  It doesn't seem like there's any safe way of handling cleaning up after an InterProcessSemaphoreV2 using canonical curator recipes.",
        "Issue Links": []
    },
    "CURATOR-174": {
        "Key": "CURATOR-174",
        "Summary": "Incompatible types between curator-rpc and the other elements",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "J.P. Koek",
        "Created": "23/Dec/14 11:50",
        "Updated": "21/Apr/15 00:11",
        "Resolved": null,
        "Description": "I'm using the curator-x-rpc module to connect Microsoft C# with zookeeper.\nIn my Java application I'm using a specific InstanceDetails object which is used in the payload of the instance.\nIf I'm using the curator-rpc module the data is set to byte[] which is not the case for my Java object, which is streamed with the JsonInstanceSerializer (default).\nBy having this I'm not able to register an service through the curator-x-rpc module and then use it in my java application.\nThe same problem occurs with the java TestClient which is part of the test set of the module.\nAlso the created instances with the Java instances can't be used by the curator-x-rpc because of the missing InstanceDetails class.",
        "Issue Links": []
    },
    "CURATOR-175": {
        "Key": "CURATOR-175",
        "Summary": "If zookeeper is down when discovery is started, it fails to register when the zookeeper comes up for the first time.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Gopi Kori",
        "Created": "28/Dec/14 12:27",
        "Updated": "12/Jan/15 21:05",
        "Resolved": "12/Jan/15 20:31",
        "Description": "If zookeeper is down when discovery is started, it fails to register when the zookeeper comes up for the first time.\nHowever, if the zookeeper is restarted again, discovery will connect and register the service instance correctly.\nThis happens because org.apache.curator.x.discovery.details.ServiceDiscoveryImpl.stateChanged() considers only ConnectionState.RECONNECTED state and not ConnectionState.CONNECTED.  This causes the first connection to be ignored, while subsequent connection recoveries work fine.",
        "Issue Links": []
    },
    "CURATOR-176": {
        "Key": "CURATOR-176",
        "Summary": "Currently difficult to write ServiceDiscovery admin console",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "30/Dec/14 14:38",
        "Updated": "05/Jan/15 15:06",
        "Resolved": "05/Jan/15 15:06",
        "Description": "In trying to write an admin console for ServiceDiscovery, some of the APIs are getting in the way. In particular, ServiceDiscovery.updateService() throws if the service isn't already registered. APIs are needed to modify/register a service irrespective to current status.",
        "Issue Links": []
    },
    "CURATOR-177": {
        "Key": "CURATOR-177",
        "Summary": "Pull request: Use Curator in thread names",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "Framework,                                            General",
        "Assignee": "Scott Blum",
        "Reporter": "Jesse Wilson",
        "Created": "02/Jan/15 16:59",
        "Updated": "12/Jan/15 21:05",
        "Resolved": "12/Jan/15 19:38",
        "Description": "I do thread dumps on my servers to look out for runaway thread pools. Curator's thread names aren't necessarily very helpful at figuring out what's going on. This should make that easier.\n(ZooKeeper itself is also guilty here.)\nhttps://github.com/apache/curator/pull/60",
        "Issue Links": [
            "/jira/browse/CURATOR-178"
        ]
    },
    "CURATOR-178": {
        "Key": "CURATOR-178",
        "Summary": "Pull request: Use Curator in thread names",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "None",
        "Component/s": "Framework,                                            General",
        "Assignee": "Scott Blum",
        "Reporter": "Jesse Wilson",
        "Created": "02/Jan/15 16:59",
        "Updated": "05/Jan/15 17:59",
        "Resolved": "05/Jan/15 17:59",
        "Description": "I do thread dumps on my servers to look out for runaway thread pools. Curator's thread names aren't necessarily very helpful at figuring out what's going on. This should make that easier.\n(ZooKeeper itself is also guilty here.)\nhttps://github.com/apache/curator/pull/60",
        "Issue Links": [
            "/jira/browse/CURATOR-177"
        ]
    },
    "CURATOR-179": {
        "Key": "CURATOR-179",
        "Summary": "Sequential path creation of /somepath/N throws exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "2.7.1",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Ryan Luecke",
        "Created": "07/Jan/15 00:33",
        "Updated": "12/Jan/15 23:05",
        "Resolved": "12/Jan/15 23:05",
        "Description": "A new error is thrown when attempting to run one of our scala services with Curator 2.7.0. The error does not throw in 2.6.0 (or in 2.5.0, which is when the original code was written. We were updating to 2.7.0).\nStack trace:\n[error] Exception in thread \"main\" java.lang.IllegalArgumentException: Path must not end with / character\n[error] \tat org.apache.curator.utils.PathUtils.validatePath(PathUtils.java:61)\n[error] \tat org.apache.curator.utils.ZKPaths.fixForNamespace(ZKPaths.java:44)\n[error] \tat org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:82)\n[error] \tat org.apache.curator.framework.imps.CuratorFrameworkImpl.fixForNamespace(CuratorFrameworkImpl.java:589)\n[error] \tat org.apache.curator.framework.imps.CreateBuilderImpl$1.forPath(CreateBuilderImpl.java:112)\n[error] \tat org.apache.curator.framework.imps.CreateBuilderImpl$1.forPath(CreateBuilderImpl.java:76)\n[error] \tat com.box.<application code>.scala:175)\n...\nThis happens when we try to create node /somepath/N (where N is sequential zk node).\nIn Curator, PathUtils.validatePath has two signatures:\n1. public static void validatePath(String path, boolean isSequential) throws IllegalArgumentException \n{\n     validatePath(isSequential? path + \"1\": path);\n   }\n2. public static String validatePath(String path) throws IllegalArgumentException {\n  ...\n  throws if path ends in '/'\n  ...\n}\nIn our case, path /somepath/ does end in '/'\nThe way #1 calls #2 appears to be a hack (appending \"1\" to path in order to validate). This would have caused path validation to succeed for our case - however, the caller MUST call this #1 signature of validatePath. The code calls actually calls #2 instead.\n2.5.0 released May 28, 2014\n2.6.0 released July 11, 2014\n\nissue introduced\n2.7.0 released November 3, 2014\n\nI believe the issue was introduced on July 29 2014, when a call to #2 above was added to fixForNamespace(String namespace, String path) in\nhttps://github.com/apache/curator/commit/96d2a55a03f8d0357f8f8cfa80a39a095d70667c\nWe've decided to workaround the issue at the moment. Our workaround is straightforward:\nChange our sequential node path from /somepath/N\nto /somepath/entry_N\nPlease get in touch if you need additional information to solve this issue. Thanks!",
        "Issue Links": []
    },
    "CURATOR-180": {
        "Key": "CURATOR-180",
        "Summary": "Rare occurences of NoNodeException in InterPRocessSemaphoreMurex.acquire",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Girard-Reydet",
        "Created": "09/Jan/15 15:04",
        "Updated": "09/Jan/15 15:04",
        "Resolved": null,
        "Description": "On some occasions, acquiring a semaphore will raise a NoNode exception (see stack below). I don't have statistics, but overs 3 weeks with about 20 test jobs/day, I observed it only twice.\nIf I refer to the code, the parent nodes (here /locks/-licence) should be created if it does not exist:\n\n    private InternalAcquireResult internalAcquire1Lease(ImmutableList.Builder<Lease> builder, long startMs, boolean hasWait, long waitMs) throws Exception\n    {\n        // ... skipping to the problematic lines\n        try\n        {\n            PathAndBytesable<String> createBuilder = client.create().creatingParentsIfNeeded().withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL);\n            String path = (nodeData != null) ? createBuilder.forPath(ZKPaths.makePath(leasesPath, LEASE_BASE_NAME), nodeData) : createBuilder.forPath(ZKPaths.makePath(leasesPath, LEASE_BASE_NAME));\n\n\nHere is the observed stack:\n\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /locks/-license/leases\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)\n\tat org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.internalAcquire1Lease(InterProcessSemaphoreV2.java:358)\n\tat org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:281)\n\tat org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.acquire(InterProcessSemaphoreV2.java:206)\n\tat org.apache.curator.framework.recipes.locks.InterProcessSemaphoreMutex.acquire(InterProcessSemaphoreMutex.java:46)\n\tat registry.backend.zk.ZooKeeperNodesManager$ZooKeeperRegistryLock.acquire(ZooKeeperNodesManager.java:223)\n\t... 11 more\n\n\nNote that I can have several processes concurrently trying to grab the lock at the same time. It is also possible that they concurrently try to grab it while the parent path does not yet exist.",
        "Issue Links": []
    },
    "CURATOR-181": {
        "Key": "CURATOR-181",
        "Summary": "discovery: registration of wrong/old service instance on RECONNECT",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.7.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "12/Jan/15 21:11",
        "Updated": "12/Jan/15 21:12",
        "Resolved": "12/Jan/15 21:12",
        "Description": "PR without a Jira: https://github.com/apache/curator/pull/43\nServiceDiscoveryImpl.registerService() remembers service registrations in a map. However updateService() does not update the map. \nThis causes a wrong registration in the case of a RECONNECT - e.g. reRegisterServices() called by the ConnectionStateListener registers the old ServiceInstance instead of the updated one.",
        "Issue Links": []
    },
    "CURATOR-182": {
        "Key": "CURATOR-182",
        "Summary": "TreeCache breaks when root node is removed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.7.0,                                            2.7.1",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Scott Blum",
        "Created": "16/Jan/15 20:39",
        "Updated": "16/Jan/15 21:44",
        "Resolved": "16/Jan/15 21:44",
        "Description": "Due to a bug in how root node existence was handled, if a root node is removed TreeCache can fail to notice when it's later re-added.",
        "Issue Links": [
            "/jira/browse/CURATOR-183"
        ]
    },
    "CURATOR-183": {
        "Key": "CURATOR-183",
        "Summary": "TreeCache not properly handling repeated add/removal of root node",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0,                                            2.7.1",
        "Fix Version/s": "2.8.0",
        "Component/s": "None",
        "Assignee": "Scott Blum",
        "Reporter": "Jason Rosenberg",
        "Created": "16/Jan/15 21:17",
        "Updated": "16/Jan/15 21:44",
        "Resolved": "16/Jan/15 21:43",
        "Description": "When using a TreeCache with just a single root node (e.g. maxDepth = 0), if I create the node, then delete it, and then repeat, it fails to send the proper events to the registered TreeCacheListener.  Instead of an event sequence like so:\nNODE_ADDED\nNODE_REMOVED\nNODE_ADDED\nNODE_REMOVED\nit instead sent these events:\nNODE_ADDED\nNODE_REMOVED\nNODE_UPDATED",
        "Issue Links": [
            "/jira/browse/CURATOR-182"
        ]
    },
    "CURATOR-184": {
        "Key": "CURATOR-184",
        "Summary": "Background retry doesn't stop with LOST event",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "18/Jan/15 20:13",
        "Updated": "10/May/23 16:11",
        "Resolved": "10/May/23 16:11",
        "Description": "Curator's background retry continues far after the LOST event connection state:\n34347 [CuratorFramework-0] INFO org.apache.curator.framework.state.ConnectionStateManager  - State change: LOST\n34348 [CuratorFramework-0] ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl  - Background operation retry gave up\n(...)\n81139 [main] DEBUG org.apache.curator.RetryLoop  - Retry policy not allowing retry\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /test\nThe LOST event is received at 34347 but the curator calls are still hanging until 81139.",
        "Issue Links": []
    },
    "CURATOR-185": {
        "Key": "CURATOR-185",
        "Summary": "Create a LOST_SESSION connection state",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "18/Jan/15 20:21",
        "Updated": "31/Aug/15 05:09",
        "Resolved": "31/Aug/15 05:09",
        "Description": "Since the LOST connection state doesn't have anything to do with the loss of the ZK session, it would be very useful to have an event for it.\nSuggested implementation to detect the loss of session:\n\nUpon reception of the SUSPENDED state, start a timer\nif we receive a RECONNECTED event, check the session ID. If we have a different session ID, then our session has been lost\nif we didn't receive a RECONNECTED event and the time reaches <sessionTimeoutMs>, our session is lost\n\nThat should inform the user of the loss of a session in a timely fashion.",
        "Issue Links": [
            "/jira/browse/CURATOR-246"
        ]
    },
    "CURATOR-186": {
        "Key": "CURATOR-186",
        "Summary": "Port Curator to Jackson FasterXML library from Jackson Codehaus library",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "3.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "mark petrovic",
        "Created": "23/Jan/15 17:41",
        "Updated": "08/May/15 18:58",
        "Resolved": "21/Apr/15 15:57",
        "Description": "There may be value in porting Curator to the Jackson FasterXML library, from Codehaus Jackson.  This lowers the possibility of accidentally mixing Jackson version 1 and 2 in Java imports if transitive dependencies use Jackson 2/FasterXML.",
        "Issue Links": []
    },
    "CURATOR-187": {
        "Key": "CURATOR-187",
        "Summary": "ChildReaper does not respect built in leader election",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.8.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "David Kesler",
        "Created": "09/Feb/15 16:27",
        "Updated": "10/Feb/15 03:47",
        "Resolved": "10/Feb/15 03:47",
        "Description": "ChildReaper has built in leader election, but ChildReaper itself doesn't actually respect it.  It merely passes it along to the Reaper.  This means that the child reaper will continue to watch its path and add nodes that need to be reaped into the reaper's queue, despite the reaper (potentially) not being active.\nThis has two negative effects:\n1) It creates a memory leak as the child reaper will continuously add paths to the reaper's list of activePaths without the reaper having any mechanism for removing them from its list.\n2) It creates a backlog of paths so that if the reaper ever does become leader it needs to churn through all the nodes that have been added to its queue since it started (or lost leadership).  In a high enough volume scenario, this can result in a substantial delay before the reaper begins successfully processing paths that still exist and need to be reaped.",
        "Issue Links": []
    },
    "CURATOR-188": {
        "Key": "CURATOR-188",
        "Summary": "Cannot determine the leader if zookeeper leader fails",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Rodrigo Nogueira",
        "Created": "12/Feb/15 16:02",
        "Updated": "26/Oct/16 17:55",
        "Resolved": null,
        "Description": "Hi,\nI'm trying to upgrade the curator framework from 2.6.0 to 2.7.1, but I'm having some problems.\nIn the 2.6.0 version almost everything works fine, but the ServiceDiscovery.updateService() that is already fixed in the 2.7.1.\nIn the 2.7.1 version, when I kill the zookeeper leader, my path for leader election becomes inconsistent. \nFor instance, I have three apps registered in the leader path (/com/myapp/leader/):\n[_c_85089ba7-0819-40a2-90b5-640bcb5e9e68-lock-0000000003, _c_070619f6-539e-4784-8068-bdc66d2a25bc-lock-0000000005, _c_54a126d3-31e8-464f-9216-5e0ad23fad1b-lock-0000000004]\nAfter killing the zookeeper leader, what I got in the /com/myapp/leader/ is:\n[_c_648d5311-a59c-4bc4-bf32-c0605dea9b6a-lock-0000000007, _c_85089ba7-0819-40a2-90b5-640bcb5e9e68-lock-0000000003, _c_f51f9660-3cbf-4ba8-8dba-c1e04ca14a93-lock-0000000008, _c_49696b77-e45a-40b6-8feb-96623c67fd85-lock-0000000006]\nSometimes I got more nodes (five or six).\nI'm aware that Curator removes and adds all nodes when a zookeeper node fails. But it seems that the previous nodes are not being removed correctly.\nIs that the expected behavior ?",
        "Issue Links": []
    },
    "CURATOR-189": {
        "Key": "CURATOR-189",
        "Summary": "Inconsistent path validation for creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4.2",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Tom Byrne",
        "Created": "19/Feb/15 20:32",
        "Updated": "19/Feb/15 20:33",
        "Resolved": null,
        "Description": "There is an inconsistency in handling paths when calling:\ncurator.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(). If the path passed ends with a \"/\", the call will succeed if the parents exist, but fail if it does not exist. \nThere is no path validation done on the parent path unless it needs to be created. \nI think the correct behavior should be that if the parent path ends with a \"/\", then just to remove the slash and create the parents anyways.",
        "Issue Links": []
    },
    "CURATOR-190": {
        "Key": "CURATOR-190",
        "Summary": "PersistentEphemeralNode.Mode.PROTECTED_EPHEMERAL creates new node on recreate",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0,                                            2.7.0,                                            2.7.1",
        "Fix Version/s": "2.8.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "John Vines",
        "Created": "27/Feb/15 01:25",
        "Updated": "28/Apr/15 04:00",
        "Resolved": "28/Apr/15 04:00",
        "Description": "PersistentEphemeralNode, when using a PROTECTED one, will create a new node with a name that looks like it padded itself again.\nex- I had a PEN, /test. With first create, it yielded:\n_c_03053942-4e11-4cf2-be75-7e689a9b2e98-test.\nI then bounced zk to emulate a disconnect and I saw the following two nodes-\n_c_03053942-4e11-4cf2-be75-7e689a9b2e98-_c_03053942-4e11-4cf2-be75-7e689a9b2e98-test\n_c_03053942-4e11-4cf2-be75-7e689a9b2e98-test\nEvery time I bounce it, I prepends another _c_03053942-4e11-4cf2-be75-7e689a9b2e98- onto the node name. All of these nodes seem to live until the PEN is closed.",
        "Issue Links": []
    },
    "CURATOR-191": {
        "Key": "CURATOR-191",
        "Summary": "Curator delivering out-of-order and duplicate events to PathChildrenCacheListener",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Himanshu Gupta",
        "Created": "02/Mar/15 20:11",
        "Updated": "02/Mar/15 20:11",
        "Resolved": null,
        "Description": "It is very hard to reproduce, but from our procudtion logs we can see that sometimes, PathChildrenCacheListener.childEvent(..) is receiving events which are out-of-order and duplicate.\nWe have following line in the code...\nChildEvent child = event.getData()\nlog.info(\"CHILD_UPDATED[%s] with version[%s]\", child.getPath(), event.getData().getStat().getVersion());\nand observed following the logs..\n2015-02-16 11:18:32,412 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[3]\n2015-02-16 11:19:04,773 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[5]\n2015-02-16 11:20:09,039 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[4]\n2015-02-16 11:20:41,420 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[5]\n2015-02-16 11:20:41,437 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[6]\n2015-02-16 11:21:13,737 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[6]\n2015-02-16 11:22:51,089 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[7]\n2015-02-16 11:23:23,400 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[8]\n2015-02-16 11:27:08,895 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[7]\n2015-02-16 11:27:41,591 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[9]\n2015-02-16 11:27:41,705 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[8]\n2015-02-16 11:32:32,751 INFO [ServerInventoryView-0] io.druid.curator.inventory.CuratorInventoryManager - CHILD_UPDATED[2015-02-11T22:01:43.441Z3] with version[10]",
        "Issue Links": []
    },
    "CURATOR-192": {
        "Key": "CURATOR-192",
        "Summary": "ChildReaper loggs error when the path to reap children from is empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Ricardo Ferreira",
        "Created": "08/Mar/15 19:40",
        "Updated": "28/Apr/23 13:59",
        "Resolved": "28/Apr/23 13:59",
        "Description": "When retrieving the children to reap, if there are no children a KeeperErrorCode = NoNode is thrown.\nThis exception is logged as an error.\nThis should be an expected behavior, so either a KeeperException.NoNodeException should be caught and silenced or logged as debug/trace.",
        "Issue Links": []
    },
    "CURATOR-193": {
        "Key": "CURATOR-193",
        "Summary": "Release note incompatibilities between Curator 2.6 and 2.7",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Implemented",
        "Affects Version/s": "2.6.0,                                            2.7.0,                                            2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sean Busbey",
        "Created": "11/Mar/15 18:01",
        "Updated": "21/Jun/15 16:26",
        "Resolved": "21/Jun/15 16:26",
        "Description": "Please release note that Curator 2.7 is incompatible with Curator 2.6 in the following ways:\n\norg.apache.curator.utils.PathUtils.validatePath(String) in curator-client\n\nIn release 2.7.0, CURATOR-136 changed the return type of this method from void to String. This is fine for a semver minor version under source compatibility, but is a violation of semver under binary compatibility. A downstream user will get NoSuchMethodError if their already compiled class uses this method.\nDownstream users should recompile their code against Curator 2.7.0+\n\norg.apache.curator.framework.recipes.shared.SharedCountReader.getVersionedValue() in curator-recipes\norg.apache.curator.framework.recipes.shared.SharedValueReader.getVersionedValue() in curator-recipes\n\nIn release 2.7.0, I think CURATOR-151 added these two methods to these interfaces as a part of improving an API.\nThe changes are fine for binary compatibility provided nothing in the framework ever calls them (doing so will result in NoSuchMethodError when called on an instance compiled against the older interface).  AFAICT, nothing in the framework accepts one of these interfaces and then calls this method.\nHowever, the addition of methods to Java interfaces breaks source compatibility. The inter-process semaphore recipes work on arbitrary SharedCountReader instances. That means that downstream folks who made their own implementation of the SharedCountReader interface under 2.6.0 will get a compilation error when they attempt to update to 2.7.0+.\nDownstream users will need to implement the added method before compiling against Curator 2.7.0+.",
        "Issue Links": []
    },
    "CURATOR-194": {
        "Key": "CURATOR-194",
        "Summary": "Deadlock in ConnectionState.checkTimeouts",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Amir Gur",
        "Created": "24/Mar/15 09:44",
        "Updated": "25/Mar/15 17:34",
        "Resolved": "25/Mar/15 17:06",
        "Description": "When ConnectionState.checkTimeouts actually detects a timeout, it calls 'reset'  \nwhich calls org.apache.zookeeper.ClientCnxn.close, which sends a ZooDefs.OpCode.closeSession request.\nThen it waits on the packet, until SendThread calls 'notifyAll' on the packet.\nAt that time, SendThread is blocked because it tries to enter the synchronized method 'ConnectionState.checkTimeouts'.\nSo it will never notify the packet.\nHere is the thread dump:\n\"job-scheduler_Worker-19-CheckHealthTask\" prio=10 tid=0x00007f260609c000 nid=0x5a97 in Object.wait() [0x00007f25723e1000]\n   java.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n\nwaiting on <0x0000000725fc0580> (a org.apache.zookeeper.ClientCnxn$Packet)\n        at java.lang.Object.wait(Object.java:503)\n        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)\nlocked <0x0000000725fc0580> (a org.apache.zookeeper.ClientCnxn$Packet)\n        at org.apache.zookeeper.ClientCnxn.close(ClientCnxn.java:1314)\n        at org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:677)\nlocked <0x0000000723949c88> (a org.apache.zookeeper.ZooKeeper)\n        at org.apache.curator.HandleHolder.internalClose(HandleHolder.java:139)\n        at org.apache.curator.HandleHolder.closeAndReset(HandleHolder.java:77)\n        at org.apache.curator.ConnectionState.reset(ConnectionState.java:218)\nlocked <0x000000071651de48> (a org.apache.curator.ConnectionState)\n        at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:194)\nlocked <0x000000071651de48> (a org.apache.curator.ConnectionState)\n        at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)\n        at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:474)\n        at org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:172)\n        at org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:161)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n        at org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:157)\n        at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:148)\n        at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:36)\n        at com.alu.dal.zooKeeper.ZooKeeperSession.checkHealth(ZooKeeperSession.java:350)\n        at com.alu.dal.zooKeeper.ZooKeeperSession.check(ZooKeeperSession.java:86)\n        at com.alu.orchestration.cluster.ClusterInstanceServiceImpl.checkQuorum(ClusterInstanceServiceImpl.java:464)\n        at com.alu.orchestration.cluster.ClusterInstanceServiceImpl.checkHealthState(ClusterInstanceServiceImpl.java:400)\n        at com.alu.tasks.health.CheckHealthTaskImpl.doWork(CheckHealthTaskImpl.java:37)\n        at com.alu.scheduler.JobSchedulerDetails$QuartzJob.executeInternal(JobSchedulerDetails.java:95)\n        at org.springframework.scheduling.quartz.QuartzJobBean.execute(QuartzJobBean.java:114)\n        at org.quartz.core.JobRunShell.run(JobRunShell.java:216)\n        at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:549)\n\n\"localhost-startStop-1-SendThread(11.1.1.11:2181)\" daemon prio=10 tid=0x00007f257c61a000 nid=0x7c3 waiting for monitor entry [0x00007f2562e65000]\n   java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:177)\n\nwaiting to lock <0x000000071651de48> (a org.apache.curator.ConnectionState)\n        at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)\n        at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:793)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.doSyncForSuspendedConnection(CuratorFrameworkImpl.java:668)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$800(CuratorFrameworkImpl.java:58)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl$7.retriesExhausted(CuratorFrameworkImpl.java:664)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:683)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:496)\n        at org.apache.curator.framework.imps.BackgroundSyncImpl$1.processResult(BackgroundSyncImpl.java:50)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:609)\n        at org.apache.zookeeper.ClientCnxn$EventThread.queuePacket(ClientCnxn.java:478)\nlocked <0x0000000714935b18> (a java.util.concurrent.LinkedBlockingQueue)\n        at org.apache.zookeeper.ClientCnxn.finishPacket(ClientCnxn.java:630)\n        at org.apache.zookeeper.ClientCnxn.conLossPacket(ClientCnxn.java:648)\n        at org.apache.zookeeper.ClientCnxn.access$2400(ClientCnxn.java:85)\n        at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1194)\nlocked <0x000000071b205bf0> (a java.util.LinkedList)\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1122)",
        "Issue Links": [
            "/jira/browse/CURATOR-172"
        ]
    },
    "CURATOR-195": {
        "Key": "CURATOR-195",
        "Summary": "LeaderSelector swallows exceptions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Henrik Nordvik",
        "Created": "30/Mar/15 12:52",
        "Updated": "08/May/16 19:45",
        "Resolved": "08/May/16 19:45",
        "Description": "Because CURATOR-71 was a bit agressive in deleting code, LeaderSelector will now swallow all exceptions from mutex.acquire();\n\nlog.error(\"mutex.acquire() threw an exception\", e);\n\n\nThe line was removed here:\nhttps://github.com/apache/curator/pull/31/files\nWhich means that the exception is now bubbling up to the Callable submitted to the executorservice. If you throw an exception inside the Callable it will be caught by FutureTask and set on the Future, and LeaderSelector never accesses the result of the future, so it is ignored.",
        "Issue Links": []
    },
    "CURATOR-196": {
        "Key": "CURATOR-196",
        "Summary": "this.client.create().creatingParentsIfNeeded()  throw Puzzling EXCEPTION",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "HuanWang",
        "Created": "31/Mar/15 07:43",
        "Updated": "10/May/23 12:04",
        "Resolved": "10/May/23 12:04",
        "Description": "Scene One\uff1aIn Single test. when I wanna register to zk. The code as below:\n\nprivate void startWorker() {\n\t\ttry {\n\n\t\t\tLOG.info(\"Start With Worker IP:\" + this.workerIP);\n\t\t\t\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH);\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_PATH);\n\t\t\t\n\t\t\tthis.workerMonitorPath = SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH + \"/\" + this.workerIP;\n\t\t\t/** Ephemeral Node: /workersMonitor/192.168.0.2 */\n\t\t\tthis.client.createEphemeralNode(this.workerMonitorPath);\n\t\t\t\n\t\t\t\n\t\t\tthis.workerPath = SuperionConstant.ZOOKEEPER_WORKER_PATH + \"/\" + this.workerIP;\n\t\t\t/** worker Node: /workers/192.168.0.2 */\n\t\t\tthis.client.makeDir(this.workerPath);\n\t\t\t\n\t\t\tString workerStatePath = this.workerPath + \"/\" + \"state\";\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state */\n\t\t\tthis.client.makeDir(workerStatePath);\n\t\t\t\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state/ProcessID */\n\t\t\tString workerStatePidPath = workerStatePath + \"/\" + \"ProcessID\";\n\t\t\tthis.client.writeInt32(workerStatePidPath, workerPID);\n\t\t\t\n\t\t\t//this.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_PATH);\n\t\t\t/** Persistent Node: /jobs/tmp   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH);\n\t\t\t/** Persistent Node: /jobs/state   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_STATE_PATH);\n\t\t\t\n\t\t\t//register the worker in Zookeeper success\n\t\t\tthis.containerManager.setBlockNewContainerRequests(false);\t\n\t\t} catch (Exception e) {\n\t\t\tString errorMsg = \"Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\";\n\t\t\tLOG.error(errorMsg, e);\n\t\t\tthrow new SuperionRuntimeException(errorMsg,e);\n\t\t}\n\t}\n\n\n==========================================================\nthe function I use is creatingParentsIfNeeded().\n==========================================================\n\npublic synchronized void writeData(String path,byte data[]) throws Exception {\n\t\t   System.out.println(path+\"  : writeData\");\n\t\tif(this.client.checkExists().forPath(path)!=null) {\n\t\t\t//node exit\n\t\t\tSystem.out.println(path+\"  : checkExist\");\n\t\t\tthis.client.setData().forPath(path, data);\n\t\t} else {\n\t\t\t//node not exit, create new\n\t\t\tSystem.out.println(path+ \"  : node not exit\");\n\t\t\tthis.client.create().creatingParentsIfNeeded()\n\t\t\t.withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t//\tthis.client.create().withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t\tSystem.out.println(path+ \"  : creatingParentsIfNeeded\");\n\t\t}\n\n\n\n======================================================\nbut sometimes (not every time) .it would throw NodeExistException:\n=======================================================\n\n015-03-31 15:29:49,452 INFO  [main-EventThread] state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: CONNECTED\n/workersMonitor  : checkExist\n/workers  : writeData\n/workers  : checkExist\n/workers/10.24.76.52  : writeData\n/workers/10.24.76.52  : node not exit\n/workers/10.24.76.52  : creatingParentsIfNeeded\n/workers/10.24.76.52/state  : writeData\n/workers/10.24.76.52/state  : node not exit\n2015-03-31 15:29:50,508 ERROR [main] zookeeper.ZookeeperService (ZookeeperService.java:startWorker(331)) - Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\n2015-03-31 15:29:50,510 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,557 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x34a75c727c204a4 closed\n2015-03-31 15:29:50,557 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n2015-03-31 15:29:50,558 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,561 INFO  [main] monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(168)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\n2015-03-31 15:29:50,562 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,562 INFO  [Public Localizer] localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(642)) - Public cache exiting\n2015-03-31 15:29:50,563 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping Worker metrics system...\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - Worker metrics system stopped.\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - Worker metrics system shutdown complete.\n2015-03-31 15:29:50,564 FATAL [main] worker.Worker (Worker.java:initAndStartNodeManager(184)) - Error starting NodeManager\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n\n\n\n===================================================\nScene Two:   When starting job \uff1a\n===================================================\n\n\nprivate void startJob(ZookeeperEvent zookeeperEvent) {\n\t\t\n\t\tStartJobZookeeperEvent startJobZookeeperEvent = (StartJobZookeeperEvent) zookeeperEvent;\n\t\tString jobInstanceId = startJobZookeeperEvent\n\t\t\t\t.getStartContainerRequest().getContainerId().getApplicationId()\n\t\t\t\t.getJobInstanceZKId();\n\t\t\n\t\tString jobTmpEphemeral = SuperionConstant.ZOOKEEPER_JOB_TMP_PATH + \"/\" + jobInstanceId;\n\t\tString jobStatePersistent = SuperionConstant.ZOOKEEPER_JOB_STATE_PATH + \"/\" + jobInstanceId;\n\t\t\n\t\tString jobStateWorkerIP = jobStatePersistent + \"/\" + SuperionConstant.JobState.WorkerIP;\n\t\tString jobStateJobStatus = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobStatus;\n\t\tString jobStateJobErrorMsg = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobErrorMsg;\n\t\tString jobStateCreateTime = jobStatePersistent + \"/\" + SuperionConstant.JobState.CreateTime;\n\t\t\n\t\ttry {\n\t\t\t/** Ephemeral Node: /job/tmp/jobInstanceId */\n\t\t\tthis.client.createEphemeralNode(jobTmpEphemeral);\n\t\t\tif(this.client.checkExists(jobTmpEphemeral) == null)\n\t\t\t\tthrow new Exception(\"ephemeral node[\"+jobTmpEphemeral+\"] create fail\");\n\t\t\t\n\t\t\t/** update job state-----------------  */\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId */\n\t\t\tthis.client.makeDir(jobStatePersistent);\n\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/WorkerIP */\n\t\t\tthis.client.writeString(jobStateWorkerIP, this.workerIP);\n\t\t\t\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/CreateTime */\n\t\t\tthis.client.writeInt64(jobStateCreateTime, System.currentTimeMillis());\n\n\t\t\t/* start container request */\n\t\t\tStartContainerResponse response = this.containerManager.startContainers(\n\t\t\t\t\tstartJobZookeeperEvent.getStartContainerRequest());\n\t\t\t\n\t\t\tint jobStatusInt = SuperionConstant.JOB_STATUS_TAKED;\n\t\t\t\n\t\t\t//TODO whtest\n\t\t\t\n\t\t\tif(!response.isSuccess()) {\n\t\t\t//\tjobStatusInt = SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR;\n\t\t\t\tLOG.error(startJobZookeeperEvent.getStartContainerRequest().getContainerId().toString() + \" start exception\", \n\t\t\t\t\t\tresponse.getFailureReason());\n\t\t\tString jobErrorMsg = response.getFailureReason().getMessage();\n\t\t\tthrow new Exception(jobErrorMsg,response.getFailureReason());\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n//       \t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\n\t\t\t} \n\t\t\t\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\n\t\t\tthis.client.writeInt32(jobStateJobStatus, jobStatusInt);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"exception happened when start job\" , e);\n\t\t\t\n\t\t\tif(e instanceof KeeperException.NodeExistsException){\n\t\t\t\t/*\n\t\t\t\t* node exit exception when /job/tmp/jobInstanceId create\n\t\t\t\t* if /job/tmp/jobInstanceId create then return\n\t\t\t\t* */\n\t\t\t\tKeeperException.NodeExistsException nodeExists = (KeeperException.NodeExistsException)e;\n\t\t\t\t     String existsPath = nodeExists.getPath();\n\t\t\t\t  \n\t\t\t\tif(existsPath != null && existsPath.startsWith(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH)) {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttry{\n\t\t\t\tString jobErrorMsg = e.getMessage();\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n\t\t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\t\t\n\t\t\t\tthis.client.writeInt32(jobStateJobStatus, SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR);\n\t\t\t} catch(Exception ignoreE) {\n\t\t\t\tLOG.warn(\"Ignore Exception\", ignoreE);//ignore\n\t\t\t} finally {\n\t\t\t\ttry {\n\t\t\t\t\tthis.client.deleteEphemeralNode(jobTmpEphemeral);\n\t\t\t\t} catch(Exception exception) {\n\t\t\t\t\tLOG.warn(\"Ignore Exception\", exception);//ignore\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\n\n====================================================\nWhen we saw logs.we find some jobs(not every one) throw the Exception\n==================================================\n\n\nource_visiblity as resource9_2_ from job_depend_resource jobdependr0_ where jobdependr0_.job_id=?\n2015-03-28 00:01:58,651 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(319)) - Start request for container_20150327000156_5755_0299_0144_ by user bicbt\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(343)) - Creating a new application reference for app application_20150327000156_5755\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] worker.WorkerAuditLogger (WorkerAuditLogger.java:logSuccess(98)) - USER=bicbt     OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_20150327000156_5755   CONTAINERID=container_20150327000156_5755_0299_0144_\n2015-03-28 00:01:58,675 ERROR [AsyncDispatcher event handler] zookeeper.ZookeeperService (ZookeeperService.java:startJob(178)) - exception happened when start job\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /jobs/state/1_299_20150328000156_144_0/JobStatus\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:119)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeInt32(ZookeeperClient.java:126)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startJob(ZookeeperService.java:176)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:104)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:30)\n        at com.suning.cybertron.superion.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:138)\n        at com.suning.cybertron.superion.event.AsyncDispatcher$1.run(AsyncDispatcher.java:85)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/CURATOR-197",
            "/jira/browse/CURATOR-198"
        ]
    },
    "CURATOR-197": {
        "Key": "CURATOR-197",
        "Summary": "this.client.create().creatingParentsIfNeeded()  throw Puzzling EXCEPTION",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "HuanWang",
        "Created": "31/Mar/15 07:53",
        "Updated": "31/Mar/15 15:50",
        "Resolved": "31/Mar/15 15:50",
        "Description": "Scene One\uff1aIn Single test. when I wanna register to zk. The code as below:\nprivate void startWorker() {\n\t\ttry \n{\n\n\t\t\tLOG.info(\"Start With Worker IP:\" + this.workerIP);\n\t\t\t\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH);\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_PATH);\n\t\t\t\n\t\t\tthis.workerMonitorPath = SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH + \"/\" + this.workerIP;\n\t\t\t/** Ephemeral Node: /workersMonitor/192.168.0.2 */\n\t\t\tthis.client.createEphemeralNode(this.workerMonitorPath);\n\t\t\t\n\t\t\t\n\t\t\tthis.workerPath = SuperionConstant.ZOOKEEPER_WORKER_PATH + \"/\" + this.workerIP;\n\t\t\t/** worker Node: /workers/192.168.0.2 */\n\t\t\tthis.client.makeDir(this.workerPath);\n\t\t\t\n\t\t\tString workerStatePath = this.workerPath + \"/\" + \"state\";\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state */\n\t\t\tthis.client.makeDir(workerStatePath);\n\t\t\t\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state/ProcessID */\n\t\t\tString workerStatePidPath = workerStatePath + \"/\" + \"ProcessID\";\n\t\t\tthis.client.writeInt32(workerStatePidPath, workerPID);\n\t\t\t\n\t\t\t//this.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_PATH);\n\t\t\t/** Persistent Node: /jobs/tmp   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH);\n\t\t\t/** Persistent Node: /jobs/state   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_STATE_PATH);\n\t\t\t\n\t\t\t//register the worker in Zookeeper success\n\t\t\tthis.containerManager.setBlockNewContainerRequests(false);\t\n\t\t}\n catch (Exception e) \n{\n\t\t\tString errorMsg = \"Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\";\n\t\t\tLOG.error(errorMsg, e);\n\t\t\tthrow new SuperionRuntimeException(errorMsg,e);\n\t\t}\n\t}\n==========================================================\nthe function I use is creatingParentsIfNeeded().\n==========================================================\npublic synchronized void writeData(String path,byte data[]) throws Exception {\n\t\t   System.out.println(path+\"  : writeData\");\n\t\tif(this.client.checkExists().forPath(path)!=null) \n{\n\t\t\t//node exit\n\t\t\tSystem.out.println(path+\"  : checkExist\");\n\t\t\tthis.client.setData().forPath(path, data);\n\t\t}\n else \n{\n\t\t\t//node not exit, create new\n\t\t\tSystem.out.println(path+ \"  : node not exit\");\n\t\t\tthis.client.create().creatingParentsIfNeeded()\n\t\t\t.withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t//\tthis.client.create().withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t\tSystem.out.println(path+ \"  : creatingParentsIfNeeded\");\n\t\t}\n\n\n======================================================\nbut sometimes (not every time) .it would throw NodeExistException:\n=======================================================\n015-03-31 15:29:49,452 INFO  [main-EventThread] state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: CONNECTED\n/workersMonitor  : checkExist\n/workers  : writeData\n/workers  : checkExist\n/workers/10.24.76.52  : writeData\n/workers/10.24.76.52  : node not exit\n/workers/10.24.76.52  : creatingParentsIfNeeded\n/workers/10.24.76.52/state  : writeData\n/workers/10.24.76.52/state  : node not exit\n2015-03-31 15:29:50,508 ERROR [main] zookeeper.ZookeeperService (ZookeeperService.java:startWorker(331)) - Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\n2015-03-31 15:29:50,510 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,557 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x34a75c727c204a4 closed\n2015-03-31 15:29:50,557 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n2015-03-31 15:29:50,558 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,561 INFO  [main] monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(168)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\n2015-03-31 15:29:50,562 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,562 INFO  [Public Localizer] localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(642)) - Public cache exiting\n2015-03-31 15:29:50,563 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping Worker metrics system...\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - Worker metrics system stopped.\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - Worker metrics system shutdown complete.\n2015-03-31 15:29:50,564 FATAL [main] worker.Worker (Worker.java:initAndStartNodeManager(184)) - Error starting NodeManager\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n===================================================\nScene Two:   When starting job \uff1a\n===================================================\nprivate void startJob(ZookeeperEvent zookeeperEvent) {\n\t\tStartJobZookeeperEvent startJobZookeeperEvent = (StartJobZookeeperEvent) zookeeperEvent;\n\t\tString jobInstanceId = startJobZookeeperEvent\n\t\t\t\t.getStartContainerRequest().getContainerId().getApplicationId()\n\t\t\t\t.getJobInstanceZKId();\n\t\tString jobTmpEphemeral = SuperionConstant.ZOOKEEPER_JOB_TMP_PATH + \"/\" + jobInstanceId;\n\t\tString jobStatePersistent = SuperionConstant.ZOOKEEPER_JOB_STATE_PATH + \"/\" + jobInstanceId;\n\t\tString jobStateWorkerIP = jobStatePersistent + \"/\" + SuperionConstant.JobState.WorkerIP;\n\t\tString jobStateJobStatus = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobStatus;\n\t\tString jobStateJobErrorMsg = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobErrorMsg;\n\t\tString jobStateCreateTime = jobStatePersistent + \"/\" + SuperionConstant.JobState.CreateTime;\n\t\ttry {\n\t\t\t/** Ephemeral Node: /job/tmp/jobInstanceId */\n\t\t\tthis.client.createEphemeralNode(jobTmpEphemeral);\n\t\t\tif(this.client.checkExists(jobTmpEphemeral) == null)\n\t\t\t\tthrow new Exception(\"ephemeral node[\"+jobTmpEphemeral+\"] create fail\");\n\t\t\t/** update job state-----------------  */\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId */\n\t\t\tthis.client.makeDir(jobStatePersistent);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/WorkerIP */\n\t\t\tthis.client.writeString(jobStateWorkerIP, this.workerIP);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/CreateTime */\n\t\t\tthis.client.writeInt64(jobStateCreateTime, System.currentTimeMillis());\n\t\t\t/* start container request */\n\t\t\tStartContainerResponse response = this.containerManager.startContainers(\n\t\t\t\t\tstartJobZookeeperEvent.getStartContainerRequest());\n\t\t\tint jobStatusInt = SuperionConstant.JOB_STATUS_TAKED;\n\t\t\t//TODO whtest\n\t\t\tif(!response.isSuccess()) \n{\n\t\t\t//\tjobStatusInt = SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR;\n\t\t\t\tLOG.error(startJobZookeeperEvent.getStartContainerRequest().getContainerId().toString() + \" start exception\", \n\t\t\t\t\t\tresponse.getFailureReason());\n\t\t\tString jobErrorMsg = response.getFailureReason().getMessage();\n\t\t\tthrow new Exception(jobErrorMsg,response.getFailureReason());\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n//       \t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\n\t\t\t}\n \n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\n\t\t\tthis.client.writeInt32(jobStateJobStatus, jobStatusInt);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"exception happened when start job\" , e);\n\t\t\tif(e instanceof KeeperException.NodeExistsException){\n\t\t\t\t/*\n\nnode exit exception when /job/tmp/jobInstanceId create\nif /job/tmp/jobInstanceId create then return\n*/\n\t\t\t\tKeeperException.NodeExistsException nodeExists = (KeeperException.NodeExistsException)e;\n\t\t\t\t     String existsPath = nodeExists.getPath();\n\n\t\t\t\tif(existsPath != null && existsPath.startsWith(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH)) \n{\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttry\n{\n\t\t\t\tString jobErrorMsg = e.getMessage();\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n\t\t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\t\t\n\t\t\t\tthis.client.writeInt32(jobStateJobStatus, SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR);\n\t\t\t}\n catch(Exception ignoreE) \n{\n\t\t\t\tLOG.warn(\"Ignore Exception\", ignoreE);//ignore\n\t\t\t}\n finally {\n\t\t\t\ttry \n{\n\t\t\t\t\tthis.client.deleteEphemeralNode(jobTmpEphemeral);\n\t\t\t\t}\n catch(Exception exception) \n{\n\t\t\t\t\tLOG.warn(\"Ignore Exception\", exception);//ignore\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n====================================================\nWhen we saw logs.we find some jobs(not every one) throw the Exception\n==================================================\nource_visiblity as resource9_2_ from job_depend_resource jobdependr0_ where jobdependr0_.job_id=?\n2015-03-28 00:01:58,651 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(319)) - Start request for container_20150327000156_5755_0299_0144_ by user bicbt\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(343)) - Creating a new application reference for app application_20150327000156_5755\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] worker.WorkerAuditLogger (WorkerAuditLogger.java:logSuccess(98)) - USER=bicbt     OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_20150327000156_5755   CONTAINERID=container_20150327000156_5755_0299_0144_\n2015-03-28 00:01:58,675 ERROR [AsyncDispatcher event handler] zookeeper.ZookeeperService (ZookeeperService.java:startJob(178)) - exception happened when start job\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /jobs/state/1_299_20150328000156_144_0/JobStatus\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:119)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeInt32(ZookeeperClient.java:126)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startJob(ZookeeperService.java:176)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:104)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:30)\n        at com.suning.cybertron.superion.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:138)\n        at com.suning.cybertron.superion.event.AsyncDispatcher$1.run(AsyncDispatcher.java:85)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/CURATOR-196"
        ]
    },
    "CURATOR-198": {
        "Key": "CURATOR-198",
        "Summary": "this.client.create().creatingParentsIfNeeded()  throw Puzzling EXCEPTION",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "HuanWang",
        "Created": "31/Mar/15 07:54",
        "Updated": "31/Mar/15 15:51",
        "Resolved": "31/Mar/15 15:51",
        "Description": "Scene One\uff1aIn Single test. when I wanna register to zk. The code as below:\nprivate void startWorker() {\n\t\ttry \n{\n\n\t\t\tLOG.info(\"Start With Worker IP:\" + this.workerIP);\n\t\t\t\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH);\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_PATH);\n\t\t\t\n\t\t\tthis.workerMonitorPath = SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH + \"/\" + this.workerIP;\n\t\t\t/** Ephemeral Node: /workersMonitor/192.168.0.2 */\n\t\t\tthis.client.createEphemeralNode(this.workerMonitorPath);\n\t\t\t\n\t\t\t\n\t\t\tthis.workerPath = SuperionConstant.ZOOKEEPER_WORKER_PATH + \"/\" + this.workerIP;\n\t\t\t/** worker Node: /workers/192.168.0.2 */\n\t\t\tthis.client.makeDir(this.workerPath);\n\t\t\t\n\t\t\tString workerStatePath = this.workerPath + \"/\" + \"state\";\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state */\n\t\t\tthis.client.makeDir(workerStatePath);\n\t\t\t\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state/ProcessID */\n\t\t\tString workerStatePidPath = workerStatePath + \"/\" + \"ProcessID\";\n\t\t\tthis.client.writeInt32(workerStatePidPath, workerPID);\n\t\t\t\n\t\t\t//this.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_PATH);\n\t\t\t/** Persistent Node: /jobs/tmp   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH);\n\t\t\t/** Persistent Node: /jobs/state   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_STATE_PATH);\n\t\t\t\n\t\t\t//register the worker in Zookeeper success\n\t\t\tthis.containerManager.setBlockNewContainerRequests(false);\t\n\t\t}\n catch (Exception e) \n{\n\t\t\tString errorMsg = \"Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\";\n\t\t\tLOG.error(errorMsg, e);\n\t\t\tthrow new SuperionRuntimeException(errorMsg,e);\n\t\t}\n\t}\n==========================================================\nthe function I use is creatingParentsIfNeeded().\n==========================================================\npublic synchronized void writeData(String path,byte data[]) throws Exception {\n\t\t   System.out.println(path+\"  : writeData\");\n\t\tif(this.client.checkExists().forPath(path)!=null) \n{\n\t\t\t//node exit\n\t\t\tSystem.out.println(path+\"  : checkExist\");\n\t\t\tthis.client.setData().forPath(path, data);\n\t\t}\n else \n{\n\t\t\t//node not exit, create new\n\t\t\tSystem.out.println(path+ \"  : node not exit\");\n\t\t\tthis.client.create().creatingParentsIfNeeded()\n\t\t\t.withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t//\tthis.client.create().withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t\tSystem.out.println(path+ \"  : creatingParentsIfNeeded\");\n\t\t}\n\n\n======================================================\nbut sometimes (not every time) .it would throw NodeExistException:\n=======================================================\n015-03-31 15:29:49,452 INFO  [main-EventThread] state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: CONNECTED\n/workersMonitor  : checkExist\n/workers  : writeData\n/workers  : checkExist\n/workers/10.24.76.52  : writeData\n/workers/10.24.76.52  : node not exit\n/workers/10.24.76.52  : creatingParentsIfNeeded\n/workers/10.24.76.52/state  : writeData\n/workers/10.24.76.52/state  : node not exit\n2015-03-31 15:29:50,508 ERROR [main] zookeeper.ZookeeperService (ZookeeperService.java:startWorker(331)) - Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\n2015-03-31 15:29:50,510 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,557 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x34a75c727c204a4 closed\n2015-03-31 15:29:50,557 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n2015-03-31 15:29:50,558 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,561 INFO  [main] monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(168)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\n2015-03-31 15:29:50,562 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,562 INFO  [Public Localizer] localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(642)) - Public cache exiting\n2015-03-31 15:29:50,563 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping Worker metrics system...\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - Worker metrics system stopped.\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - Worker metrics system shutdown complete.\n2015-03-31 15:29:50,564 FATAL [main] worker.Worker (Worker.java:initAndStartNodeManager(184)) - Error starting NodeManager\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n===================================================\nScene Two:   When starting job \uff1a\n===================================================\nprivate void startJob(ZookeeperEvent zookeeperEvent) {\n\t\tStartJobZookeeperEvent startJobZookeeperEvent = (StartJobZookeeperEvent) zookeeperEvent;\n\t\tString jobInstanceId = startJobZookeeperEvent\n\t\t\t\t.getStartContainerRequest().getContainerId().getApplicationId()\n\t\t\t\t.getJobInstanceZKId();\n\t\tString jobTmpEphemeral = SuperionConstant.ZOOKEEPER_JOB_TMP_PATH + \"/\" + jobInstanceId;\n\t\tString jobStatePersistent = SuperionConstant.ZOOKEEPER_JOB_STATE_PATH + \"/\" + jobInstanceId;\n\t\tString jobStateWorkerIP = jobStatePersistent + \"/\" + SuperionConstant.JobState.WorkerIP;\n\t\tString jobStateJobStatus = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobStatus;\n\t\tString jobStateJobErrorMsg = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobErrorMsg;\n\t\tString jobStateCreateTime = jobStatePersistent + \"/\" + SuperionConstant.JobState.CreateTime;\n\t\ttry {\n\t\t\t/** Ephemeral Node: /job/tmp/jobInstanceId */\n\t\t\tthis.client.createEphemeralNode(jobTmpEphemeral);\n\t\t\tif(this.client.checkExists(jobTmpEphemeral) == null)\n\t\t\t\tthrow new Exception(\"ephemeral node[\"+jobTmpEphemeral+\"] create fail\");\n\t\t\t/** update job state-----------------  */\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId */\n\t\t\tthis.client.makeDir(jobStatePersistent);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/WorkerIP */\n\t\t\tthis.client.writeString(jobStateWorkerIP, this.workerIP);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/CreateTime */\n\t\t\tthis.client.writeInt64(jobStateCreateTime, System.currentTimeMillis());\n\t\t\t/* start container request */\n\t\t\tStartContainerResponse response = this.containerManager.startContainers(\n\t\t\t\t\tstartJobZookeeperEvent.getStartContainerRequest());\n\t\t\tint jobStatusInt = SuperionConstant.JOB_STATUS_TAKED;\n\t\t\t//TODO whtest\n\t\t\tif(!response.isSuccess()) \n{\n\t\t\t//\tjobStatusInt = SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR;\n\t\t\t\tLOG.error(startJobZookeeperEvent.getStartContainerRequest().getContainerId().toString() + \" start exception\", \n\t\t\t\t\t\tresponse.getFailureReason());\n\t\t\tString jobErrorMsg = response.getFailureReason().getMessage();\n\t\t\tthrow new Exception(jobErrorMsg,response.getFailureReason());\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n//       \t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\n\t\t\t}\n \n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\n\t\t\tthis.client.writeInt32(jobStateJobStatus, jobStatusInt);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"exception happened when start job\" , e);\n\t\t\tif(e instanceof KeeperException.NodeExistsException){\n\t\t\t\t/*\n\nnode exit exception when /job/tmp/jobInstanceId create\nif /job/tmp/jobInstanceId create then return\n*/\n\t\t\t\tKeeperException.NodeExistsException nodeExists = (KeeperException.NodeExistsException)e;\n\t\t\t\t     String existsPath = nodeExists.getPath();\n\n\t\t\t\tif(existsPath != null && existsPath.startsWith(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH)) \n{\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttry\n{\n\t\t\t\tString jobErrorMsg = e.getMessage();\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n\t\t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\t\t\n\t\t\t\tthis.client.writeInt32(jobStateJobStatus, SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR);\n\t\t\t}\n catch(Exception ignoreE) \n{\n\t\t\t\tLOG.warn(\"Ignore Exception\", ignoreE);//ignore\n\t\t\t}\n finally {\n\t\t\t\ttry \n{\n\t\t\t\t\tthis.client.deleteEphemeralNode(jobTmpEphemeral);\n\t\t\t\t}\n catch(Exception exception) \n{\n\t\t\t\t\tLOG.warn(\"Ignore Exception\", exception);//ignore\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n====================================================\nWhen we saw logs.we find some jobs(not every one) throw the Exception\n==================================================\nource_visiblity as resource9_2_ from job_depend_resource jobdependr0_ where jobdependr0_.job_id=?\n2015-03-28 00:01:58,651 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(319)) - Start request for container_20150327000156_5755_0299_0144_ by user bicbt\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(343)) - Creating a new application reference for app application_20150327000156_5755\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] worker.WorkerAuditLogger (WorkerAuditLogger.java:logSuccess(98)) - USER=bicbt     OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_20150327000156_5755   CONTAINERID=container_20150327000156_5755_0299_0144_\n2015-03-28 00:01:58,675 ERROR [AsyncDispatcher event handler] zookeeper.ZookeeperService (ZookeeperService.java:startJob(178)) - exception happened when start job\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /jobs/state/1_299_20150328000156_144_0/JobStatus\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:119)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeInt32(ZookeeperClient.java:126)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startJob(ZookeeperService.java:176)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:104)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:30)\n        at com.suning.cybertron.superion.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:138)\n        at com.suning.cybertron.superion.event.AsyncDispatcher$1.run(AsyncDispatcher.java:85)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/CURATOR-196"
        ]
    },
    "CURATOR-199": {
        "Key": "CURATOR-199",
        "Summary": "this.client.create().creatingParentsIfNeeded() throw puzzling Exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "HuanWang",
        "Created": "31/Mar/15 07:56",
        "Updated": "31/Mar/15 07:59",
        "Resolved": "31/Mar/15 07:59",
        "Description": "Scene One\uff1aIn Single test. when I wanna register to zk. The code as below:\nprivate void startWorker() {\n\t\ttry \n{\n\n\t\t\tLOG.info(\"Start With Worker IP:\" + this.workerIP);\n\t\t\t\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH);\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_WORKER_PATH);\n\t\t\t\n\t\t\tthis.workerMonitorPath = SuperionConstant.ZOOKEEPER_WORKER_MONITOR_PATH + \"/\" + this.workerIP;\n\t\t\t/** Ephemeral Node: /workersMonitor/192.168.0.2 */\n\t\t\tthis.client.createEphemeralNode(this.workerMonitorPath);\n\t\t\t\n\t\t\t\n\t\t\tthis.workerPath = SuperionConstant.ZOOKEEPER_WORKER_PATH + \"/\" + this.workerIP;\n\t\t\t/** worker Node: /workers/192.168.0.2 */\n\t\t\tthis.client.makeDir(this.workerPath);\n\t\t\t\n\t\t\tString workerStatePath = this.workerPath + \"/\" + \"state\";\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state */\n\t\t\tthis.client.makeDir(workerStatePath);\n\t\t\t\n\t\t\t/** Persistent Node:  /workers/192.168.0.2/state/ProcessID */\n\t\t\tString workerStatePidPath = workerStatePath + \"/\" + \"ProcessID\";\n\t\t\tthis.client.writeInt32(workerStatePidPath, workerPID);\n\t\t\t\n\t\t\t//this.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_PATH);\n\t\t\t/** Persistent Node: /jobs/tmp   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH);\n\t\t\t/** Persistent Node: /jobs/state   */\n\t\t\tthis.client.makeDir(SuperionConstant.ZOOKEEPER_JOB_STATE_PATH);\n\t\t\t\n\t\t\t//register the worker in Zookeeper success\n\t\t\tthis.containerManager.setBlockNewContainerRequests(false);\t\n\t\t}\n catch (Exception e) \n{\n\t\t\tString errorMsg = \"Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\";\n\t\t\tLOG.error(errorMsg, e);\n\t\t\tthrow new SuperionRuntimeException(errorMsg,e);\n\t\t}\n\t}\n==========================================================\nthe function I use is creatingParentsIfNeeded().\n==========================================================\npublic synchronized void writeData(String path,byte data[]) throws Exception {\n\t\t   System.out.println(path+\"  : writeData\");\n\t\tif(this.client.checkExists().forPath(path)!=null) \n{\n\t\t\t//node exit\n\t\t\tSystem.out.println(path+\"  : checkExist\");\n\t\t\tthis.client.setData().forPath(path, data);\n\t\t}\n else \n{\n\t\t\t//node not exit, create new\n\t\t\tSystem.out.println(path+ \"  : node not exit\");\n\t\t\tthis.client.create().creatingParentsIfNeeded()\n\t\t\t.withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t//\tthis.client.create().withMode(CreateMode.PERSISTENT).forPath(path, data);\n\t\t\tSystem.out.println(path+ \"  : creatingParentsIfNeeded\");\n\t\t}\n\n\n======================================================\nbut sometimes (not every time) .it would throw NodeExistException:\n=======================================================\n015-03-31 15:29:49,452 INFO  [main-EventThread] state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: CONNECTED\n/workersMonitor  : checkExist\n/workers  : writeData\n/workers  : checkExist\n/workers/10.24.76.52  : writeData\n/workers/10.24.76.52  : node not exit\n/workers/10.24.76.52  : creatingParentsIfNeeded\n/workers/10.24.76.52/state  : writeData\n/workers/10.24.76.52/state  : node not exit\n2015-03-31 15:29:50,508 ERROR [main] zookeeper.ZookeeperService (ZookeeperService.java:startWorker(331)) - Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\n2015-03-31 15:29:50,510 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,557 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x34a75c727c204a4 closed\n2015-03-31 15:29:50,557 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n2015-03-31 15:29:50,558 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,561 INFO  [main] monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(168)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\n2015-03-31 15:29:50,562 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STARTED; cause: com.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n2015-03-31 15:29:50,562 INFO  [Public Localizer] localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(642)) - Public cache exiting\n2015-03-31 15:29:50,563 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(200)) - Stopping Worker metrics system...\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(206)) - Worker metrics system stopped.\n2015-03-31 15:29:50,564 INFO  [main] impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(572)) - Worker metrics system shutdown complete.\n2015-03-31 15:29:50,564 FATAL [main] worker.Worker (Worker.java:initAndStartNodeManager(184)) - Error starting NodeManager\ncom.suning.cybertron.superion.exception.SuperionRuntimeException: Worker Register Error Happen, Maker Sure Zookeeper Server Can Be Connected\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:332)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.serviceStart(ZookeeperService.java:86)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:230)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n\tat com.suning.cybertron.superion.worker.Worker.serviceStart(Worker.java:143)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat com.suning.cybertron.superion.worker.Worker.initAndStartNodeManager(Worker.java:182)\n\tat com.suning.cybertron.superion.worker.Worker.main(Worker.java:227)\nCaused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /workers/10.24.76.52/state\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:125)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.makeDir(ZookeeperClient.java:169)\n\tat com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startWorker(ZookeeperService.java:315)\n\t... 10 more\n===================================================\nScene Two:   When starting job \uff1a\n===================================================\nprivate void startJob(ZookeeperEvent zookeeperEvent) {\n\t\tStartJobZookeeperEvent startJobZookeeperEvent = (StartJobZookeeperEvent) zookeeperEvent;\n\t\tString jobInstanceId = startJobZookeeperEvent\n\t\t\t\t.getStartContainerRequest().getContainerId().getApplicationId()\n\t\t\t\t.getJobInstanceZKId();\n\t\tString jobTmpEphemeral = SuperionConstant.ZOOKEEPER_JOB_TMP_PATH + \"/\" + jobInstanceId;\n\t\tString jobStatePersistent = SuperionConstant.ZOOKEEPER_JOB_STATE_PATH + \"/\" + jobInstanceId;\n\t\tString jobStateWorkerIP = jobStatePersistent + \"/\" + SuperionConstant.JobState.WorkerIP;\n\t\tString jobStateJobStatus = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobStatus;\n\t\tString jobStateJobErrorMsg = jobStatePersistent + \"/\" + SuperionConstant.JobState.JobErrorMsg;\n\t\tString jobStateCreateTime = jobStatePersistent + \"/\" + SuperionConstant.JobState.CreateTime;\n\t\ttry {\n\t\t\t/** Ephemeral Node: /job/tmp/jobInstanceId */\n\t\t\tthis.client.createEphemeralNode(jobTmpEphemeral);\n\t\t\tif(this.client.checkExists(jobTmpEphemeral) == null)\n\t\t\t\tthrow new Exception(\"ephemeral node[\"+jobTmpEphemeral+\"] create fail\");\n\t\t\t/** update job state-----------------  */\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId */\n\t\t\tthis.client.makeDir(jobStatePersistent);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/WorkerIP */\n\t\t\tthis.client.writeString(jobStateWorkerIP, this.workerIP);\n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/CreateTime */\n\t\t\tthis.client.writeInt64(jobStateCreateTime, System.currentTimeMillis());\n\t\t\t/* start container request */\n\t\t\tStartContainerResponse response = this.containerManager.startContainers(\n\t\t\t\t\tstartJobZookeeperEvent.getStartContainerRequest());\n\t\t\tint jobStatusInt = SuperionConstant.JOB_STATUS_TAKED;\n\t\t\t//TODO whtest\n\t\t\tif(!response.isSuccess()) \n{\n\t\t\t//\tjobStatusInt = SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR;\n\t\t\t\tLOG.error(startJobZookeeperEvent.getStartContainerRequest().getContainerId().toString() + \" start exception\", \n\t\t\t\t\t\tresponse.getFailureReason());\n\t\t\tString jobErrorMsg = response.getFailureReason().getMessage();\n\t\t\tthrow new Exception(jobErrorMsg,response.getFailureReason());\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n//       \t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\n\t\t\t}\n \n\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\n\t\t\tthis.client.writeInt32(jobStateJobStatus, jobStatusInt);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"exception happened when start job\" , e);\n\t\t\tif(e instanceof KeeperException.NodeExistsException){\n\t\t\t\t/*\n\nnode exit exception when /job/tmp/jobInstanceId create\nif /job/tmp/jobInstanceId create then return\n*/\n\t\t\t\tKeeperException.NodeExistsException nodeExists = (KeeperException.NodeExistsException)e;\n\t\t\t\t     String existsPath = nodeExists.getPath();\n\n\t\t\t\tif(existsPath != null && existsPath.startsWith(SuperionConstant.ZOOKEEPER_JOB_TMP_PATH)) \n{\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\ttry\n{\n\t\t\t\tString jobErrorMsg = e.getMessage();\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobErrorMsg */\n\t\t\t\tthis.client.writeString(jobStateJobErrorMsg, jobErrorMsg);\n\t\t\t\t/** Persistent Node: /jobs/state/jobInstanceId/JobStatus */\t\t\n\t\t\t\tthis.client.writeInt32(jobStateJobStatus, SuperionConstant.JOB_STATUS_PARAMETER_CHECK_ERROR);\n\t\t\t}\n catch(Exception ignoreE) \n{\n\t\t\t\tLOG.warn(\"Ignore Exception\", ignoreE);//ignore\n\t\t\t}\n finally {\n\t\t\t\ttry \n{\n\t\t\t\t\tthis.client.deleteEphemeralNode(jobTmpEphemeral);\n\t\t\t\t}\n catch(Exception exception) \n{\n\t\t\t\t\tLOG.warn(\"Ignore Exception\", exception);//ignore\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n====================================================\nWhen we saw logs.we find some jobs(not every one) throw the Exception\n==================================================\nource_visiblity as resource9_2_ from job_depend_resource jobdependr0_ where jobdependr0_.job_id=?\n2015-03-28 00:01:58,651 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(319)) - Start request for container_20150327000156_5755_0299_0144_ by user bicbt\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(343)) - Creating a new application reference for app application_20150327000156_5755\n2015-03-28 00:01:58,652 INFO  [AsyncDispatcher event handler] worker.WorkerAuditLogger (WorkerAuditLogger.java:logSuccess(98)) - USER=bicbt     OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_20150327000156_5755   CONTAINERID=container_20150327000156_5755_0299_0144_\n2015-03-28 00:01:58,675 ERROR [AsyncDispatcher event handler] zookeeper.ZookeeperService (ZookeeperService.java:startJob(178)) - exception happened when start job\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /jobs/state/1_299_20150328000156_144_0/JobStatus\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:688)\n        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:672)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:668)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:453)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:443)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeData(ZookeeperClient.java:119)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperClient.writeInt32(ZookeeperClient.java:126)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.startJob(ZookeeperService.java:176)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:104)\n        at com.suning.cybertron.superion.worker.containermanager.zookeeper.ZookeeperService.handle(ZookeeperService.java:30)\n        at com.suning.cybertron.superion.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:138)\n        at com.suning.cybertron.superion.event.AsyncDispatcher$1.run(AsyncDispatcher.java:85)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "CURATOR-200": {
        "Key": "CURATOR-200",
        "Summary": "Proposal: Remove references to guava library from public APIs",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "01/Apr/15 22:24",
        "Updated": "10/Apr/17 19:06",
        "Resolved": "16/Jan/17 18:27",
        "Description": "There was a good discussion on the mailing list: http://curator.markmail.org/thread/yjete2ozm32jmz5u\nThe critical portion copied here:\n\nThe problem classes that I have found are:\n\ncurator-framework: org.apache.curator.framework.listen.ListenerContainer : method forEach takes a parameter of type com.google.common.base.Function\ncurator-framework: org.apache.curator.framework.api.transaction.CuratorTransactionResult : method ofTypeAndPath returns com.google.common.base.Predicate\ncurator-x-discovery-server: org.apache.curator.x.discovery.server.contexts.GenericDiscoveryContext : constructor takes param of type com.google.common.reflect.TypeToken\ncurator-x-discovery: org.apache.curator.x.discovery.InstanceFilter : inherits from com.google.common.base.Predicate\n\n\nIn the ensuing discussion, it sounded like we'd need to get started on an implementation before we had enough information to determine whether the changes are too intrusive or not.",
        "Issue Links": [
            "/jira/browse/YARN-3774",
            "/jira/browse/CURATOR-256",
            "/jira/browse/CURATOR-263",
            "/jira/browse/CURATOR-370",
            "https://github.com/apache/curator/pull/190",
            "https://github.com/apache/curator/pull/196",
            "https://github.com/apache/curator/pull/199",
            "https://github.com/apache/curator/pull/212"
        ]
    },
    "CURATOR-201": {
        "Key": "CURATOR-201",
        "Summary": "Update to latest maven-bundle-plugin : fixes OSGi \"uses\" constraints",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": "Ioannis Canellos",
        "Reporter": "Simon Kitching",
        "Created": "02/Apr/15 06:28",
        "Updated": "18/Apr/15 06:21",
        "Resolved": null,
        "Description": "The curator main pom currently specifies version 2.3.7 for plugin maven-bundle-plugin. This plugin is responsible for generating the OSGi-specific parts of the MANIFEST.MF file. Unfortunately that version of maven-bundle-plugin is buggy and generates too many \"uses\" constraints into the MANIFEST.MF. The latest version of maven-bundle-plugin (2.5.3) fixes this.",
        "Issue Links": []
    },
    "CURATOR-202": {
        "Key": "CURATOR-202",
        "Summary": "LeaderSelector node is not removed on connection loss if reconnected to the same session",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "2.8.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sergey Serebryanik",
        "Created": "02/Apr/15 19:53",
        "Updated": "10/May/23 15:20",
        "Resolved": "19/Apr/15 18:15",
        "Description": "Leader selection fails when connection is lost but restored before session timed out.\nTC:\n\nthe client looses connection to the zookeeper server\nthe client looses leadership\nthe client reconnects to the zookeeper server using the same session\nthe leader node wasn't deleted because the connection was not available on deleteOurPath() call, and because the client had reconnected to the same session, so the ephemeral node wasn't deleted by zookeeper\nthe client creates the new node\nno one is the leader because the old leader node stays forever.",
        "Issue Links": [
            "/jira/browse/CURATOR-205"
        ]
    },
    "CURATOR-203": {
        "Key": "CURATOR-203",
        "Summary": "ChildReaper can be unstable when there are large ZNodes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Apr/15 21:40",
        "Updated": "21/Apr/15 15:32",
        "Resolved": "21/Apr/15 15:31",
        "Description": "If ChildReaper is asked to reap a node that has grown beyond the jute.maxbuffer limits, it can cause client instability and, possibly, stop reaping nodes.",
        "Issue Links": []
    },
    "CURATOR-204": {
        "Key": "CURATOR-204",
        "Summary": "Use the latest apache parent pom",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.8.0",
        "Component/s": "Apache",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "07/Apr/15 17:00",
        "Updated": "21/Apr/15 20:06",
        "Resolved": "21/Apr/15 16:19",
        "Description": "We are currently on apache parent pom 14 and should update our build to use the latest one (currently 16).\nIt includes updates to versions of various maven plugins for bug fixes and other useful things.",
        "Issue Links": []
    },
    "CURATOR-205": {
        "Key": "CURATOR-205",
        "Summary": "Repeated InterruptedExceptions during mutex acquire leads to LeaderSelector deadlock",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Stephen Ingram",
        "Created": "08/Apr/15 18:28",
        "Updated": "10/May/23 15:20",
        "Resolved": "10/May/23 15:20",
        "Description": "When an InterruptedException is thrown during the internalLockLoop that is called during mutex.acquire, internalLockLoop will set a flag \"doDelete\" which signals during a finally clause to delete the lock path that we are trying to create.\nHowever, in the pathInForeground function of DeleteBuilderImpl, a second InterruptedException may occur before zookeeper can delete the specified path.  The RetryLoop machinery contained in the function will only retry if it is a Retryable Exception, an equivalence class which does not include InterruptedExceptions.  \nThe second InterruptedException exception then causes an exit of the pathInForeground function without deleting the path, leading to a deadlock where no one can acquire the mutex.\nIn my test, I am certain that both of these InterruptedExceptions are due to repeated fluctuation in the ConnectionStateManager's connection state.  When the state ceases to fluctuate, no leader can be selected due to the persistence of the node we failed to delete.\nI was able to address this bug with a solution similar to CURATOR-45:  if the pathInForeground function is interrupted with an InterruptedException, I schedule a BackgroundCallback to attempt pathInForeground again.  This task is able to delete the path when the connection is stable and the mutex is acquired by the new leader.\nI have a repro and a fix if needed.",
        "Issue Links": [
            "/jira/browse/CURATOR-202"
        ]
    },
    "CURATOR-206": {
        "Key": "CURATOR-206",
        "Summary": "2 clients aquired the same InterProcessLock?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Huahang Liu",
        "Created": "09/Apr/15 07:21",
        "Updated": "21/Apr/15 07:32",
        "Resolved": "21/Apr/15 07:32",
        "Description": "When a curator client acquires an InterProcessMutex, it creates an ephemeral node on zookeeper. But if we disconnect the network for some time long enough so that the ephemeral node expires, the thread that has the lock will not get interrupted and still \u201cthinks\u201d it has the lock. And if an other curator client tries to acquire the lock with the same path, it will acquired the lock while the first client still \u201cthinks\u201d it has the lock.\nIs it a defect? Or is it by design and this is not a proper way to use curator?\nThe snippet to reproduce this behaviour is uploaded as the following gist: \nhttps://gist.github.com/huahang/e6ebf948804fd7ea7c13\nRun the code and wait until client #0 gets the lock:\nClient #0 trying to acquire lock\nClient #1 trying to acquire lock\nClient #0 lock acquired\nDisconnect the network and reconnect after the ephemeral node expires, and then the following output will show in the command line:\nClient #1 lock acquired",
        "Issue Links": []
    },
    "CURATOR-207": {
        "Key": "CURATOR-207",
        "Summary": "Consolidate test setup/teardown",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "10/Apr/15 04:42",
        "Updated": "10/Apr/15 04:42",
        "Resolved": null,
        "Description": "There's a lot of boilerplate in the test code where each method does the same set-up and tear-down steps. And when writing new tests, the easiest way to get started is by copying an existing test.\nExamine if we can consolidate any of the test code. This might have a side-effect of speeding tests up too, if done properly.",
        "Issue Links": []
    },
    "CURATOR-208": {
        "Key": "CURATOR-208",
        "Summary": "InterProcessSemaphoreV2 swallows InterruptedException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0,                                            2.7.1",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Alexei Osipov",
        "Created": "10/Apr/15 09:53",
        "Updated": "18/Jan/16 00:47",
        "Resolved": "18/Jan/16 00:47",
        "Description": "InterProcessSemaphoreV2 incorrectly processes InterruptedException.\nMethod `InterProcessSemaphoreV2#makeLease(final String path)` contains code block\n\n                try\n                {\n                    client.delete().guaranteed().forPath(path);\n                }\n                catch ( KeeperException.NoNodeException e )\n                {\n                    log.warn(\"Lease already released\", e);\n                }\n                catch ( Exception e )\n                {\n                    throw new IOException(e);\n                }\n\n\nThe problem is that code in try block may throw an InterruptedException and this exception gets wrapped into IOException so it becomes very problematic to handle it properly.",
        "Issue Links": [
            "/jira/browse/CURATOR-109"
        ]
    },
    "CURATOR-209": {
        "Key": "CURATOR-209",
        "Summary": "Background retry falls into infinite loop of reconnection after connection loss",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "2.10.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Ryan Anderson",
        "Created": "10/Apr/15 20:57",
        "Updated": "05/Jan/17 14:57",
        "Resolved": "11/Jan/16 00:31",
        "Description": "We've been unable to replicate this in our test environments, but approximately once a week in production (~50 machine cluster using curator/zk for service discovery) we will get a machine falling into a loop and spewing tens of thousands of errors that look like:\n\nBackground operation retry gave uporg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\nat org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:695) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:496) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CreateBuilderImpl.sendBackgroundResponse(CreateBuilderImpl.java:538) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CreateBuilderImpl.access$700(CreateBuilderImpl.java:44) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CreateBuilderImpl$6.processResult(CreateBuilderImpl.java:497) [curator-framework-2.6.0.jar:na]\nat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605) [zookeeper-3.4.6.jar:3.4.6-1569965]\nat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n\n\nThe rate at which we get these errors seems to increase linearly until we stop the process (starts at 10-20/sec, when we kill the box it's typically generating 1,000+/sec)\nWhen the error first occurs, there's a slightly different stack trace:\n\nBackground operation retry gave uporg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\nat org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:695) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:813) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:779) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.access$400(CuratorFrameworkImpl.java:58) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:265) [curator-framework-2.6.0.jar:na]\nat java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55]\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_55]\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_55]\nat java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]\n\n\nfollowed very closely by:\n\nBackground retry gave uporg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.performBackgroundOperation(CuratorFrameworkImpl.java:796) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.backgroundOperationsLoop(CuratorFrameworkImpl.java:779) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.access$400(CuratorFrameworkImpl.java:58) [curator-framework-2.6.0.jar:na]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl$4.call(CuratorFrameworkImpl.java:265) [curator-framework-2.6.0.jar:na]\nat java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_55]\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_55]\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_55]\nat java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]\n\n\nAfter which it begins spewing the stack trace I first posted above. We're assuming that some sort of networking hiccup is occurring in EC2 that's causing the ConnectionLoss, which seems entirely momentary (none of our other boxes see it, and when we check the box it can connect to all the zk servers without any issues.)",
        "Issue Links": []
    },
    "CURATOR-210": {
        "Key": "CURATOR-210",
        "Summary": "Unit tests using server.close() are fragile.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.8.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "Cam McKenzie",
        "Created": "28/Apr/15 02:58",
        "Updated": "28/Apr/15 03:59",
        "Resolved": "28/Apr/15 03:59",
        "Description": "Unit tests that use the pattern of server.close() then recreating a new TestingServer instance to simulate connection loss are fragile and will fail periodically.\nThis appears to be due to a ZooKeeper bug (https://issues.apache.org/jira/browse/ZOOKEEPER-832). It can be worked around by using the server.restart() method instead of close and recreate.\nAll unit tests need to be investigated and fixed as necessary.",
        "Issue Links": []
    },
    "CURATOR-211": {
        "Key": "CURATOR-211",
        "Summary": "Fix InterProcessReadWriteLock#readLockPredicate() index validation issue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "awaiting-response",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Alexander Makarenko",
        "Created": "29/Apr/15 22:33",
        "Updated": "17/Jul/22 06:49",
        "Resolved": "05/May/15 20:08",
        "Description": "Hello, guys! I noticed a small bug in InterProcessReadWriteLock#readLockPredicate() method:\nInterProcessReadWriteLock.java, lines 196-211\nint         ourIndex = Integer.MAX_VALUE;\nfor ( String node : children )\n{\n    if ( node.contains(WRITE_LOCK_NAME) )\n    {\n        firstWriteIndex = Math.min(index, firstWriteIndex);\n    }\n    else if ( node.startsWith(sequenceNodeName) )\n    {\n        ourIndex = index;\n        break;\n    }\n\n    ++index;\n}\nStandardLockInternalsDriver.validateOurIndex(sequenceNodeName, ourIndex);\n\n\nIn case if sequenceNodeName is not in children list, StandardLockInternalsDriver.validateOurIndex() won't throw exception, because ourIndex in this case equals to Integer.MAX_VALUE.\nI'm creating pull request on Github to fix this issue.\nI know it's a very rare case, but why not fix it anyways?",
        "Issue Links": [
            "/jira/browse/CURATOR-307"
        ]
    },
    "CURATOR-212": {
        "Key": "CURATOR-212",
        "Summary": "checkState will set a new connection to old connected state",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "TBD",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Flier Lu",
        "Created": "30/Apr/15 08:00",
        "Updated": "29/Mar/16 00:42",
        "Resolved": null,
        "Description": "consider the situation, after the EnsembleProvider changed the connection string, ConnectionState.process received a session event, which will invoke checkState to check whether the client is connected.\n\n    @Override\n    public void process(WatchedEvent event)\n    {\n        //...\n        boolean wasConnected = isConnected.get();\n        boolean newIsConnected = wasConnected;\n        if ( event.getType() == Watcher.Event.EventType.None )\n        {\n            newIsConnected = checkState(event.getState(), wasConnected);\n        }\n\n        if ( newIsConnected != wasConnected )\n        {\n            isConnected.set(newIsConnected);\n            connectionStartMs = System.currentTimeMillis();\n        }\n    }\n\n\nif the old connection state is SyncConnected or ConnectedReadOnly, isConnected will be set true, even the handleNewConnectionString() was invoked, and the state was reset to create a new connection.\nAfter checkState return, process will set the new connection to the old connected state.\n\n        if ( checkNewConnectionString && zooKeeper.hasNewConnectionString() )\n        {\n            isConnected = false; // force to set it\n            handleNewConnectionString();\n        }\n\n\nWe could force the state to disconnected when we found a new connection will be create.",
        "Issue Links": []
    },
    "CURATOR-213": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-214": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support new create() APIs",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Cam McKenzie",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/May/15 15:08",
        "Updated": "31/Aug/15 23:47",
        "Resolved": "31/Aug/15 23:47",
        "Description": "ZooKeeper 3.5 adds new versions of create() that return the Stat object of the newly created ZNode.",
        "Issue Links": []
    },
    "CURATOR-215": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support async multi/transaction APIs",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/May/15 15:09",
        "Updated": "13/May/15 12:04",
        "Resolved": "13/May/15 12:04",
        "Description": "ZooKeeper 3.5 adds async multi/transaction APIs. Add support in Curator.",
        "Issue Links": []
    },
    "CURATOR-216": {
        "Key": "CURATOR-216",
        "Summary": "PathChildrenCacheListener could not receive CHILD_UPDATED events",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Later",
        "Affects Version/s": "2.6.0,                                            2.7.0,                                            2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Xiaoshuang LU",
        "Created": "09/May/15 11:19",
        "Updated": "21/Jun/15 16:43",
        "Resolved": "21/Jun/15 16:43",
        "Description": "Here is the scenario which can reproduce this issue.\n\n    public void testChildUpdated() throws Exception\n    {\n        Timing timing = new Timing();\n        CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString(), timing.session(), timing.connection(), new RetryOneTime(1));\n        client.start();\n        try\n        {\n            final CountDownLatch updatedLatch = new CountDownLatch(1);\n            client.create().creatingParentsIfNeeded().forPath(\"/test\");\n            PathChildrenCache cache = new PathChildrenCache(client, \"/test\", false);\n            cache.getListenable().addListener\n                (\n                    new PathChildrenCacheListener()\n                    {\n                        @Override\n                        public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception\n                        {\n                            if ( event.getType() == PathChildrenCacheEvent.Type.CHILD_UPDATED )\n                            {\n                                updatedLatch.countDown();\n                            }\n                        }\n                    }\n                );\n            cache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n            client.create().forPath(\"/test/foo\", \"first\".getBytes());\n            client.setData().forPath(\"/test/foo\", \"something new\".getBytes());\n            updatedLatch.await();\n        }\n        finally\n        {\n            CloseableUtils.closeQuietly(client);\n        }\n    }\n\n\nThe function will be blocked on \"updatedLatch.await();\".",
        "Issue Links": []
    },
    "CURATOR-217": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Use new Watcher Removal APIs in Curator Recipes",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "09/May/15 17:09",
        "Updated": "18/Jan/16 22:25",
        "Resolved": "24/Aug/15 16:27",
        "Description": "Once the new Watcher Removal APIs are available, every Curator Recipe should be reviewed so that they clean up watchers as appropriate.",
        "Issue Links": [
            "/jira/browse/CURATOR-161",
            "/jira/browse/CURATOR-290",
            "/jira/browse/CURATOR-167",
            "/jira/browse/CURATOR-286"
        ]
    },
    "CURATOR-218": {
        "Key": "CURATOR-218",
        "Summary": "Race condition in client.blockUntilConnected()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0,                                            2.7.1,                                            2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Mike Drob",
        "Reporter": "Samuel Garc\u00eda Mart\u00ednez",
        "Created": "12/May/15 10:24",
        "Updated": "26/Aug/15 17:59",
        "Resolved": "24/Aug/15 12:24",
        "Description": "I'm implementing a Guava service relying on the startAsync feature. I'm using a listener to wait for the INITIALIZED event to fire the guava service notifyStarted method, but sometimes the listener never receives the event if the root path does not exist.\nI've created a repository with a failing test (looping over the failing test to force it).\nrepo with tests: https://github.com/samuelgmartinez/curator-treecache-tests.git\nfailing test: mvn -Dtest=TreeCacheTest#testListenerNonExistentRootLoop test",
        "Issue Links": []
    },
    "CURATOR-219": {
        "Key": "CURATOR-219",
        "Summary": "Avoid use of Throwable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "12/May/15 17:17",
        "Updated": "14/Jul/22 03:10",
        "Resolved": "14/Jul/22 03:10",
        "Description": "We liberally catch Throwable in our code, which can mask more troubling issues like OutOfMemory or other errors. In most cases we should be catching Exception instead.",
        "Issue Links": []
    },
    "CURATOR-220": {
        "Key": "CURATOR-220",
        "Summary": "LOST state is sometimes not reported to ConnectionStateListener",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4.2,                                            2.8.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Akos Gyimesi",
        "Created": "15/May/15 20:11",
        "Updated": "16/Oct/15 19:34",
        "Resolved": null,
        "Description": "I used iptables to drop all outgoing packets to ZooKeeper, and logged the connection state changes. Most of the time I got only CONNECTED->SUSPENDED->RECONNECTED states, even though the ZooKeeper session expired and the ephemeral nodes of the session disappeared. (I expected CONNECTED->SUSPENDED->LOST->RECONNECTED)\nAccording to CURATOR-185 this may not be a bug because Curator connection states are not in 1-to-1 relation with ZooKeeper connection events. However, it causes problems in the LeaderSelector recipe (and possibly in others): without the LOST event the leader will never know if it really lose leadership or not. If it does not resign leadership at the SUSPENDED event (which is recommended, but not required according to the docs) the session will be in leader state forever.\nSee the following gist: https://gist.github.com/gyim/caea1b73cc8fa8f6997b\nThe code tests the LeaderSelector recipe, but the behavior is the same with a simple ConnectionStateListener.\nTested with Curator 2.4.2 and 2.8.0, it probably affects several other versions as well.",
        "Issue Links": []
    },
    "CURATOR-221": {
        "Key": "CURATOR-221",
        "Summary": "ACLProvider is not respected on parent nodes after calling CuratorFramework.usingNamespace",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Gregory Chanan",
        "Created": "18/May/15 21:33",
        "Updated": "18/May/15 23:10",
        "Resolved": null,
        "Description": "See HADOOP-11973 for some background information.\nHere's a high level overview of the issue I'm seeing:\n1) I create a CuratorFramework with a non-default ACLProvider\n2) I call framework.usingNamespace(\"/solr/zkdtsm\") on the curator framework\n3) I do some action, like starting a new SharedCount on a subnode of the namespace /testPath/ZKDTSMRoot/ZKDTSMSeqNumRoot\nResult: the tree is created, but neither /solr nor /solr/zkdtsm have ACLs, while /testPath/ZKDTSMRoot/ZKDTSMSeqNumRoot does.  I would expect all those nodes would have ACLs because I only performed zookeeper actions on a CuratorFramework with an ACLProvider specified.\nThis seems to happen because NamespaceImpl does not save the ACLProvider, see this line:\nhttps://github.com/apache/curator/blob/7f2098654a26e2f593801a586ce68300f54abf15/curator-framework/src/main/java/org/apache/curator/framework/imps/NamespaceImpl.java#L47\nThen, when an action is performed on the namespace, fixForNamespace is called:\nhttps://github.com/apache/curator/blob/7f2098654a26e2f593801a586ce68300f54abf15/curator-framework/src/main/java/org/apache/curator/framework/imps/NamespaceImpl.java#L74\nresulting in the parents being created without an ACL.",
        "Issue Links": [
            "/jira/browse/HADOOP-11973"
        ]
    },
    "CURATOR-222": {
        "Key": "CURATOR-222",
        "Summary": "Support Container Nodes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "19/May/15 20:11",
        "Updated": "01/Jul/15 23:02",
        "Resolved": "01/Jul/15 23:02",
        "Description": "ZooKeeper will most likely add support for \"Container\" nodes per https://issues.apache.org/jira/browse/ZOOKEEPER-2163 - Curator must support this feature\n\nBackwards compatible to 3.4.6. If Containers are available, they\u2019re used in all recipes that create parent nodes.\nThe use of EnsurePath is now deprecated",
        "Issue Links": []
    },
    "CURATOR-223": {
        "Key": "CURATOR-223",
        "Summary": "PathChildrenCache (wastefully) creates a thread per monitored node.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Tom Dyas",
        "Created": "15/Jun/15 20:46",
        "Updated": "20/Jun/15 13:09",
        "Resolved": "20/Jun/15 13:09",
        "Description": "PathChildrenCache creates a single-threaded executor. In the aggregate, this means there is a thread for every monitored node.\nIn my company's use case, we use ServiceCache (which uses PathChildrenCache) to monitor service discovery nodes for the locations of many sharded immutable key/value stores used by our services. We saw in excess of 250 threads devoted to the PathChildrenCache's used by ServiceCache. These threads were all parked so there was negligible CPU impact, but there is still the memory/stack impact of having so many idle threads. We would like to avoid that impact.\nI'd like to modify PathChildrenCache to take an ExecutorService as an alternate to the ThreadFactory it currently takes. This would allow me to pass in a thread pool for my company's use case.\nAre there any issues with which I should be concerned concerning PathChildrenCache's use of separate threads? (Non-reentrancy from watcher-invoked code? Binary compatibility?)",
        "Issue Links": []
    },
    "CURATOR-224": {
        "Key": "CURATOR-224",
        "Summary": "Locking Not Working with DistributedIdQueue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Apache,                                            Framework",
        "Assignee": null,
        "Reporter": "Zhihong Zhang",
        "Created": "18/Jun/15 21:31",
        "Updated": "21/Aug/15 18:32",
        "Resolved": "23/Jun/15 00:09",
        "Description": "The locking doesn't work with DistributedIdQeue.\n    2015-06-18 17:19:47.859  INFO 18244 \u2014 [    TaskCache-0] com.pixia.gi.zookeeper.TaskLockingQueue  : Path cache event: path=/task_queue/queue-|7qb89wjddu|0000000001, type=CHILD_REMOVED\n    2015-06-18 17:19:47.864  INFO 18244 \u2014 [    TaskCache-0] com.pixia.gi.zookeeper.TaskLockingQueue  : Path cache event: path=/task_queue/queue-0000000005, type=CHILD_ADDED\nWhen the DistributedQueue does the requeue, it recreates a new node path without the ID. The bug is in following code block,\n            if ( requeue )\n            {\n                client.inTransaction()\n                    .delete().forPath(itemPath)\n                    .and()\n                    .create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(makeItemPath(), bytes)\n                    .and()\n                    .commit();\n            }\n\nShould replace makeItemPath() with itemPath.",
        "Issue Links": [
            "/jira/browse/CURATOR-231"
        ]
    },
    "CURATOR-225": {
        "Key": "CURATOR-225",
        "Summary": "Add a new mode ErrorMode.KEEP",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Zhihong Zhang",
        "Created": "24/Jun/15 17:55",
        "Updated": "29/Mar/16 14:07",
        "Resolved": "29/Mar/16 14:07",
        "Description": "When locking is used and consumer throws exception, there are currently 2 modes, REQUEUE and DELETE.  In our use-cases, the items in the queue need to keep in the same FIFO order, even in case of the error. A 3rd mode is needed to keep the queue order intact.",
        "Issue Links": []
    },
    "CURATOR-226": {
        "Key": "CURATOR-226",
        "Summary": "A serious recursive error",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.0,                                            2.7.1,                                            2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "coder_czp",
        "Created": "30/Jun/15 02:21",
        "Updated": "30/Jun/15 02:21",
        "Resolved": null,
        "Description": "When used asynchronous way to create a temporary node multi directory, if the client does not have the zookeeper write permissions, the head will get into a cycle of death, the client will has been submitted to the server write message, but never succeed. Error code logic is as follows:",
        "Issue Links": []
    },
    "CURATOR-227": {
        "Key": "CURATOR-227",
        "Summary": "A serious death cycle error(\u4e00\u4e2a\u4e25\u91cd\u7684\u6b7b\u5faa\u73af\u9519\u8bef)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.0,                                            2.7.1,                                            2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "coder_czp",
        "Created": "30/Jun/15 02:23",
        "Updated": "07/Jun/18 13:52",
        "Resolved": null,
        "Description": "When used asynchronous way to create a temporary node multi directory, if the client does not have the zookeeper write permissions, the head will get into a cycle of death, the client will has been submitted to the server write message, but never succeed. Error code logic is as follows:",
        "Issue Links": []
    },
    "CURATOR-228": {
        "Key": "CURATOR-228",
        "Summary": "A serious death cycle error(\u4e00\u4e2a\u4e25\u91cd\u7684\u6b7b\u5faa\u73af\u9519\u8bef)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.0,                                            2.7.1,                                            2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Framework",
        "Assignee": "Cam McKenzie",
        "Reporter": "coder_czp",
        "Created": "30/Jun/15 02:39",
        "Updated": "31/Aug/15 23:19",
        "Resolved": "31/Aug/15 23:19",
        "Description": "\u5f53\u7528\u5f02\u6b65\u7684\u65b9\u5f0f\u521b\u5efa\u5e26\u6709\u591a\u5c42\u76ee\u5f55\u7684\u4e34\u65f6\u8282\u70b9\u65f6\uff0c\u5982\u679c\u5ba2\u6237\u7aef\u6ca1\u6709zookeeper\u7684\u5199\u5165\u6743\u9650\uff0ccurator \u5c31\u4f1a\u9677\u5165\u6b7b\u5faa\u73af,\u5ba2\u6237\u7aef\u4f1a\u4e00\u76f4\u5411\u670d\u52a1\u5668\u63d0\u4ea4\u5199\u5165\u62a5\u6587\uff0c\u4f46\u662f\u6c38\u8fdc\u4e0d\u4f1a\u6210\u529f\u3002\u51fa\u9519\u7684\u4ee3\u7801\u903b\u8f91\u5982\u4e0b:\nWhen used asynchronous way to create a temporary node multi directory, if the client does not have the zookeeper write permissions, the curator will get into a cycle of death, the client will has been submitted to the server write message, but never succeed. Error code logic is as follows:\n1  PersistentEphemeralNode sessionNode = new PersistentEphemeralNode(curatorClient, Mode.EPHEMERAL,\"/a/b/c\", \"test\");\n   sessionNode.start();\n     CreateBuilderImpl:\n       forPath(xx)->pathInBackground(xx)->\n\t\t    CuratorFrameworkImpl:processBackgroundOperation(operationAndData, null);\n\t\t\t\t                    --->performBackgroundOperation(xx)\n\t\t\t\tOperationAndData:callPerformBackgroundOperation(xx)-->\n                         CreateBuilderImpl:performBackgroundOperation(xx)->backgroundCreateParentsThenNode(xx)\n                                             -->queueOperation(xx)\n{backgroundOperations.offer(operationAndData);}\n \n  //\u8fd9\u4e2a\u5faa\u73af\u4f1a\u89e6\u53d11\uff0c\u5bfc\u81f4\u5faa\u73af\u65e0\u6cd5\u9000\u51fa\n  //This cycle will trigger 1, causing the loop to exit.  \n  //CuratorFrameworkImpl\n 2 private void backgroundOperationsLoop()\n    {\n        while ( !Thread.currentThread().isInterrupted() )\n        {\n            OperationAndData<?> operationAndData;\n            try\n            {\n                operationAndData = backgroundOperations.take();\n                if ( debugListener != null )\n                {\n                    debugListener.listen(operationAndData);\n                }\n            }\n            catch ( InterruptedException e )\n            {\n                Thread.currentThread().interrupt();\n                break;\n            }\n\n            performBackgroundOperation(operationAndData);\n        }\n    }\t\n \u5982\u679c\u9700\u8981\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u7ed9\u6211\u53d1\u90ae\u4ef6: coder_czp@126.com\t\n If you need more details, please email me:coder_czp@126.com",
        "Issue Links": []
    },
    "CURATOR-229": {
        "Key": "CURATOR-229",
        "Summary": "No retry on DNS lookup failure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.7.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Michael Putters",
        "Created": "02/Jul/15 11:46",
        "Updated": "17/May/23 10:59",
        "Resolved": "17/May/23 10:59",
        "Description": "Our environment is setup so that host names (rather than IP addresses) are used when registering services.\nWhen disconnecting a node from the network, it will attempt to reconnect and - in order to do this - attempts to resolve a host name, which fails (since we have no network connectivity and a DNS server is used).\nIt appears this type of exception is not retryable, and the node simply gives up and never reconnects, even when the network connectivity is back.\nIs this the expected behavior? Is there any way to configure Curator so that this type of exception is retryable? I had a look at CuratorFrameworkImpl.java around line 768 but there doesn't seem to be anything configurable.\nIf this is not the expected behavior (or if it is but you don't mind making it configurable), I should be able to provide a patch via a pull request.",
        "Issue Links": [
            "/jira/browse/CURATOR-293"
        ]
    },
    "CURATOR-230": {
        "Key": "CURATOR-230",
        "Summary": "Get remaining semaphore count upon lease acquisition",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Dmitry Minkovsky",
        "Created": "07/Jul/15 23:11",
        "Updated": "07/Jul/15 23:12",
        "Resolved": null,
        "Description": "I'm using Curator to help generate IDs in a distributed environment. My IDs are pretty typical: 64 bits with three components: timestamp, worker, count. The worker component comes from a Curator DistributedAtomicNumber modulo the max number of workers. To make sure that at any given time the number of workers don't exceed the max number of workers, each worker needs to take an InterProcessSemaphoreV2 before it gets its worker ID. There are some inherent issues with this method, but it's serving me fine at the moment. However, it seems that this use-case could be made entirely much simpler if I could just get a lease count upon acquiring an InterProcessSemaphoreV2. Would this be possible? Thank you.",
        "Issue Links": []
    },
    "CURATOR-231": {
        "Key": "CURATOR-231",
        "Summary": "TestDistributedQueue.testRetryAfterFailure_Curator56 fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes,                                            Tests",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "08/Jul/15 18:29",
        "Updated": "26/Aug/15 18:00",
        "Resolved": "21/Aug/15 18:32",
        "Description": "Per Jenkins: https://builds.apache.org/job/Curator/lastBuild/org.apache.curator$curator-recipes/testReport/org.apache.curator.framework.recipes.queue/TestDistributedQueue/testRetryAfterFailure_Curator56/",
        "Issue Links": [
            "/jira/browse/CURATOR-224",
            "/jira/browse/CURATOR-250",
            "/jira/browse/CURATOR-56"
        ]
    },
    "CURATOR-232": {
        "Key": "CURATOR-232",
        "Summary": "Consolidate test code",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "08/Jul/15 23:14",
        "Updated": "06/Sep/22 13:34",
        "Resolved": null,
        "Description": "Writing additional tests for Curator can be fairly intimidating to new users, and it looks like things just get copy and pasted a lot. We can consolidate a lot of test code, starting with how we expose Timing and CuratorFramework objects. This is a probably going to be a longer term effort with several JIRAs, so I'm not concerned with this one issue covering absolutely everything, I just want to make new tests easier to write.\nOne easy route is to make heavier use of annotations, like the @BeforeMethod and @AfterMethod instead of try/finally when making sure resources are cleaned up.",
        "Issue Links": []
    },
    "CURATOR-233": {
        "Key": "CURATOR-233",
        "Summary": "Bug in double barrier",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Mike Drob",
        "Reporter": "J D",
        "Created": "11/Jul/15 21:41",
        "Updated": "17/Jul/22 06:35",
        "Resolved": "17/Jul/22 06:35",
        "Description": "Hi,\nI think I discovered a bug in the internalLeave method of the double barrier implementation.\nWhen a client is told to leave the barrier after maxWait it does not do so. A flag is set but the client does not leave the barrier, instead it keeps iterating through the control loop and drives CPU usage to 100%.\nI have attached an example.\nBest regards\nLianro",
        "Issue Links": [
            "/jira/browse/CURATOR-536",
            "https://github.com/apache/curator/pull/207"
        ]
    },
    "CURATOR-234": {
        "Key": "CURATOR-234",
        "Summary": "Add test logging back to modules",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.0",
        "Component/s": "Tests",
        "Assignee": "Mike Drob",
        "Reporter": "Mike Drob",
        "Created": "13/Jul/15 12:52",
        "Updated": "26/Aug/15 17:59",
        "Resolved": "24/Aug/15 15:43",
        "Description": "Commit db06634 removed the slf4j-log4j12 dependency from the parent pom, so now we no longer have test logging easily available. This makes it difficult to debug various failing tests.\n\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <scope>test</scope>\n        </dependency>",
        "Issue Links": []
    },
    "CURATOR-235": {
        "Key": "CURATOR-235",
        "Summary": "LeaderSelector.internalRequeue should be private",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Nandor Kracser",
        "Created": "14/Jul/15 17:38",
        "Updated": "06/Sep/22 13:32",
        "Resolved": "18/Jul/15 17:42",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-236": {
        "Key": "CURATOR-236",
        "Summary": "TreeCache throws IllegalArgumentException when node is a substring of the requested path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Joe Littlejohn",
        "Created": "16/Jul/15 14:57",
        "Updated": "16/Jul/15 22:21",
        "Resolved": "16/Jul/15 21:55",
        "Description": "The javadocs for the new TreeCache state that getCurrentChildren will return null when the given path is not found in the cache. This seems to be the case for most paths, however if the given path contains an existing node as a substring, an exception is thrown:\n\nUnhandled java.lang.IllegalArgumentException\n   Path must start with / character\n\n                PathUtils.java:   54  org.apache.curator.utils.PathUtils/validatePath\n                  ZKPaths.java:  149  org.apache.curator.utils.ZKPaths/split\n                TreeCache.java:  597  org.apache.curator.framework.recipes.cache.TreeCache/find\n                TreeCache.java:  625  org.apache.curator.framework.recipes.cache.TreeCache/getCurrentChildren\n                           nil:   -1  sun.reflect.GeneratedMethodAccessor13/invoke\nDelegatingMethodAccessorImpl.java:   43  sun.reflect.DelegatingMethodAccessorImpl/invoke\n                   Method.java:  606  java.lang.reflect.Method/invoke\n                Reflector.java:   93  clojure.lang.Reflector/invokeMatchingMethod\n                Reflector.java:   28  clojure.lang.Reflector/invokeInstanceMethod\n                          REPL:    1  healthy.zookeeper/eval20736\n\n\n(please ignore the strangely formatted exception, I'm running a Clojure repl in Emacs)\nSo, lets imagine that I have the following nodes in zookeeper:\n\n/foo\n/foo/bar\n/foo/baz\n\n\nIf I have a TreeCache t pointing at /foo then I will get the following results:\n\nt.getCurrentChildren(\"/sss\") => nil\nt.getCurrentChildren(\"/foo/sss\") => nil\nt.getCurrentChildren(\"/foo/barsss\") => IllegalArgumentException: Path must start with / character",
        "Issue Links": []
    },
    "CURATOR-237": {
        "Key": "CURATOR-237",
        "Summary": "[Parent] Create code style files for other IDEs",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "20/Jul/15 21:15",
        "Updated": "20/Jul/15 21:15",
        "Resolved": null,
        "Description": "We currently have a code style file for IntelliJ IDEA users. It would be nice to have files for other environments as well. Different editors can be tracked as subtasks of this parent task.",
        "Issue Links": []
    },
    "CURATOR-238": {
        "Key": "CURATOR-237 [Parent] Create code style files for other IDEs",
        "Summary": "Create code style file for NetBeans",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "20/Jul/15 21:17",
        "Updated": "20/Jul/15 21:42",
        "Resolved": null,
        "Description": "See parent task for context.\nCreate an importable code style file for NetBeans that mirrors the existing \"JZ\" style.",
        "Issue Links": []
    },
    "CURATOR-239": {
        "Key": "CURATOR-239",
        "Summary": "RetryPolicy that retries forever",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Leandro Nunes",
        "Created": "23/Jul/15 11:28",
        "Updated": "31/Aug/15 14:20",
        "Resolved": "31/Aug/15 14:20",
        "Description": "I'd like to have a RetryPolicy that never ceases to retry. I find having to come up with number for maxRetries a pain. I mean, I guess it makes sense to have my clients repeatedly trying to connect to zookeeper on most cases. I can be missing something though (this seems so obvious to me that I most definitely am).",
        "Issue Links": []
    },
    "CURATOR-240": {
        "Key": "CURATOR-240",
        "Summary": "Severe memory leak in TreeCache when path does not exist",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Joe Littlejohn",
        "Created": "31/Jul/15 13:25",
        "Updated": "05/Aug/15 07:57",
        "Resolved": "31/Jul/15 21:21",
        "Description": "When creating a TreeCache, if the path supplied does not exist in Zookeeper then the heap is quickly exhausted. The problem appears to be instances of org.apache.curator.framework.imps.NamespaceWatcher.\nTry running the following test:\n\npackage org.apache.curator.framework.recipes.cache;\n\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.framework.recipes.cache.TreeCache;\nimport org.apache.curator.retry.RetryOneTime;\nimport org.apache.curator.test.TestingServer;\n\npublic class TreeCacheLeak {\n\n    public static void main(String[] args) throws Exception {\n\n        TestingServer server = new TestingServer();\n        \n        final CuratorFramework curatorFramework = CuratorFrameworkFactory.newClient(server.getConnectString(), new RetryOneTime(1));\n        curatorFramework.start();\n\n        final TreeCache cache = new TreeCache(curatorFramework, \"/foo/bar/baz\");\n        cache.start();\n\n        try {\n            while (true) {\n                Thread.sleep(1000);\n            }\n        } finally {\n            cache.close();\n            server.close();\n        }\n    }\n\n}\n\n\nLaunch the class then connect e.g. jvisualvm. You'll see the heap growing and if you watch for a few minutes then take a heap dump, you'll see millions of instances of org.apache.curator.framework.imps.NamespaceWatcher are present.",
        "Issue Links": []
    },
    "CURATOR-241": {
        "Key": "CURATOR-241",
        "Summary": "PersistentEphemeralNode writes initial data on reconnect",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Alex Brasetvik",
        "Created": "02/Aug/15 23:00",
        "Updated": "11/Aug/15 22:30",
        "Resolved": "11/Aug/15 22:30",
        "Description": "A `PersistentEphemeralNode` initialised with \"initial data\", then updated using `setData(\"updated data\")` may overwrite the updated data with \"initial data\" on reconnect.",
        "Issue Links": []
    },
    "CURATOR-242": {
        "Key": "CURATOR-242",
        "Summary": "TestDistributedQueue.testCustomExecutor() is failing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "11/Aug/15 17:09",
        "Updated": "26/Aug/15 17:59",
        "Resolved": "24/Aug/15 13:13",
        "Description": "https://builds.apache.org/job/Curator/940/org.apache.curator$curator-recipes/testReport/org.apache.curator.framework.recipes.queue/TestDistributedQueue/testCustomExecutor/\n\njava.lang.AssertionError: expected [true] but found [false]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertTrue(Assert.java:42)\n\tat org.testng.Assert.assertTrue(Assert.java:52)\n\tat org.apache.curator.framework.recipes.queue.TestDistributedQueue.testCustomExecutor(TestDistributedQueue.java:203)",
        "Issue Links": []
    },
    "CURATOR-243": {
        "Key": "CURATOR-243",
        "Summary": "TestDistributedIdQueue.testRequeuingWithLock is failing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "11/Aug/15 17:13",
        "Updated": "26/Aug/15 17:59",
        "Resolved": "21/Aug/15 17:35",
        "Description": "https://builds.apache.org/job/Curator/940/org.apache.curator$curator-recipes/testReport/org.apache.curator.framework.recipes.queue/TestDistributedIdQueue/testRequeuingWithLock/\n\njava.lang.AssertionError: expected [1] but found [0]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertEquals(Assert.java:123)\n\tat org.testng.Assert.assertEquals(Assert.java:370)\n\tat org.testng.Assert.assertEquals(Assert.java:380)\n\tat org.apache.curator.framework.recipes.queue.TestDistributedIdQueue.testRequeuingWithLock(TestDistributedIdQueue.java:165)",
        "Issue Links": []
    },
    "CURATOR-244": {
        "Key": "CURATOR-244",
        "Summary": "Creating parents with ACLProvider puts wrong ACLs on znodes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.6.0",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "John Vines",
        "Created": "11/Aug/15 18:23",
        "Updated": "18/Jan/16 23:05",
        "Resolved": "18/Jan/16 23:05",
        "Description": "Currently, if I create /foo/bar/baz and /foo doesn't exist it will create the whole hierarchy, but it will create /foo and /foo/bar with the ACLs the provider provides for /foo/bar/baz. I would expect it to consult the ACLProvider for the right znode (which seems totally doable but for some reason it passes in the wrong path to the provider when creating parents) or not ACL the parent znode.\nThis is similar to CURATOR-221, but I think this behavior makes for a pretty bad user experience as it's not simply a case of checking how you namespace a curatorframework.",
        "Issue Links": [
            "/jira/browse/CURATOR-58"
        ]
    },
    "CURATOR-245": {
        "Key": "CURATOR-245",
        "Summary": "Long sleep when setting up TestingServer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Harald Musum",
        "Created": "17/Aug/15 12:37",
        "Updated": "09/Sep/15 12:29",
        "Resolved": "09/Sep/15 12:29",
        "Description": "We have unit tests with TestingServer which started taking much longer tie after upgrading to 2.8.0.  I found that the following had been added to TestingZooKeeperMain.blockUntilStarted() since the previous version we used (2.4.1):\n        Thread.sleep(1000);\nIn our case this made the tests use a lot more time than previously.  \nIs the sleep really necessary?  Is it possible to workaround it (couldn't find any easy way)",
        "Issue Links": [
            "/jira/browse/CURATOR-257"
        ]
    },
    "CURATOR-246": {
        "Key": "CURATOR-246",
        "Summary": "Parent task for adding a SESSION_LOST connection state, etc.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": null,
        "Reporter": "Dong Lei",
        "Created": "20/Aug/15 06:02",
        "Updated": "28/Dec/15 18:55",
        "Resolved": "28/Dec/15 18:55",
        "Description": "Spark now leverage curator to help manage the connections to ZK and do leader election. \nCurrently, whenever a ZK session gets disassociated, the ConnectionStateManager will be aware and mark the state to be SUSPENDED and a new leader election will be triggered. \nEven though a ZK session is able to reconnect to another machine very soon. \nI wonder if we can tolerate such unstable network trembling and do not trigger a leader election. Because the upper layer application's (like spark) reaction of new leader can be very costly.",
        "Issue Links": [
            "/jira/browse/CURATOR-185"
        ]
    },
    "CURATOR-247": {
        "Key": "CURATOR-246 Parent task for adding a SESSION_LOST connection state, etc.",
        "Summary": "Extend Curator's connection state to support SESSION_LOST",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Aug/15 02:29",
        "Updated": "08/Sep/15 18:24",
        "Resolved": "01/Sep/15 13:03",
        "Description": "Currently, Curator has a connection state for LOST that confuses users. It does not mean that the session is lost. Instead it means that the retry policy has given up retrying. Introduce a new connection state that roughly corresponds to the ZooKeeper session expiring. Possibly require that clients request this support via a new new builder method in CuratorFrameworkFactory",
        "Issue Links": [
            "/jira/browse/CURATOR-248",
            "/jira/browse/CURATOR-134"
        ]
    },
    "CURATOR-248": {
        "Key": "CURATOR-246 Parent task for adding a SESSION_LOST connection state, etc.",
        "Summary": "Introduce pluggable error handlers",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Aug/15 02:30",
        "Updated": "24/Jan/17 01:39",
        "Resolved": "07/Sep/15 12:57",
        "Description": "Currently, Curator recipes handle error conditions by listening for the SUSPENDED connection state. Some users would like Curator to try to maintain the session if possible. Introduce a pluggable error handler system. Have an implementation that does the current behavior. Create another that attempts to wait until session lost.",
        "Issue Links": [
            "/jira/browse/CURATOR-247",
            "/jira/browse/CURATOR-249"
        ]
    },
    "CURATOR-249": {
        "Key": "CURATOR-246 Parent task for adding a SESSION_LOST connection state, etc.",
        "Summary": "Update all Curator recipes to use new pluggable Error Handler",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Aug/15 02:33",
        "Updated": "24/Aug/15 19:31",
        "Resolved": "24/Aug/15 19:31",
        "Description": "CURATOR-248 adds new pluggable error handlers. Apply this to all recipes.",
        "Issue Links": [
            "/jira/browse/CURATOR-248"
        ]
    },
    "CURATOR-250": {
        "Key": "CURATOR-250",
        "Summary": "CURATOR-224 caused regression of CURATOR-56",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Aug/15 18:14",
        "Updated": "21/Aug/15 18:17",
        "Resolved": "21/Aug/15 18:17",
        "Description": "Since CURATOR-224, testRetryAfterFailure_Curator56() has been failing.",
        "Issue Links": [
            "/jira/browse/CURATOR-231"
        ]
    },
    "CURATOR-251": {
        "Key": "CURATOR-251",
        "Summary": "TestResetConnectionWithBackgroundFailure::testConnectionStateListener failing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "22/Aug/15 04:05",
        "Updated": "08/Sep/15 18:45",
        "Resolved": "08/Sep/15 18:45",
        "Description": "https://builds.apache.org/job/Curator-3.0/1/org.apache.curator$curator-recipes/testReport/org.apache.curator.framework.client/TestResetConnectionWithBackgroundFailure/testConnectionStateListener/\n\njava.lang.AssertionError: expected [-CONNECTED-SUSPENDED-LOST-RECONNECTED-SUSPENDED-LOST] but found [-CONNECTED-SUSPENDED-LOST-RECONNECTED-SUSPENDED]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertEquals(Assert.java:123)\n\tat org.testng.Assert.assertEquals(Assert.java:176)\n\tat org.testng.Assert.assertEquals(Assert.java:186)\n\tat org.apache.curator.framework.client.TestResetConnectionWithBackgroundFailure.testConnectionStateListener(TestResetConnectionWithBackgroundFailure.java:103)",
        "Issue Links": []
    },
    "CURATOR-252": {
        "Key": "CURATOR-252",
        "Summary": "TestLeaderSelectorEdges::createProtectedNodeInBackgroundTest failed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mike Drob",
        "Created": "22/Aug/15 04:10",
        "Updated": "08/Sep/15 18:42",
        "Resolved": "08/Sep/15 18:42",
        "Description": "Running the full test suite locally, this test failed for me ~20% of the time.\n\njava.lang.AssertionError: Callback has not been called expected [true] but found [false]\n        at org.testng.Assert.fail(Assert.java:94)\n        at org.testng.Assert.failNotEquals(Assert.java:494)\n        at org.testng.Assert.assertTrue(Assert.java:42)\n        at org.apache.curator.framework.recipes.leader.TestLeaderSelectorEdges.createProtectedNodeInBackgroundTest(TestLeaderSelectorEdges.java:189)\n\n\nUnfortunately, I do not have full logs available at this time.",
        "Issue Links": []
    },
    "CURATOR-253": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Use new injectSessionExpiration() in the test module",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "24/Aug/15 02:24",
        "Updated": "06/Sep/15 20:32",
        "Resolved": "06/Sep/15 19:51",
        "Description": "ZK 3.5.0 adds the Testable interface and the injectSessionExpiration() method. Curator's KillSession utils should use this instead of what's currently done.",
        "Issue Links": [
            "/jira/browse/CURATOR-260"
        ]
    },
    "CURATOR-254": {
        "Key": "CURATOR-254",
        "Summary": "TestBoundedDistributedQueue:testMulti fails intermittently",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "Cam McKenzie",
        "Created": "26/Aug/15 00:44",
        "Updated": "31/Aug/15 23:31",
        "Resolved": "31/Aug/15 23:30",
        "Description": "The TestBoundedDistributedQueue:testMulti fails intermittently. See attached test report.",
        "Issue Links": []
    },
    "CURATOR-255": {
        "Key": "CURATOR-255",
        "Summary": "Creating CuratorFrameworkFactory.builder with empty connect string does not fail",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Framework",
        "Assignee": "Fangmin Lv",
        "Reporter": "Harald Musum",
        "Created": "27/Aug/15 08:46",
        "Updated": "11/Mar/17 23:52",
        "Resolved": "11/Mar/17 23:52",
        "Description": "Creating CuratorFrameworkFactory.builder with empty connect string does not fail, it just logs an error. If you do an operation after this, it will retry when\nconnection fails and not throw an exception, although the client will never be able to recover.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/203"
        ]
    },
    "CURATOR-256": {
        "Key": "CURATOR-256",
        "Summary": "Provide curator artifact in Maven Central with Guava shaded away",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Robert Metzger",
        "Created": "31/Aug/15 11:23",
        "Updated": "08/Jan/17 01:04",
        "Resolved": "08/Jan/17 01:04",
        "Description": "Guava is used by a lot of projects, often leading to version conflicts (Guava releases are incompatible).\nA common solution to resolve this issue is to use the \"maven-shade-plugin\" and pack the (Apache licensed) guava classes into the curator jars.\nThe Flink project is currently adding a flink-shaded-curator Maven module which is shading Curator's Guava away, so that it does not affect our users or code: https://github.com/apache/flink/pull/1076/files\nIt would be great if Curator would directly provide an artifact in Maven central without a (visible) Guava dependency.\nIf the project agrees to add this feature, I'm willing to provide a patch/pull request for this.",
        "Issue Links": [
            "/jira/browse/CURATOR-200"
        ]
    },
    "CURATOR-257": {
        "Key": "CURATOR-257",
        "Summary": "Thread.sleep in TestingZooKeeperMain.blockUntilStarted is costly for unit tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Martin Serrano",
        "Created": "01/Sep/15 03:03",
        "Updated": "09/Sep/15 13:31",
        "Resolved": "09/Sep/15 13:31",
        "Description": "The TestingZooKeeperMain.blockUntilStarted() contains a Thread.sleep(1000) call.  In a large battery of unit level tests (which are otherwise quick and depend on curator service discovery) this time adds up.\nRecent communication from JZ regarding the code:\n\nAs I recall, it takes some time for the server to start up and this was a hack to make sure it\u2019s ready.  However, I no longer remember the details. Do tests work with the timeout removed?\nWe are in the midst of running our battery of tests to see if removal of this sleep call causes any issues.   Our set of tests starts and stops the testing server a few hundred times within the same process, so I think it will show any such issues within a few runs.  If no issues appear (and the curator tests pass of course) I will post a pull request.  \nA sleep of this sort is unreliable to ensure startup anyway.  While it may be very unlikely for the server not to be up after 1 second, I've found with similar approaches that these types of solutions will still fail once it a while, leading to odd and hard to reproduce test failures.",
        "Issue Links": [
            "/jira/browse/CURATOR-245"
        ]
    },
    "CURATOR-258": {
        "Key": "CURATOR-258",
        "Summary": "PersistentEphemeralNode stops watching after first WatchedEvent",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.8.0,                                            2.9.0,                                            2.9.1",
        "Fix Version/s": "3.0.0,                                            2.9.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Njal Karevoll",
        "Created": "02/Sep/15 18:04",
        "Updated": "11/Oct/15 20:51",
        "Resolved": "11/Oct/15 20:51",
        "Description": "The `Watcher` in the `PersistentEphemeralNode` recipe only reacts to the `NodeDeleted` event. But it's possible to receive a `NodeDataChanged` event as well due to\n 1. The node has been created before the recipe has been started, so the first event it receives is it's own update.\n 2. The node has been created by another session, and the recipe accepts updating nodes belonging to a different session. In this case, it also receives it's own update in the `Watcher`, and no longer watches the node when the `NodeDeleted` event comes through (after the session of the other client expires). This is easily reproducible by restarting a service using a the recipe when the session timeout is higher than the service restart time.",
        "Issue Links": []
    },
    "CURATOR-259": {
        "Key": "CURATOR-259",
        "Summary": "Add try-with-resources class for Curator locks",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "05/Sep/15 21:21",
        "Updated": "08/Mar/17 12:43",
        "Resolved": "07/Sep/15 06:03",
        "Description": "For Java7+, try-with-resources makes using locks safer. Curator should have a utility for this. Something like:\n\npublic class SafeLock implements AutoCloseable {\n  private final InterProcessLock lock;\n  private final boolean acquired;\n  public SafeLock(InterProcessLock lock, long timeout, TimeUnit unit) {\n        this.lock = lock;\n        acquired = lock.acquire(timeout, unit);\n  }\n\n  public void close() throws Exception {\n      if ( acquired ) {\n           lock.release();\n      }\n   }\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/103"
        ]
    },
    "CURATOR-260": {
        "Key": "CURATOR-260",
        "Summary": "Fix tests in CURATOR-3.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Sep/15 20:32",
        "Updated": "09/Sep/15 00:23",
        "Resolved": "09/Sep/15 00:23",
        "Description": "TestSessionFailRetryLoop fails intermittently (needed some sleep due to new speed of KillSession)\nTests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.748 sec <<< FAILURE! - in org.apache.curator.TestSessionFailRetryLoop\ntestBasic(org.apache.curator.TestSessionFailRetryLoop)  Time elapsed: 0.182 sec  <<< FAILURE!\njava.lang.AssertionError: null\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.fail(Assert.java:101)\n\tat org.apache.curator.TestSessionFailRetryLoop.testBasic(TestSessionFailRetryLoop.java:216)\n\ntestKillSession (killed sessions go directly to LOST)\ntestKillSession(org.apache.curator.framework.imps.TestEnabledSessionExpiredState)  Time elapsed: 0.097 sec  <<< FAILURE!\njava.lang.AssertionError: expected [SUSPENDED] but found [LOST]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertEquals(Assert.java:123)\n\tat org.testng.Assert.assertEquals(Assert.java:165)\n\tat org.apache.curator.framework.imps.TestEnabledSessionExpiredState.testKillSession(TestEnabledSessionExpiredState.java:127)\n\nTestFramework.teardown (ignore any shutdown errors)\nTests run: 173, Failures: 1, Errors: 0, Skipped: 171, Time elapsed: 2.676 sec <<< FAILURE! - in org.apache.curator.framework.imps.TestFramework\nteardown(org.apache.curator.framework.imps.TestFramework)  Time elapsed: 0.023 sec  <<< FAILURE!\njava.lang.NullPointerException: null\n\tat org.apache.zookeeper.server.ZooKeeperServerMain.shutdown(ZooKeeperServerMain.java:147)\n\tat org.apache.curator.test.TestingZooKeeperMain.close(TestingZooKeeperMain.java:122)\n\tat org.apache.curator.test.TestingZooKeeperServer.stop(TestingZooKeeperServer.java:110)\n\tat org.apache.curator.test.TestingZooKeeperServer.close(TestingZooKeeperServer.java:122)\n\tat org.apache.curator.test.TestingServer.close(TestingServer.java:183)\n\tat org.apache.curator.test.BaseClassForTests.teardown(BaseClassForTests.java:140)\n\tat org.apache.curator.framework.imps.TestFramework.teardown(TestFramework.java:71)\n\nTestEnabledSessionExpiredState.testKillSession\nFailed tests: \n  TestEnabledSessionExpiredState.testKillSession:127 expected [SUSPENDED] but found [LOST]\n  TestFramework.teardown:71->BaseClassForTests.teardown:140 \u00bb NullPointer\n\nflappingTest (ChaosMonkeyCnxnFactory didn't know about create2 opcode)\nTests run: 6, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 79.297 sec <<< FAILURE! - in org.apache.curator.framework.recipes.leader.TestLeaderSelectorEdges\nflappingTest(org.apache.curator.framework.recipes.leader.TestLeaderSelectorEdges)  Time elapsed: 10.278 sec  <<< FAILURE!\njava.lang.AssertionError: Connection has not been lost expected [true] but found [false]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertTrue(Assert.java:42)\n\tat org.apache.curator.framework.recipes.leader.TestLeaderSelectorEdges.flappingTest(TestLeaderSelectorEdges.java:88)\n\nTestPersistentEphemeralNode (sleep a bit so that session can actually expire)\nTests run: 40, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 312.767 sec <<< FAILURE! - in org.apache.curator.framework.recipes.nodes.TestPersistentEphemeralNode\ntestRecreatesNodeWhenSessionReconnectsMultipleTimes(org.apache.curator.framework.recipes.nodes.TestPersistentEphemeralNode)  Time elapsed: 85.994 sec  <<< FAILURE!\njava.lang.AssertionError: expected [true] but found [false]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertTrue(Assert.java:42)\n\tat org.testng.Assert.assertTrue(Assert.java:52)\n\tat org.apache.curator.framework.recipes.nodes.TestPersistentEphemeralNode.testRecreatesNodeWhenSessionReconnectsMultipleTimes(TestPersistentEphemeralNode.java:382)\n\ntestCallbackNotifyLeader (seems intermittent - watch it)\ntestCallbackNotifyLeader(org.apache.curator.framework.recipes.leader.TestLeaderLatch)  Time elapsed: 0.053 sec  <<< FAILURE!\njava.lang.AssertionError: expected [17] but found [16]\n\tat org.testng.Assert.fail(Assert.java:94)\n\tat org.testng.Assert.failNotEquals(Assert.java:494)\n\tat org.testng.Assert.assertEquals(Assert.java:123)\n\tat org.testng.Assert.assertEquals(Assert.java:265)\n\tat org.testng.Assert.assertEquals(Assert.java:275)\n\tat org.apache.curator.framework.recipes.leader.TestLeaderLatch.testCallbackNotifyLeader(TestLeaderLatch.java:658)",
        "Issue Links": [
            "/jira/browse/CURATOR-253"
        ]
    },
    "CURATOR-261": {
        "Key": "CURATOR-261",
        "Summary": "ZooKeeper 3.5.x has enough APIs so that Javassist can be removed",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "10/Sep/15 01:12",
        "Updated": "26/Sep/15 00:19",
        "Resolved": "26/Sep/15 00:19",
        "Description": "The test module has used Javassist to fix unit-test-incompatible portions of ZK server. With ZK 3.5.x it's possible to eliminate this.",
        "Issue Links": []
    },
    "CURATOR-262": {
        "Key": "CURATOR-262",
        "Summary": "Issue with ExponentialBackoffRetry",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "Dimitar Dyankov",
        "Created": "12/Sep/15 20:08",
        "Updated": "24/May/16 22:34",
        "Resolved": "24/May/16 22:34",
        "Description": "Hi,\nLooking at the ExponentialBackOff Strategy for the apache curator I found this issue :\n    @Override\n    protected int getSleepTimeMs(int retryCount, long elapsedTimeMs)\n    {\n        // copied from Hadoop's RetryPolicies.java\n        int sleepMs = baseSleepTimeMs * Math.max(1, random.nextInt(1 << (retryCount + 1)));\n        if ( sleepMs > maxSleepMs )\n        {\n            log.warn(String.format(\"Sleep extension too large (%d). Pinning to %d\", sleepMs, maxSleepMs));\n            sleepMs = maxSleepMs;\n        }\n        return sleepMs;\n    }\nsince sleepMs is an Integer and retryCount could be as large as 29 we could fall into an integer overflow case and therefore sleepMs being a negative number.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/136"
        ]
    },
    "CURATOR-263": {
        "Key": "CURATOR-263",
        "Summary": "Update to Guava 18",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Andrew Winterman",
        "Created": "16/Sep/15 21:32",
        "Updated": "16/Feb/17 13:33",
        "Resolved": "08/Jan/17 01:05",
        "Description": "We just saw dependency conflicts between guava 18 and curator in our internal rpc protocol (which talks to zookeeper via curator for storing system state). I'd like to upgrade Guava to version 18.0 in curator.",
        "Issue Links": [
            "/jira/browse/CURATOR-370",
            "/jira/browse/CURATOR-200"
        ]
    },
    "CURATOR-264": {
        "Key": "CURATOR-264",
        "Summary": "Leader election: Duplicate ephemeral nodes with same owner id",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "2.9.1",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ole Hjalmar Herje",
        "Created": "22/Sep/15 06:57",
        "Updated": "23/Sep/15 13:22",
        "Resolved": "23/Sep/15 13:22",
        "Description": "We sometimes experience failure in our leader-election functionality when we have network issues. When this situation occurs we see that there are two ephemeral nodes in the zookeeper cluster for the same session but there is no active leader. \nI have managed to recreate the same scenario by running a test locally and use iptables to simulate network issues. The debug log (see attachment) shows that findAndDeleteProtectedNodeInBackground does not delete the node because processResult in FindProtectedNodeCB receives a -101 (NoNode) resultcode. I suspect this can happen if the read is not synched? (http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkGuarantees)\nThis also seems to be related to: \nhttps://issues.apache.org/jira/browse/CURATOR-45 and\nhttps://issues.apache.org/jira/browse/CURATOR-79",
        "Issue Links": []
    },
    "CURATOR-265": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Dynamic Reconfig APIs needs some work and more accurate testing",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "25/Sep/15 23:32",
        "Updated": "06/Sep/22 13:35",
        "Resolved": "09/Oct/15 00:12",
        "Description": "The new dynamic reconfig APIs don't have all permutations and the tests are flakey.",
        "Issue Links": [
            "/jira/browse/CURATOR-266"
        ]
    },
    "CURATOR-266": {
        "Key": "CURATOR-159 Parent Task for ZooKeeper 3.5.0 support",
        "Summary": "Support updateServerList",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Sep/15 17:19",
        "Updated": "14/Oct/15 17:32",
        "Resolved": "09/Oct/15 00:12",
        "Description": "ZK 3.5 adds updateServerList(). This should be exposed in the CuratorFramework interface. Also, Curator should watch for Config changes and update the server list when it happens",
        "Issue Links": [
            "/jira/browse/CURATOR-265"
        ]
    },
    "CURATOR-267": {
        "Key": "CURATOR-267",
        "Summary": "A group membership recipe would be useful",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "2.9.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "27/Sep/15 21:59",
        "Updated": "16/Oct/15 18:41",
        "Resolved": "16/Oct/15 18:41",
        "Description": "ZooKeeper already supports groups via ephemeral nodes. But, Curator's PersistentEphemeralNode is more complete. Combined with a Cache, PersistentEphemeralNode makes for a good group membership recipe.",
        "Issue Links": []
    },
    "CURATOR-268": {
        "Key": "CURATOR-268",
        "Summary": "Curator client doesn't behave well when it loses connection: CRUD operation fail",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.7.1,                                            2.9.0",
        "Fix Version/s": "3.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "01/Oct/15 22:54",
        "Updated": "09/Aug/16 11:13",
        "Resolved": "06/Oct/15 01:43",
        "Description": "I am doing a basic stress test :\n\na TestingServer that keeps restarting in one thread\na Curator client that keeps creating/deleting a node in another\n\nAfter a few seconds, ether the create or the delete fail:\n\nThu Oct 01 15:35:10 PDT 2015 - Node /test has successfully been removed.\nThu Oct 01 15:35:10 PDT 2015 - Recreating /test\nThu Oct 01 15:35:10 PDT 2015 - Restarting server...\nThu Oct 01 15:35:14 PDT 2015 - Restarting server...\nThu Oct 01 15:35:15 PDT 2015 - ERROR: node should be removed.\nThu Oct 01 15:35:15 PDT 2015 - Data of node is: test\nNode has been mysteriously created...\norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /test\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:123)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1209)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:720)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:703)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:700)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:477)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:467)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat TestCuratorSaveConnLoss$3.run(TestCuratorSaveConnLoss.java:87)\n\nThe create shouldn't fail, we're within 5 seconds of the create and the connection timeout is 10 secs.\nThis is surprising as this basic scenario is - AFAIK - the reason Curator exists in the first place.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-2288",
            "/jira/browse/CURATOR-341"
        ]
    },
    "CURATOR-269": {
        "Key": "CURATOR-269",
        "Summary": "InterProcessSemaphoreV2: decrease level of logging when path not found",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Natalia Gorchakova",
        "Created": "03/Oct/15 14:47",
        "Updated": "10/May/23 14:37",
        "Resolved": null,
        "Description": "line 362\nlog.error(\"Sequential path not found: \" + path);\nError level is not required here, as the exception handled by caller",
        "Issue Links": []
    },
    "CURATOR-270": {
        "Key": "CURATOR-270",
        "Summary": "createContainers does not work correctly with usingNamespace",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "2.9.1",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Alexey Serba",
        "Created": "05/Oct/15 21:10",
        "Updated": "06/Oct/15 13:23",
        "Resolved": "06/Oct/15 13:23",
        "Description": "I just recently upgraded my project to 2.9.0 and noticed that PathChildrenCache recipe started failing with\n\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /parent\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214) ~[curator-framework-2.9.0.jar:?]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) ~[curator-framework-2.9.0.jar:?]\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.9.0.jar:?]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200) ~[curator-framework-2.9.0.jar:?]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) ~[curator-framework-2.9.0.jar:?]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) ~[curator-framework-2.9.0.jar:?]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.rebuild(PathChildrenCache.java:326) ~[curator-recipes-2.9.0.jar:?]\n        at org.apache.curator.framework.recipes.cache.PathChildrenCache.start(PathChildrenCache.java:299) ~[curator-recipes-2.9.0.jar:?]\n\n\nIt turns out that PathChildrenCache recently started using new implementation createContainers for creating parent directories/znodes and this method has a bug in NamespaceFacade.createContainers implementation, because it does not account for namespace name.\nI'll create a PR with a test and fix shortly.",
        "Issue Links": []
    },
    "CURATOR-271": {
        "Key": "CURATOR-271",
        "Summary": "Curator does not run sync callback on supplied Executor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.9.1,                                            3.1.0",
        "Component/s": "Framework",
        "Assignee": "Cam McKenzie",
        "Reporter": "Glen Wallace",
        "Created": "13/Oct/15 20:14",
        "Updated": "15/Oct/15 02:30",
        "Resolved": "15/Oct/15 02:24",
        "Description": "It appears that Apache Curator does not execute the sync callback on the supplied Executor in at least one case.\nThis issue described is currently (2015-10-14) present in master\nSample code outline:\n\nBackgroundCallback myCallback = ...;\nExecutor myExecutor = ...;\nString myZkPath = ...;\nCuratorFramework myCurator = ...;\ncurator.sync()\n  .inBackground(myCallback, myExecutor)\n  .forPath(myZkPath);\n\n\nThis should execute myCallback on the myExecutor executor, it does not and instead executes myCallback on what is presumeably a ZooKeeper thread.\nLooking at the org.apache.curator.framework.imps.SyncBuilderImpl code we have\n\npublic Pathable inBackground(BackgroundCallback callback, Executor executor) {\n  backgrounding = new Backgrounding(callback, executor);\n  return this;\n}\n\n\nHowever the only matching constructor in org.apache.curator.framework.imps.Backgrounding is Backgrounding(BackgroundCallback callback, Object context)\nIt seems that SyncBuilderImpl#inBackground(BackgroundCallback,Executor) should be using the following Backgrounding constructor Backgrounding(CuratorFrameworkImpl, BackgroundCallback, Executor)\nA workaround looks to be using the following method and passing a null context object:\n\n  public Pathable inBackground(BackgroundCallback callback, Object context, Executor executor)\n\n\nA very quick skim suggests that it is only SyncBuilderImpl that has the issue the other org.apache.curator.framework.imps.*BuilderImpl classes seem to be fine.",
        "Issue Links": []
    },
    "CURATOR-272": {
        "Key": "CURATOR-272",
        "Summary": "Spamming of  Background operation retry gave up",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "MatjazCuk",
        "Created": "15/Oct/15 07:42",
        "Updated": "06/Mar/18 19:22",
        "Resolved": "06/Mar/18 02:39",
        "Description": "Hi,\nwe are having problems with curator when disconnect happens. It starts to log insane amounts of logs:\n2015-10-14 16:58:54,511 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,511 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,511 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,512 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,513 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,514 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,514 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,524 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,524 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,526 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,527 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,528 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,535 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,536 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,537 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,538 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,547 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:166) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:593) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\n2015-10-14 16:58:54,548 [Thread-3-EventThread] [ERROR] [o.a.c.f.i.CuratorFrameworkImpl] - Background operation retry gave up\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.checkBackgroundRetry(CuratorFrameworkImpl.java:723) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:514) [curator-framework-2.9.0.jar:na]\n        at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254) [curator-framework-2.9.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564) [zookeeper-3.4.6.jar:3.4.6-1569965]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]\nSo couple per millisecond. This quickly amounts to huge number of logs. \nThis happens only when using TreeCache. Attached some parts of our code.",
        "Issue Links": []
    },
    "CURATOR-273": {
        "Key": "CURATOR-273",
        "Summary": "Namespaces cannot start with /",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Tom Dyas",
        "Created": "20/Oct/15 19:38",
        "Updated": "28/Apr/23 13:54",
        "Resolved": "28/Apr/23 13:54",
        "Description": "Not so much of a bug as an API quirk: It is an error for namespaces to start with a / since Curator will always blindly append a / in front of the namespace. For example, see `curator-framework/src/main/java/org/apache/curator/framework/imps/NamespaceImpl.java`. Any reason why namespaces starting with a / are not supported? This requires application code to know whether the namespace starts with a / or not.",
        "Issue Links": []
    },
    "CURATOR-274": {
        "Key": "CURATOR-274",
        "Summary": "version 2.9.0 incorrectly calls mkdirs to create container nodes, when running against zk 3.4.6",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.0",
        "Fix Version/s": "2.9.1,                                            3.1.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jason Rosenberg",
        "Created": "21/Oct/15 05:17",
        "Updated": "22/Oct/15 22:57",
        "Resolved": "22/Oct/15 22:57",
        "Description": "I recently attempted to upgrade to use curator 2.9.0 (we had been using 2.7.0). We are using zookeeper 3.4.6.\nWe use the PathChildrenCache, to watch nodes, that can be deleted.  Our tests fail with 2.9.0 because after we delete a node, the PathChildrenCache immediately re-creates the parent directories for the deleted node.\nIn the logs, we see:\n\n2015-10-21 04:51:39,642  WARN [path-cache /parent1/parent2/parent3-0] utils.ZKPaths - The version of ZooKeeper being used doesn't support Container nodes. CreateMode.PERSISTENT will be used instead.\n\n\nUnfortunately, this results in persistent nodes being created.  But since container mode is not available in 3.4.6, it should not have attempted to create the container nodes in the first place.\nThe behavior starts with PathChildrenCache.refresh(), with call sequence below, which happens after the node is deleted:\n\nPathChildrenCache.refresh(); -> \nPathChildrenCache.ensurePath(); ->\nPathChildrenCache.client.createContainers(path); ->\nCuratorFrameworkImpl.checkExists().creatingParentContainersIfNeeded().forPath(ZKPaths.makePath(path, \"foo\")); ->\n...\nExistsBulderImpl.ZKPaths.mkdirs(client.getZooKeeper(), parent, true, client.getAclProvider(), true);\n\n\nIt seems to be rather unexpected behavior to default to creating persistent nodes if container nodes aren't available.  Instead, if container nodes are not available, the behavior should be to do nothing (this is the behavior we've been living with 'til now).\nThis is a Blocker, because it prevents upgrading, for anyone who might delete a node watched by a PathChildrenCache.\nI did not look to see if a similar issue might exist in the other cache recipes, etc.",
        "Issue Links": []
    },
    "CURATOR-275": {
        "Key": "CURATOR-275",
        "Summary": "Allow service instances to be disabled temporarily",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Joe Littlejohn",
        "Created": "02/Nov/15 22:44",
        "Updated": "25/Mar/17 00:26",
        "Resolved": "24/Jan/17 02:08",
        "Description": "Allow service instances to be temporarily removed from discovery (when using the Service Discovery recipe). \nThis can be achieved with a new 'enabled' flag that is included as part of the service instance node data in Zookeeper. Updating this flag via e.g. Exhibitor will allow an instance to be disabled. When the flag is omitted, the service is assumed to be enabled, hence this change is backward compatible.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/116",
            "https://github.com/apache/curator/pull/208"
        ]
    },
    "CURATOR-276": {
        "Key": "CURATOR-276",
        "Summary": "Downgrade pattern doesn't work with \"Shared Reentrant Read Write Lock\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Maxim Sidnin",
        "Created": "04/Nov/15 08:21",
        "Updated": "28/Apr/23 13:15",
        "Resolved": "28/Apr/23 13:15",
        "Description": "Issue: Downgrade pattern doesn't block already existed WRITE lock waiters.\nScenario:\n\nFirst thread takes a WRITE lock and waits\nSecond thread starts and it is trying to obtain a WRITE lock. It waits because the WRITE lock is hold by the first thread.\nFirst thread does downgrade:\n\t\nTakes the READ lock\nReleases the WRITE lock\n\n\n\n At this point the first thread still holds the READ lock but the second one will get the WRITE lock. \nIt is reproducible for:\n\n2 threads use the same CuratorFramework instance\n2 threads use the different sessions\n2 separate JVMs\n\n There is attached test class: DowngradeTest.java and zoo.cfg\n Quote from Shared Reentrant Read Write Lock description\nhttp://curator.apache.org/curator-recipes/shared-reentrant-read-write-lock.html\n\nLock Downgrading Re-entrancy also allows downgrading from the write lock to a read lock, by acquiring the write lock, then the read lock and then releasing the write lock. However, upgrading from a read lock to the write lock is not possible.",
        "Issue Links": [
            "/jira/browse/CURATOR-621"
        ]
    },
    "CURATOR-277": {
        "Key": "CURATOR-277",
        "Summary": "Fluent API doesn't support  compressed option with some fluent api combinations during create",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Grant Henke",
        "Created": "04/Nov/15 21:06",
        "Updated": "04/Nov/15 21:23",
        "Resolved": "04/Nov/15 21:23",
        "Description": "client.create().compressed().creatingParentsIfNeeded() will not compile. I can't find a way to use both options together.\nNote: client.create().compressed() and _client.create().creatingParentsIfNeeded() do work independently",
        "Issue Links": []
    },
    "CURATOR-278": {
        "Key": "CURATOR-278",
        "Summary": "In transactions compressed option does not work with some fluent api combinations",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "2.10.0",
        "Component/s": "Framework",
        "Assignee": "Cam McKenzie",
        "Reporter": "Grant Henke",
        "Created": "04/Nov/15 21:17",
        "Updated": "18/Jan/16 00:13",
        "Resolved": "18/Jan/16 00:13",
        "Description": "I can't find a way to use compression with the following options in transactions. These do not compile:\n\ntransaction.setData().compressed().withVersion(1).forPath(\"/path\")\ntransaction.create().compressed().withMode(CreateMode.PERSISTENT).forPath(\"path\")\ntransaction.create().compressed().withMode(CreateMode.PERSISTENT).withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE).forPath(\"path\")",
        "Issue Links": []
    },
    "CURATOR-279": {
        "Key": "CURATOR-279",
        "Summary": "Semaphore Leases should return the node name",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.9.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "05/Nov/15 21:00",
        "Updated": "08/Jan/17 16:11",
        "Resolved": "08/Jan/17 16:11",
        "Description": "It would be useful if InterProcessSemaphore Leases included the node name",
        "Issue Links": [
            "https://github.com/apache/curator/pull/191"
        ]
    },
    "CURATOR-280": {
        "Key": "CURATOR-280",
        "Summary": "LeaderLatch doesn't work when using a zookeeper chroot",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.9.0,                                            2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Vincent Bernat",
        "Created": "10/Nov/15 11:00",
        "Updated": "10/May/23 10:40",
        "Resolved": "10/May/23 10:40",
        "Description": "Hey!\nWhen using a ZK connection-string with a chroot (for example localhost:2181/chroot), the leader election by LeaderLatch doesn't work. This may be similar to CURATOR-270. If I query .getParticipants, I get:\n\n      Actual: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /test4\n              org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n              org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n              org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)\n              org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)\n              org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)\n              org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n              org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)\n              org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)\n              org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)\n              org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:150)\n              org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:132)\n              org.apache.curator.framework.recipes.leader.LeaderLatch.getParticipants(LeaderLatch.java:430)",
        "Issue Links": [
            "/jira/browse/CURATOR-357",
            "/jira/browse/CURATOR-357"
        ]
    },
    "CURATOR-281": {
        "Key": "CURATOR-281",
        "Summary": "Usage of TestingServer fails when upgrading to 3.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.1.0",
        "Component/s": "Tests",
        "Assignee": "Mike Drob",
        "Reporter": "Grant Henke",
        "Created": "13/Nov/15 04:39",
        "Updated": "27/Jan/17 18:55",
        "Resolved": "17/Nov/15 19:40",
        "Description": "If I upgrade from 2.9.2 to 3.0 and run my existing unit tests I get this error on all tests:\n\norg.scurator.TestGetChildren > initializationError FAILED\n    java.lang.NoClassDefFoundError: org/testng/Assert\n        at org.apache.curator.test.TestingZooKeeperMain.blockUntilStarted(TestingZooKeeperMain.java:135)\n        at org.apache.curator.test.TestingZooKeeperServer.start(TestingZooKeeperServer.java:159)\n        at org.apache.curator.test.TestingServer.<init>(TestingServer.java:117)\n        at org.apache.curator.test.TestingServer.<init>(TestingServer.java:100)\n        at org.apache.curator.test.TestingServer.<init>(TestingServer.java:41)\n        at org.scurator.BaseSCuratorTest$class.beforeEach(BaseSCuratorTest.scala:33)\n        at org.scurator.TestGetChildren.org$scurator$SCuratorTestClient$$super$beforeEach(TestGetChildren.scala:14)\n        at org.scurator.SCuratorTestClient$class.beforeEach(SCuratorTestClient.scala:20)\n        at org.scurator.TestGetChildren.beforeEach(TestGetChildren.scala:14)\n        at org.scalatest.BeforeAndAfterEach$class.beforeEach(BeforeAndAfterEach.scala:154)\n        at org.scurator.TestGetChildren.beforeEach(TestGetChildren.scala:14)\n        at org.scalatest.BeforeAndAfterEach$class.beforeEach(BeforeAndAfterEach.scala:173)\n        at org.scurator.TestGetChildren.beforeEach(TestGetChildren.scala:14)\n        at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:253)\n        at org.scurator.TestGetChildren.runTest(TestGetChildren.scala:14)\n\n        Caused by:\n        java.lang.ClassNotFoundException: org.testng.Assert\n            at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n            at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n            at java.security.AccessController.doPrivileged(Native Method)\n            at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n            at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n            at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n            at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n            ... 15 more\n\n\nThe code it points to is here: \nhttps://github.com/apache/curator/blob/apache-curator-3.0.0/curator-test/src/main/java/org/apache/curator/test/TestingZooKeeperMain.java#L135\nIts caused because I am an external user from the project and testng is marked as provided here:\nhttps://github.com/apache/curator/blob/apache-curator-3.0.0/curator-test/pom.xml#L53-L57\nThe only other place testng is used is BaseClassForTests.java, which makes sense. I propose using a standard java assertion here to avoid the dependency for those using TestingServer.",
        "Issue Links": []
    },
    "CURATOR-282": {
        "Key": "CURATOR-282",
        "Summary": "In consist behavior for create EPHEMERAL_SEQUENTIAL with usingNamespace",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "\u9648\u6768\u6587",
        "Created": "26/Nov/15 13:11",
        "Updated": "26/Nov/15 13:11",
        "Resolved": null,
        "Description": "Here is a junit test write in groovy.\n\nimport org.apache.curator.framework.CuratorFramework\nimport org.apache.curator.framework.CuratorFrameworkFactory\nimport org.apache.curator.retry.RetryOneTime\nimport org.apache.curator.test.TestingServer\nimport org.apache.zookeeper.CreateMode\nimport org.junit.After\nimport org.junit.Before\nimport org.junit.Test\n\npublic class PlayZK {\n    private TestingServer server\n    private CuratorFramework client\n\n    @Before\n    public void setup() {\n        server = new TestingServer(8788)\n        server.start();\n        println \"ConnectString: \" + server.getConnectString()\n        client = CuratorFrameworkFactory.newClient(server.getConnectString(), new RetryOneTime(8000))\n        client.start()\n    }\n\n    @Test\n    public void issues() {\n        println \"Create \" + client.create().creatingParentContainersIfNeeded().forPath(\"/test/data/sub\");\n        println \"Create \" + client.create().creatingParentContainersIfNeeded().forPath(\"/sub\");\n        println \"Create sub \" + client.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(\"/sub/\")\n        println \"Create sub \" + client.usingNamespace(\"test/data\").create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(\"/sub/\")\n\n        println \"Expected sub\"\n        client.children.forPath(\"/test/data\").each {println it}\n        println \"Expected 0000000000\"\n        client.children.forPath(\"/test/data/sub\").each {println it}\n        println \"Expected 0000000000\"\n        client.children.forPath(\"/sub\").each {println it}\n    }\n\n    @After\n    public void down() {\n        server.stop()\n    }\n}\n\n\n\n// will create /test/data/sub0000000001\nclient.usingNamespace(\"test/data\").create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(\"/sub/\") \n\n// will create /test/data/sub/0000000000\nclient.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(\"/sub/\")\n\n\nWhich one is expected ?",
        "Issue Links": []
    },
    "CURATOR-283": {
        "Key": "CURATOR-283",
        "Summary": "LockInternals does not deal with spurious weakup?",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "csulyj",
        "Created": "22/Dec/15 15:14",
        "Updated": "23/Dec/15 13:41",
        "Resolved": "22/Dec/15 15:22",
        "Description": "I don't understand why method internalLockLoop doesn't not deal with spurious weakup?\nI think it should be like this : \n synchronized (obj) \n{\n\n     while (condition does not hold)\n\n        obj.wait(timeout);\n\n         ... // Perform action appropriate to condition\n\n }",
        "Issue Links": []
    },
    "CURATOR-284": {
        "Key": "CURATOR-284",
        "Summary": "NullPointerException at EnsembleTracker.processConfigData",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.1.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "G",
        "Created": "06/Jan/16 19:47",
        "Updated": "06/Oct/17 14:24",
        "Resolved": "06/Jan/16 20:54",
        "Description": "While using Zookeeper 3.5.1 with Curator 3.0.0, we hit this issue:\norg.apache.curator.framework.imps.CuratorFrameworkImpl: Background exception was not retry-able or retry gave up\n! java.lang.NullPointerException: null\n! at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n! at org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:163)\n! at org.apache.curator.framework.imps.EnsembleTracker.access$200(EnsembleTracker.java:48)\n! at org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:134)\n! at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:829)\n! at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:611)\n! at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:151)\n! at org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:210)\n! at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:619)\n! at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:528)",
        "Issue Links": [
            "/jira/browse/CURATOR-285"
        ]
    },
    "CURATOR-285": {
        "Key": "CURATOR-285",
        "Summary": "LeaderElection with 3.0.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Kunal Bharati",
        "Created": "08/Jan/16 10:07",
        "Updated": "06/Oct/17 14:24",
        "Resolved": "06/Oct/17 14:24",
        "Description": "I am running 3 instances of ZooKeeper and the config is this:\ntickTime=2000\ninitLimit=10\nsyncLimit=5\ndataDir=/tmp/zookeeper1\nclientPort=2181\nmaxClientCnxns=1000\nserver.1=127.0.0.1:2888:3888\nserver.2=127.0.0.1:2889:3889\nserver.3=127.0.0.1:2890:3890\nI am using the leader election example code given here: https://git-wip-us.apache.org/repos/asf?p=curator.git;a=tree;f=curator-examples/src/main/java/leader;h=73b547eadb98995c0ccbd06a5b76d0741ffef263;hb=HEAD\nThe code runs fine with TestingServer but when I change connection string to : \"127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183\", I get the exceptions:\n[main-SendThread(127.0.0.1:2183)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2183. Will not attempt to authenticate using SASL (unknown error)\n[main-SendThread(127.0.0.1:2183)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /127.0.0.1:56111, server: 127.0.0.1/127.0.0.1:2183\n[main-SendThread(127.0.0.1:2183)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server 127.0.0.1/127.0.0.1:2183, sessionid = 0x3521552283c0000, negotiated timeout = 40000\n[main-EventThread] INFO org.apache.curator.framework.state.ConnectionStateManager - State change: CONNECTED\n[main-SendThread(127.0.0.1:2183)] INFO org.apache.zookeeper.ClientCnxn - Unable to read additional data from server sessionid 0x3521552283c0000, likely server has closed socket, closing socket connection and attempting reconnect\n[main-EventThread] INFO org.apache.curator.framework.imps.EnsembleTracker - New config event received: null\n[main-EventThread] ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl - Background exception was not retry-able or retry gave up\njava.lang.NullPointerException\n    at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n    at org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:163)\n    at org.apache.curator.framework.imps.EnsembleTracker.access$200(EnsembleTracker.java:48)\n    at org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:134)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:829)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:611)\n    at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:151)\n    at org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:210)\n    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:619)\n    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:528)\n[main-EventThread] INFO org.apache.curator.framework.state.ConnectionStateManager - State change: SUSPENDED",
        "Issue Links": [
            "/jira/browse/CURATOR-284"
        ]
    },
    "CURATOR-286": {
        "Key": "CURATOR-286",
        "Summary": "Memory leak in service discovery",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "3.0.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Joe Littlejohn",
        "Created": "11/Jan/16 12:35",
        "Updated": "08/Feb/16 18:21",
        "Resolved": "11/Jan/16 17:59",
        "Description": "Hi\nI'm seeing a memory leak in my application which makes use of service discovery.\nI've taken heap dumps and I see:\n\nHundreds of thousands of NamespaceWatcher instances. The client, actualWatcher and curatorWatcher fields are all null, so these are closed NamespaceWatchers.\nThousands of PathChildrenCache instances. Each one has a 'path' value that refers to one of the services I'm lookup up (using a service provider). The state fields shows that all these PathChildrenCache instances are CLOSED.\n\nIn my application I'm using a service provider to get an instance, then closing that service provider. It seems that even after doing this, there's still a reference to the NamespaceWatcher held in ZKWatchManager field childWatches.",
        "Issue Links": [
            "/jira/browse/CURATOR-217"
        ]
    },
    "CURATOR-287": {
        "Key": "CURATOR-287",
        "Summary": "PersistentEphemeralNode should be generalized to accept all create modes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.9.1",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "12/Jan/16 16:11",
        "Updated": "19/Jan/16 02:06",
        "Resolved": "19/Jan/16 02:06",
        "Description": "With very little change, PersistentEphemeralNode could work with non-ephemeral nodes. There is a good use case for this: permanent nodes that must always exist with some data. It's actually a pain to do this manually with ZK.",
        "Issue Links": [
            "/jira/browse/CURATOR-25"
        ]
    },
    "CURATOR-288": {
        "Key": "CURATOR-288",
        "Summary": "Provide a mechanism to limit tree traversal in TreeCache + Return old values in NODE_REMOVED event",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.9.1",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "12/Jan/16 18:14",
        "Updated": "20/May/16 17:55",
        "Resolved": "20/May/16 17:55",
        "Description": "1. Currently, the only way to limit TreeCache's traversal is with maxDepth. It would be nice to also have a functional way of doing this. i.e. an policy object that is called to determine if a node should be traversed.\n2. NODE_REMOVED should return the old values of the node",
        "Issue Links": [
            "/jira/browse/CURATOR-145"
        ]
    },
    "CURATOR-289": {
        "Key": "CURATOR-289",
        "Summary": "Implement CURATOR-278 in Curator 3.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.1.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Cam McKenzie",
        "Created": "18/Jan/16 03:30",
        "Updated": "18/Jan/16 21:17",
        "Resolved": "18/Jan/16 21:17",
        "Description": "CURATOR-278 cannot be directly merged into the Curator 3.0 code base as the transaction API has been rewritten in Curator 3.0. The support for compressed transactions should be implemented in Curator 3.0.",
        "Issue Links": []
    },
    "CURATOR-290": {
        "Key": "CURATOR-290",
        "Summary": "Watcher removal is not removing internal Curator data structures",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.1.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "18/Jan/16 22:25",
        "Updated": "19/Jan/16 02:48",
        "Resolved": "18/Jan/16 23:12",
        "Description": "In the new RemoveWatchesBuilder APIs, the watchers should also be removed from Curator's NamespaceWatcherMap.",
        "Issue Links": [
            "/jira/browse/CURATOR-291",
            "/jira/browse/CURATOR-217"
        ]
    },
    "CURATOR-291": {
        "Key": "CURATOR-291",
        "Summary": "Fix numerous \"NamespaceWatcherMap is not empty\" errors in test",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "TBD",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "19/Jan/16 02:22",
        "Updated": "20/May/16 21:52",
        "Resolved": null,
        "Description": "Since enabled check for \"NamespaceWatcherMap is not empty\" numerous cases have shown up. Fix these.",
        "Issue Links": [
            "/jira/browse/CURATOR-290"
        ]
    },
    "CURATOR-292": {
        "Key": "CURATOR-292",
        "Summary": "ConnectionState did not reset the zookeeperclient when session expired",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "wudan",
        "Created": "21/Jan/16 08:25",
        "Updated": "21/Jan/16 08:42",
        "Resolved": null,
        "Description": "The log as follow:\n2016-01-21 09:50:11,133 INFO [ClientCnxn.java] - Client session timed out, have not heard from server in 6667ms for sessionid 0x151e7817be340be, closing socket connection and attempting reconnect\n2016-01-21 09:50:11,239 DEBUG [RetryLoop.java] - Retry-able exception received\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /dispatcher/list\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:1)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:1)\n\tat com.trs.dev3.bdc.util.ZKUtil.getChildren(ZKUtil.java:323)\n\tat com.trs.dev3.bdc.dispatcher.TimerHandler.checkDispatcher(TimerHandler.java:119)\n\tat com.trs.dev3.bdc.dispatcher.TimerHandler.run(TimerHandler.java:69)\n\tat java.lang.Thread.run(Unknown Source)\n2016-01-21 09:50:12,387 INFO [ClientCnxn.java] - Opening socket connection to server 192.168.51.153/192.168.51.153:2181. Will not attempt to authenticate using SASL (unknown error)\n2016-01-21 09:50:21,234 INFO [ClientCnxn.java] - Client session timed out, have not heard from server in 10000ms for sessionid 0x151e7817be340be, closing socket connection and attempting reconnect\n2016-01-21 09:50:21,236 DEBUG [ClientCnxnSocketNIO.java] - Ignoring exception during shutdown input\njava.net.SocketException: Socket is not connected\n\tat sun.nio.ch.Net.translateToSocketException(Unknown Source)\n\tat sun.nio.ch.Net.translateException(Unknown Source)\n\tat sun.nio.ch.Net.translateException(Unknown Source)\n\tat sun.nio.ch.SocketAdaptor.shutdownInput(Unknown Source)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.cleanup(ClientCnxnSocketNIO.java:200)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:1185)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1110)\nCaused by: java.nio.channels.NotYetConnectedException\n\tat sun.nio.ch.SocketChannelImpl.shutdownInput(Unknown Source)\n\t... 4 more\n2016-01-21 09:50:21,262 DEBUG [RetryLoop.java] - Retrying operation\n2016-01-21 09:50:21,345 DEBUG [RetryLoop.java] - Retry-able exception received\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /dispatcher/list\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:1)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:1)\n\tat com.trs.dev3.bdc.util.ZKUtil.getChildren(ZKUtil.java:323)\n\tat com.trs.dev3.bdc.dispatcher.TimerHandler.checkDispatcher(TimerHandler.java:119)\n\tat com.trs.dev3.bdc.dispatcher.TimerHandler.run(TimerHandler.java:69)\n\tat java.lang.Thread.run(Unknown Source)\n2016-01-21 09:50:21,345 DEBUG [RetryLoop.java] - Retry-able exception received\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /dispatcher/list\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:1)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)\n\tat org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:1)\n\tat com.trs.dev3.bdc.util.ZKUtil.watchedGetChildren(ZKUtil.java:342)\n\tat com.trs.dev3.bdc.dispatcher.Tools.zkWatcherList(Tools.java:74)\n\tat com.trs.dev3.bdc.dispatcher.Tools$1.process(Tools.java:70)\n\tat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:61)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n...\n...\n2016-01-21 09:51:10,514 DEBUG [RetryLoop.java] - Retry policy not allowing retry\n2016-01-21 09:51:10,515 ERROR [ZKUtil.java] - KeeperErrorCode = Session expired for /dispatcher/list com.trs.dev3.bdc.util.ZKUtil.watchedGetChildren(ZKUtil.java:342) com.trs.dev3.bdc.dispatcher.Tools.zkWatcherList(Tools.java:74) com.trs.dev3.bdc.dispatcher.Tools$1.process(Tools.java:70)",
        "Issue Links": []
    },
    "CURATOR-293": {
        "Key": "CURATOR-293",
        "Summary": "Curator can NOT reconnect after connection lost and session expired when the connection come up while the DNS server is not ready yet.(zookeeper connection string using domain names)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "huanhuan li",
        "Created": "26/Jan/16 03:57",
        "Updated": "17/May/23 10:59",
        "Resolved": "10/May/23 13:58",
        "Description": "1. Add following lines to the /etc/hosts:\nx.x.x.x zk1.test.com\nx.x.x.x  zk2.test.com\nx.x.x.x  zk3.test.com\n2. RUN the test programme\n3. shutdown the network connection to x.x.x.x\n4. wait until the session expires (for example 10 min)\n5. remove the added 3 lines in /etc/hosts\n6. open the network connection to x.x.x.x\n7. watch that curator cannot reconnect\n8. add the 3 lines to /etc/hosts\n9. watch that curator cannot reconnect either\nThe log may look like the following:\n[main-SendThread(172.24.2.35:2181)][INFO ]2016-01-26 11:07:45.005 [ClientCnxn.logStartConnect] - Opening socket connection to server 172.24.2.35/172.24.2.35:2181. Will not attempt to authenticate using SASL (unknown error)\n[main-SendThread(172.24.2.35:2181)][INFO ]2016-01-26 11:07:45.050 [ClientCnxn.primeConnection] - Socket connection established to 172.24.2.35/172.24.2.35:2181, initiating session\n[main-EventThread][WARN ]2016-01-26 11:07:45.093 [ConnectionState.handleExpiredSession] - Session expired event received\n[main-EventThread][DEBUG]2016-01-26 11:07:45.093 [ConnectionState.reset] - reset\n[main-SendThread(172.24.2.35:2181)][INFO ]2016-01-26 11:07:45.093 [ClientCnxn.run] - Unable to reconnect to ZooKeeper service, session 0x1525d9593a537af has expired, closing socket connection\n[main-EventThread][INFO ]2016-01-26 11:07:45.095 [ZooKeeper.<init>] - Initiating client connection, connectString=zk1.test.com:2181,zk2.test.com:2181,zk3.test.com:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@7e7d611f\n[main-EventThread][INFO ]2016-01-26 11:07:45.488 [ClientCnxn.run] - EventThread shut down\n[main-SendThread(111.206.227.147:2181)][INFO ]2016-01-26 11:07:45.615 [ClientCnxn.logStartConnect] - Opening socket connection to server 111.206.227.147/111.206.227.147:2181. Will not attempt to authenticate using SASL (unknown error)\n[Curator-ConnectionStateManager-0][DEBUG]2016-01-26 11:07:58.523 [CuratorZookeeperClient.blockUntilConnectedOrTimedOut] - blockUntilConnectedOrTimedOut() end. isConnected: false",
        "Issue Links": [
            "/jira/browse/CURATOR-229",
            "/jira/browse/ZOOKEEPER-1576"
        ]
    },
    "CURATOR-294": {
        "Key": "CURATOR-294",
        "Summary": "Optimize TreeCache, fix possible concurrency issue",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Nick Hill",
        "Created": "28/Jan/16 16:41",
        "Updated": "01/Feb/16 19:30",
        "Resolved": "01/Feb/16 19:16",
        "Description": "Hi, I have been looking at the TreeCache impl and have some questions.\nIt doesn't look right to me that there's separate atomic refs for a node's data and stat. It seems the stat in a ChildData object obtained from getCurrentData() might not correspond to the data that it's with. This could be problematic when doing conditional state changes given assumptions about that data.\nAn obvious and simple solution to this would be to have a single AtomicReference<ChildData> field instead, which would have the additional significant benefit of eliminating ChildData obj creation on every cache access. PathChildrenCache works this way, but my understanding was that TreeCache is intended to be a (more flexible) replacement.\nFurthermore I'd propose that the data field of ChildData be just a final byte[] instead of an AtomicReference. This would avoid needing two volatile reads to get to the data, and mean that \"sharing\" these (per above) is a bit safer. The ChildData byte[] AtomicReference is only used by PathChildrenCache.clearDataBytes() (not currently used by TreeCache at all), and that capability could be easily maintained by having PathChildrenCache use it's own simple subclass of ChildData containing the atomic reference.\nIf similar capability were to be added to TreeCache, I'd suggest it would be better to just replace the node's ChildData object with a copy that has the byte[] field nulled out (but same stat ref).\nI'm fairly new to the code so apologies if there's something I've missed/misunderstood! But if there is agreement, I'd also be happy to prepare a PR.\nRegards,\nNick",
        "Issue Links": []
    },
    "CURATOR-295": {
        "Key": "CURATOR-295",
        "Summary": "Service discovery close attempts to double-close caches",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Martin Serrano",
        "Created": "30/Jan/16 14:21",
        "Updated": "11/Jul/17 15:07",
        "Resolved": "11/Jul/17 15:07",
        "Description": "The ServiceDiscoveryImpl class close method closes all caches followed by closing all providers.  The ServiceProviderImpl close method calls close on its associated cache as well.  Since the cache is already closed, the close call throws an IllegalArgumentException which bubbles up.  This causes the ServiceDiscoveryImpl close method to abort after the first provider is attempted to be closed, resulting in not closing the rest of the providers or any of the listeners associated with the discovery instance.\nThe ServiceDiscoveryImpl close method should not close caches and it should wrap the provider closes with try/catch so that failure to close one does not prevent others from being closed or the listeners.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/225"
        ]
    },
    "CURATOR-296": {
        "Key": "CURATOR-296",
        "Summary": "Notify cache listeners when re-init completes after reconnect",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Chris Conroy",
        "Created": "01/Feb/16 19:05",
        "Updated": "01/Feb/16 19:30",
        "Resolved": "01/Feb/16 19:22",
        "Description": "The cache recipes notify listeners when the cache has finished materializing the subtree it is responsible for with an INITIALIZED event. \nListeners are also notified of connection loss and reconnect. However, a listener should also be notified of the subsequent re-initialization after a reconnect.",
        "Issue Links": []
    },
    "CURATOR-297": {
        "Key": "CURATOR-297",
        "Summary": "Curator quickly starting+stopping a persistent ephemeral node causes dangling ephemeral node",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.9.1",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Feb/16 03:37",
        "Updated": "02/Feb/16 21:49",
        "Resolved": "02/Feb/16 21:49",
        "Description": "Start a PEN and then immediately close it. In most cases, the async node created during start will not execute before the close executes and, thus, the node will be leaked.",
        "Issue Links": []
    },
    "CURATOR-298": {
        "Key": "CURATOR-298",
        "Summary": "Curator NodeCache.start() is blocking even with buildInitial=false",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0,                                            2.9.1",
        "Fix Version/s": "3.1.0,                                            2.10.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Feb/16 03:39",
        "Updated": "02/Feb/16 19:43",
        "Resolved": "02/Feb/16 19:43",
        "Description": "NodeCache start() calls client.checkExists().creatingParentContainersIfNeeded().forPath(path) in the foreground. It would be better if this were async",
        "Issue Links": []
    },
    "CURATOR-299": {
        "Key": "CURATOR-299",
        "Summary": "Copious \"Failed to find watcher\" log messages when closing Curator 3.0 recipes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Feb/16 03:40",
        "Updated": "02/Jun/16 21:13",
        "Resolved": "30/Apr/16 19:56",
        "Description": "[2016-02-01 14:49:47,005][ERROR][org.apache.zookeeper.ClientCnxn] Failed to find watcher!\norg.apache.zookeeper.KeeperException$NoWatcherException: KeeperErrorCode = No such watcher for /zookeeper/config\n    at org.apache.zookeeper.ZooKeeper$ZKWatchManager.containsWatcher(ZooKeeper.java:377) ~[zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ZooKeeper$ZKWatchManager.removeWatcher(ZooKeeper.java:252) ~[zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.WatchDeregistration.unregister(WatchDeregistration.java:58) ~[zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxn.finishPacket(ClientCnxn.java:712) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxn.access$1500(ClientCnxn.java:97) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxn$SendThread.readResponse(ClientCnxn.java:948) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:99) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n    at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1236) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]",
        "Issue Links": []
    },
    "CURATOR-300": {
        "Key": "CURATOR-300",
        "Summary": "Creating a node using storingStatIn breaks Curator with a logged NullPointerException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.1.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Feb/16 03:41",
        "Updated": "02/Feb/16 14:33",
        "Resolved": "02/Feb/16 14:33",
        "Description": "java.lang.NullPointerException: null\n        at org.apache.curator.framework.imps.CreateBuilderImpl$7.processResult(CreateBuilderImpl.java:577) ~[curator-framework-3.0.0.jar:na]\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:672) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:528) [zookeeper-3.5.1-alpha.jar:3.5.1-alpha-1693007]",
        "Issue Links": []
    },
    "CURATOR-301": {
        "Key": "CURATOR-301",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0,                                            2.4.0,                                            2.4.1,                                            2.4.2,                                            2.5.0,                                            2.6.0,                                            2.7.0,                                            2.7.1,                                            2.8.0,                                            2.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sebb",
        "Created": "02/Feb/16 21:02",
        "Updated": "02/Feb/16 22:18",
        "Resolved": "02/Feb/16 21:58",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\ni.e. the ones listed as affected.\nThanks!\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "CURATOR-302": {
        "Key": "CURATOR-302",
        "Summary": "TestTreeCache.testKilledSession() fails in 3.0 branch",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "3.1.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Scott Blum",
        "Created": "06/Feb/16 18:41",
        "Updated": "08/Feb/16 17:26",
        "Resolved": "08/Feb/16 17:26",
        "Description": "Jordan Zimmerman:\n\nIn this branch, TestTreeCache.testKilledSession() is failing at:\n\n        assertEvent(TreeCacheEvent.Type.NODE_REMOVED, \"/test/me\", \"data\".getBytes());\n\n\nHowever, if I change the two asserts to:\n\n        assertEvent(TreeCacheEvent.Type.INITIALIZED);\n        assertEvent(TreeCacheEvent.Type.NODE_REMOVED, \"/test/me\", \"data\".getBytes());\n\n\nit works.\nScott:\nIt's looking like some kind of internal behavior change in the framework.  I'm going to have to add some tracing and compare the behaviors between trunk and 3.0 to uncover what changed.",
        "Issue Links": []
    },
    "CURATOR-303": {
        "Key": "CURATOR-303",
        "Summary": "Find/fix all thread leaks",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "TBD",
        "Component/s": "Client,                                            Framework,                                            Recipes,                                            Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "22/Feb/16 20:03",
        "Updated": "20/May/16 17:54",
        "Resolved": null,
        "Description": "Curator probably has some thread leaks. The tests should be enhanced to find them and fixes should be applied.",
        "Issue Links": []
    },
    "CURATOR-304": {
        "Key": "CURATOR-304",
        "Summary": "Support Sequential Nodes in ServiceDiscovery",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "awaiting-response",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Nir Dothan",
        "Created": "02/Mar/16 12:01",
        "Updated": "02/Jun/16 21:16",
        "Resolved": "28/Mar/16 22:40",
        "Description": "Would like to support EPHEMERAL SEQUENTIAL zk nodes in Service Discovery.\nSubmitting PR",
        "Issue Links": []
    },
    "CURATOR-305": {
        "Key": "CURATOR-305",
        "Summary": "Curator Transaction APIs lose error codes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "3.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Mar/16 18:52",
        "Updated": "02/Jun/16 21:13",
        "Resolved": "02/Mar/16 21:27",
        "Description": "CuratorTransactionImpl.makeCuratorResult() needs to check for ErrorResult and return it in CuratorTransactionResult (new field probably needed).",
        "Issue Links": []
    },
    "CURATOR-306": {
        "Key": "CURATOR-306",
        "Summary": "Background operations would benefit from an optional error handler",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "03/Mar/16 18:00",
        "Updated": "02/Jun/16 21:15",
        "Resolved": "10/Mar/16 22:56",
        "Description": "If an exception is thrown during processing of a background operation in the Framework, the error eventually goes to the global UnhandledErrorListener. This makes correlating the error with the site of the error difficult. It would be nice to have a call-specific error handler that is called in this case.",
        "Issue Links": []
    },
    "CURATOR-307": {
        "Key": "CURATOR-307",
        "Summary": "InterProcessReadWriteLock can throw IndexOutOfBoundsException when connection is lost",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4.0",
        "Fix Version/s": "2.8.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jakub Strolen\u00fd",
        "Created": "04/Mar/16 13:41",
        "Updated": "17/Jul/22 06:50",
        "Resolved": "17/Jul/22 06:49",
        "Description": "We are using recipe InterProcessMutex as lock for our cluster environment.\nWe found that it is possible that acquiring and/or releasing of lock can fail when connection to ZooKeeper is lost or changed.\nIf you have have following sample code:\n\nInterProcessMutex.acquire();\ndoSomething();\nInterProcessMutex.release();\n\n\nYou can get following exception in release() method, when connection is lost after lock is acquired.\nStack trace:\n\njava.lang.IllegalMonitorStateException: You do not own the lock: /clusters/Production/something/lock\nat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:128)\n\n\nand following stack trace when lock directory is empty (because emphemeral nodes are deleted, because of connection was lost)\n\njava.lang.IndexOutOfBoundsException: Index: 2147483647, Size: 0\nat java.util.ArrayList.rangeCheck(ArrayList.java:653)\nat java.util.ArrayList.get(ArrayList.java:429)\nat org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock.readLockPredicate(InterProcessReadWriteLock.java:190)\nat org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock.access$0(InterProcessReadWriteLock.java:163)\nat org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock$2.getsTheLock(InterProcessReadWriteLock.java:137)\nat org.apache.curator.framework.recipes.locks.LockInternals.internalLockLoop(LockInternals.java:287)\nat org.apache.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:226)\nat org.apache.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:221)\nat org.apache.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:77)\n\n\nThese operations should fail more gracefully, maybe with dedicated exception, when connection is lost or changed.",
        "Issue Links": [
            "/jira/browse/CURATOR-211"
        ]
    },
    "CURATOR-308": {
        "Key": "CURATOR-308",
        "Summary": "SimpleDistributedQueue::take() hangs if container nodes are removed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Philip Searle",
        "Created": "14/Mar/16 13:35",
        "Updated": "02/Jun/16 21:13",
        "Resolved": "23/May/16 03:33",
        "Description": "SimpleDistributedQueue creates the queue using container nodes if the ZooKeeper instance supports this feature. If ZooKeeper runs the container node cleanup task while SimpleDistributedQueue::take() is blocking, the call will not ever return.\nA similar issue occurs when calling poll(), resulting in it delaying until the timeout has elapsed, even if a queue item was inserted after the container cleanup occurs.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/149"
        ]
    },
    "CURATOR-309": {
        "Key": "CURATOR-309",
        "Summary": "ConnectionState:244 - Authentication failed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "15/Mar/16 16:23",
        "Updated": "10/Dec/18 17:26",
        "Resolved": null,
        "Description": "I keep seeing those in my logs:\n2016-01-06 12:02:09,228 ERROR ConnectionState:244 - Authentication failed\nI think that's because the ZookeeperFactory doesn't support credentials:\nzookeeperFactory.newZooKeeper(connectionString, sessionTimeout, watcher,\ncanBeReadOnly);",
        "Issue Links": [
            "/jira/browse/DRILL-6892"
        ]
    },
    "CURATOR-310": {
        "Key": "CURATOR-310",
        "Summary": "Race in PersistentNode startup",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Gerd Behrmann",
        "Created": "17/Mar/16 14:26",
        "Updated": "14/Oct/16 22:37",
        "Resolved": "29/Mar/16 17:21",
        "Description": "We ran into what looks like a race in PersisentNode startup:\njava.lang.IllegalArgumentException: Path cannot be null\n        at org.apache.curator.utils.PathUtils.validatePath(PathUtils.java:48) ~[curator-client-2.10.0.jar:na]\n        at org.apache.curator.utils.PathUtils.validatePath(PathUtils.java:37) ~[curator-client-2.10.0.jar:na]\n        at org.apache.curator.utils.ZKPaths.fixForNamespace(ZKPaths.java:105) ~[curator-client-2.10.0.jar:na]\n        at org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:104) ~[curator-framework-2.10.0.jar:na]\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.fixForNamespace(CuratorFrameworkImpl.java:594) ~[curator-framework-2.10.0.jar:na]\n        at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:244) ~[curator-framework-2.10.0.jar:na]\n        at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41) ~[curator-framework-2.10.0.jar:na]\n        at dmg.cells.zookeeper.CellCuratorFramework$PathAndBytesableDecorator.forPath(CellCuratorFramework.java:1369) ~[cells-2.16.0-SNAPSHOT.jar:2.16.0-SNAPSHOT]\n        at org.apache.curator.framework.recipes.nodes.PersistentNode.setData(PersistentNode.java:323) ~[curator-recipes-2.10.0.jar:na]\nThe problem here is that PersistentNode#setData calls PersistentNode#getActualPath, however the nodePath field accessed by PersistentNode#getActualPath isn't set until PersistentNode#processBackgroundCallback is called as a result of the createNode call in PersistentNode#start. \nI.e. if one calls PersistentNode#setData right after calling start, there is a race between the node creation initializing the actual path and PersistentNode#setData accessing it.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/140"
        ]
    },
    "CURATOR-311": {
        "Key": "CURATOR-311",
        "Summary": "SharedValue could hold stall data when quourm membership changes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jian Fang",
        "Created": "22/Mar/16 18:45",
        "Updated": "10/Jul/17 17:11",
        "Resolved": "10/Jul/17 17:09",
        "Description": "We run a Zookeeper 3.5.1-alpha quorum on EC2 instances and the quorum members could be changed, for example, one peer could be replaced by a new EC2 instance due to EC2 instance termination. We use Apache Curator 3.1.0 as the zookeeper client. During our testing, we found the SharedValue data structure could hold stall data during and after one peer is replaced and thus led to the system failure. \nWe look into the SharedValue code. Seems it always returns the value from an in-memory reference variable and the value is only updated by a watcher. If for any reason, the watch is lost, then the value would never get a chance to be updated again.\nRight now, we added a connection state listener to force SharedValue to call readValue(), i.e., read the data from zookeeper directly, if the connection state has been changed to RECONNECTED to work around this issue.\nIt would be great if this issue could be fixed in Curator directly.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/193",
            "https://github.com/apache/curator/pull/200"
        ]
    },
    "CURATOR-312": {
        "Key": "CURATOR-312",
        "Summary": "Memory leak of NamespaceWatcher when register TreeCacheListener to non-existing path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Pavan Kumar M S",
        "Created": "25/Mar/16 07:04",
        "Updated": "29/Mar/16 04:09",
        "Resolved": "29/Mar/16 04:09",
        "Description": "Hi, \n     As per the Curator API document its allowed to register the TreeCacheListener to the path which is not present in the broker, but when we try to register its found that object 'NamespaceWatcher' keep on increasing and leading to out of memory in very short time. \n   Following is the sample code which can be used to reproduce the issue. \nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.framework.CuratorFrameworkFactory.Builder;\nimport org.apache.curator.framework.recipes.cache.TreeCache;\nimport org.apache.curator.framework.recipes.cache.TreeCacheEvent;\nimport org.apache.curator.framework.recipes.cache.TreeCacheListener;\nimport org.apache.curator.retry.RetryNTimes;\npublic class TreeCacheProblem\n{\n    private static final int RETRYTIMES = 3;\n    private static final int RETRYINTERVAL = 1000;\n    private static final int CONNECTION_TIME_OUT = 3000;\n    private static final int SESSION_TIME_OUT = 5000;\n    public static void main ( String [] args ) throws Exception\n    {\n        TreeCacheProblem treeCache = new TreeCacheProblem();\n        treeCache.init ( \"10.18.104.149:2189\" );\n        String zkPath = \"/home/sample\";\n        treeCache.addTreeCacheListener ( zkPath, new TreeCacheListener(){\n            public void childEvent ( CuratorFramework client, TreeCacheEvent event )\n                throws Exception\n            {\n                 System.out.println (event);\n            }\n\n        });\n        System.in.read();\n    }\n    private CuratorFramework client;\n    private void init ( String zkURL )\n    {\n        Builder connect = CuratorFrameworkFactory.builder ().connectString ( zkURL );\n\n        this.client = connect.retryPolicy ( new RetryNTimes ( RETRYTIMES, RETRYINTERVAL ) )\n            .connectionTimeoutMs ( CONNECTION_TIME_OUT ).sessionTimeoutMs ( SESSION_TIME_OUT )\n            .build ();\n        this.client.start ();\n    }\n\n    public void addTreeCacheListener ( String zkPath, TreeCacheListener treeCacheListener ) throws Exception\n    {\n\n        org.apache.curator.framework.recipes.cache.TreeCache.Builder builder = TreeCache\n            .newBuilder ( client, zkPath );\n        builder.setCacheData ( true );\n        TreeCache treeCache = builder.build ();\n        treeCache.getListenable ().addListener ( treeCacheListener );\n        treeCache.start ();\n\n    }\n}\nCurator API Doc for reference \nhttps://curator.apache.org/apidocs/org/apache/curator/framework/recipes/cache/TreeCache.html\npublic TreeCache(CuratorFramework client,\n                 String path)\nCreate a TreeCache for the given client and path with default options.\nIf the client is namespaced, all operations on the resulting TreeCache will be in terms of the namespace, including all published events. The given path is the root at which the TreeCache will watch and explore. If no node exists at the given path, the TreeCache will be initially empty.\nParameters:\nclient - the client to use; may be namespaced\npath - the path to the root node to watch/explore; this path need not actually exist on the server\nSee Also:\nnewBuilder(CuratorFramework, String)\nWith Regards,\nPavan",
        "Issue Links": []
    },
    "CURATOR-313": {
        "Key": "CURATOR-313",
        "Summary": "Verify that Curator logging is appropriate",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Mike Drob",
        "Created": "29/Mar/16 16:57",
        "Updated": "23/Jul/17 16:55",
        "Resolved": "23/Jul/17 16:55",
        "Description": "From a customer:\n\nWhen Curator looses a connection with Zookeeper it does print a rather large amount of stack traces into the logs. I'm unsure if this log spamming should be investigated to determine if there is a way of rate limiting the messages, or move the stack trace to DEBUG and have a simple 1 liner stating Zookeeper is unavailable in WARN.\nThis might have already been taken care of in newer version, so this is at least 50% a reminder to myself to investigate it more thoroughly.",
        "Issue Links": []
    },
    "CURATOR-314": {
        "Key": "CURATOR-314",
        "Summary": "Listenable support for GroupMember recipe",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Shivram Khandeparker",
        "Created": "04/Apr/16 08:29",
        "Updated": "06/Sep/22 13:39",
        "Resolved": null,
        "Description": "The GroupMember recipe currently doesn't provide any membership change events. I wanted this feature in my project and I've implemented it by making a copy of GroupMember in my project and adding the necessary classes and interfaces. I wanted to contribute this back to curator",
        "Issue Links": []
    },
    "CURATOR-315": {
        "Key": "CURATOR-315",
        "Summary": "Reconnect during InterProcessSemaphoreV2.acquire can lead to orphaned node",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ulrich Geilmann",
        "Created": "11/Apr/16 20:55",
        "Updated": "10/Apr/18 12:10",
        "Resolved": "08/May/16 18:54",
        "Description": "Acquiring and releasing a lease can lead to a leftover node when the client has to reconnect. Subsequent attempts to acquire a lease on the same path fail when max leases is reached due to these nodes.\nHere's an excerpt of the client log when that happened:\n\n2016-04-11 10:50:03.634+0000 UTC - INFO org.apache.curator.ConnectionState - Connection attempt unsuccessful after 81663 (greater than max timeout of 60000). Resetting connection and trying again with a new connection. \n2016-04-11 10:50:03.638+0000 UTC - INFO org.apache.zookeeper.ZooKeeper - Session: 0x15404ee16da0015 closed \n2016-04-11 10:50:03.638+0000 UTC - INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=172.16.45.128:2181,172.16.45.192:2181,172.16.45.56:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@34129c78 \n2016-04-11 10:50:03.640+0000 UTC - INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server 172.16.45.128/172.16.45.128:2181. Will not attempt to authenticate using SASL (unknown error) \n2016-04-11 10:50:03.640+0000 UTC - INFO org.apache.zookeeper.ClientCnxn - Socket connection established to 172.16.45.128/172.16.45.128:2181, initiating session \n2016-04-11 10:50:03.643+0000 UTC - INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server 172.16.45.128/172.16.45.128:2181, sessionid = 0x25404f0136d0011, negotiated timeout = 40000 \n2016-04-11 10:50:03.643+0000 UTC - INFO o.a.c.f.s.ConnectionStateManager - State change: RECONNECTED \n2016-04-11 10:50:03.644+0000 UTC - INFO o.a.c.f.r.l.InterProcessSemaphoreV2 - Sequential path not found: /locking/leases/_c_32448fde-1af3-472f-91ff-ec8a4c4b5034-lease-0000000002 \n2016-04-11 10:50:03.644+0000 UTC - INFO org.apache.zookeeper.ClientCnxn - EventThread shut down \n2016-04-11 10:50:06.625+0000 UTC - INFO o.a.c.f.r.l.InterProcessSemaphoreV2 - Lease already released",
        "Issue Links": [
            "/jira/browse/CURATOR-462"
        ]
    },
    "CURATOR-316": {
        "Key": "CURATOR-316",
        "Summary": "getQueuedThreads for InterProcessMutex",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "awaiting-response",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Purshotam Shah",
        "Created": "12/Apr/16 18:15",
        "Updated": "08/May/16 18:31",
        "Resolved": null,
        "Description": "We need store InterProcessMutex (so that it can be used for Reentrant) in a map when we acquire a lock and remove it when we release it.\nIt good to have function getQueuedThreads which will return the number of threads waiting to acquire this lock. \nOnce should not remove the lock from the map if other there are waiting threads to acquire the same lock.",
        "Issue Links": []
    },
    "CURATOR-317": {
        "Key": "CURATOR-317",
        "Summary": "Deprecation comment suggests code that is not equivalent",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Jorge Montero",
        "Created": "20/Apr/16 14:18",
        "Updated": "02/Jun/16 21:15",
        "Resolved": "21/Apr/16 00:37",
        "Description": "An old deprecation (from 2012) recommended replacing withProtectedEphemeralSequential() with \nwithProtection().withMode(CreateMode.PERSISTENT_SEQUENTIAL), which does not do the same thing! Anyone making that replacement will get persistent nodes instead of ephemeral modes, which can cause all kinds of trouble (it definitely did for us!)\nIt should be replaced with code that actually does the same thing as the old implementation: \t\t\t\t.withProtection().withMode(CreateMode.EPHEMERAL_SEQUENTIAL)",
        "Issue Links": []
    },
    "CURATOR-318": {
        "Key": "CURATOR-318",
        "Summary": "Threads may return different boolean values when entering same double barrier",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Shiliang Cao",
        "Created": "27/Apr/16 09:28",
        "Updated": "14/Jul/18 06:54",
        "Resolved": null,
        "Description": "To my understanding, when all threads are trying enter an barrier, they should all success or fail, which means their return values should be the same.\nBut actually they may get different return values in this situation (reproduce steps):\n0. Some preparing works such as running a zk server, basic curator connecting codes;\n1. Prepare 3 threads: thread1/ thread2/ thread3;\n2. Thread1 sleep 20 seconds then enter barrier, thread2 and thread3 try to enter barrier right now, with timeout value set to 5 seconds;\n3. Result: thread2 and thread3 returned false due to timeout as expected, but thread1 (the sleeping one) just return true, which I think should be false too.\nPossible root cause as I observed via zkCli:\nWhen thread1 and thread2 enter methods returned, their path nodes remained, so when thread3 came, it just think other threads are still waiting, so it just created the ready node and return with true.\nIf this is not by design, it should be a design defect.",
        "Issue Links": []
    },
    "CURATOR-319": {
        "Key": "CURATOR-319",
        "Summary": "NodeCache recreates deleted parents of the node being cached",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Tommy Becker",
        "Created": "28/Apr/16 18:42",
        "Updated": "02/May/16 18:26",
        "Resolved": null,
        "Description": "Starting with Curator 2.10.0, the NodeCache will spontaneously re-create parents of the node being cached if one of them is deleted.  As an example, if there is a NodeCache watching node /a/b/c and node b is deleted, the cache will immediately recreate node b (but not c) when the change is processed. This seems to stem from this commit: https://github.com/apache/curator/commit/f02fb225d531506444b54cdf5effb0529a35cdb0\nThis was not the behavior prior to 2.10.0, and seems pretty unexpected.",
        "Issue Links": []
    },
    "CURATOR-320": {
        "Key": "CURATOR-320",
        "Summary": "Discovery reregister triggered even if retry policy suceeds. Connection looping condition.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "TBD,                                            2.10.0",
        "Fix Version/s": "TBD",
        "Component/s": "Client,                                            Framework",
        "Assignee": null,
        "Reporter": "Running Fly",
        "Created": "29/Apr/16 18:38",
        "Updated": "24/Jan/17 01:42",
        "Resolved": null,
        "Description": "ServiceDiscoveryImpl.reRegisterServices() can be trigger  on ConnectionState events: RECONNECTED and CONNECTED. Causing the reRegisterServices() method to be run on ConnectionStateManager thread. If a connection drops while running reRegisterServices() it will be recovered by the retry policy. However the ConnectionState SUSPENDED followed by RECONNECTED events will be queued but not fired until reRegisterServices() completes(ConnectionStateManager Thread fires these events but is in use). When it does complete the RECONNECTED event in the queue will fire and reRegisterServices() will rerun.\n    When zookeeper's server connection is interrupted all of the clients will simultaneously call reRegisterServices(). This overloads the server with requests causing connections to timeout and reset. Thus queuing up more RECONNECTED events. This state can persist indefinitely.\n    Because the reRegisterServices() will most likely receive a NodeExistsException. It deletes and recreates the node. Effectively causing the services to thrash up and down. Wreaking havoc on our service dependency chain.",
        "Issue Links": []
    },
    "CURATOR-321": {
        "Key": "CURATOR-321",
        "Summary": "Recipe for Group Membership with Partition/Shard Assignment",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Drew Kutcharian",
        "Created": "30/Apr/16 18:43",
        "Updated": "03/May/16 14:39",
        "Resolved": "03/May/16 14:39",
        "Description": "It'd be great if Curator had a recipe for partition/shard assignment, similar to Kafka's consumer rebalancing. It could be thought of as the generalization of what Kafka consumers do using Zookeeper.\nUse Case:\nYou have 5 worker machines that need to process data from 10 database shards, but you want each worker to be pinned to a single database so you don't have to worry about race conditions.\nSolution:\nA more advanced group management recipe that:\n1. Has a concept of slots (or paritions). You can create a group with say 1000 slots.\n2. When a node joins the group, it gets assigned a range of the slots. If it's the first node, then it will get all the slots. If there are already nodes registered in the group, then they will split the slots equally between them.\n3. When a node leaves the group, the rest of the nodes rebalance so they each end up with a equal number of remaining slots.\nI am aware that something like this can be done using Storm et al but I feel that this primitive would be useful on its own in Curator.",
        "Issue Links": []
    },
    "CURATOR-322": {
        "Key": "CURATOR-322",
        "Summary": "Schema support - path validation and documentation",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "3.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/May/16 21:19",
        "Updated": "02/Jun/16 21:13",
        "Resolved": "19/May/16 19:03",
        "Description": "ZooKeeper applications are heavily dependent on correct usage of paths, watchers, ZNode data, etc. Currently, there is no mechanism for validating and documenting this. It would be nice to have some kind of schema system that allows for this.\nThis issue provides several items of functionality: a) ZNode path documentation; b) ZNode path validation; c) keyed reference to ZNode paths. Both items are defined in a new class, Schema. A Curator Schema specifies a ZNode path (or regex pattern), documentation for that path, and the operations that are allowed on that path. Additionally, a set of Schema objects are collected into a SchemaSet object that allows any Schema (and thereby its path) to be retrieved via a symbolic name. This should make path management in ZK applications easier. Lastly, a utility is provided to load SchemaSets from a file/stream in JSON format.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/147"
        ]
    },
    "CURATOR-323": {
        "Key": "CURATOR-323",
        "Summary": "Use threadpool executor with prio queue",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Rich Alberth",
        "Created": "06/May/16 13:10",
        "Updated": "21/May/16 17:47",
        "Resolved": "21/May/16 17:47",
        "Description": "(couldn't find a forum link to ask this question, trying here, please advise if this is not the right venue)\nI'm creating a priority queue in Spring appcontext.xml, and wiring a thread pool object as the executor.  My intention is to have a single Curator framework connection, a single priority queue object, and a thread pool to control parallel execution.  My hope is as elements are dequeued, they would be given to the thread pool, which would be responsible for executing them in parallel.  I'm not using a lock with the queue.\nFrom testing, a single thread is always used, regardless of my executor.\nCan you confirm this is intended?",
        "Issue Links": []
    },
    "CURATOR-324": {
        "Key": "CURATOR-324",
        "Summary": "TreeCache sending multiple NODE_ADDED events for same node",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Jordan Zimmerman",
        "Created": "19/May/16 20:25",
        "Updated": "02/Jun/16 21:15",
        "Resolved": "20/May/16 02:39",
        "Description": "Randomly creating, updating, deleting nodes and recording events as they come through exhibits a bug with TreeCache. TreeCache will occasionally send TreeCacheEvent.Type.NODE_ADDED for the same node. It appears to always happen after that node has been deleted.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/148"
        ]
    },
    "CURATOR-325": {
        "Key": "CURATOR-325",
        "Summary": "Background retry falls into infinite loop of SessionExpiredException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.9.1,                                            2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "clive du",
        "Created": "20/May/16 05:27",
        "Updated": "06/Oct/17 14:22",
        "Resolved": null,
        "Description": "after long time gc pause,which longer than zookeeper session time,the zookeeper cluster invalidate the session id holding by the client and waiting the client to reconnect,but client consider the  SessionExpiredException as retry exception and re-put to the background queue,so wo get the stacktrace infinitely.\n12:50:54.337 [configuration-0-EventThread] DEBUG org.apache.curator.RetryLoop - Retrying operation\n12:50:54.337 [configuration-0-EventThread] DEBUG org.apache.curator.RetryLoop - Retry-able exception received\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /dynamic/apps/258741001/DEV\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:127) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n    at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304) ~[curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293) ~[curator-framework-2.10.0.jar:na]\n    at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) ~[curator-client-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$1.forPath(GetDataBuilderImpl.java:105) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$1.forPath(GetDataBuilderImpl.java:65) [curator-framework-2.10.0.jar:na]\n    at com.ctrip.flight.configuration.client.AbstractZookeeperClient.getData(AbstractZookeeperClient.java:68) [classes/:na]\n    at com.ctrip.flight.configuration.client.ZooKeeperConfigurationSource.getPublishNodeValue(ZooKeeperConfigurationSource.java:258) [classes/:na]\n    at com.ctrip.flight.configuration.client.ZooKeeperConfigurationSource.access$100(ZooKeeperConfigurationSource.java:45) [classes/:na]\n    at com.ctrip.flight.configuration.client.ZooKeeperConfigurationSource$1.nodeChanged(ZooKeeperConfigurationSource.java:105) [classes/:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:310) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache$4.apply(NodeCache.java:304) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [curator-framework-2.10.0.jar:na]\n    at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:310) [guava-19.0.jar:na]\n    at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache.setNewData(NodeCache.java:302) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache.processBackgroundResult(NodeCache.java:269) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache.access$300(NodeCache.java:56) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.recipes.cache.NodeCache$3.processResult(NodeCache.java:122) [curator-recipes-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:749) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:522) [curator-framework-2.10.0.jar:na]\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:256) [curator-framework-2.10.0.jar:na]\n    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:561) [zookeeper-3.4.6.jar:3.4.6-1569965]\n    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]",
        "Issue Links": []
    },
    "CURATOR-326": {
        "Key": "CURATOR-326",
        "Summary": "createContainers fails silently if client is not connected",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Gerd Behrmann",
        "Created": "20/May/16 20:41",
        "Updated": "25/May/16 01:21",
        "Resolved": "23/May/16 03:32",
        "Description": "Calling CuratorFramework#createContainers on a non-existing path while the client is not yet connected to the server (e.g. server is down while client is starting) fails silently if enough time goes by before the server is started. \nThe following unit test demonstrates the issue:\n\npackage dmg.cells.zookeeper;\n\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.retry.RetryForever;\nimport org.apache.curator.test.TestingServer;\nimport org.apache.curator.test.Timing;\nimport org.junit.Test;\n\nimport static junit.framework.TestCase.assertNotNull;\n\npublic class CuratorTest\n{\n    @Test\n    public void createContainersTest() throws Exception\n    {\n        TestingServer server = new TestingServer(false);\n\n        Timing timing = new Timing();\n        CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString(), timing.session(), timing.connection(), new RetryForever(100));\n        try {\n            new Thread() {\n                @Override\n                public void run()\n                {\n                    try {\n                        Thread.sleep(30000);\n                        server.start();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            }.start();\n\n            client.start();\n            client.createContainers(\"/this/does/not/exist\");\n            assertNotNull(client.checkExists().forPath(\"/this/does/not/exist\"));\n        } finally {\n            client.close();\n            server.stop();\n        }\n    }\n}\n\n\nThe delay before starting the server is significant. If only sleeping for 10 seconds, the unit test passes. Sleeping for 30 seconds triggers a code path in Curator that will cause CuratorFramework#createContainers to wait until the server is started, yet it returns without exception and without creating the path. The assertion fails.\nI tracked down the issue to ExistingBuilderImpl#pathInForeground in which the call to ZKPath#mkdirs is wrapped with a try-catch that ignores the exception. Thus the failed operation is neither retried nor propagated.\nSpecifically this causes silent problems with PathAndChildren cache as it uses EnsureContainer#ensure during startup to ensure that the path exists. This internally calls the above createContainers. When it fails silently, the recipe fails to register a watcher on the non-existing path and consequently the cache stays empty even when the server finally is started and the path is populated by another client.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/151"
        ]
    },
    "CURATOR-327": {
        "Key": "CURATOR-327",
        "Summary": "NamespaceImpl does not report full error message to caller",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Raymond Mauge",
        "Created": "20/May/16 20:58",
        "Updated": "25/May/16 01:21",
        "Resolved": "20/May/16 21:23",
        "Description": "NamespaceImpl calls PathUtils.validatePath(\"/\" + namespace) which returns a detailed error message to NamespaceImpl. But NamespaceImpl completely ignores the message detail and simply uses a generic message which is less than helpful.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/150"
        ]
    },
    "CURATOR-328": {
        "Key": "CURATOR-328",
        "Summary": "PathChildrenCache fails silently if server is unavailable for sufficient time when client starts",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Gerd Behrmann",
        "Created": "20/May/16 21:22",
        "Updated": "25/May/16 01:21",
        "Resolved": "23/May/16 03:25",
        "Description": "When initializing the PathChildrenCache, if the curator client is not yet connected to the ZooKeeper server (e.g. the server is down or the network connection is unavailable), then the internal initialization of the cache will eventually fail silently and the cache stays empty even after the client finally connects to the server and the path is populated with znodes.\nThe following unit test demonstrates the problem (the unit test is ugly as the problem depends on timing, but it suffices to demonstrate the issue):\n\n    @Test\n    public void pathChildrenCacheTest() throws Exception\n    {\n        TestingServer server = new TestingServer(false);\n\n        Timing timing = new Timing();\n        CuratorFramework client = CuratorFrameworkFactory.newClient(\n                server.getConnectString(), timing.session(), timing.connection(), new ExponentialBackoffRetry(1000, 3));\n        try {\n            new Thread() {\n                @Override\n                public void run()\n                {\n                    try {\n                        Thread.sleep(60000);\n                        server.start();\n                    } catch (Exception e) {\n                        e.printStackTrace();\n                    }\n                }\n            }.start();\n\n            client.start();\n\n            PathChildrenCache cache = new PathChildrenCache(client, \"/\", true);\n            cache.start();\n\n            client.blockUntilConnected();\n\n            client.create().creatingParentContainersIfNeeded().forPath(\"/baz\", new byte[] {1,2,3});\n\n            assertNotNull(\"/baz does not exist\", client.checkExists().forPath(\"/baz\"));\n\n            /* Ugly hack for this test to ensure the cache got time to update itself. */\n            Thread.sleep(1000);\n\n            assertNotNull(\"cache doesn't see /baz\", cache.getCurrentData(\"/baz\"));\n        } finally {\n            client.close();\n            server.stop();\n        }\n    }\n\n\nHere the server startup is delayed until some point after the curator client was started and after the recipe has been created. Eventually the server starts and the path is populated with data - some time is given for the cache to update itself, yet no data is visible: The second assertion fails.\nIf the startup time is reduced to - say - 20 seconds, the test passes.\nIf the client is allowed to first connect to the server before creating the recipe and then disconnect and reconnect after creating the recipe, then the test passes too.\nI tracked down the problem to the state change listener of the recipe: If the connection to the server is down for long enough, the refresh call during the background initialization will eventually fail (ensurePath throws an exception). This isn't a problem as the recipe has a state change listener, so it gets notified when the client eventually connects to the server. The handleStateChange method however doesn't react to a CONNECTED event - only to a RECONNECTED event. Thus if the client has been connected to the server in the past, everything works, however if this is the first time it connects, the recipe will not react to the event and thus not refresh itself.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/152"
        ]
    },
    "CURATOR-329": {
        "Key": "CURATOR-329",
        "Summary": "SharedValue.start() method does not update to current value if version of value currently stored in zookeeper is 0 (zero)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Algirdas Rascius",
        "Created": "24/May/16 13:37",
        "Updated": "24/May/16 22:53",
        "Resolved": "24/May/16 22:53",
        "Description": "When zookeeper node stores value with version 0 and SharedCount instance is created and started for this node, getCount() method returns seedValue passed to SharedCount constructor instead of actual value stored in zookeeper node. This only happens when zookeeper node version is 0 (zero).\nFor example:\n\n// Node /count does not exists before the test\nSharedCount count1 = new SharedCount(client, \"/count\", 10);\nSharedCount count2 = new SharedCount(client, \"/count\", 20);\ncount1.start();\ncount2.start();\nAssert.assertEquals(count1.getCount(), 10);\nAssert.assertEquals(count2.getCount(), 10); // Should return 10, but returns 20 instead\n\n\nIssue arrises because when constructed class SharedValue initializes instance variable currentValue with version 0 and does not update value in updateValue(..) method if version returned by zookeeper is 0.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/153"
        ]
    },
    "CURATOR-330": {
        "Key": "CURATOR-330",
        "Summary": "Need a way to handle connection lost while entering double barrier",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Simon Wang",
        "Created": "24/May/16 20:40",
        "Updated": "26/May/16 18:51",
        "Resolved": null,
        "Description": "Here is the problem I\u2019m meeting:\nAssuming 3 node ensemble, my application has 3 clients and each one runs on same zk node (Client 1, 2 and 3). They use double barrier for coordination. \nClient 1 is entering the barrier and waiting for the other 2. Now the other 2 nodes are down and then the ensemble gets crashed and the client 1 gets LostConnectionException from enter(). That\u2019s expected.\nAfter while the other 2 nodes come back,  all clients need to retry operation and reenter the same barrier (It might become more complex if creating a new barrier). Here is the problem:\nIf the session for client 1 is still alive, Client 1 calling enter method will get NodeExistException as the ephemeral node corresponding to that session is not deleted yet. \nI wonder in this case what should I do from application side? Or I\u2019m thinking can we add a mechanism to reenter the barrier but skip creating child node for this client if that exists?\nThanks,\nSimon",
        "Issue Links": []
    },
    "CURATOR-331": {
        "Key": "CURATOR-331",
        "Summary": "Enable to add an UnhandledErrorListener for TreeCache errorHandlers",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Julie Kim",
        "Created": "26/May/16 16:09",
        "Updated": "10/Jan/17 16:41",
        "Resolved": "14/Jun/16 22:42",
        "Description": "Currently, errorHandlers in TreeCache is not configurable and a treeCacheListener cannot handle exceptions properly. PTAL https://github.com/apache/curator/blob/b4da5f5ca279905097b1fe7d5eb9710c6dc8bdd2/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L633",
        "Issue Links": [
            "https://github.com/apache/curator/pull/156",
            "https://github.com/apache/curator/pull/192"
        ]
    },
    "CURATOR-332": {
        "Key": "CURATOR-332",
        "Summary": "Internal watcher tracking for new WatcherRemoveCuratorFramework feature is not correct",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "3.2.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/May/16 18:04",
        "Updated": "01/Jun/16 00:09",
        "Resolved": "01/Jun/16 00:09",
        "Description": "The WatcherRemoveCuratorFramework introduced in 3.0.0 doesn't track watchers correctly and can sometimes lose track of some watchers and, thus, not delete them when requested to. Examples:\n\nExists watchers stay set even when there is a NoNode exception\nIf a watcher is successfully set and then the same watcher is used during a network failure, WatcherRemoveCuratorFramework will clear the watcher internally.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/154"
        ]
    },
    "CURATOR-333": {
        "Key": "CURATOR-333",
        "Summary": "TestCleanState can hide test assertions",
        "Type": "Test",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.1.0",
        "Fix Version/s": "TBD",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/May/16 22:03",
        "Updated": "01/Jun/16 00:44",
        "Resolved": null,
        "Description": "TestCleanState is used in finally blocks in lots of tests to close the Curator handle and check for cleaned watchers. However, being in a finally, it always runs. If there is an exception or assertion, it will run and always fail hiding the real error.",
        "Issue Links": []
    },
    "CURATOR-334": {
        "Key": "CURATOR-334",
        "Summary": "Curator doesn't retry if GC pause > session timeout",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Purshotam Shah",
        "Created": "01/Jun/16 17:57",
        "Updated": "01/Jun/16 18:00",
        "Resolved": null,
        "Description": "We noticed that if GC pause > session timeout and even if retry policy is set, Curator doesn't retry.\nWe use CuratorFramework.getConnectionStateListenable to listen on connection state. ConnectionState.LOST is received without retry.\nIdeally, Curator should retry before notifying as connection lost.",
        "Issue Links": [
            "/jira/browse/OOZIE-2549"
        ]
    },
    "CURATOR-335": {
        "Key": "CURATOR-335",
        "Summary": "InterProcessSemaphoreV2 can deadlock under network stress",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            2.10.0",
        "Fix Version/s": "2.11.0,                                            3.2.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/Jun/16 21:12",
        "Updated": "06/Jun/16 00:57",
        "Resolved": "06/Jun/16 00:57",
        "Description": "Under network stress, InterProcessSemaphoreV2 can stop acquiring new leases. This test (by cammckenzie) shows the issues :\n\npackage org.apache.curator.framework.recipes.locks;\n\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport org.apache.curator.framework.CuratorFramework;\nimport org.apache.curator.framework.CuratorFrameworkFactory;\nimport org.apache.curator.framework.state.ConnectionState;\nimport org.apache.curator.framework.state.ConnectionStateListener;\nimport org.apache.curator.retry.RetryOneTime;\nimport org.apache.curator.test.BaseClassForTests;\nimport org.apache.curator.utils.CloseableUtils;\nimport org.apache.zookeeper.KeeperException;\nimport org.testng.Assert;\nimport org.testng.ITestContext;\nimport org.testng.annotations.BeforeSuite;\nimport org.testng.annotations.Test;\n\npublic class TestInterProcessMutexNotReconnecting extends BaseClassForTests\n{\n    @Test\n    public void test() throws Exception\n    {\n        final String SEMAPHORE_PATH = \"/test\";\n        final int MAX_SEMAPHORES = 1;\n        final int NUM_CLIENTS = 10;\n        \n        server.start();\n        \n        CuratorFramework client = null;\n\n        ExecutorService executor = Executors.newFixedThreadPool(NUM_CLIENTS);\n        \n        final AtomicInteger counter = new AtomicInteger(0);\n        final AtomicBoolean run = new AtomicBoolean(true);\n        \n        try {\n            client = CuratorFrameworkFactory.newClient(server.getConnectString(), 5000, 5000, new RetryOneTime(1));\n            client.start();\n            \n            final CuratorFramework lClient = client;\n            \n            for(int i = 0; i < NUM_CLIENTS; ++i)\n            {\n                executor.execute(new Runnable()\n                    {\n                    \n                    @Override\n                    public void run()\n                    {\n                        while(run.get())\n                        {\n                            InterProcessSemaphoreV2 semaphore = new InterProcessSemaphoreV2(lClient, SEMAPHORE_PATH, MAX_SEMAPHORES);\n                            System.err.println(Thread.currentThread() + \"Acquiring\");\n                            Lease lease = null;\n                            try\n                            {\n                                lease = semaphore.acquire();\n                                System.err.println(Thread.currentThread() + \"Acquired\");\n                                counter.incrementAndGet();\n                                Thread.sleep(2000);\n                            }\n                            catch(InterruptedException e)\n                            {\n                                System.err.println(\"Interrupted\");\n                                Thread.currentThread().interrupt();\n                                break;\n                            }\n                            catch(KeeperException e)\n                            {\n                                try\n                                {\n                                    Thread.sleep(2000);\n                                }\n                                catch(InterruptedException e2)\n                                {\n                                    System.err.println(\"Interrupted\");\n                                    Thread.currentThread().interrupt();\n                                    break;\n                                }\n                            }\n                            catch(Exception e)\n                            {\n                                e.printStackTrace();\n                            }\n                            finally\n                            {\n                                if(lease != null) {\n                                    semaphore.returnLease(lease);\n                                }\n                            }\n                        }\n                    }\n                    });\n            }\n            \n\n            final AtomicBoolean lost = new AtomicBoolean(false);\n            client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n                \n                @Override\n                public void stateChanged(CuratorFramework client, ConnectionState newState) {\n                   System.err.println(\"New state : \" + newState);\n                   \n                   if(newState == ConnectionState.LOST) {\n                       lost.set(true);\n                   }\n                }\n            });\n            \n            Thread.sleep(2000);\n            \n            System.err.println(\"Stopping server\");\n            server.stop();\n            System.err.println(\"Stopped server\");\n            \n            while(!lost.get())\n            {\n                Thread.sleep(1000);\n            }\n            \n            int preRestartCount = counter.get();\n            \n            System.err.println(\"Restarting server\");\n            server.restart();\n            \n            long startCheckTime = System.currentTimeMillis();\n            while(true)\n            {\n                if(counter.get() > preRestartCount)\n                {\n                    break;\n                }\n                else if((System.currentTimeMillis() - startCheckTime) > 30000)\n                {\n                    Assert.fail(\"Semaphores not reacquired after restart\");\n                }\n            }\n\n        }\n        finally\n        {\n            run.set(false);\n            executor.shutdownNow();\n            CloseableUtils.closeQuietly(client);\n        }\n    }\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/155"
        ]
    },
    "CURATOR-336": {
        "Key": "CURATOR-336",
        "Summary": "QueueConsumer does not get any connection state updates",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "LIU Long",
        "Created": "09/Jul/16 11:32",
        "Updated": "09/Jul/16 11:32",
        "Resolved": null,
        "Description": "According to http://curator.apache.org/curator-recipes/distributed-queue.html#Error_Handling, the QueueConsumer implementer can receive connection state updates. However, in fact, my consumer is never notified of any state updates when the connection is down. By checking the source code of DistributedQueue, I found that indeed no class is invoking the \"stateChanged\" callback of QueueConsumer.\nAm I missing something?",
        "Issue Links": []
    },
    "CURATOR-337": {
        "Key": "CURATOR-337",
        "Summary": "LeaderSelector logs stack trace with \"The leader threw an exception\" message erroneously",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.2.1,                                            2.11.1",
        "Component/s": "Client",
        "Assignee": "Cam McKenzie",
        "Reporter": "Mark Payne",
        "Created": "27/Jul/16 19:56",
        "Updated": "01/Jun/18 22:18",
        "Resolved": "28/Jul/16 00:55",
        "Description": "I am using the LeaderSelector to choose a leader for a particular role in my cluster. Every time that I call close() on the leader selector, I end up with the following stack trace in my logs:\n\n2016-07-20 20:20:47,814 ERROR [Leader Election Notification Thread-1] o.a.c.f.recipes.leader.LeaderSelector The leader threw an exception\njava.lang.IllegalMonitorStateException: You do not own the lock: /leaders/Cluster Coordinator\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:140) ~[curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:425) [curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:441) [curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64) [curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245) [curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239) [curator-recipes-2.10.0.jar:na]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_74]\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002at java.lang.Thread.run(Thread.java:745) [na:1.8.0_74]\n\n\nThis appears to be due to the fact that in LeaderSelector.doWork(), we call mutex.acquire() and then if an InterruptedException is thrown, the finally block calls mutex.release() even though the mutex has not been acquired. The finally block, then, should read:\n\nif ( hasLeadership )\n{\n    hasLeadership = false;\n    try\n    {\n         mutex.release();\n    }\n    catch ( Exception e )\n    {\n        ThreadUtils.checkInterrupted(e);\n        log.error(\"The leader threw an exception\", e);\n        // ignore errors - this is just a safety\n    }\n}\n\n\nThat is, we should execute the body of the finally block only if hasLeadership == true.",
        "Issue Links": [
            "/jira/browse/CURATOR-468",
            "/jira/browse/NIFI-2337",
            "https://github.com/apache/curator/pull/158",
            "https://github.com/apache/curator/pull/159"
        ]
    },
    "CURATOR-338": {
        "Key": "CURATOR-338",
        "Summary": "SessionFailRetryLoop doesn't work correctly",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Rohit Agarwal",
        "Created": "04/Aug/16 12:34",
        "Updated": "04/Aug/16 13:25",
        "Resolved": null,
        "Description": "According to the documentation at https://curator.apache.org/apidocs/org/apache/curator/SessionFailRetryLoop.html\nthe canonical usage of SessionFailRetryLoop is:\n\n SessionFailRetryLoop    retryLoop = client.newSessionFailRetryLoop(mode);\n retryLoop.start();\n try\n {\n     while ( retryLoop.shouldContinue() )\n     {\n         try\n         {\n             // do work\n         }\n         catch ( Exception e )\n         {\n             retryLoop.takeException(e);\n         }\n     }\n }\n finally\n {\n     retryLoop.close();\n }\n\n\nLet's say there is an exception which can be retried (i.e. it's not session expiry.) If my understanding of the code is correct, it will never be retried.\nThis is because when you call shouldContinue() for the first time, isDone is set to true (https://github.com/apache/curator/blob/apache-curator-2.11.0/curator-client/src/main/java/org/apache/curator/SessionFailRetryLoop.java#L204)\nSo, the next time shouldContinue() will return false, unless isDone is set to false in the interim. However, isDone is only updated when the session has expired and the mode is retry https://github.com/apache/curator/blob/apache-curator-2.11.0/curator-client/src/main/java/org/apache/curator/SessionFailRetryLoop.java#L241",
        "Issue Links": [
            "/jira/browse/CURATOR-339"
        ]
    },
    "CURATOR-339": {
        "Key": "CURATOR-339",
        "Summary": "CuratorFramework misses session expired events",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Rohit Agarwal",
        "Created": "04/Aug/16 13:25",
        "Updated": "04/Aug/16 13:25",
        "Resolved": null,
        "Description": "I was using CuratorFramework and was unable to see some expected session expiration events.\nHere's a simpler demonstration of these missed events (without using CuratorFramework)\n\nCode that works:\nThis code doesn't misses any events, note that here I am holding onto a particular zk instance through out.\n\nimport org.apache.curator.CuratorZookeeperClient;\nimport org.apache.curator.retry.RetryOneTime;\nimport org.apache.zookeeper.CreateMode;\nimport org.apache.zookeeper.KeeperException;\nimport org.apache.zookeeper.WatchedEvent;\nimport org.apache.zookeeper.Watcher;\nimport org.apache.zookeeper.ZooDefs;\nimport org.apache.zookeeper.ZooKeeper;\n\npublic class CuClient implements Watcher {\n  public static void main(String[] args) throws Exception {\n    CuClient zkc = new CuClient();\n    zkc.connect(args[0]);\n  }\n\n  private void connect(String zkConnect) throws Exception {\n    CuratorZookeeperClient czc = new CuratorZookeeperClient(\n        zkConnect,\n        4000,\n        1000,\n        this,\n        new RetryOneTime(1000)\n    );\n    czc.start();\n\n    ZooKeeper zk = czc.getZooKeeper();\n\n    for (int i = 0; i < 100; i++) {\n      Thread.sleep(1000);\n      System.out.println(\"creating \" + i);\n      try {\n        zk.create(\"/rollup/\" + i, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n      } catch (KeeperException ke) {\n        System.out.println(ke.getMessage());\n      }\n    }\n\n    czc.close();\n  }\n\n  @Override\n  public void process(WatchedEvent watchedEvent) {\n    System.out.println(watchedEvent);\n  }\n}\n\n\nI caused a network partition after node 6 and then healed it after the session expired\n\nWatchedEvent state:SyncConnected type:None path:null\ncreating 0\ncreating 1\ncreating 2\ncreating 3\ncreating 4\ncreating 5\ncreating 6\nWatchedEvent state:Disconnected type:None path:null\ncreating 7\nKeeperErrorCode = ConnectionLoss for /rollup/7\ncreating 8\nKeeperErrorCode = ConnectionLoss for /rollup/8\ncreating 9\nKeeperErrorCode = ConnectionLoss for /rollup/9\ncreating 10\nKeeperErrorCode = ConnectionLoss for /rollup/10\ncreating 11\nWatchedEvent state:Expired type:None path:null\nWatchedEvent state:SyncConnected type:None path:null\nKeeperErrorCode = Session expired for /rollup/11\ncreating 12\nKeeperErrorCode = Session expired for /rollup/12\ncreating 13\nKeeperErrorCode = Session expired for /rollup/13\ncreating 14\nKeeperErrorCode = Session expired for /rollup/14\n^C\n\n\n\nCode that doesn't work:\nThis is the same code, except instead of holding onto a zookeeper instance, I am calling getZookeeper again and again.\n\nimport org.apache.curator.CuratorZookeeperClient;\nimport org.apache.curator.retry.RetryOneTime;\nimport org.apache.zookeeper.CreateMode;\nimport org.apache.zookeeper.KeeperException;\nimport org.apache.zookeeper.WatchedEvent;\nimport org.apache.zookeeper.Watcher;\nimport org.apache.zookeeper.ZooDefs;\nimport org.apache.zookeeper.ZooKeeper;\n\npublic class CuClient implements Watcher {\n  public static void main(String[] args) throws Exception {\n    CuClient zkc = new CuClient();\n    zkc.connect(args[0]);\n  }\n\n  private void connect(String zkConnect) throws Exception {\n    CuratorZookeeperClient czc = new CuratorZookeeperClient(\n        zkConnect,\n        4000,\n        1000,\n        this,\n        new RetryOneTime(1000)\n    );\n    czc.start();\n\n    for (int i = 0; i < 100; i++) {\n      Thread.sleep(1000);\n      System.out.println(\"creating \" + i);\n      try {\n        czc.getZooKeeper().create(\"/rollup/\" + i, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n      } catch (KeeperException ke) {\n        System.out.println(ke.getMessage());\n      }\n    }\n\n    czc.close();\n  }\n\n  @Override\n  public void process(WatchedEvent watchedEvent) {\n    System.out.println(watchedEvent);\n  }\n}\n\n\nAgain, network partitioned after node 5 and then healed once the session expired. Note the missing 'WatchedEvent state:Expired type:None path:null' line.\n\nWatchedEvent state:SyncConnected type:None path:null\ncreating 0\ncreating 1\ncreating 2\ncreating 3\ncreating 4\ncreating 5\nWatchedEvent state:Disconnected type:None path:null\ncreating 6\nKeeperErrorCode = ConnectionLoss for /rollup/6\ncreating 7\nKeeperErrorCode = ConnectionLoss\ncreating 8\nKeeperErrorCode = ConnectionLoss\ncreating 9\nKeeperErrorCode = ConnectionLoss for /rollup/9\ncreating 10\nKeeperErrorCode = ConnectionLoss\ncreating 11\nKeeperErrorCode = ConnectionLoss\nWatchedEvent state:SyncConnected type:None path:null\ncreating 12\ncreating 13\ncreating 14\ncreating 15\ncreating 16\ncreating 17\n^C\n\n\nThe same happens when I use CuratorFramework (I get SyncConnected events but no Expired events). Am I doing something wrong? I want to do all the operations in one session (because I am creating some ephemeral nodes etc.) I want a reliable way of getting notified when the session expired.",
        "Issue Links": [
            "/jira/browse/CURATOR-338"
        ]
    },
    "CURATOR-340": {
        "Key": "CURATOR-340",
        "Summary": "Upgrade to ZooKeeper 3.4.8",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.0,                                            2.11.1",
        "Fix Version/s": "2.11.1",
        "Component/s": "General",
        "Assignee": "Cam McKenzie",
        "Reporter": "Rahul S",
        "Created": "07/Aug/16 06:11",
        "Updated": "08/Aug/16 03:45",
        "Resolved": "08/Aug/16 03:45",
        "Description": "Curator-Client pulls in zookeeper 3.4.6 by default which is more than 2 yrs old (released 10 March, 2014)\nCan this be upgraded to pull in the latest stable release 3.4.8?",
        "Issue Links": []
    },
    "CURATOR-341": {
        "Key": "CURATOR-341",
        "Summary": "create().orSetData() retries indefinitely if node exists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.0.0",
        "Fix Version/s": "3.2.1",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Henrik Nordvik",
        "Created": "09/Aug/16 11:12",
        "Updated": "14/Aug/16 23:35",
        "Resolved": "14/Aug/16 23:35",
        "Description": "Symptom:\nOne call to create.orSetData().inBackground(callback) writes the data to zookeeper infinitely many times. Version numbers in stat increase quickly. Callback is also called multiple times.\nAdded the following to TestFramework.testCreateOrSetData() to make it fail:\n\n\nCuratorEvent event2 = queue.poll(new Timing().milliseconds(), TimeUnit.MILLISECONDS);\nAssert.assertNull(event2);\n\n\n(slows down test, but just to demonstrate it)\nSeems like CreateBuilderImpl.backgroundSetData() shouldn't requeue mainOperation after doing setData.\n\nclient.queueOperation(mainOperationAndData);",
        "Issue Links": [
            "/jira/browse/CURATOR-268",
            "https://github.com/apache/curator/pull/160"
        ]
    },
    "CURATOR-342": {
        "Key": "CURATOR-342",
        "Summary": "Not possible to create Schema for root path",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "09/Aug/16 16:12",
        "Updated": "09/Aug/16 16:12",
        "Resolved": null,
        "Description": "If you try to create a schema for \"/\", it gets converted to \"\" due to bug in Schema.fixPath()",
        "Issue Links": []
    },
    "CURATOR-343": {
        "Key": "CURATOR-343",
        "Summary": "Schema exceptions need more context",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "09/Aug/16 16:17",
        "Updated": "09/Aug/16 16:18",
        "Resolved": null,
        "Description": "Current schema violations look like this:\n\nSchema violation: Data is not valid for schema: Schema{name='950eb5e3-b64f-43a4-833e-4262be0f3018', pathRegex=/clusters/.*, path='null', documentation='', dataValidator=InternalSchemaValidator(List(root, ip, readonly)), ephemeral=CAN, sequential=CAN, watched=CAN, canBeDeleted=true, metadata={*=1, allocators=1}}\n\n\nIt lacks information about the invalid aspect. It merely shows what was expected.",
        "Issue Links": []
    },
    "CURATOR-344": {
        "Key": "CURATOR-344",
        "Summary": "SharedValue watcher should limit work to valid data change events",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "2.11.0,                                            3.2.1",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Gary Tully",
        "Created": "18/Aug/16 18:32",
        "Updated": "31/Aug/16 01:02",
        "Resolved": "31/Aug/16 01:02",
        "Description": "With a RetryNTimes retry policy and a disconnect. The event thread can get blocked on retries from the SharedValue watcher readValue, blocking other listeners from getting the SUSPENDED event till retry completes.\nSeems the watcher should limit work and notifications to valid change events and ignore a disconnect. The ConnectionStateListener already handles those.\nSample thread stack that blocks other listeners:\n\nmain-EventThread\" daemon prio=10 tid=0x00007f95d009f000 nid=0x3429 waiting on condition [0x00007f959d6d5000]\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\n\tat java.lang.Thread.sleep(Native Method)\n\tat java.lang.Thread.sleep(Thread.java:340)\n\tat java.util.concurrent.TimeUnit.sleep(TimeUnit.java:360)\n\tat org.apache.curator.RetryLoop$1.sleepFor(RetryLoop.java:74)\n\tat org.apache.curator.retry.SleepingRetry.allowRetry(SleepingRetry.java:46)\n\tat org.apache.curator.retry.RetryNTimes.allowRetry(RetryNTimes.java:24)\n\tat org.apache.curator.RetryLoop.takeException(RetryLoop.java:188)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:112)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)\n\tat org.apache.curator.framework.recipes.shared.SharedValue.readValue(SharedValue.java:192)\n\t- locked <0x000000074326fb50> (a org.apache.curator.framework.recipes.shared.SharedValue)\n\tat org.apache.curator.framework.recipes.shared.SharedValue.access$100(SharedValue.java:42)\n\tat org.apache.curator.framework.recipes.shared.SharedValue$1.process(SharedValue.java:58)\n\tat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/161"
        ]
    },
    "CURATOR-345": {
        "Key": "CURATOR-345",
        "Summary": "NPE in EnsembleTracker when starting Curator",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.1.0,                                            3.2.0",
        "Fix Version/s": "3.2.1",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Alex Rankin",
        "Created": "24/Aug/16 07:44",
        "Updated": "30/Sep/16 12:44",
        "Resolved": "30/Sep/16 12:44",
        "Description": "We're currently getting a NPE when running Curator 3.1.0 and 3.2.0 in our environment. I don't believe this was occurring before we upgraded to ZooKeeper 3.5.1-alpha and Curator 3.1.0.\n\n java.lang.NullPointerException\n \tat org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:171) ~[curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:186) ~[curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) ~[curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:144) ~[curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:835) [curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:618) [curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:214) [curator-framework-3.2.0.jar:3.2.0]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:587) [zookeeper-3.5.1-alpha.jar:?]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499) [zookeeper-3.5.1-alpha.jar:?]\n\n\nThe problem is that the QuorumServer.clientAddr here is null. This seems to be filled by parsing the ZooKeeper server properties, which look something like:\n\nserver.1=zoo1:2888:3888\nserver.2=zoo2:2888:3888\nserver.3=zoo3:2888:3888\nIn the QuorumServer constructor, we parse each server line into its respective tasks. The clientAddr, however, is filled by splitting the line around a semicolon and taking the second part:\n\nString[] serverClientParts = addressStr.split(\";\");\n...\nif(serverClientParts.length == 2) {\n    String[] e = splitWithLeadingHostname(serverClientParts[1]);\n    if(e.length > 2) {\n        throw new ConfigException(addressStr + \" does not have the form server_config or server_config;client_config where server_config is host:port:port or host:port:port:type and client_config is port or host:port\");\n    }\n\n    String hostname = e.length == 2?e[0]:\"0.0.0.0\";\n    ...\n}\n\n\nThis suggests that the server string should be followed by a client host/port (or just port), e.g.:\n\nserver.1=zoo1:2888:3888;2181\nserver.2=zoo2:2888:3888;2181\nserver.3=zoo3:2888:3888;localhost:2181\nI can't find anything in the ZooKeeper documentation about this though, and it isn't mandatory in the properties (so shouldn't throw a NPE).\nThis occurs every time we start the server, but doesn't affect functionality, hence the Minor flag. It would be nice to know what exactly is expected here though, so we can get rid of the exception.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/166"
        ]
    },
    "CURATOR-346": {
        "Key": "CURATOR-346",
        "Summary": "LeaderSelector getParticipants reports incorrect number of nodes after connection is suspended",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.7.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Bowen Zhou",
        "Created": "30/Aug/16 18:32",
        "Updated": "30/Aug/16 18:41",
        "Resolved": null,
        "Description": "This is potentially a bug in Curator 2.7.1 where LeaderSelector's getParticipants() reports one less participants than the actual number. On the node missing from the result of getParticipants(), I found something strange in the logs below: ClientCnxn reset the zookeeper connection, then the SUSPENDED state was delivered and the leadership was interrupted and the participant recreated a lock node and called Object.wait() to sleep and wait for its turn to become the leader. However, the RECONNECTED state was not delivered until ConnectionState.checkTimeouts() reset the zookeeper connection again, which also had the side effect of deleting the ephemeral lock znode created earlier by the leader selection participant. Now, we are missing the participant permanently since it is sleeping in wait() and watching only the previous lock node.\nCallstack\n\n\u201cCurator-LeaderSelector-0\" #319 daemon prio=5 os_prio=0 tid=0x00007f892408f800 nid=0x491f in Object.wait() [0x00007f88f4eb2000]\n   java.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at org.apache.curator.framework.recipes.locks.LockInternals.internalLockLoop(LockInternals.java:307)\n        - locked <0x000000059ee53af0> (a org.apache.curator.framework.recipes.locks.LockInternals)\n        at org.apache.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:217)\n        at org.apache.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:232)\n        at org.apache.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:89)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:386)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:64)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:245)\n        at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:239)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n\nLog\n\n2016/08/29 16:46:49.478 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1255.prod.acompany.com:12916)] [searcher] [] Client session timed out, have not heard from server in 26680ms for sessionid 0x255531b1ff4732e, closing socket connection and attempting reconnect\n2016/08/29 16:46:49.796 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1215.prod.acompany.com:12916)] [searcher] [] Opening socket connection to server lva1-app1215.prod.acompany.com/10.132.48.90:12916. Will not attempt to authenticate using SASL (unknown error)\n2016/08/29 16:46:49.797 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1215.prod.acompany.com:12916)] [searcher] [] Socket connection established to lva1-app1215.prod.acompany.com/10.132.48.90:12916, initiating session\n2016/08/29 16:46:49.800 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1215.prod.acompany.com:12916)] [searcher] [] Session establishment complete on server lva1-app1215.prod.acompany.com/10.132.48.90:12916, sessionid = 0x255531b1ff4732e, negotiated timeout = 40000\n2016/08/29 16:46:49.801 INFO [ConnectionStateManager] [SomeService STARTING-EventThread] [searcher] [] State change: SUSPENDED\n2016/08/29 16:46:49.803 INFO [MinReplicasSemaphore] [Curator-LeaderSelector-0] [searcher] [] I'm no longer the leader .. currently there are 1 permits\n2016/08/29 16:47:04.809 ERROR [ConnectionState] [SomeService STARTING-EventThread] [searcher] [] Connection timed out for connection string (lva1-app1215.prod.acompany.com:12916,lva1-app1255.prod.acompany.com:12916,lva1-app1295.prod.acompany.com:12916,lva1-app1335.prod.acompany.com:12916,lva1-app1375.prod.acompany.com:12916) and timeout (15000) / elapsed (15002)\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:87) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:477) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:288) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.readValue(SharedValue.java:244) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.access$100(SharedValue.java:44) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue$1.process(SharedValue.java:61) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n2016/08/29 16:47:20.812 ERROR [ConnectionState] [SomeService STARTING-EventThread] [searcher] [] Connection timed out for connection string (lva1-app1215.prod.acompany.com:12916,lva1-app1255.prod.acompany.com:12916,lva1-app1295.prod.acompany.com:12916,lva1-app1335.prod.acompany.com:12916,lva1-app1375.prod.acompany.com:12916) and timeout (15000) / elapsed (31006)\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:87) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:477) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:288) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.readValue(SharedValue.java:244) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.access$100(SharedValue.java:44) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue$1.process(SharedValue.java:61) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n2016/08/29 16:47:36.815 ERROR [ConnectionState] [SomeService STARTING-EventThread] [searcher] [] Connection timed out for connection string (lva1-app1215.prod.acompany.com:12916,lva1-app1255.prod.acompany.com:12916,lva1-app1295.prod.acompany.com:12916,lva1-app1335.prod.acompany.com:12916,lva1-app1375.prod.acompany.com:12916) and timeout (15000) / elapsed (47008)\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:87) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:477) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:288) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.readValue(SharedValue.java:244) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue.access$100(SharedValue.java:44) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.recipes.shared.SharedValue$1.process(SharedValue.java:61) ~[curator-recipes-2.7.1.jar:?]\n\tat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67) ~[curator-framework-2.7.1.jar:?]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]\n2016/08/29 16:47:53.817 WARN [ConnectionState] [SomeService STARTING-EventThread] [searcher] [] Connection attempt unsuccessful after 64011 (greater than max timeout of 60000). Resetting connection and trying again with a new connection.\n2016/08/29 16:47:53.819 INFO [ZooKeeper] [SomeService STARTING-EventThread] [searcher] [] Session: 0x255531b1ff4732e closed\n2016/08/29 16:47:53.819 INFO [ZooKeeper] [SomeService STARTING-EventThread] [searcher] [] Initiating client connection, connectString=lva1-app1215.prod.acompany.com:12916,lva1-app1255.prod.acompany.com:12916,lva1-app1295.prod.acompany.com:12916,lva1-app1335.prod.acompany.com:12916,lva1-app1375.prod.acompany.com:12916 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@2eee547b\n2016/08/29 16:47:53.824 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1295.prod.acompany.com:12916)] [searcher] [] Opening socket connection to server lva1-app1295.prod.acompany.com/10.132.48.218:12916. Will not attempt to authenticate using SASL (unknown error)\n2016/08/29 16:47:53.824 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1295.prod.acompany.com:12916)] [searcher] [] Socket connection established to lva1-app1295.prod.acompany.com/10.132.48.218:12916, initiating session\n2016/08/29 16:47:53.827 INFO [ClientCnxn] [SomeService STARTING-SendThread(lva1-app1295.prod.acompany.com:12916)] [searcher] [] Session establishment complete on server lva1-app1295.prod.acompany.com/10.132.48.218:12916, sessionid = 0x355531abd0978d1, negotiated timeout = 40000\n2016/08/29 16:47:53.827 INFO [ConnectionStateManager] [SomeService STARTING-EventThread] [searcher] [] State change: RECONNECTED\n2016/08/29 16:47:53.827 INFO [ClientCnxn] [SomeService STARTING-EventThread] [searcher] [] EventThread shut down",
        "Issue Links": []
    },
    "CURATOR-347": {
        "Key": "CURATOR-347",
        "Summary": "Shared read/write lock acquired by one thread cannot be released by another",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Meni Hillel",
        "Created": "31/Aug/16 23:32",
        "Updated": "25/Oct/16 21:43",
        "Resolved": null,
        "Description": "Consider the following: lock is being acquired by main thread but released by another thread. This throws an exception:\njava.lang.IllegalMonitorStateException: You do not own the lock: /locks/abc\n\tat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:140)\nAre locks thread specific? That wouldn't make sense. How else can I achieve this? Also would be nice to have a non-reentrant read/write shared lock.\npublic static void main(String[] args) throws Exception {\n    final CuratorFramework client = CuratorFrameworkFactory.builder()\n            .connectString(ApplicationProperties.getConfig().getMessagebusSyncServers()).sessionTimeoutMs(5000)\n            .connectionTimeoutMs(3000).retryPolicy(new ExponentialBackoffRetry(1000, 3)).build();\n    client.start();\n    InterProcessReadWriteLock lock = new InterProcessReadWriteLock(client, \"/locks/abc\");\n    lock.writeLock().acquire();\n    Thread r = new Thread() {\n        @Override\n        public void run() {\n            try \n{\n                InterProcessReadWriteLock lock = new InterProcessReadWriteLock(client, \"/locks/abc\");\n                lock.writeLock().release();\n\n            }\n catch (Exception e) \n{\n                e.printStackTrace();\n            }\n        }\n    };\n    r.start();\n    r.join();\n}",
        "Issue Links": []
    },
    "CURATOR-348": {
        "Key": "CURATOR-348",
        "Summary": "Schema.validateCreate() should use the path",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "3.2.1",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Sep/16 15:41",
        "Updated": "07/Sep/16 00:26",
        "Resolved": "07/Sep/16 00:26",
        "Description": "For some reason, I forgot to put the path in the Schema.validateCreate() - this is a serious mistake.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/162"
        ]
    },
    "CURATOR-349": {
        "Key": "CURATOR-349",
        "Summary": "Expose extra metrics in TracerDriver",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.2.1,                                            2.11.1",
        "Component/s": "Framework",
        "Assignee": "Fangmin Lv",
        "Reporter": "Fangmin Lv",
        "Created": "15/Sep/16 08:32",
        "Updated": "11/Mar/17 23:58",
        "Resolved": "25/Oct/16 15:18",
        "Description": "Currently, the TracerDriver exposed the latency of ZK operations, in multi-tenant environment, extra metrics are required to help tracing and monitoring:\n\nthe bytes being sent and received, so we can monitor the client usage scenarios.\nwhich ensemble participant the client is talking to, used to find out the problematic Zk server when the issue happened.\nthe z-node path, to easily find out which z-node caused the problem, like high load, etc.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/165",
            "https://github.com/apache/curator/pull/169",
            "https://github.com/apache/curator/pull/172"
        ]
    },
    "CURATOR-350": {
        "Key": "CURATOR-350",
        "Summary": "Allow to close executor service on shutdown",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Olivier Liegeon",
        "Created": "19/Sep/16 19:59",
        "Updated": "21/Sep/16 15:30",
        "Resolved": "21/Sep/16 15:30",
        "Description": "In Issue 251, we got the ability to create a ChildrenCache and passing an ExecutorService. \nThis change does allow to close this service when closing the childrenCache. This should be a parameter, false by default to keep current behavior\nin CloseableExecutorService\n\n/**\n     * @param executorService the service to decorate\n     */\n    public CloseableExecutorService(ExecutorService executorService)\n    {\n       this(executorService, false);\n    }\n\n\n\nif (shutdownOnClose) {\n            executorService.shutdownNow();\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/163"
        ]
    },
    "CURATOR-351": {
        "Key": "CURATOR-351",
        "Summary": "ZOOKEEPER-2169 will add support TTL Nodes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "28/Sep/16 12:11",
        "Updated": "25/Apr/17 15:01",
        "Resolved": "25/Apr/17 14:52",
        "Description": "ZOOKEEPER-2169 will add support TTL Nodes. Upgrade the Curator DSL to support TTLs.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/171"
        ]
    },
    "CURATOR-352": {
        "Key": "CURATOR-352",
        "Summary": "SchemaViolation errors do not contain enough information",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "3.2.1",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "03/Oct/16 12:04",
        "Updated": "10/Oct/16 09:53",
        "Resolved": "10/Oct/16 09:53",
        "Description": "When a SchemaViolation is thrown, it only has information about the schema entry. It would be much more useful if it also had information about the bad action - path, acl, etc. so that the problem can be debugged.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/167"
        ]
    },
    "CURATOR-353": {
        "Key": "CURATOR-353",
        "Summary": "PathChildrenCache and PersistentNode interrupt each other if sharing the same executor",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Simon Cooper",
        "Created": "04/Oct/16 15:37",
        "Updated": "05/Jan/17 14:08",
        "Resolved": null,
        "Description": "In our example, we've got a PathChildrenCache and PersistentNode sharing the same executor.\nThis interacts badly with interrupts - when PathChildrenCache is closed, this calls CloseableExecutorService.close(), which interrupts all the current pending tasks via Future.cancel(true).\nIf PersistentNode is then closed, there's a chance that the deleteNode() will happen on a thread on the shared executor that was interrupted by the closing executor service. This throws InterruptedException, which is caught and rethrown as an IOException. This results in spurious failures in our tests when the close randomly throws IOException.\nI'm not sure that re-throwing an InterruptedException as an IOException is a good idea - there's already code that re-sets the interrupt status on the thread, should it just leave it at that? Else it looks like the thread has been interrupted twice.",
        "Issue Links": [
            "/jira/browse/CURATOR-375"
        ]
    },
    "CURATOR-354": {
        "Key": "CURATOR-354",
        "Summary": "GzipCompressionProvider leaks native memory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.0",
        "Fix Version/s": "3.2.1,                                            2.11.1",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Evan Pollan",
        "Created": "06/Oct/16 20:27",
        "Updated": "13/Oct/16 02:26",
        "Resolved": "13/Oct/16 02:26",
        "Description": "Both compress and decompress instantiate GZIP streams, use them, and don't close them.  The java util deflate/inflate streams use JNI to leverage zlib, including the allocation of a native, off-heap buffer.  \nThese JNI buffer handles are released when the containing stream is closed.  They're also released by the finalize() method on java.util.zip.Inflater/Deflater, but it looks as if the native memory can pile up quickly enough in certain use cases where the JVM is OOM killed before the finalizer does its thing.\nFix is to explicitly close these streams after they're used.\nThis was discovered in version 2.11.0, but it looks like the problem affects all versions for which GzipCompressionProvider existed.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/168"
        ]
    },
    "CURATOR-355": {
        "Key": "CURATOR-355",
        "Summary": "Curator client fails when connecting to read-only ensemble",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Benjamin Jaton",
        "Created": "10/Oct/16 17:09",
        "Updated": "28/Mar/19 22:17",
        "Resolved": null,
        "Description": "ZK is 3.5.1-alpha\nI have a 3 nodes ZK cluster , readonly mode is enabled.\n2 nodes are down, so one of them (QA-E8WIN11) is in read-only (verified by using the ZK API manually). All the machines of the ensemble can be pinged from the client.\nI'm using this piece of code:\n\n\t\tBuilder curatorClientBuilder = CuratorFrameworkFactory.builder()\n\t\t\t\t.connectString(\"QA-E8WIN11:2181,QA-E8WIN12:2181\")\n\t\t\t\t.sessionTimeoutMs(45000).connectionTimeoutMs(15000)\n\t\t\t\t.retryPolicy(new RetryNTimes(3, 5000)).canBeReadOnly(true);\n\n\t\tCuratorFramework client = curatorClientBuilder.build();\n\t\tclient.start();\n\t\tclient.getZookeeperClient().blockUntilConnectedOrTimedOut();\n\t\tSystem.out.println(\"Successfully established the connection with ZooKeeper\");\n\t\t\n\t\tclient.getData().forPath(\"/\");\n\t\tSystem.out.println(\"Done.\");\n\nWhen curator pick the host that is UP first, it goes through very quickly. When it picks the host that is down first (QA-E8WIN12), it seems to be stuck at the getData() call for a very long time, and then eventually fail with a ConnectionLossException. (see attached log)",
        "Issue Links": []
    },
    "CURATOR-356": {
        "Key": "CURATOR-356",
        "Summary": "Allow SASL configuration for TestingServer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.0",
        "Fix Version/s": "3.2.1,                                            2.11.1",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Enrico Olivelli",
        "Created": "14/Oct/16 12:56",
        "Updated": "21/Nov/16 16:28",
        "Resolved": "26/Oct/16 22:36",
        "Description": "I would like to set the folloing properties on my TestingServer\n\nproperties.setProperty(\"authProvider.1\", \n\"org.apache.zookeeper.server.auth.SASLAuthenticationProvider\");\nproperties.setProperty(\"kerberos.removeHostFromPrincipal\", \"true\");\nproperties.setProperty(\"kerberos.removeRealmFromPrincipal\", \"true\");\n\n\nWould it  be better to let QuorumConfigBuilder (and InstanceSpec) accept a custom Properties object to be merged to the actual configuration ?",
        "Issue Links": [
            "https://github.com/apache/curator/pull/170"
        ]
    },
    "CURATOR-357": {
        "Key": "CURATOR-357",
        "Summary": "creatingParentsIfNeeded do not create \"/\"",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Vitalii Tymchyshyn",
        "Created": "11/Nov/16 00:49",
        "Updated": "10/May/23 14:16",
        "Resolved": "10/May/23 10:56",
        "Description": "In Zookeeper there is no guaranty that \"/\" would exist. In a case when chroot feature is used, \"/\" path may not exists.\nCurrently Curator incorrectly assumes that \"/\" always exists. It leads to NoNodeException when there is no \"/\" available.\nE.g. see CURATOR-280 for example.\nA solution would be to create \"/\" in  ZKPaths#mkdirs. \nPrimary concern is that it would slow down general case when entity name do not have path separator. Currently it does not require any additional calls, now \"/\" check would be needed.\nA solution can be to change logic to create parents only if NoNode received for node creation call. This would also speed up all the cases where entity name has separator, including when namespaces are used.\nTest:\n\npublic class TestChroot {\n    private TestingServer server = new TestingServer();\n\n    public TestChroot() throws Exception {\n    }\n\n    @Test\n    public void testCurator() throws Exception {\n        CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString() + \"/chrootCurator\", new RetryOneTime(1000));\n        client.start();\n        client.create().creatingParentsIfNeeded().forPath(\"/test\", new byte[]{1});\n        Assert.assertEquals(1, client.getData().forPath(\"/test\")[0]);\n    }\n\n    @Test\n    public void testZookeeper() throws Exception {\n        CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString() + \"/chrootZoo\", new RetryOneTime(1000));\n        client.start();\n        ZooKeeper zooKeeper = client.getZookeeperClient().getZooKeeper();\n        zooKeeper.create(\"/\", new byte[]{}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        zooKeeper.create(\"/test\", new byte[]{1}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        Assert.assertEquals(1, client.getData().forPath(\"/test\")[0]);\n    }\n}",
        "Issue Links": [
            "/jira/browse/CURATOR-280",
            "/jira/browse/CURATOR-50",
            "/jira/browse/CURATOR-280"
        ]
    },
    "CURATOR-358": {
        "Key": "CURATOR-358",
        "Summary": "Receiving KeeperException with NoNode when LeaderLatch#getLeader()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Satish Duggana",
        "Created": "20/Nov/16 08:35",
        "Updated": "12/Oct/17 10:08",
        "Resolved": "22/Nov/16 22:11",
        "Description": "org.apache.curator.framework.recipes.leader.LeaderLatch#getLeader() throws KeeperException with Code#NONODE intermittently as mentioned in the stack trace below. It may be possible  participant's ephemeral ZK node is removed because its connection/session is closed. \nYou can see the below code at https://github.com/apache/curator/blob/master/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L451\npublic Participant getLeader() throws Exception\n{\n    Collection<String> participantNodes = LockInternals.getParticipantNodes(client, latchPath, LOCK_NAME, sorter);\n    return LeaderSelector.getLeader(client, participantNodes);\n}\nI guess it hits a race condition where a participant node is retrieved but when it invokes LeaderSelector#getLeader() it would have been removed because of session timeout and it throws KeeperException with NoNode code. It does not retry as the RetryLoop retries only for connection/session timeouts. But in this case, NoNode should have been retried. I could not find any APIs on CuratorClient to configure the kind of KeeperException codes to be retried. It may be good to have a way to take what kind of errors should be retried in org.apache.curator.framework.CuratorFrameworkFactory.Builder APIs. \nIntermittent Exception found with the stack trace:\n2016-11-15 06:09:33.954 o.a.s.d.nimbus [ERROR] Error when processing event\norg.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock/_c_97c09eed-5bba-4ac8-a05f-abdc4e8e95cf-latch-0000000002\n     at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n     at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n     at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n     at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)\n     at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)\n     at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)\n     at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)\n     at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)\n     at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)\n     at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)\n     at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)\n     at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:454)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/173"
        ]
    },
    "CURATOR-359": {
        "Key": "CURATOR-359",
        "Summary": "Curator can't connect to Read Only ensemble",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "23/Nov/16 23:15",
        "Updated": "27/Nov/16 21:27",
        "Resolved": null,
        "Description": "The read requests fail when connecting to a read only ensemble.\nI made a class for this using TestingCluster, but it requires to set iptables rules in order to mimic a real (remote) read-only ensemble. The test will do this:\n1) start a TestingCluster\n2) stop 2 of the 3 nodes\n3) user has to run the iptables commands (specified in the logs)\n4) a read request is issued every 3 seconds\nAll the read requests block for approx 2 minutes, then they all throw an exception (ConnectionLoss).",
        "Issue Links": []
    },
    "CURATOR-360": {
        "Key": "CURATOR-360",
        "Summary": "Allow Zookeeper servers in TestingCluster to listen on network interfaces other than localhost",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Imesha Sudasingha",
        "Created": "29/Nov/16 23:47",
        "Updated": "05/Dec/16 05:09",
        "Resolved": "05/Dec/16 02:09",
        "Description": "Currently, when we create a TestingCluster, all the TestingServers will be bound to the local interface (127.0.0.1). This becomes a constraint if we want to run a TestingCluster which can be used for testing over a network (say LAN).\nIn order to address that we can add a hostname to be provided optionally in the InstanceSpec.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/175"
        ]
    },
    "CURATOR-361": {
        "Key": "CURATOR-361",
        "Summary": "NamespaceImpl.unfixForNamespace() removes leading slash if namespace is empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Charles Ruhland",
        "Created": "07/Dec/16 01:37",
        "Updated": "07/Dec/16 02:51",
        "Resolved": "07/Dec/16 02:50",
        "Description": "When using a CuratorFramework client with a namespace set explicitly to the empty string via the builder, calls to unfixForNamespace() on its namespace field will result in invalid paths that are missing their leading slash; e.g. /foo/bar becomes foo/bar.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/176"
        ]
    },
    "CURATOR-362": {
        "Key": "CURATOR-362",
        "Summary": "Curator framework create API with ACL Mode is not adding the ACL to the parent nodes if select creatingParentsIfNeeded",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.9.1,                                            3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Pavan",
        "Created": "07/Dec/16 09:32",
        "Updated": "19/Jul/17 14:41",
        "Resolved": "19/Jul/17 14:41",
        "Description": "1) We are trying to create the node in the zookeeper using the curator framework 2.9.1 version by using the following api \n CuratorFramework.create().creatingParentsIfNeeded().withACL(authACList())\n                                .forPath(\"/abc/xyz/\", value)\n2) Here ACL is applied only for xyz and the parent node like abc there is no ACL info. So any client can add/remove data without ACL \nSummary :  CuratorClient even though ACL is specified its not applying for the parents.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/223"
        ]
    },
    "CURATOR-363": {
        "Key": "CURATOR-363",
        "Summary": "Adding of the Node/Path Cache listener is creating the zookeeper path without ACL",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Pavan",
        "Created": "07/Dec/16 09:50",
        "Updated": "07/Dec/16 09:53",
        "Resolved": null,
        "Description": "1) Recently inorder to avoid the OutOfMemory Issues, curator creates the path if the path is not present while adding the PathCacheListner. But there is no API to take the ACL details its creating the path without ACL \n2) NodeCache nodeCache = new NodeCache(client, path);\n    nodeCache.start(true);    \n   PathChildrenCache childrenCache = new PathChildrenCache(client, path,true);\n   childrenCache.start(PathChildrenCache.StartMode.NORMAL);\n3) So curator should provide the API to take the ACL and use the same when creating the path in case the path is not present",
        "Issue Links": []
    },
    "CURATOR-364": {
        "Key": "CURATOR-364",
        "Summary": "Persistent Node Constructor Requires a Started ZK Connection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1,                                            2.11.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ilya Biryukov",
        "Created": "14/Dec/16 11:07",
        "Updated": "09/Jan/17 19:26",
        "Resolved": "09/Jan/17 16:28",
        "Description": "Steps to reproduce:\n1. Initialise an instance of PersistentNode\n2. Ensure that CuratorFramework is Not Started (Latent)\nExpected behaviour:\nPersistentNode constructor should work without an active CuratorFramework connection to ZK\nActual behaviour:\nAn Exception is thrown:\n\nCaused by: java.lang.IllegalStateException: instance must be started before calling this methodDEBUG [2016-12-14 10:25:25,641] akka.event.EventStream: logger log1-Slf4jLogger started\n\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:150)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.create(CuratorFrameworkImpl.java:351)\n\tat org.apache.curator.framework.recipes.nodes.PersistentNode.<init>(PersistentNode.java:173)\n\n\nThoughts\nThis behaviour is inconsistent with other Zk recepies based on Closable interface. PersistentNode should not attempt any Zk communication until start() method is called.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/177"
        ]
    },
    "CURATOR-365": {
        "Key": "CURATOR-365",
        "Summary": "backgroundCreateParentsThenNode() ignores exceptions too crudely",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.0,                                            2.11.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "16/Dec/16 22:26",
        "Updated": "19/Dec/16 20:03",
        "Resolved": "19/Dec/16 20:03",
        "Description": "backgroundCreateParentsThenNode() in CreateBuilderImpl,java is ignoring all KeeperExceptions. This can cause an infinite loop if the KeeperException is not an ignorable one. \nHere's a code snippet that shows the problem:\n\n@Test\npublic void testIt() throws Exception\n{\n    ACLProvider provider = new ACLProvider()\n    {\n        @Override\n        public List<ACL> getDefaultAcl()\n        {\n            return ZooDefs.Ids.OPEN_ACL_UNSAFE;\n        }\n\n        @Override\n        public List<ACL> getAclForPath(String path)\n        {\n            if ( path.equals(\"/ns/one\") )\n            {\n                try\n                {\n                    return Collections.singletonList(new ACL(ZooDefs.Perms.ALL, new Id(\"digest\", DigestAuthenticationProvider.generateDigest(\"test\"))));\n                }\n                catch ( NoSuchAlgorithmException e )\n                {\n                    e.printStackTrace();\n                }\n            }\n            return getDefaultAcl();\n        }\n    };\n    try ( CuratorFramework client = createClient(provider, new AuthInfo(\"digest\", DigestAuthenticationProvider.generateDigest(\"test\").getBytes())) )\n    {\n        LeaderLatch latch = new LeaderLatch(client, \"/one/two\");\n        latch.start();\n        latch.await();\n        System.err.println(\"hdeherherhe\");\n    }\n}\n\nprivate CuratorFramework createClient(ACLProvider provider, AuthInfo authInfo) throws Exception\n{\n    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n        .namespace(\"ns\")\n        .connectString(server.getConnectString())\n        .retryPolicy(retryPolicy)\n        ;\n    if ( provider != null )\n    {\n        builder = builder.aclProvider(provider);\n    }\n    if ( authInfo != null )\n    {\n        builder = builder.authorization(authInfo.getScheme(), authInfo.getAuth());\n    }\n    CuratorFramework client = builder.build();\n    client.start();\n    return client;\n}\n\n\nWhile this is running you can put a breakpoint on CreateBuilderImpl#findProtectedNodeInForeground() and see it get called over and over.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/178"
        ]
    },
    "CURATOR-366": {
        "Key": "CURATOR-366",
        "Summary": "curator hangup and throw a CuratorConnectionLossException after timeout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Apache,                                            Framework",
        "Assignee": null,
        "Reporter": "xsank mar",
        "Created": "21/Dec/16 11:51",
        "Updated": "22/Dec/16 02:28",
        "Resolved": "22/Dec/16 02:28",
        "Description": "here is the traceback:\n\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:197)\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:116)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:489)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:243)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:237)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:234)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:218)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42)\n\n\ni have tried the curator version of 2.6.0, 2.7.1, 2.9.1, 2.10.0,  the version of zookeeper server is 3.3.5",
        "Issue Links": []
    },
    "CURATOR-367": {
        "Key": "CURATOR-367",
        "Summary": "Curator may deliver RECONNECTED before LOST in case of session expiry",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "2.12.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Zoltan Szekeres",
        "Created": "21/Dec/16 15:40",
        "Updated": "28/Feb/17 22:43",
        "Resolved": "28/Feb/17 22:43",
        "Description": "Behaviour:\nWe saw our code blocked at client.blockUntilConnected() after reconnected on session expiry.\nPossible reason:\nAfter receiving a session expired event ConnectionState first resets the connection then notifies the parent watchers, where the CuratorEvent is created. In this case it seems the execution of the first zookeeper event thread was delayed before calling the parent watchers. Meanwhile a new zookeeper event thread was created due to calling reset and this new thread sent the SyncConnected event earlier than SessionExpired was sent to parent watchers in the first thread. This resulted ConnectionStateListener instances seeing the RECONNECTED before the LOST.\nLogs:\n2016-11-17T20:23:28.527Z [Thread-0-SendThread(]  INFO              ClientCnxn: Opening socket connection to server _\n2016-11-17T20:23:28.535Z [Thread-0-SendThread(]  INFO              ClientCnxn: Socket connection established to _, initiating session\n2016-11-17T20:23:28.576Z [Thread-0-SendThread(]  INFO              ClientCnxn: Unable to reconnect to ZooKeeper service, session 0xc585ba1e7b6adc2 has expired, closing socket connection\n2016-11-17T20:23:28.576Z [Thread-0-EventThread]  WARN         ConnectionState: Session expired event received\n2016-11-17T20:23:28.673Z [Thread-0-EventThread]  INFO               ZooKeeper: Initiating client connection, connectString=_ sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@6ddf3f9d\n2016-11-17T20:23:28.691Z [Thread-0-SendThread(]  INFO              ClientCnxn: Opening socket connection to server _\n2016-11-17T20:23:28.693Z [Thread-0-SendThread(]  INFO              ClientCnxn: Socket connection established to _, initiating session\n2016-11-17T20:23:28.701Z [Thread-0-SendThread(]  INFO              ClientCnxn: Session establishment complete on server _, sessionid = 0x2585ba1e69ffeca, negotiated timeout = 30000\n2016-11-17T20:23:28.701Z [Thread-0-EventThread]  INFO  ConnectionStateManager: State change: RECONNECTED\n2016-11-17T20:23:28.715Z [Thread-0-EventThread]  INFO  ConnectionStateManager: State change: LOST\n2016-11-17T20:23:28.715Z [Thread-0-EventThread]  INFO              ClientCnxn: EventThread shut down\nReproduction:\nI was only able to reproduce the behaviour by adding artificial Thread.sleep in ConnectionState#process before calling the parent watchers if the event is session expired.\nConnectionState#process\n@Override\npublic void process(WatchedEvent event)\n{\n\tif ( LOG_EVENTS )\n\t{\n\t\tlog.debug(\"ConnectState watcher: \" + event);\n\t}\n\n\tif ( event.getType() == Watcher.Event.EventType.None )\n\t{\n\t\tboolean wasConnected = isConnected.get();\n\t\tboolean newIsConnected = checkState(event.getState(), wasConnected);\n\t\tif ( newIsConnected != wasConnected )\n\t\t{\n\t\t\tisConnected.set(newIsConnected);\n\t\t\tconnectionStartMs = System.currentTimeMillis();\n\t\t}\n\t}\n\n\tif (event.getState() == KeeperState.Expired)\n\t{\n\t\tSystem.err.println(\"::> sleep in ConnectionState#process\");\n\t\ttry {\n\t\t\tThread.sleep(1000);\n\t\t} catch (InterruptedException e) {}\n\t}\n\n\tfor ( Watcher parentWatcher : parentWatchers )\n\t{\n\t\tTimeTrace timeTrace = new TimeTrace(\"connection-state-parent-process\", tracer.get());\n\t\tparentWatcher.process(event);\n\t\ttimeTrace.commit();\n\t}\n}\n\n\nSome ideas for fix:\n\nAdd the event handling and calling parent watchers into a synchronized block.\nChange the order of handling watched event and calling parent watchers (I'm not aware of the behaviour implications of this).\nMove only calling reset to the end of the method \"process\".",
        "Issue Links": [
            "https://github.com/apache/curator/pull/197"
        ]
    },
    "CURATOR-368": {
        "Key": "CURATOR-368",
        "Summary": "powermock will cause curator CONNECTIONLOSS exception",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "xsank mar",
        "Created": "22/Dec/16 02:55",
        "Updated": "22/Dec/16 06:04",
        "Resolved": null,
        "Description": "client code\n\npublic class CuratorClient {\n    private CuratorFramework client;\n\n    public CuratorClient(String url) {\n        this.client = CuratorFrameworkFactory.builder().connectString(url).namespace(\"test\")\n                .retryPolicy(new RetryNTimes(3, 5000))\n                .connectionTimeoutMs(3000).build();\n        this.client.start();\n    }\n\n    public void create(String path) {\n        try {\n            this.client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    public void lsChildren(String path) {\n        try {\n            List<String> children = this.client.getChildren().forPath(path);\n            for (String child : children) {\n                System.out.println(child);\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    public void close() {\n        this.client.close();\n    }\n}\n\n\nTest code\n\n@RunWith(PowerMockRunner.class)\n@PrepareForTest({Static.class})\npublic class CuratorTest {\n    private CuratorClient client;\n    private TestingServer server;\n\n    @Before\n    public void setUp() throws Exception {\n        server = new TestingServer();\n        server.start();\n        client = new CuratorClient(server.getConnectString());\n    }\n\n    @After\n    public void tearDown() throws Exception {\n        client.close();\n        server.close();\n    }\n\n    @Test\n    public void testCreate() throws Exception {\n        client.create(\"/curator\");\n    }\n\n    @Test\n    public void lsChildren() throws Exception {\n        client.lsChildren(\"/curator\");\n    }\n}\n\n\nnow start testing, you will catch the exception everytime:\n\norg.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss\n\tat org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:195)\n\tat org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:87)\n\tat org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:487)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:720)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:703)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:699)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:477)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:467)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:447)\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n\tat org.alibaba.search.client.CuratorClient.create(CuratorClient.java:25)\n\tat org.alibaba.search.client.CuratorTest.testCreate(CuratorTest.java:35)",
        "Issue Links": []
    },
    "CURATOR-369": {
        "Key": "CURATOR-369",
        "Summary": "Improve log format in EnsembleTracker.processConfigData()",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "3.3.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sylvain Wallez",
        "Created": "22/Dec/16 15:25",
        "Updated": "28/Dec/16 17:42",
        "Resolved": "28/Dec/16 17:42",
        "Description": "EnsembleFrameworkTracker has a log statement that doesn't format its value correctly and is adding noise in log files.\nSample output of this statement is as follows:\n\n[2016-12-09 12:16:26,046][INFO ][org.apache.curator.framework.imps.EnsembleTracker][] New config event received: [115, 101, 114, 118, 101, 114, 46, 49, 48, 61, 108, 111, 99, 97, 108, 104, 111, 115, 116, 58, 50, 56, 57, 56, 58, 51, 56, 57, 56, 58, 112, 97, 114, 116, 105, 99, 105, 112, 97, 110, 116, 59, 48, 46, 48, 46, 48, 46, 48, 58, 50, 49, 57, 49, 10, 118, 101, 114, 115, 105, 111, 110, 61, 49, 48, 48, 48, 48, 48, 48, 48, 48]\n\n\nI'd like to provide a fix for this by logging the properties object once it's been parsed, but I'm confused about what branch to patch against. This class exists in the CURATOR-3.0 branch, but not in master.\nIs creating a PR against the CURATOR-3.0 branch the way to go?",
        "Issue Links": [
            "https://github.com/apache/curator/pull/179"
        ]
    },
    "CURATOR-370": {
        "Key": "CURATOR-370",
        "Summary": "Replace use of MoreExecutors.sameThreadExecutor",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "3.2.1,                                            2.11.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Simon Cooper",
        "Created": "23/Dec/16 13:23",
        "Updated": "08/Jan/17 05:21",
        "Resolved": "08/Jan/17 01:05",
        "Description": "ListenerContainer has a reference to MoreExecutors.sameThreadExecutor in guava. This method has been deprecated since guava version 18.0, and is being removed in version 21.0. This should be changed to use MoreExecutors.directExecutor instead.\nThe use of this method means that all projects using Curator can't use a guava version past 20.0, as the method no longer exists in 21, so I would say this is quite high priority to fix, especially since 21.0 is the first Java 8 version of guava.",
        "Issue Links": [
            "/jira/browse/CURATOR-200",
            "/jira/browse/CURATOR-263",
            "https://github.com/apache/curator/pull/188"
        ]
    },
    "CURATOR-371": {
        "Key": "CURATOR-371",
        "Summary": "Reduce logging level for fallback being used instead of features not available in stable ZooKeeper",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "27/Dec/16 01:46",
        "Updated": "17/Jul/22 07:02",
        "Resolved": "17/Jul/22 07:02",
        "Description": "E.g. https://github.com/apache/curator/blob/master/curator-client/src/main/java/org/apache/curator/utils/ZKPaths.java#L76 outputs annoying warning, although Apache Curator 2.x supports ZooKeeper 3.4.x and there is no other stable Apache ZooKeeper version to which one can upgrade which supports container nodes, at least until ZOOKEEPER-2163 is merged.\nPlease consider using debug log level for this message instead.",
        "Issue Links": []
    },
    "CURATOR-372": {
        "Key": "CURATOR-372",
        "Summary": "CompressionProvider should not take path as an Arguement",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Mridul Verma",
        "Created": "31/Dec/16 07:43",
        "Updated": "09/Jan/17 16:53",
        "Resolved": "09/Jan/17 15:21",
        "Description": "Currently in Curator-Framework in Compression Provider, we \n\ncompress(String path, byte[] data)\n\n\nwe use the path, but the `path` is not required for this functionality.\nSo we should clean this up.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/180"
        ]
    },
    "CURATOR-373": {
        "Key": "CURATOR-373",
        "Summary": "Add option to PersistentNode to not overwrite/delete existing node",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "P\u00e9ter Lakos",
        "Created": "02/Jan/17 16:05",
        "Updated": "06/Sep/22 13:40",
        "Resolved": null,
        "Description": "Currently, when PersistentNode.start() is called, and a node at the specified path already exists, this existing node's data is overwritten with the PersistentNode's data.\nSimilarly, when PersistentNode.close() is called, the node is deleted, even if a node existed at the specified path before PersistentNode.start() was called.\nAdd an option to PersistentNode to not overwrite/delete an existing node.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/182"
        ]
    },
    "CURATOR-374": {
        "Key": "CURATOR-374",
        "Summary": "Reduce memory usage in TreeCache",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": "Scott Blum",
        "Reporter": "Sylvain Wallez",
        "Created": "04/Jan/17 10:36",
        "Updated": "08/Jan/17 16:15",
        "Resolved": "05/Jan/17 17:48",
        "Description": "TreeCache uses 3 AtomicReference objects in its tree nodes to ensure atomic updates. On large trees (we have 1M nodes) this can cause a significant memory overhead.\nUsing volatile fields with a AtomicReferenceFieldUpdater would avoid this while keeping the atomicity garantees.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/184",
            "https://github.com/apache/curator/pull/185"
        ]
    },
    "CURATOR-375": {
        "Key": "CURATOR-375",
        "Summary": "Fix thread interruption being reported twice",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Simon Cooper",
        "Created": "05/Jan/17 14:06",
        "Updated": "10/Jan/17 18:02",
        "Resolved": null,
        "Description": "When a curator operation thread is interrupted, some classes (PersistentNode ConnectionState primarily) report the interruption in two ways at the same time - by re-marking the thread interruption status and throwing InterruptedException - this makes it look like the thread has been interrupted twice, rather than once.",
        "Issue Links": [
            "/jira/browse/CURATOR-353",
            "https://github.com/apache/curator/pull/186"
        ]
    },
    "CURATOR-376": {
        "Key": "CURATOR-376",
        "Summary": "InterProcessSemaphoreMutex is not thread safe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1,                                            2.11.1",
        "Fix Version/s": "3.3.0,                                            2.12.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Yang Tianyang",
        "Created": "06/Jan/17 07:13",
        "Updated": "08/Jan/17 05:44",
        "Resolved": "08/Jan/17 05:44",
        "Description": "In `release` method, `lease.close()` will cause `semaphore.acquire` return, then two threads will write on same `this.lease` (one set it to `acquiredLease` while the other set it to `null`), so `acquire` method will return true and `this.lease` is still null, which can't be `release`.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/187"
        ]
    },
    "CURATOR-377": {
        "Key": "CURATOR-377",
        "Summary": "Curator should have an option to send KeeperState.Expired to watchers when a ZK handle is closed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Jan/17 22:34",
        "Updated": "07/Jan/17 19:27",
        "Resolved": "07/Jan/17 19:27",
        "Description": "Curator manages the ZooKeeper handle internally. Under certain circumstances, Curator will close the current ZooKeeper handle and create a new one. However, any set Watchers in the old ZooKeeper handle will never get triggered. If code is expecting a trigger - even for KeeperState.Expired - it will never happen. Curator should have an option to, when closing the ZooKeeper handle, fake KeeperState.Expired messages to any pending Watchers so that they know to exit.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-2368"
        ]
    },
    "CURATOR-378": {
        "Key": "CURATOR-378",
        "Summary": "Dependency Refresh",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Fangmin Lv",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Jan/17 14:47",
        "Updated": "21/Jul/17 17:51",
        "Resolved": "21/Jul/17 17:51",
        "Description": "It's about time we update the versions of our dependencies. Get the latest Apache parent POM, Maven plugins, etc.",
        "Issue Links": [
            "/jira/browse/CURATOR-398",
            "https://github.com/apache/curator/pull/204"
        ]
    },
    "CURATOR-379": {
        "Key": "CURATOR-379",
        "Summary": "unfixForNamespace corrupts child path when it contains namespace",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Marvin Bredal Lillehaug",
        "Created": "11/Jan/17 09:58",
        "Updated": "06/Sep/22 13:40",
        "Resolved": null,
        "Description": "When a child path starts with the namespace unfixForNamespace removes the namespace string, thus creating a invalid path.\nWith namespace \"foo\" the result of unfixForNamespace(\"/foobar\") is \"bar\".",
        "Issue Links": [
            "https://github.com/apache/curator/pull/194"
        ]
    },
    "CURATOR-380": {
        "Key": "CURATOR-380",
        "Summary": "Service Discovery: ServiceCacheListener should receive the added or deleted service instance when notified",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "july",
        "Created": "18/Jan/17 14:28",
        "Updated": "18/Jan/17 14:28",
        "Resolved": null,
        "Description": "I think it's reasonable to receive the added/deleted service instance when ServiceCacheListener is notified, currently ServiceCacheListener only has one no-arg method:\n public void cacheChanged();\nMaybe we should add(or replace with) another two methods in ServiceCacheListener, like:\nvoid onInstanceAdded(ServiceInstance<T> instance);\nvoid onInstancedRemoved(ServiceInstance<T> instance);",
        "Issue Links": []
    },
    "CURATOR-381": {
        "Key": "CURATOR-381",
        "Summary": "PersistentNode.start() fails if CuratorFramework unable to connect, when using namespace",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Rhys Yarranton",
        "Created": "27/Jan/17 23:28",
        "Updated": "27/Jan/17 23:28",
        "Resolved": null,
        "Description": "Encountered the following exception\n\n2017-01-27 17:41:49.515 ERROR [org.apache.curator.framework.imps.CuratorFrameworkImpl] {main} Ensure path threw exception\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /bids\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1846)\n        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1874)\n        at org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:274)\n        at org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90)\n        at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:67)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)\n        at org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83)\n        at org.apache.curator.framework.imps.CuratorFrameworkImpl.fixForNamespace(CuratorFrameworkImpl.java:695)\n        at org.apache.curator.framework.imps.WatcherRemovalFacade.fixForNamespace(WatcherRemovalFacade.java:176)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:488)\n        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)\n        at org.apache.curator.framework.recipes.nodes.PersistentNode.createNode(PersistentNode.java:414)\n        at org.apache.curator.framework.recipes.nodes.PersistentNode.start(PersistentNode.java:276)\n        ....\n\n\nFor context, the calling code started a CuratorFramework, and then started a PersistentNode.  There was no quorum of servers available at that time.\nFrom the PersistentNode code, you would expect start() to operate in the background.  However, the step NamespaceImpl.fixForNamespace is in the foreground.  In addition to fixing up the path, this method also does an ensure if required, in a retry loop.\nThe retry loop eventually terminated w/ the above error, apparently because the CuratorFramework decided it wasn't going to be able to connect any time soon.  (And until that point the PersistentNode.start() call would not have returned.)",
        "Issue Links": []
    },
    "CURATOR-382": {
        "Key": "CURATOR-382",
        "Summary": "ConnectionState does not sync startup of ExhibitorEnsembleProvider and Zookeeper connection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Egor Ryashin",
        "Created": "28/Jan/17 13:14",
        "Updated": "30/Jan/17 16:26",
        "Resolved": "30/Jan/17 16:26",
        "Description": "Use CuratorFrameworkFactory.Builder and specify ExhibitorEnsembleProvider.\nCall build() and start().\nInternal ConnectionState.start() calls ensembleProvider.start() which should poll for hostnames to produce connectionString.\nWithout waiting (for connectionString) ConnectionState calls zooKeeper.closeAndReset() and ClientCnxn is created with empty connectionString. That leads to lame zooKeeper sending requests to localhost.\n\n2017-01-27T22:56:17,618 INFO  [Agents-0] org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting\n2017-01-27T22:56:17,619 INFO  [Agents-0] org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString= sessionTimeout=60001 watcher=org.apache.curator.ConnectionState@4402fad2\n2017-01-27T22:56:17,625 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2017-01-27T22:56:18,632 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused: no further information\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]\n2017-01-27T22:56:19,733 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2017-01-27T22:56:19,807 INFO  [Curator-ExhibitorEnsembleProvider-0] org.apache.curator.ensemble.exhibitor.ExhibitorEnsembleProvider - Connection string has changed. Old value (), new value (172.19.2.158:2181,172.19.2.15:2181,172.19.2.177:2181,172.19.2.4:2181,172.19.2.89:2181,172.19.2.72:2181)\n2017-01-27T22:56:20,734 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused: no further information\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]\n2017-01-27T22:56:21,835 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)",
        "Issue Links": []
    },
    "CURATOR-383": {
        "Key": "CURATOR-383",
        "Summary": "Log unexpected response codes in PersistentNode recipe",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1,                                            2.11.1",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Zoltan Szekeres",
        "Created": "30/Jan/17 11:59",
        "Updated": "09/Mar/17 22:31",
        "Resolved": "09/Mar/17 22:31",
        "Description": "With CURATOR-228 NoAuth response code is logged in the PersistentNode recipe if the client has no authorisation to create the node. However it can happen that the \"create\" permission has been given, but not \"write\" persmission, so further setData calls fail silently. I know this is a client error and the permissions should be fixed, but we would like to give better visibility to our users if they face this issue. Since we don't have access to the background callback within the recipe I thought logging the unexpected response code would be useful.\nThis method could be called from the background callback.\n\nprivate void logUnexecpedResponseCode(int responseCode)\n{\n  Code code = Code.get(responseCode);\n  log.warn(\"Client received unexpected response code '{}' while creating/writing node on path {}', code, getActualPath());\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/201"
        ]
    },
    "CURATOR-384": {
        "Key": "CURATOR-384",
        "Summary": "EnsembleTacker.configToConnectionString can render connection impossible",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Rhys Yarranton",
        "Created": "06/Feb/17 21:26",
        "Updated": "02/Apr/23 06:58",
        "Resolved": "02/Apr/23 06:58",
        "Description": "When Curator starts, it uses the connection string supplied by the user.  However, with Ensembles enabled by default that connection string will later be overriden by the value constructed in EnsembleTacker.configToConnectionString.\nHowever, in some cases configToConnectionString produces an unusable value.  If the connection is lost, the reconnect will attempt to use the computed value, and will fail.  That node will be forever lost.\nExample:  The following is a valid ZooKeeper server configuration entry:\n\nserver.1: somehost:22888:32888;22181\n\nZooKeeper will default the client address host to 0.0.0.0 (i.e., listen on all interfaces).  The ZooKeeper server reports this as localhost:22888:32888:participant;0.0.0.0:22181.  configToConnectionString then turns this into 0.0.0.0:22181.  Which will lead to connection refused exceptions.\nTwo possible workarounds.  One is to change the server config to listen on a specific host address for client connections.  Another would be to explicitly provide the EnsembleProvider, e.g., FixedEnsembleProvider with updateServerListEnabled set to false.",
        "Issue Links": [
            "/jira/browse/CURATOR-664"
        ]
    },
    "CURATOR-385": {
        "Key": "CURATOR-385",
        "Summary": "Synchronize startup of ExhibitorEnsembleProvider and Zookeeper connection",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "2.10.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Egor Ryashin",
        "Created": "07/Feb/17 22:59",
        "Updated": "10/May/23 15:27",
        "Resolved": "10/May/23 15:27",
        "Description": "Right now starting CuratorFramework with ExhibitorEnsembleProvider could cause faulty zookeeper connection.\nTo check this use CuratorFrameworkFactory.Builder and specify ExhibitorEnsembleProvider.\nCall build() and start().\nInternal ConnectionState.start() calls ensembleProvider.start() which should poll for hostnames to produce connectionString.\nWithout waiting (for connectionString) ConnectionState calls zooKeeper.closeAndReset() and ClientCnxn is created with empty connectionString. That leads to lame zooKeeper sending requests to localhost.\n\n2017-01-27T22:56:17,618 INFO  [Agents-0] org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting\n2017-01-27T22:56:17,619 INFO  [Agents-0] org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString= sessionTimeout=60001 watcher=org.apache.curator.ConnectionState@4402fad2\n2017-01-27T22:56:17,625 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2017-01-27T22:56:18,632 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused: no further information\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]\n2017-01-27T22:56:19,733 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2017-01-27T22:56:19,807 INFO  [Curator-ExhibitorEnsembleProvider-0] org.apache.curator.ensemble.exhibitor.ExhibitorEnsembleProvider - Connection string has changed. Old value (), new value (172.19.2.158:2181,172.19.2.15:2181,172.19.2.177:2181,172.19.2.4:2181,172.19.2.89:2181,172.19.2.72:2181)\n2017-01-27T22:56:20,734 WARN  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused: no further information\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_74]\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_74]\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.4.5.jar:3.4.5-1392090]\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068) [zookeeper-3.4.5.jar:3.4.5-1392090]\n2017-01-27T22:56:21,835 INFO  [Agents-0-SendThread(127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n\n\nAs a workaround a user could poll for connection string using that provider beforehand.\nThe goal of the improvement is to allow user to call CuratorFramework.start() and got a working framework without an additional poll method call. Using poll before start() actually uncovers implementation details and is a vague technique for a general user.",
        "Issue Links": []
    },
    "CURATOR-386": {
        "Key": "CURATOR-386",
        "Summary": "Allow listener to be passed in to PersistentNode to notify for node creation events",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Alex Kira",
        "Created": "10/Feb/17 18:31",
        "Updated": "25/Apr/17 03:09",
        "Resolved": null,
        "Description": "I think it would be useful to allow a listener to be passed in to the PersistentNode that would notify when the new node is created. This is useful as some cases such as disconnect / reconnect or when an ephemeral node is deleted and recreated by PersistentNode. In this case, I would like to be able to listen to these even so I can do something like issue a watch on the node.  \nFor example:\n```\npublic interface PersistentNodeListener {\n    /**\n\nCalled on a persistentNode event when node is created\n     *\n@param path Path of the znode\n@throws Exception errors\n     */\n    void nodeCreated(String path) throws Exception;\n}\n```\n\nI have a code change implementing this and can issue a pull request for this.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/198"
        ]
    },
    "CURATOR-387": {
        "Key": "CURATOR-387",
        "Summary": "API to validate file path in zookeeper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Information Provided",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": "Zili Chen",
        "Reporter": "Abhijeet Jadhav",
        "Created": "17/Feb/17 14:54",
        "Updated": "17/Jul/22 07:16",
        "Resolved": "17/Jul/22 07:16",
        "Description": "Is there a java API to validate file path in zookeeper or curator framework conforming to ZooKeeper data model? If not, is it planned? it is required validate filepath in zookeeper.",
        "Issue Links": []
    },
    "CURATOR-388": {
        "Key": "CURATOR-388",
        "Summary": "PathChildrenCache stops working if container node is auto-removed and later recreated",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "4.0.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Rhys Yarranton",
        "Created": "26/Feb/17 23:03",
        "Updated": "22/Dec/17 05:22",
        "Resolved": "22/Dec/17 05:22",
        "Description": "PathChildrenCache uses EnsureContainers to create the path as a container(if it does not already exist).  If at some point that container is empty, ZooKeeper may remove it.  If at some later point the path is recreated and a child node added, the PathChildrenCache will not detect it.  No event will be fired, nor will the child node appear in getCurrentData().\nThe attached test reproduces the problem most of the time if the ZooKeeper server has znode.container.checkIntervalMs reduced to 1000.\nA workaround is to explicitly create the path as a non-container before starting the PathChildrenCache.\nNB there are two related problems here.  One is that it has trouble handling containers.  More serious is that it creates a container, thus running itself into trouble.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/249"
        ]
    },
    "CURATOR-389": {
        "Key": "CURATOR-389",
        "Summary": "Factory and Default ThreadFactory inconsistencies and poor default thread names",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Fangmin Lv",
        "Reporter": "Michael Pawliszyn",
        "Created": "01/Mar/17 15:44",
        "Updated": "16/Mar/17 18:13",
        "Resolved": null,
        "Description": "If you pass in a ThreadFactory to the CuratorFrameworkFactory that ThreadFactory is used for both the Framework and the ConnectionStateManager. If no ThreadFactory is passed in two thread factories are created so the threads are named based on what class is using the thread, for example:\n\"Curator-ConnectionStateManager-0\" #62 daemon prio=5 os_prio=0 tid=0x00007f3d1272d800 nid=0x5466c waiting on condition [0x00007f3c46620000]\n\"Curator-Framework-0\" #418 daemon prio=5 os_prio=0 tid=0x00007f3d12d29800 nid=0x54ece waiting on condition [0x00007f3ade6ef000]\nIf there is more than one curator there is no indication on what Zookeeper connections they are curating by looking at the thread dumps. Overall better default thread naming that uses information from the EnsembleProvider would help investigations.",
        "Issue Links": []
    },
    "CURATOR-390": {
        "Key": "CURATOR-390",
        "Summary": "Create better summary of what Apache Curator is/does",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation,                                            Website",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "07/Mar/17 15:21",
        "Updated": "18/Mar/17 14:44",
        "Resolved": "18/Mar/17 14:44",
        "Description": "Since moving to Apache, Curator's emails and website describe Curator as: \nThe Apache Curator Java libraries make using Apache ZooKeeper much easier and more reliable.\nThis is too minimal for new users. Try to come up with something that new users can more easily understand.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/206"
        ]
    },
    "CURATOR-391": {
        "Key": "CURATOR-391",
        "Summary": "PathChildrenCache missing events upon reconnection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Jaton",
        "Created": "07/Mar/17 20:00",
        "Updated": "20/Mar/17 14:12",
        "Resolved": "20/Mar/17 14:12",
        "Description": "PathChildrenCache attempts to check for any change that occurred during a connection loss in applyNewData():\nPathChildrenCache.java\n    else if ( previousData.getStat().getVersion() != stat.getVersion() )\n\n\nThe way to compare stats is erroneous. version will be reset if the node is recreated. So the above code only works if an update has been made, but it may miss changes that involve a delete event.\nExample:\n\nnode /tmp is just created, version=0\nconnection loss\nnode /tmp is deleted\nnode /tmp is recreated with new data, version=0\nreconnect\nno data change event from PathChildrenCache",
        "Issue Links": [
            "https://github.com/apache/curator/pull/202"
        ]
    },
    "CURATOR-392": {
        "Key": "CURATOR-392",
        "Summary": "Zookeeper Ensemble Get Incorrect Address",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Alex Rankin",
        "Created": "08/Mar/17 15:24",
        "Updated": "18/Jul/17 13:21",
        "Resolved": "18/Jul/17 13:21",
        "Description": "I've noticed an issue with Curator 3.2.1 which relates to the fix from CURATOR-345 (also reported by me).\nWhen we would reconnect after losing connection to Zookeeper (due to network issues), our services would always have the wrong connection string, and never manage to reconnect to the Zookeeper cluster. Assuming that 10.1.2.3 is our zookeeper server, and we have two scenarios (with different zoo.cfg files) we were seeing the following results when a reconnection was established:\n\nScenario 1: ClientCnxn - Opening socket connection to server 0.0.0.0/0.0.0.0:2181.\nScenario 2: ClientCnxn - Opening socket connection to server 10.1.2.3/10.1.2.3:2888.\nObviously these are both undesirable connection strings, as both are wrong. The issue arises in the EnsembleTracker.processConfigData() when we reconnect to Zookeeper. The config coming from zookeeper is in the format:\n\nserver.<positive id> = <address1>:<port1>:<port2>[:role];[<client port address>:]<client port>\nAs we can see, both [:role] and [<client port address>:] are optional. Hence, the following string is perfectly valid:\n\nserver.1=10.1.2.3:2888:3888:participant;2181\nWhen Zookeeper sends this, it defaults the clientAddress to 0.0.0.0, so we retrieve the following value in EnsembleTracker:\n\nserver.1=10.1.2.3:2888:3888:participant;0.0.0.0:2181\nThe resulting connection string, therefore, turns in to 0.0.0.0:2181 instead of 10.1.2.3:2181, and Curator creates a new ZooKeeper to connect to that IP - which obviously never works.\nIn the second scenario, our connection string looks a bit different. It is wrong according to the docs, but is valid:\n\nserver.1=10.1.2.3:2888:3888:participant\nNow, this is missing the client port and address. That means that the resulting string from the EnsembleTracker is 10.1.2.3:2888 - which isn't desired. Including the port would just lead to the above scenario.\nFrom what I can see, the EnsembleTracker.configToConnectionString() method is the issue here:\n\nInetSocketAddress address = Objects.firstNonNull(server.clientAddr, server.addr);\n            sb.append(address.getAddress().getHostAddress()).append(\":\").append(address.getPort());\n\n\nIn the above cases, both the server.Addr and server.clientAddr values are wrong. We also prefer the value of clientAddr for some reason, which doesn't look right to me (given that it can be 0.0.0.0 or 127.0.0.1).\nIt seems to me that Curator should use server.Addr.getHostAddress() with server.clientAddr.getPort(). When the clientAddr is missing, however, I'm not sure what should be done.",
        "Issue Links": [
            "/jira/browse/CURATOR-402",
            "https://github.com/apache/curator/pull/205"
        ]
    },
    "CURATOR-393": {
        "Key": "CURATOR-393",
        "Summary": "Integrate Jenkins into our Github",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "15/Mar/17 14:56",
        "Updated": "17/May/23 11:59",
        "Resolved": "17/May/23 11:59",
        "Description": "It would be nice if a Jenkins build was triggered for PRs on Github. There's a nice integration that other projects have so it seems Apache INFRA support this.",
        "Issue Links": []
    },
    "CURATOR-394": {
        "Key": "CURATOR-394",
        "Summary": "UnrecognizedPropertyException: \"enabled\" incompatibility",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Gian Merlino",
        "Created": "23/Mar/17 16:44",
        "Updated": "04/Jan/18 19:39",
        "Resolved": "26/May/17 09:42",
        "Description": "When doing a rolling upgrade of our services from Curator 2.10.0 to Curator 2.12.0 we noticed some of the non-upgraded services started throwing this error. The cause seems to the combination of a new field added to ServiceInstance in CURATOR-275, and the lack of an \"ignore unknown properties\" setting on the ObjectMapper used by Curator 2.10.0.\nThis makes it so clients need to be upgraded before services. But if you have a cluster where some services are also clients of other services (or of a service they are a part of) then I don't see a way to do a rolling upgrade with the way things currently are. Whatever instance you upgrade first will start announcing itself in a way that breaks instances running the previous version of the code. This doesn't seem to be configurable either, so the new \"enabled\" field can't be omitted from the serialized form.\nOne possible solution is to revert CURATOR-275, add an \"ignore unknown properties\" to the ObjectMapper, and then re-introduce CURATOR-275 in a future release. That'd create a \"you must go through release X first to upgrade to release Y\" situation, but it would at least make it possible to do rolling updates.\nAnother possible solution is to accept that clients always need to be upgraded before services, and accept that if you can't do this, it's impossible to update without downtime. This seems like something that'd be good to avoid though.\n\n2017-03-23 16:22:35,652 [DruidTaskResolver[com.metamx.tranquility.druid.IndexService@37a0ec3c]] WARN  c.m.t.finagle.DruidTaskResolver - Poll failed, trying again at[2017-03-23T16:23:10.899Z].\norg.codehaus.jackson.map.exc.UnrecognizedPropertyException: Unrecognized field \"enabled\" (Class org.apache.curator.x.discovery.ServiceInstance), not marked as ignorable\n at [Source: [B@e9cde06; line: 1, column: 226] (through reference chain: org.apache.curator.x.discovery.ServiceInstance[\"enabled\"])\n        at org.codehaus.jackson.map.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:53) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.StdDeserializationContext.unknownFieldException(StdDeserializationContext.java:267) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.std.StdDeserializer.reportUnknownProperty(StdDeserializer.java:673) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:659) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.BeanDeserializer.handleUnknownProperty(BeanDeserializer.java:1365) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.BeanDeserializer._handleUnknown(BeanDeserializer.java:725) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:703) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.deser.BeanDeserializer.deserialize(BeanDeserializer.java:580) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1973) ~[org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar:1.9.13]\n        at org.apache.curator.x.discovery.details.JsonInstanceSerializer.deserialize(JsonInstanceSerializer.java:50) ~[org.apache.curator.curator-x-discovery-2.10.0.jar:na]\n        at org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:193) ~[org.apache.curator.curator-x-discovery-2.10.0.jar:na]\n        at org.apache.curator.x.discovery.details.ServiceCacheImpl.start(ServiceCacheImpl.java:96) ~[org.apache.curator.curator-x-discovery-2.10.0.jar:na]",
        "Issue Links": [
            "https://github.com/apache/curator/pull/208"
        ]
    },
    "CURATOR-395": {
        "Key": "CURATOR-395",
        "Summary": "Potential null dereference in PersistentNode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ted Yu",
        "Created": "23/Mar/17 22:02",
        "Updated": "18/Apr/17 00:00",
        "Resolved": "18/Apr/17 00:00",
        "Description": "if ( localCreateMethod == null )\n            {\n                CreateModable<ACLBackgroundPathAndBytesable<String>> tempCreateMethod = useProtection ? client.create().creatingParentContainersIfNeeded().withProtection() : client.create().creatingParentContainersIfNeeded();\n                if ( createMethod.compareAndSet(null, tempCreateMethod) )\n                {\n                    localCreateMethod = tempCreateMethod;\n                }\n            }\n            localCreateMethod.withMode(getCreateMode(existingPath != null)).inBackground(backgroundCallback).forPath(createPath, data.get());\n\n\nBefore calling withMode(), localCreateMethod should be checked against null.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/213"
        ]
    },
    "CURATOR-396": {
        "Key": "CURATOR-396",
        "Summary": "the node don't disappear when I kill the process in leaderLatch",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.9.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "finira",
        "Created": "07/Apr/17 10:31",
        "Updated": "07/Apr/17 10:31",
        "Resolved": null,
        "Description": "Userd LeaderLatch to make cluster leader.\nbut when I kill the process , the node made by leaderLatch do not disappear, so when I restart a process ,it can not get leader anymore. \nI started two process A and B , they fight for leader ,A got leader.\nand I killed A and B,but the node in zookeeper does not disapper.\nso when I restart A and B,the leader is always the old A.",
        "Issue Links": []
    },
    "CURATOR-397": {
        "Key": "CURATOR-397",
        "Summary": "Created strongly typed model DSL",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Apr/17 18:51",
        "Updated": "23/Jul/17 05:47",
        "Resolved": "14/Jul/17 15:10",
        "Description": "All of Curator's CRUD methods work with raw byte arrays. It would be nice to be able to use strongly type Models.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/211",
            "https://github.com/apache/curator/pull/229"
        ]
    },
    "CURATOR-398": {
        "Key": "CURATOR-398",
        "Summary": "Upgrade from Guava 16 to 21 or later",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.10.0,                                            3.3.0",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": null,
        "Reporter": "Misagh Moayyed",
        "Created": "10/Apr/17 15:18",
        "Updated": "10/Apr/17 15:47",
        "Resolved": "10/Apr/17 15:47",
        "Description": "Please upgrade the guava dependency from version 16 to 21 or later. Existing projects that depend on Guava 21 are not able to use Zookeeper and/or Curator given the dependency conflict, and v16 is very very old. \nPS I started out with 2.10 and when ran into this issue, upgraded to 3.3 to see if that might make a difference.",
        "Issue Links": [
            "/jira/browse/CURATOR-378"
        ]
    },
    "CURATOR-399": {
        "Key": "CURATOR-399",
        "Summary": "curator-client doesn't use shaded Guava",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Tsuyoshi Ozawa",
        "Created": "10/Apr/17 16:24",
        "Updated": "10/Apr/17 17:02",
        "Resolved": "10/Apr/17 16:31",
        "Description": "It seems that curator-client(low-level API) doesn't use shaded Guava although curator-client includes shaded Guava.",
        "Issue Links": []
    },
    "CURATOR-400": {
        "Key": "CURATOR-400",
        "Summary": "Upgrade clirr to fix the check error on java 8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "3.3.0",
        "Component/s": "Framework",
        "Assignee": "Fangmin Lv",
        "Reporter": "Fangmin Lv",
        "Created": "13/Apr/17 06:07",
        "Updated": "21/Jul/17 17:53",
        "Resolved": "21/Jul/17 17:53",
        "Description": "Currently, the mvn compile failed on CURATOR-3.0 branch, it turns out clirr maven plugin has bug and can fail on Java 8, the latest 2.8 version fixed those bug, and compiled without error.\nError details:\n--------------------------\n[ERROR] Failed to execute goal org.codehaus.mojo:clirr-maven-plugin:2.6.1:check (default) on project curator-x-async: Execution default of goal org.codehaus.mojo:clirr-maven-plugin:2.6.1:check failed: Invalid byte tag in constant pool: 15",
        "Issue Links": [
            "https://github.com/apache/curator/pull/210",
            "https://github.com/apache/curator/pull/210"
        ]
    },
    "CURATOR-401": {
        "Key": "CURATOR-401",
        "Summary": "Make InterProcessMutex.isOwnedByCurrentThread public",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Stig Rohde D\u00f8ssing",
        "Created": "18/Apr/17 16:17",
        "Updated": "11/Nov/18 16:07",
        "Resolved": "11/Nov/18 16:07",
        "Description": "It is useful in some cases to be able to tell if the current thread holds the lock before calling acquire. I have some components using InterProcessMutex for coordination that are intended to run in different JVMs in production. I'd like to be able to write an integration test that runs them in the same JVM in different threads, and verify that the lock is held by the expected component at certain points in the code.\nUsing InterProcessMutex.isAcquiredInThisProcess is not a good replacement in this case, and acquiring/releasing the lock unnecessarily also doesn't seem like a good solution.\nInterProcessMutex already supports this functionality, but the method is package private.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/214"
        ]
    },
    "CURATOR-402": {
        "Key": "CURATOR-402",
        "Summary": "Curator should not use QuorumServer.addr when updating the connect string dynamically",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "3.2.1,                                            3.3.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Stig Rohde D\u00f8ssing",
        "Created": "24/Apr/17 18:16",
        "Updated": "12/Jul/17 19:08",
        "Resolved": "12/Jul/17 19:08",
        "Description": "https://issues.apache.org/jira/browse/CURATOR-345 made a change to EnsembleTracker to read the QuorumServer.addr in case the QuorumServer.clientAddr field is null. This doesn't seem right to me. \nWhen the Zookeeper cluster is configured with the new clientPort syntax (http://zookeeper.apache.org/doc/r3.5.3-beta/zookeeperReconfig.html#sc_reconfig_clientport), the clientAddr is correctly set:\n\n19:54:10.691 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr /0.0.0.0:2181 and server addr localhost/127.0.0.1:2888\n19:54:10.691 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr /0.0.0.0:2182 and server addr localhost/127.0.0.1:2889\n19:54:10.691 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr /0.0.0.0:2183 and server addr localhost/127.0.0.1:2890\n\n\nIf the ensemble is configured with the old clientPort property, the clientAddr fields will be null:\n\n19:59:23.801 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr null and server addr localhost/127.0.0.1:2888\n19:59:23.801 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr null and server addr localhost/127.0.0.1:2889\n19:59:23.801 [main-EventThread] ERROR org.apache.curator.framework.imps.EnsembleTracker - Getting clientAddr null and server addr localhost/127.0.0.1:2890\n\n\nBefore https://issues.apache.org/jira/browse/CURATOR-345, this was okay, since using the old clientPort property just wound up making the EnsembleTracker.configToConnectionString method return an empty string, which effectively disables Curators ability to update the connection string.\nWith the new behavior, the connect string will be updated to the server addresses (i.e. the peer ports) if the clientAddr fields are null, which will result in Curator being unable to reconnect if the Zookeeper instance needs to be replaced.\nAs far as I'm aware the client has no use for the Zookeeper peer ports, so the change from https://issues.apache.org/jira/browse/CURATOR-345 should probably be reverted.",
        "Issue Links": [
            "/jira/browse/CURATOR-392",
            "https://github.com/apache/curator/pull/216"
        ]
    },
    "CURATOR-403": {
        "Key": "CURATOR-403",
        "Summary": "Curator framework incompatible with guava 21",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "3.3.0,                                            2.12.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Vladimir Dergachev",
        "Created": "27/Apr/17 10:45",
        "Updated": "02/May/17 18:41",
        "Resolved": "28/Apr/17 13:13",
        "Description": "Guava version 21 incompatible with curator framework. Now it uses version 16.0.1 of guava, and compatible up to version 20 inclusive. To reproduce a problem just put 21 version in dependencies management and you will get message like this in runtime.\n\nCaused by: java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.sameThreadExecutor()Lcom/google/common/util/concurrent/ListeningExecutorService;\n\tat org.apache.curator.framework.listen.ListenerContainer.addListener(ListenerContainer.java:41)",
        "Issue Links": []
    },
    "CURATOR-404": {
        "Key": "CURATOR-404",
        "Summary": "AsyncTransactionOp#create returns the wrong DSL type",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "29/Apr/17 18:36",
        "Updated": "01/May/17 19:56",
        "Resolved": "01/May/17 19:56",
        "Description": "The AsyncTransactionOp.create() methods should return AsyncPathAndBytesable not AsyncPathable.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/217"
        ]
    },
    "CURATOR-405": {
        "Key": "CURATOR-405",
        "Summary": "Connection loss not leading to ConnectionState.LOST and causing logs to fill",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.1.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/May/17 19:31",
        "Updated": "09/Dec/18 22:06",
        "Resolved": "09/Dec/18 22:06",
        "Description": "It's unclear how it got into this state, but a client started endlessly logging \"Session timeout has elapsed while SUSPENDED\". Curator 3.0 is supposed to go to LOST and inject a session expiration but it's not working in this case.\nThere are millions of these:\n\n[2017-05-01 18:29:34,636][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368206. Adjusted session timeout ms: 15000 {}\n[2017-05-01 18:29:34,637][INFO ][XXXXX] Resolved connection string from [http://localhost:2180/zookeeper/clients/ensemble/connection-string?namespace=/v1] to [localhost:22191,localhost:22194,localhost:22192,localhost:22193/v1] with local namespace [/v1] {}\n[2017-05-01 18:29:34,637][INFO ][org.apache.zookeeper.ZooKeeperTestable] injectSessionExpiration() called {}\n[2017-05-01 18:29:34,637][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368207. Adjusted session timeout ms: 15000 {}\n[2017-05-01 18:29:34,637][INFO ][XXXXX] Resolved connection string from [http://localhost:2180/zookeeper/clients/ensemble/connection-string?namespace=/v1] to [localhost:22191,localhost:22194,localhost:22192,localhost:22193/v1] with local namespace [/v1] {}\n[2017-05-01 18:29:34,637][INFO ][org.apache.zookeeper.ZooKeeperTestable] injectSessionExpiration() called {}\n[2017-05-01 18:29:34,637][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368207. Adjusted session timeout ms: 15000 {}\n[2017-05-01 18:29:34,637][INFO ][XXXXX] Resolved connection string from [http://localhost:2180/zookeeper/clients/ensemble/connection-string?namespace=/v1] to [localhost:22191,localhost:22194,localhost:22192,localhost:22193/v1] with local namespace [/v1] {}\n[2017-05-01 18:29:34,637][INFO ][org.apache.zookeeper.ZooKeeperTestable] injectSessionExpiration() called {}\n[2017-05-01 18:29:34,637][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368207. Adjusted session timeout ms: 15000 {}\n[2017-05-01 18:29:34,637][INFO ][XXXXX] Resolved connection string from [http://localhost:2180/zookeeper/clients/ensemble/connection-string?namespace=/v1] to [localhost:22191,localhost:22194,localhost:22192,localhost:22193/v1] with local namespace [/v1] {}\n[2017-05-01 18:29:34,637][INFO ][org.apache.zookeeper.ZooKeeperTestable] injectSessionExpiration() called {}\n[2017-05-01 18:29:34,637][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368207. Adjusted session timeout ms: 15000 {}\n[2017-05-01 18:29:34,638][INFO ][XXXXX] Resolved connection string from [http://localhost:2180/zookeeper/clients/ensemble/connection-string?namespace=/v1] to [localhost:22191,localhost:22194,localhost:22192,localhost:22193/v1] with local namespace [/v1] {}\n[2017-05-01 18:29:34,638][INFO ][org.apache.zookeeper.ZooKeeperTestable] injectSessionExpiration() called {}\n[2017-05-01 18:29:34,638][WARN ][org.apache.curator.framework.state.ConnectionStateManager] Session timeout has elapsed while SUSPENDED. Injecting a session expiration. Elapsed ms: 250368208. Adjusted session timeout ms: 15000 {}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/292"
        ]
    },
    "CURATOR-406": {
        "Key": "CURATOR-406",
        "Summary": "The CURATOR-3.0 branch should become master",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0,                                            2.12.0",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": "Scott Blum",
        "Reporter": "Jordan Zimmerman",
        "Created": "02/May/17 19:40",
        "Updated": "02/May/17 20:15",
        "Resolved": "02/May/17 20:08",
        "Description": "Currently, we have two main git branches, master (corresponding to Curator 2.x) and CURATOR-3.0. master should become master-2.0 and CURATOR-3.0 should become master.",
        "Issue Links": []
    },
    "CURATOR-407": {
        "Key": "CURATOR-407",
        "Summary": "Create Transaction DSL is missing TTL support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "03/May/17 12:41",
        "Updated": "04/May/17 03:04",
        "Resolved": "04/May/17 03:04",
        "Description": "TransactionCreateBuilder is missing TTL support. i.e. you should be able to specify a TTL when doing a create transaction.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/218"
        ]
    },
    "CURATOR-408": {
        "Key": "CURATOR-408",
        "Summary": "Handle graceful close of ZookKeeper client waiting for all resources to be released",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "04/May/17 11:27",
        "Updated": "28/Jun/18 06:41",
        "Resolved": "28/Jun/18 06:04",
        "Description": "Ths idea is to leverage the new ZooKeeper#close(timeoutMs) method introduced with ZOOKEEPER-2697.\nThis new method waits for the internal threads to finish, this way the client is sure that all internal resources handled but low-level ZooKeeper client have been released.\nThis is very useful in tests because the user can wait for the test environment to be cleared.\nIn some cases you want to return from the 'close' method as soon as possibile. In ZooKeeper a new specific method as been added in order to let the user ask for a specific behaviour.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-2697",
            "https://github.com/apache/curator/pull/266"
        ]
    },
    "CURATOR-409": {
        "Key": "CURATOR-409",
        "Summary": "TestingQuorumPeerMain does not work with Zookeeper 3.5.3-beta",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Stig Rohde D\u00f8ssing",
        "Created": "05/May/17 18:43",
        "Updated": "03/Dec/18 02:58",
        "Resolved": "08/May/17 04:13",
        "Description": "Zookeeper 3.5.3 has added a getQuorumPeer method to QuorumPeerMain https://github.com/apache/zookeeper/blob/release-3.5.3/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeerMain.java#L194. TestingQuorumPeerMain has an identically named method, which is now unintentionally overridding the one in the base class.\nThis causes TestingCluster to be unusable.\n\n20:36:41.199 [Thread-1] ERROR org.apache.curator.test.TestingZooKeeperServer - From testing server (random state: false) for instance: InstanceSpec{dataDirectory=C:\\Users\\Esran\\AppData\\Local\\Temp\\1494009401090-0, port=65306, electionPort=65307, quorumPort=65308, deleteDataDirectoryOnClose=true, serverId=1, tickTime=-1, maxClientCnxns=-1, customProperties={}, hostname=127.0.0.1} org.apache.curator.test.InstanceSpec@59c4b497\njava.lang.NullPointerException: null\n\tat org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:158) ~[zookeeper-3.5.3-beta.jar:3.5.3-beta-8ce24f9e675cbefffb8f21a47e06b42864475a60]\n\tat org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:150) [curator-test-3.3.0.jar:3.3.0]\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_73]\n\n\nThe method in TestingQuorumPeerMain should be renamed.",
        "Issue Links": [
            "/jira/browse/HADOOP-15974",
            "/jira/browse/ZOOKEEPER-3181",
            "/jira/browse/KNOX-1599",
            "/jira/browse/ZOOKEEPER-2355",
            "https://github.com/apache/curator/pull/219"
        ]
    },
    "CURATOR-410": {
        "Key": "CURATOR-410",
        "Summary": "testPathsFromProtectingInBackground fails since upgrade to Zookeeper 3.5.3",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Stig Rohde D\u00f8ssing",
        "Created": "06/May/17 07:51",
        "Updated": "09/May/17 15:08",
        "Resolved": "08/May/17 04:10",
        "Description": "testPathsFromProtectingInBackground iterates over all CreateModes, and tries to create paths with protection. Since Zookeeper 3.5.3, two new CreateModes requiring non-negative TTLs have been added. Since the test doesn't set TTLs for those CreateModes, Zookeeper throws an exception.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/220"
        ]
    },
    "CURATOR-411": {
        "Key": "CURATOR-411",
        "Summary": "Make Tests Great Again",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/May/17 03:39",
        "Updated": "30/May/17 11:54",
        "Resolved": "30/May/17 11:54",
        "Description": "The current master branch fails many tests and hangs on some. Let's get this fixed.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/221"
        ]
    },
    "CURATOR-412": {
        "Key": "CURATOR-412",
        "Summary": "Deprecate curator-x-rpc",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.1,                                            3.3.0,                                            2.12.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/May/17 04:21",
        "Updated": "21/Jul/17 11:31",
        "Resolved": "21/Jul/17 11:30",
        "Description": "It seems to me that Curator RPC Proxy hasn't been accepted. It hasn't been maintained either. So, barring objections, I'd like to deprecate and then remove it.\nI'll leave this issue open past at least several releases in case there are objections/comments.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/231"
        ]
    },
    "CURATOR-413": {
        "Key": "CURATOR-413",
        "Summary": "Set up a Travis or Jenkins PR builder",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Stig Rohde D\u00f8ssing",
        "Created": "09/May/17 15:08",
        "Updated": "28/Apr/23 13:25",
        "Resolved": "28/Apr/23 13:01",
        "Description": "It would be good to add a PR builder to the project, similar to those used by other ASF projects like Kafka, Storm and Zookeeper. Building PR code before review helps avoid accidentally merging broken code or code that breaks the tests.\nThe ASF has a Jenkins server at https://builds.apache.org/ that this project could probably get access to. It looks like Apache also has a Travis account at https://travis-ci.org/apache which is another option.",
        "Issue Links": [
            "/jira/browse/CURATOR-624"
        ]
    },
    "CURATOR-414": {
        "Key": "CURATOR-414",
        "Summary": "Typo in TestingZooKeeperMain",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.0",
        "Component/s": "None",
        "Assignee": "Mike Drob",
        "Reporter": "Colm O hEigeartaigh",
        "Created": "15/May/17 16:52",
        "Updated": "16/May/17 20:58",
        "Resolved": "16/May/17 20:58",
        "Description": "There is a Typo in TestingZooKeeperMain which appears at info level if there is a problem starting Zookeeper - \"Could not server.\".",
        "Issue Links": [
            "https://github.com/apache/curator/pull/222"
        ]
    },
    "CURATOR-415": {
        "Key": "CURATOR-415",
        "Summary": "SharedValue doesn't retry on session expiration and the approach to retries is not clear",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sergey Shelukhin",
        "Created": "22/May/17 18:30",
        "Updated": "22/May/17 18:31",
        "Resolved": null,
        "Description": "When the session expires, seemingly on a background thread, trying to set a shared value instantly fails (in our logs there's no delay between the previous activity of the calling thread, and the exception). Judging by the code in RetryLoop that is the default behavior.\nSeems like there should be some mechanism to wait and/or retry. Also, in absence of such mechanism it's not clear whether external retries should be used, or whether SharedValue object is entirely unusable after session expiration (since if it were usable across sessions, one would assume it would retry internally )\n\n2017-05-17T21:33:33,863 ERROR [b99e518c-9777-4c99-b5e7-c9a371aba3af HiveServer2-Handler-Pool: Thread-36331-EventThread]: imps.CuratorFrameworkImpl (CuratorFrameworkImpl.java:logError(546)) - Watcher exception\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for ...\n...\n2017-05-17T21:33:39,935 \n... \nCaused by: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$SessionExpiredException:KeeperErrorCode = Session expired for /zkdtsm_hive_llap0/ZKDTSMRoot/ZKDTSMSeqNumRoot\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)\n\tat org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:274)\n\tat org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:270)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:267)\n\tat org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:253)\n\tat org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:41)\n\tat org.apache.curator.framework.recipes.shared.SharedValue.trySetValue(SharedValue.java:168)\n\tat org.apache.curator.framework.recipes.shared.SharedCount.trySetCount(SharedCount.java:111)",
        "Issue Links": []
    },
    "CURATOR-416": {
        "Key": "CURATOR-416",
        "Summary": "Set JDK to 1.7 for CURATOR-2.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "2.13.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "31/May/17 01:01",
        "Updated": "31/May/17 01:03",
        "Resolved": "31/May/17 01:03",
        "Description": "JDK 6 has been EOL for a long time. I think some JDK 7 features have crept in anyway.",
        "Issue Links": []
    },
    "CURATOR-417": {
        "Key": "CURATOR-417",
        "Summary": "Update to latest Guava dependecies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "3.3.0,                                            2.12.0",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            General,                                            Recipes,                                            Tests",
        "Assignee": null,
        "Reporter": "Ajay",
        "Created": "11/Jun/17 01:42",
        "Updated": "11/Jun/17 04:34",
        "Resolved": "11/Jun/17 04:34",
        "Description": "guava dependency version update from 16 to 22-android\nMoving to 22-android as curator still builds from jdk 1.7 \nFrom the Google Guava Docs ,\nIf you need support for JDK 1.7 or Android, use the Android flavor. You can find the Android Guava source in the android directory.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/224"
        ]
    },
    "CURATOR-418": {
        "Key": "CURATOR-418",
        "Summary": "ServiceCacheImpl.close() should be idempotent",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Roman Leventov",
        "Created": "22/Jun/17 21:59",
        "Updated": "22/Jun/17 21:59",
        "Resolved": null,
        "Description": "https://github.com/apache/curator/blob/6ca77776d3d2c71b1e541c0edd60d2c17efe9c66/curator-x-discovery/src/main/java/org/apache/curator/x/discovery/details/ServiceCacheImpl.java#L104 emits IllegalStateException if ServiceCacheImpl is already closed, that contradicts Closeable contract, it states that close() call should be idempotent: https://docs.oracle.com/javase/7/docs/api/java/io/Closeable.html\nMight be applicable to other Closeable implementations in the Curator project.\nAnyway, the issue is that we have a lot of errors like this in logs:\nException in thread \"Thread-113\" java.lang.IllegalStateException: Already closed or has not been started\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:176)\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.close(ServiceCacheImpl.java:104)\n\tat org.apache.curator.x.discovery.details.ServiceProviderImpl.close(ServiceProviderImpl.java:78)\n\tat com.google.common.io.Closeables.close(Closeables.java:77)\n\tat org.apache.curator.utils.CloseableUtils.closeQuietly(CloseableUtils.java:59)\n\tat org.apache.curator.x.discovery.details.ServiceDiscoveryImpl.close(ServiceDiscoveryImpl.java:149)\n\tat io.druid.curator.discovery.DiscoveryModule$5.stop(DiscoveryModule.java:227)\n\tat io.druid.java.util.common.lifecycle.Lifecycle.stop(Lifecycle.java:284)\n\tat io.druid.java.util.common.lifecycle.Lifecycle$1.run(Lifecycle.java:310)\n\tat java.lang.Thread.run(Thread.java:748)",
        "Issue Links": []
    },
    "CURATOR-419": {
        "Key": "CURATOR-419",
        "Summary": "CURATOR-397 broke some tests (and introduced new problems) - Fix these tests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Jul/17 16:23",
        "Updated": "18/Jul/17 12:49",
        "Resolved": "18/Jul/17 12:49",
        "Description": "CURATOR-397 broke some tests (and introduced new problems) - Fix these tests\nhttps://builds.apache.org/job/Curator/1048/",
        "Issue Links": []
    },
    "CURATOR-420": {
        "Key": "CURATOR-420",
        "Summary": "SharedCount.removeListener has no effect",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "4.0.0,                                            2.13.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Niv Singer",
        "Created": "11/Jul/17 13:26",
        "Updated": "11/Jul/17 14:00",
        "Resolved": "11/Jul/17 13:58",
        "Description": "SharedCount.removeListener only removes the listener from the SharedCount's listeners member, which has no effect.\nFor each SharedCountListener, a SharedValueListener wrapper is created, and should be removed  from the SharedValue's listeners.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/226"
        ]
    },
    "CURATOR-421": {
        "Key": "CURATOR-421",
        "Summary": "Add Data Migration Feature",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "12/Jul/17 20:40",
        "Updated": "18/Jul/17 13:16",
        "Resolved": "18/Jul/17 13:16",
        "Description": "With CURATOR-397 we have a strongly typed DSL. This implies the need to be able to migrate data models. A framework for data migration would be useful.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/228"
        ]
    },
    "CURATOR-422": {
        "Key": "CURATOR-422",
        "Summary": "PathChildrenCache is not tolerant to failed connection to ZK on startup",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Dmitry Konstantinov",
        "Created": "13/Jul/17 20:38",
        "Updated": "13/Jul/17 20:38",
        "Resolved": null,
        "Description": "If PathChildrenCache is started when Zookeeper is not available for a quite long time (to exceed operations retries) and parent node did not exist - when the connection to Zookeeper is resumed PathChildrenCache does not watch for changes anymore.\nRoot cause: PathChildrenCache uses EnsureContainers which has the following logic:\n\nprivate synchronized void internalEnsure() throws Exception\n    {\n        if ( ensureNeeded.compareAndSet(true, false) )\n        {\n            client.createContainers(path);\n        }\n    }\n\n\nThis logic is not aware about operation result, even if client.createContainers throws an exception and the nodes are not created EnsureContainers next time will not try to do it.\nExample of the exception:\n\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /test\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)\n\tat org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1073)\n\tat org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:274)\n\tat org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:199)\n\tat org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:193)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)\n\tat org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:190)\n\tat org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:175)\n\tat org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:32)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.createContainers(CuratorFrameworkImpl.java:194)\n\tat org.apache.curator.framework.EnsureContainers.internalEnsure(EnsureContainers.java:61)\n\tat org.apache.curator.framework.EnsureContainers.ensure(EnsureContainers.java:53)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.ensurePath(PathChildrenCache.java:576)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.refresh(PathChildrenCache.java:490)\n\tat org.apache.curator.framework.recipes.cache.RefreshOperation.invoke(RefreshOperation.java:35)\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache$9.run(PathChildrenCache.java:773)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\nAs a result the watcher registered in org.apache.curator.framework.recipes.cache.PathChildrenCache#refresh is not triggered.\nTest to reproduce:\n\n@Test\npublic void test() throws Exception {\n    TestingServer zkTestServer = new TestingServer(2181, false);\n\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.newClient(\n            zkTestServer.getConnectString(),\n            5000,\n            1000,\n            new RetryOneTime(100)\n    );\n    curatorFramework.start();\n    PathChildrenCache cache = new PathChildrenCache(curatorFramework, \"/test\", true);\n    cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT);\n\n    Thread.sleep(5000);\n\n    zkTestServer.start();\n    curatorFramework.create().creatingParentContainersIfNeeded().forPath(\"/test/example\");\n\n    while(true) {\n        Thread.sleep(1000);\n        System.out.println(cache.getCurrentData());\n    }\n}",
        "Issue Links": []
    },
    "CURATOR-423": {
        "Key": "CURATOR-423",
        "Summary": "AsyncCuratorFramework.transactionOp().create() is ignoring create modes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "14/Jul/17 04:43",
        "Updated": "14/Jul/17 13:10",
        "Resolved": "14/Jul/17 13:10",
        "Description": "AsyncCuratorFramework.transactionOp().create() with a non-default create mode is not getting used.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/227"
        ]
    },
    "CURATOR-424": {
        "Key": "CURATOR-424",
        "Summary": "storingStatIn() methods are not working in async/background",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "16/Jul/17 19:21",
        "Updated": "16/Jul/17 19:45",
        "Resolved": "16/Jul/17 19:45",
        "Description": "E.g.\nAsync:\n\nStat stat = new Stat();\ncomplete(client.getData().storingStatIn(stat).forPath(\"/test\"));\n\n\nThe above code will not set the values for stat",
        "Issue Links": []
    },
    "CURATOR-425": {
        "Key": "CURATOR-425",
        "Summary": "Create ZooKeeper 3.4.x compatibility mode for master branch (Curator 3.x)",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0,                                            2.12.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Client,                                            Documentation,                                            Framework,                                            General,                                            Recipes,                                            Tests,                                            Website",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "19/Jul/17 15:57",
        "Updated": "21/Jul/17 19:27",
        "Resolved": "21/Jul/17 19:27",
        "Description": "Maintaining the CURATOR-2.0 branch is becoming untenable. Investigate if it's possible to have a version of Curator 3.x that works with both ZooKeeper 3.5.x and ZooKeeper 3.4.x",
        "Issue Links": [
            "https://github.com/apache/curator/pull/230"
        ]
    },
    "CURATOR-426": {
        "Key": "CURATOR-426",
        "Summary": "Prep Apache Curator 4.0.0",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Apache",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "20/Jul/17 18:57",
        "Updated": "23/Jul/17 02:41",
        "Resolved": "23/Jul/17 02:41",
        "Description": "Prepare and process Apache Curator 4.0.0",
        "Issue Links": [
            "https://github.com/apache/curator/pull/232"
        ]
    },
    "CURATOR-427": {
        "Key": "CURATOR-427",
        "Summary": "Remove ClassicConnectionHandling",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0",
        "Fix Version/s": "4.0.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "24/Jul/17 04:41",
        "Updated": "26/Jul/17 13:55",
        "Resolved": "26/Jul/17 13:55",
        "Description": "ClassicConnectionHandlingPolicy was a stop-gap from Curator 2.x to Curator 3.x. Now that we're at 4.0 there's no reason to maintain it. It necessitates running the tests twice and is a headache to stay compatible. StandardConnectionHandlingPolicy is what users expect anyway.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/233"
        ]
    },
    "CURATOR-428": {
        "Key": "CURATOR-428",
        "Summary": "NoSuchFieldError for configFileStr in QuorumConfigBuilder",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Jacky Chan",
        "Created": "30/Jul/17 03:07",
        "Updated": "31/Jul/17 03:33",
        "Resolved": "31/Jul/17 03:33",
        "Description": "Exception in thread \"Thread-0\" java.lang.NoSuchFieldError: configFileStr\n\tat org.apache.curator.test.QuorumConfigBuilder$1.<init>(QuorumConfigBuilder.java:142)\n\tat org.apache.curator.test.QuorumConfigBuilder.buildConfig(QuorumConfigBuilder.java:137)\n\tat org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:157)\n\tat java.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "CURATOR-429": {
        "Key": "CURATOR-429",
        "Summary": "Make Curator 4.x compatible with Zookeeper 3.4.x in OSGi too",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.3.0,                                            5.0.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Andrea Cosentino",
        "Created": "04/Aug/17 11:36",
        "Updated": "28/Apr/23 13:40",
        "Resolved": "28/Apr/23 13:40",
        "Description": "Currently in OSGi curator need Zookeeper >= 3.5 to work. Maybe you can think of relaxing this constraint by adding a version range.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/311"
        ]
    },
    "CURATOR-430": {
        "Key": "CURATOR-430",
        "Summary": "deletingChildrenIfNeeded maybe cannot delete children completely when multi-client delete concurrently",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.1",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "hebelala",
        "Created": "12/Aug/17 04:58",
        "Updated": "09/Dec/18 07:17",
        "Resolved": "24/Sep/17 20:56",
        "Description": "use curatorFramework.delete().deletingChildrenIfNeeded().forPath(path), this sync api doesn't ignore the NoNodeException, causes the rest of children nodes will not be deleted perhaps.\nzookeeper.getChildren(path, null) maybe throw NoNodeException, if the path doesn't exist.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/235"
        ]
    },
    "CURATOR-431": {
        "Key": "CURATOR-431",
        "Summary": "Stat structure not filled when using the idiom create().orSetData()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.1",
        "Component/s": "Client",
        "Assignee": "Cam McKenzie",
        "Reporter": "Andr\u00e9s Pipicello",
        "Created": "06/Sep/17 12:45",
        "Updated": "12/Sep/17 23:51",
        "Resolved": "12/Sep/17 23:51",
        "Description": "Hello\nIt seems that when using the idiom client.create().orSetData(), the Stat structure set with storingStatIn does not get filled if the path already exists.\nI attached a unit test that exemplifies the behaviour.\nRegards",
        "Issue Links": [
            "https://github.com/apache/curator/pull/236"
        ]
    },
    "CURATOR-432": {
        "Key": "CURATOR-432",
        "Summary": "OSGI bundles should not import org.apache.curator",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Christopher Johnson",
        "Created": "14/Sep/17 19:37",
        "Updated": "10/Jan/18 11:21",
        "Resolved": null,
        "Description": "Curator bundles will not deploy to an OSGI container like Karaf with an import requirement on a non-existent package.  The problem is highlighted in the excerpt from the MANIFEST.MF below\n\nImport-Package: com.google.common.base;version=\"[20.0,21)\",com.google.\n common.collect;version=\"[20.0,21)\",com.google.common.util.concurrent;\n version=\"[20.0,21)\",*org.apache.curator;version=\"[4.0,5)\"*,org.apache.c\n urator.framework;version=\"[4.0,5)\",org.apache.curator.framework.api;v\n ersion=\"[4.0,5)\",org.apache.curator.framework.api.transaction;version\n =\"[4.0,5)\",org.apache.curator.framework.imps;version=\"[4.0,5)\",org.ap\n ache.curator.framework.listen;version=\"[4.0,5)\",org.apache.curator.fr\n amework.state;version=\"[4.0,5)\",org.apache.curator.retry;version=\"[4.\n 0,5)\",org.apache.curator.utils;version=\"[4.0,5)\",org.apache.zookeeper\n ;version=\"[3.5,4)\",org.apache.zookeeper.data;version=\"[3.5,4)\",org.sl\n f4j;version=\"[1.7,2)\"",
        "Issue Links": [
            "https://github.com/apache/curator/pull/237"
        ]
    },
    "CURATOR-433": {
        "Key": "CURATOR-433",
        "Summary": "Test/Validate Curator with Java 11",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework,                                            General,                                            Recipes,                                            Tests",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "24/Sep/17 20:47",
        "Updated": "10/May/23 11:49",
        "Resolved": "10/May/23 11:49",
        "Description": "We should verify that Curator works with Java 9. Note: this may need to wait on ZooKeeper itself. I'm creating this now as a TBD.",
        "Issue Links": [
            "/jira/browse/CURATOR-523",
            "/jira/browse/CURATOR-672"
        ]
    },
    "CURATOR-434": {
        "Key": "CURATOR-434",
        "Summary": "API changes report for Curator Framework",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Andrey Ponomarenko",
        "Created": "02/Oct/17 06:59",
        "Updated": "02/Oct/17 06:59",
        "Resolved": null,
        "Description": "The review of API changes for the Curator Framework library since 2.3.0 version: https://abi-laboratory.pro/java/tracker/timeline/curator-framework/\nHope it will be helpful for users and maintainers of the library.\nThe report is generated by the https://github.com/lvc/japi-tracker tool for jars found at http://central.maven.org/maven2/org/apache/curator/curator-framework/ according to https://wiki.eclipse.org/Evolving_Java-based_APIs_2.\nThank you.",
        "Issue Links": []
    },
    "CURATOR-435": {
        "Key": "CURATOR-435",
        "Summary": "ZNodes are created with incompatible CreateMode under racing conditions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Recipes",
        "Assignee": null,
        "Reporter": "Daniel Klessing",
        "Created": "06/Oct/17 15:06",
        "Updated": "16/Jun/23 06:02",
        "Resolved": "16/Jun/23 06:02",
        "Description": "We have a microservice-based software stack that starts its core services relatively simultaneously. For this we rely on Docker and Rancher.\nWe noticed that occasionally the Apache Storm Nimbus, that relies on Apache Curator's LeaderLatch, cannot agree on a leader. This comes due a missing ZNode:\n\norg.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock\n\tat org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]\n\tat clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]\n\tat clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]\n\tat org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]\n\tat clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]\n\tat clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]\n\tat org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1544) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getClusterInfo(nimbus.clj:2006) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]\n\tat org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\n11:29:13.229 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments\n11:29:13.229 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup\n\n\neven though a create request has been issued for this ZNode:\n\n2017-10-06 11:27:04,387 [myid:3] - INFO  [ProcessThread(sid:3 cport:-1)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x35ef170475e0001 type:create cxid:0x1 zxid:0x100000013 txntype:-1 reqpath:n/a Error Path:/storm/leader-lock Error:KeeperErrorCode = NoNode for /storm/leader-lock\n\n\nVerifying the Zookeeper data via zkCli.sh shows that the ZNode is in fact not present:\n\n[zk: localhost:2181(CONNECTED) 0] ls /storm\n[assignments, backpressure, nimbuses, logconfigs, storms, errors, supervisors, workerbeats, blobstore]\n\n\nWe noticed, that in such a case, the following log entry in the Nimbus log is missing\n\n[Curator-Framework-0] WARN  org.apache.storm.shade.org.apache.curator.utils.ZKPaths - The version of ZooKeeper being used doesn't support Container nodes. CreateMode.PERSISTENT will be used instead.\n\n\nand if everything works, the log entry is present.\nWe assume that the correct CreateMode for the ZNode someone cannot be determined (as \"Container\" ZNodes are only available in Zookeeper 3.5.1 onwards).\nPlease notice the startup phase in which several attempts to connect to Zookeeper are not successful until it is fully reachable.\nFull logs are attached. Zookeeper is run as a 3-node cluster in this setup. Happens with a single node instance also.",
        "Issue Links": [
            "/jira/browse/CURATOR-436"
        ]
    },
    "CURATOR-436": {
        "Key": "CURATOR-436",
        "Summary": "NoNodeException: KeeperErrorCode KeeperException in storm-nimbus",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0,                                            4.0.0",
        "Fix Version/s": "4.0.1",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Thierry G\u00f6ckel",
        "Created": "12/Oct/17 10:21",
        "Updated": "16/Jun/23 06:02",
        "Resolved": "19/Oct/17 08:41",
        "Description": "Seeing this exception when using Storm Nimbus and Zookeeper:\n\nERROR org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!\r\norg.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock\r\n        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) ~[?:?]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]\r\n        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]\r\n        at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]\r\n        at org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]\r\n        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]\r\n        at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]\r\n        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getLeader(nimbus.clj:2412) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3944) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3928) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]\r\n\n\nThere is a similar looking ticket at https://issues.apache.org/jira/browse/CURATOR-358 but that dealt with LeaderSelector whereas this exception seems to be caused by LeaderLatch and LockInternals.\nLet me know if more details and/or logs are needed.",
        "Issue Links": [
            "/jira/browse/CURATOR-435",
            "https://github.com/apache/curator/pull/238"
        ]
    },
    "CURATOR-437": {
        "Key": "CURATOR-437",
        "Summary": "zookeeper connection leak when session expires.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "zealot",
        "Created": "19/Oct/17 12:36",
        "Updated": "20/Oct/17 08:45",
        "Resolved": null,
        "Description": "https://github.com/apache/curator/blob/master/curator-client/src/main/java/org/apache/curator/utils/InjectSessionExpiration.java#L97\nCurator inject will set zookeeper state to CLOSED when session expires without close zk associated threads.\nIf state set to CLOSED, ZooKeeper.close() function won't be able to release resources properly, which lead to memory and connection leak.\nTo reproduce, create a curator client, then shutdown zk server, wait for session timeout, restart the zk server. There will be two ZooKeeper instances and two connections to the server.",
        "Issue Links": []
    },
    "CURATOR-438": {
        "Key": "CURATOR-438",
        "Summary": "Remove calls to MoreExecutors.sameThreadExecutor (deprecated)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.1",
        "Component/s": "Apache",
        "Assignee": null,
        "Reporter": "Kenneth McFarland",
        "Created": "21/Oct/17 04:12",
        "Updated": "08/Nov/17 05:06",
        "Resolved": "07/Nov/17 21:07",
        "Description": "There are existing calls to MoreExecutors.sameThreadExecutor(), this issue has been opened and closed before. A quick look shows CURATOR-370, CURATOR-200 etc.. \nWhile these other issues address the same problem, the patches did not remove all calls to the method so I took the liberty of doing it while getting familiar with the codebase.\nPlease accept my humble first PR for Curator. Because this is my first PR I will let you know fyi that I have signed the ICLA.\nhttps://github.com/apache/curator/pull/240",
        "Issue Links": [
            "https://github.com/apache/curator/pull/240"
        ]
    },
    "CURATOR-439": {
        "Key": "CURATOR-439",
        "Summary": "CuratorFrameworkState STARTED, but ZookeeperClient not connected",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "3.2.1",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Alex Rankin",
        "Created": "24/Oct/17 10:49",
        "Updated": "26/Apr/18 08:08",
        "Resolved": null,
        "Description": "I recently ran into an issue on some of our nodes caused by network issues between a service and Zookeeper. I have been unable to recreate them as of yet, but I'm still trying.\nSetup\n 5x services using Curator 3.2.1 to talk to Zookeeper 3.5.3 cluster (also 5 nodes).\nNetwork issues caused the services to disconnect from Zookeeper.\nThere's a check in our code to see if the Zookeeper connection is available before sending a request:\npublic boolean isConnected() \nUnknown macro: {\n return curatorFramework.getZookeeperClient().isConnected();\n } \nAfter the network issues resolved, we noticed that all calls to Zookeeper from 4 of the services were still failing (the fifth was fine). Checking the logs, we saw that CuratorFramework.getState() was reporting the state as STARTED, but curatorFramework.getZookeeperClient().isConnected(); was returning false. Restarting the service fixed everything, but I want to obviously avoid this issue in future.\nProblem\n I couldn't find any documentation stating whether the CuratorZookeeperClient.isConnected() should be used, or if CuratorFramework.getState() == CuratorFrameworkState.STARTED (the functionality of the deprecated CuratorFramework.isConnected()) would be the better check, or if these should both be equivalent, and there's a bug that let one be true while the other was false.\nIf my own check is wrong, and I shouldn't be using CuratorZookeeperClient.isConnected(), then I can easily fix that. I wanted to check the expected behaviour before diving too deep into this, in case this is normal and I am just using Curator incorrectly.\nEdit\nThis was a misunderstanding on my part. I'm leaving it open so that I can submit a documentation/example update shortly to hopefully clarify things a bit better for others.",
        "Issue Links": []
    },
    "CURATOR-440": {
        "Key": "CURATOR-440",
        "Summary": "curator-framework is unable to load in OSGi",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Workaround",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "zealot",
        "Created": "07/Nov/17 07:34",
        "Updated": "22/Apr/20 03:39",
        "Resolved": "22/Apr/20 03:39",
        "Description": "java.lang.NoClassDefFoundError: org/apache/curator/shaded/com/google/common/cache/CacheLoader                                                  \r\n        at org.apache.curator.framework.CuratorFrameworkFactory$Builder.<init>(CuratorFrameworkFactory.java:148) ~[171:curator-framework:4.0.0]             \r\n        at org.apache.curator.framework.CuratorFrameworkFactory$Builder.<init>(CuratorFrameworkFactory.java:130) ~[171:curator-framework:4.0.0]\r\n        at org.apache.curator.framework.CuratorFrameworkFactory.builder(CuratorFrameworkFactory.java:78) ~[171:curator-framework:4.0.0]\r\n        at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:104) ~[171:curator-framework:4.0.0] \r\n        at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:90) ~[171:curator-framework:4.0.0]",
        "Issue Links": [
            "/jira/browse/CURATOR-464",
            "https://github.com/apache/curator/pull/241"
        ]
    },
    "CURATOR-441": {
        "Key": "CURATOR-441",
        "Summary": "Recipes such as TreeCache are susceptible to herding on reconnections",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Do",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "TBD",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Nov/17 02:29",
        "Updated": "22/Apr/20 03:37",
        "Resolved": "22/Apr/20 03:37",
        "Description": "Many of Curator's recipes (TreeCache, etc.) have to refresh their state when a connection problem is repaired. In larger systems, this could cause a thundering herd as all these clients press the ZK ensemble. It would be nice to introduce an optional Jitter mechanism into the Framework connection management so that reconnections aren't immediately reported to the recipes, but are instead delayed slightly with some randomness.",
        "Issue Links": [
            "/jira/browse/CURATOR-533"
        ]
    },
    "CURATOR-442": {
        "Key": "CURATOR-442",
        "Summary": "System.currentTimeMillis() is used for timing - use nanos instead",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Nov/17 02:44",
        "Updated": "08/Nov/17 02:44",
        "Resolved": null,
        "Description": "There are known problems with System.currentTimeMillis() - use nanos instead",
        "Issue Links": []
    },
    "CURATOR-443": {
        "Key": "CURATOR-443",
        "Summary": "Sleep too long when connection to zookeeper has not been established yet after construction",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.1",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Duo Zhang",
        "Created": "21/Nov/17 07:56",
        "Updated": "02/Jan/18 16:04",
        "Resolved": "02/Jan/18 16:04",
        "Description": "Recently we purge the complicated RecoverableZooKeeper dependency in hbase-client and use curator to get data from zookeeper.\nBut when debugging HBASE-19266, we found that if we get data immediately after the creation of CuratorFramework, the request will always cost 100x ms to complete.\nAfter digging, we found that there is a sleep in CuratorFrameworkImpl if the connection is not established yet\nhttps://github.com/apache/curator/blob/6d36a4793b31cdacaf4bbf6554e05d68bc680001/curator-framework/src/main/java/org/apache/curator/framework/imps/CuratorFrameworkImpl.java#L943\nThis is really a bad news for us. We decide to make the async hbase client fully asynchronous. You can see this example, where we execute the request to hbase directly in the netty event loop thread.\nhttps://github.com/apache/hbase/blob/master/hbase-examples/src/main/java/org/apache/hadoop/hbase/client/example/HttpProxyExample.java\nSo if there is a sleep in foreground then the event loop will be stuck for 1 second which will cause very bad performance impact...",
        "Issue Links": [
            "/jira/browse/HBASE-19312",
            "https://github.com/apache/curator/pull/242"
        ]
    },
    "CURATOR-444": {
        "Key": "CURATOR-444",
        "Summary": "LeaderLatch sends events that leads to simultaneously leadership after blocking zookeeper peer communication",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.12.0,                                            4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Borja Bravo Alf\u00e9rez",
        "Created": "27/Nov/17 11:13",
        "Updated": "06/Feb/19 02:34",
        "Resolved": null,
        "Description": "How to reproduce the error:\n\n3 Zookeeper nodes.\nUsing LeaderLatch recipe with a listener.\nWe block the zookeeper network with the attached script. It blocks communication between zookeepers. Note that comunication is lost only for 10 seconds that is exactly our default session timeout.\n\nOur curator configuration: \n\nBase sleep 100ms\nMax sleep 5000ms\nDefault connection timeout 15000ms\nDefault Session timeout  10000ms\n\nI am working with a pull request. Fix seems trivial but creating the test not so much.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/243"
        ]
    },
    "CURATOR-445": {
        "Key": "CURATOR-445",
        "Summary": "Curator failed to detect ZooKeeper 3.5.x under OSGi",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "zealot",
        "Created": "14/Dec/17 10:25",
        "Updated": "25/Sep/22 14:48",
        "Resolved": "25/Sep/22 14:48",
        "Description": "Curator detect ZooKeeper 3.5.x using Class.forName(\"org.apache.zookeeper.admin.ZooKeeperAdmin\")\nDynamicImport-Package is required to use import class by string under OSGi.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/246"
        ]
    },
    "CURATOR-446": {
        "Key": "CURATOR-446",
        "Summary": "Compile error with tests using 4.0.0 and ZK 3.4.11 - need to update docs on how to use old test lib",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.0.1",
        "Component/s": "Documentation,                                            Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Greg Moulliet",
        "Created": "14/Dec/17 19:06",
        "Updated": "02/Jan/18 16:21",
        "Resolved": "20/Dec/17 23:51",
        "Description": "Attempting to start a test ZK gives the following error:\nException in thread \"Thread-0\" java.lang.NoSuchFieldError: configFileStr\n\tat org.apache.curator.test.QuorumConfigBuilder$1.<init>(QuorumConfigBuilder.java:142)\n\tat org.apache.curator.test.QuorumConfigBuilder.buildConfig(QuorumConfigBuilder.java:137)\n\tat org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:157)\n\tat java.lang.Thread.run(Thread.java:745)\nReverting back to 2.12.0 resolves the issue for me.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/239",
            "https://github.com/apache/curator/pull/248"
        ]
    },
    "CURATOR-447": {
        "Key": "CURATOR-447",
        "Summary": "TreeCache: Improve memory usage and concurrent update logic",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0,                                            2.12.0,                                            4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Nick Hill",
        "Created": "02/Jan/18 20:10",
        "Updated": "20/Feb/18 04:08",
        "Resolved": "20/Feb/18 04:08",
        "Description": "Jira https://issues.apache.org/jira/browse/CURATOR-374 reduced per-node memory usage in TreeCache. It can be improved further via removal of the nodeState field - its LIVE state corresponds exactly to the adjacent childData field being non-null, and a sentinel ChildData value can be used for the DEAD state. This simplification also reduces the room for bugs and state inconsistencies.\nOther improvements included:\n\nA further simplification to have TreeNode extend AtomicReference, which obviates the need for an explicit childData field\nMore robust cache update logic (in get-children and get-data event callbacks)\nAvoid overhead of incrementing/decrementing the outstandingOps atomic integer post-initialization",
        "Issue Links": [
            "https://github.com/apache/curator/pull/250"
        ]
    },
    "CURATOR-448": {
        "Key": "CURATOR-448",
        "Summary": "Include curator framework state in error messages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Keith Turner",
        "Created": "23/Jan/18 16:18",
        "Updated": "24/Jun/18 14:36",
        "Resolved": "24/Jun/18 14:36",
        "Description": "While looking into Apache Fluo #1004\u00a0I am seeing error messages like the following.\u00a0\u00a0\n\u00a0\n\n2018-01-22 02:57:23,383 [leader.LeaderSelector] ERROR: The leader threw an exception\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\nat com.google.common.base.Preconditions.checkState(Preconditions.java:149)\r\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)\r\nat org.apache.fluo.core.oracle.OracleServer.takeLeadership(OracleServer.java:426)\r\nTRUNCATED\n\n\u00a0\nWhen I see this error message I know the\u00a0 CuratorFrameworkState\u00a0is not STARTED.\u00a0 However I don't know if its LATENT or STOPPED.\u00a0 From a debugging standpoint this information would be very useful.\u00a0 An example of the code that generates this error messages is at CuratorFrameworkImpl.java line 408.\u00a0 This code could be changed to call an internal method like the following :\n\u00a0\n\n\r\nprivate void checkState() {\r\n  CuratorFrameworkState state = getState(); //store state in local var to avoid race conditions since it may be read twice\r\n  Preconditions.checkState(state == CuratorFrameworkState.STARTED, \"instance state must be %s before calling this method, however its %s\", CuratorFrameworkState.STARTED, state);\r\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/267"
        ]
    },
    "CURATOR-449": {
        "Key": "CURATOR-449",
        "Summary": "Documentation how to exclude Gradle dependencies in Site is wrong",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.0.1",
        "Component/s": "Website",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Peter Rietzler",
        "Created": "24/Jan/18 15:39",
        "Updated": "04/Feb/18 14:45",
        "Resolved": "04/Feb/18 14:45",
        "Description": "https://github.com/apache/curator/pull/251",
        "Issue Links": [
            "https://github.com/apache/curator/pull/251"
        ]
    },
    "CURATOR-450": {
        "Key": "CURATOR-450",
        "Summary": "Dead link in \"Getting Started\" documentation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "4.1.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Etienne Neveu",
        "Created": "05/Feb/18 14:30",
        "Updated": "24/Jun/18 13:03",
        "Resolved": "24/Jun/18 13:03",
        "Description": "The \"Getting Started\" page ( http://curator.apache.org/getting-started.html\u00a0) has a dead link:\u00a0http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html\n\u00a0\nI think it should be\u00a0updated to:\u00a0https://zookeeper.apache.org/doc/current/zookeeperStarted.html\n\u00a0\nI haven't yet signed\u00a0the CLA, so I didn't create a PR...",
        "Issue Links": [
            "https://github.com/apache/curator/pull/255"
        ]
    },
    "CURATOR-451": {
        "Key": "CURATOR-451",
        "Summary": "Background retry may fall into infinite loop",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Yongle Zhang",
        "Created": "06/Feb/18 01:17",
        "Updated": "06/Feb/18 15:01",
        "Resolved": null,
        "Description": "Similar to Curator-209,\u00a0addFailedOperation() function in\u00a0FailedOperationManager.java could fall into infinite loop when there's network disruption.\u00a0\nRoot cause: when there's an Exception (any type), function\u00a0addFailedOperation() simply recursively calls itself. And in every recursion, it prints a log.\u00a0\u00a0\n\u00a0\n\n\r\nvoid addFailedOperation(T details)\r\n{\r\n  ...\r\n  if ( client.getState() == CuratorFrameworkState.STARTED )\r\n  {\r\n    log.debug(\"Details being added to guaranteed operation set: \" + details);\r\n    try\r\n    {\r\n      executeGuaranteedOperationInBackground(details);\r\n    }\r\n\u00a0   catch ( Exception e )\r\n    {\r\n      ThreadUtils.checkInterrupted(e);\r\n      addFailedOperation(details);\r\n    }\r\n  }\r\n}",
        "Issue Links": []
    },
    "CURATOR-452": {
        "Key": "CURATOR-452",
        "Summary": "ChildData.getData() can be null",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Egor Ryashin",
        "Created": "08/Feb/18 23:40",
        "Updated": "27/Nov/18 19:55",
        "Resolved": "24/Jun/18 14:09",
        "Description": "Curator client tries to make ObjectMapper to parse null (ChildData.getData() can be null).\n\n\r\njava.lang.NullPointerException\r\n at org.codehaus.jackson.JsonFactory.createJsonParser(JsonFactory.java:604)\r\n at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1973)\r\n at org.apache.curator.x.discovery.details.JsonInstanceSerializer.deserialize(JsonInstanceSerializer.java:50)\r\n at org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:193)\r\n at org.apache.curator.x.discovery.details.ServiceCacheImpl.start(ServiceCacheImpl.java:96)\r\n at org.apache.curator.x.discovery.details.ServiceProviderImpl.start(ServiceProviderImpl.java:67)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/261"
        ]
    },
    "CURATOR-453": {
        "Key": "CURATOR-453",
        "Summary": "NPE in ServiceCacheImpl.start() because child.getData() returns null",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "Kislay Verma",
        "Created": "13/Feb/18 04:19",
        "Updated": "14/Feb/18 05:13",
        "Resolved": null,
        "Description": "This problem\u00a0is\u00a0in curator-discovery module where we get an NPE on ServiceProvider.start() because child.getData() can be null while starting ServiceCacheImpl. This is similar to https://issues.apache.org/jira/browse/CURATOR-452\u00a0but happens on adding null to map.\nNPE Stacktrace\njava.lang.NullPointerException at java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011) ~[?:1.8.0_112] at java.util.concurrent.ConcurrentHashMap.putIfAbsent(ConcurrentHashMap.java:1535) ~[?:1.8.0_112] at org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:196) ~[curator-x-discovery-4.0.0.jar:?] at org.apache.curator.x.discovery.details.ServiceCacheImpl.start(ServiceCacheImpl.java:96) ~[curator-x-discovery-4.0.0.jar:?] at org.apache.curator.x.discovery.details.ServiceProviderImpl.start(ServiceProviderImpl.java:75) ~[curator-x-discovery-4.0.0.jar:?]\n\u00a0\nThe code\nclient = CuratorFrameworkFactory.newClient(\"localhost:2181\", new ExponentialBackoffRetry(1000, 3));\nclient.start();\nserviceDiscovery = ServiceDiscoveryBuilder.builder(ServiceRecord.class).client(client).basePath(\"/discovery/services/\").serializer(serializer).build(); serviceDiscovery.start();\nserviceProvider = serviceDiscovery.serviceProviderBuilder().serviceName(serviceName).build();\nserviceProvider.start();",
        "Issue Links": []
    },
    "CURATOR-454": {
        "Key": "CURATOR-454",
        "Summary": "Curator 4.0.0 be used with Zookeeper 3.4.10 due to compile-time issues",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Information Provided",
        "Affects Version/s": "4.0.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Simon Billingsley",
        "Created": "15/Feb/18 13:23",
        "Updated": "17/May/23 11:46",
        "Resolved": "17/May/23 11:46",
        "Description": "I see compile issues when trying to compile a project\u00a0with\u00a0a library that depends on apache curator 4.0.0.\u00a0\nWe are trying to upgrade to a newer version of Zookeeper (3.4.10) due to security issues in ZK 3.4.9 and have been forced to upgrade to a newer version of curator (from 2.11.1 due to other curator issues) but we keep hitting compile-time issues (see below)\u00a0which mean that we need to use ZK 3.5.3-beta at compile time even though we\u00a0will\u00a0use ZK 3.4.10 at runtime.\nHere is an example of the javac verbose output (via maven):\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/transaction/CuratorOp.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar(org/apache/zookeeper/Op.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar(org/apache/zookeeper/Op$Create.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar(org/apache/zookeeper/Op$Delete.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar(org/apache/zookeeper/Op$SetData.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar(org/apache/zookeeper/Op$Check.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/transaction/TypeAndPath.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/SyncBuilder.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/RemoveWatchesBuilder.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/RemoveWatchesType.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/RemoveWatchesLocal.class)]]\n[loading ZipFileIndexFileObject[/Users/sbilling/.m2/repository/org/apache/curator/curator-framework/4.0.0/curator-framework-4.0.0.jar(org/apache/curator/framework/api/BackgroundPathableQuietlyable.class)]]java.lang.RuntimeException: com.sun.tools.javac.code.Symbol$CompletionFailure: class file for org.apache.zookeeper.Watcher$WatcherType not found\nIf I look through the source code I see that RemoveWatchesType\u00a0imports and uses\u00a0org.apache.zookeeper.Watcher$WatcherType in the 'ofType' method.\nI don't understand how the backwards-compatibility is meant to work. It is also strange that this compile error only appears for a few of the modules in the project, so it must depend on which classes are used by the library and so which classes are loaded into the compile tree.",
        "Issue Links": []
    },
    "CURATOR-455": {
        "Key": "CURATOR-455",
        "Summary": "Curator's EnsembleProvider.getConnectionString() is not re-called once the session expiration has been injected",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "20/Feb/18 14:28",
        "Updated": "22/Feb/18 15:09",
        "Resolved": "22/Feb/18 15:09",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/curator/pull/252"
        ]
    },
    "CURATOR-456": {
        "Key": "CURATOR-456",
        "Summary": "Change visible of some constructor/fields",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gali Alykov",
        "Created": "28/Feb/18 08:18",
        "Updated": "17/May/23 11:20",
        "Resolved": "17/May/23 11:20",
        "Description": "Change visible for\u00a0ServiceInstanceBuilder\u00a0constructor/fields. This is need for extensibility (for example in project\u00a0https://github.com/hazelcast/hazelcast-zookeeper). I think that package visibility is bad for extensibility. Could you please also say your opinion about change package visibility in others classes?",
        "Issue Links": []
    },
    "CURATOR-457": {
        "Key": "CURATOR-457",
        "Summary": "Configuring service discovery paths",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Aleksander Melnichnikov",
        "Created": "01/Mar/18 09:44",
        "Updated": "03/Jul/23 06:54",
        "Resolved": null,
        "Description": "Right now paths for services in service discovery hardcoded to pattern: /{basePath} /{serviceName}/{instanceId}.\u00a0And that's impossible to configure paths this way:\u00a0\u00a0/{basePathPart1} /{serviceName}/{basePathPart2}/{instanceId}\nFor example:\n If basePath = /base1/base2/base3, serviceName = serviceName, instanceId - serviceId\n then path will be /base1/base2/base3/serviceName/serviceId, that can't be configured like this:\u00a0/base1/base2/serviceName/base3/serviceId.\nBecause of strong encapsulation in ServiceDiscoveryImpl class (package private method modifiers)\n\n\r\n@VisibleForTesting\r\nString pathForName(String name) \r\n{ \r\n    return ZKPaths.makePath(basePath, name); \r\n}\r\n@VisibleForTesting \r\nString pathForInstance(String name, String id)\r\n{ \r\n   return ZKPaths.makePath(pathForName(name), id);\r\n}\r\n\n\nSo, can we rethink visibility modifiers or add some extendable entity which would be\u00a0responsible for constructing paths?\u00a0\nFor example:\nCreate interface\n\n\r\n/**\r\n * A constructor which constructs path to services for service discovering\r\n */\r\npublic interface DiscoveryPathConstructor\r\n{\r\n    /**\r\n     * Return the path where all service names registered\r\n     * @return basePath\r\n     */\r\n    String getBasePath();\r\n\r\n    /**\r\n     * Return the path where all instances of service registered\r\n     * @param name service name\r\n     * @return path to service instances\r\n     */\r\n    String getPathForInstances(String name);\r\n\r\n    /**\r\n     * Return the path where concrete instance registered\r\n     * @param name service name\r\n     * @param id concrete instance id\r\n     * @return path to concrete instance\r\n     */\r\n    String getPathForInstance(String name, String id);\r\n}\r\n\n\nAlso make a constructor and field in ServiceDiscoveryImpl:\n\n\r\n    private final DiscoveryPathConstructor pathConstructor;\r\n  /**\r\n     * @param client the client\r\n     * @param pathConstructor constructor for instances path\r\n     * @param serializer serializer for instances (e.g. {@link JsonInstanceSerializer})\r\n     * @param thisInstance instance that represents the service that is running. The instance will get auto-registered\r\n     * @param watchInstances if true, watches for changes to locally registered instances\r\n     */\r\n    public ServiceDiscoveryImpl(CuratorFramework client, DiscoveryPathConstructor pathConstructor, InstanceSerializer<T> serializer,\r\n                                ServiceInstance<T> thisInstance, boolean watchInstances)\r\n    {\r\n        this.watchInstances = watchInstances;\r\n        this.client = Preconditions.checkNotNull(client, \"client cannot be null\");\r\n        this.serializer = Preconditions.checkNotNull(serializer, \"serializer cannot be null\");\r\n        this.pathConstructor = Preconditions.checkNotNull(pathConstructor, \"pathConstructor cannot be null\");\r\n        if ( thisInstance != null )\r\n        {\r\n            Entry<T> entry = new Entry<T>(thisInstance);\r\n            entry.cache = makeNodeCache(thisInstance);\r\n            services.put(thisInstance.getId(), entry);\r\n        }\r\n    }\r\n\n\nYou can view commit in my fork of framework, if that's ok - I can make a pull request. \nhttps://github.com/Me1kaa/curator/commit/6e6a34db7d71a84e3a53d284fc94e1fe2e3c10d8",
        "Issue Links": [
            "https://github.com/apache/curator/pull/256",
            "https://github.com/apache/curator/pull/256"
        ]
    },
    "CURATOR-458": {
        "Key": "CURATOR-458",
        "Summary": "Fix Schema constructor validation logic",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Roman Leventov",
        "Created": "17/Mar/18 18:08",
        "Updated": "14/Jul/22 03:03",
        "Resolved": "14/Jul/22 03:03",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/curator/pull/257"
        ]
    },
    "CURATOR-459": {
        "Key": "CURATOR-459",
        "Summary": "Support for asynchronous locks in Curator Async",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Peter Rietzler",
        "Created": "20/Mar/18 06:41",
        "Updated": "20/Mar/18 14:06",
        "Resolved": null,
        "Description": "It would be great to have a support for asynchronous locks.\u00a0Here's an interface suggestion\n\n\r\n/**\r\n * A non-reentrant Lock with the ability to react when the lock is acquired async, so that the thread which\r\n * acquires the lock is not blocked.\r\n */\r\npublic interface CompletableLock {\r\n\r\n /**\r\n * Acquire the lock non-blocking. The resulting {@link CompletableFuture} can then be used to block\r\n * until the lock is actually acquired. The {@link CompletableFuture#cancel(boolean)} can be used\r\n * to abort the acquisition of the lock. As soon as the lock is actually acquired, this method has no\r\n * effect. The Lock itself is then released with {@link Lease#unlock()}.\r\n * <p>\r\n * Several calls results in several independent locking requests. So it is equivalent to\r\n * call this method multiple times or get a new lock and then call the method once.\r\n *\r\n * @return an instance of {@link CompletableFuture} which represents the actual lock.\r\n */\r\n CompletableFuture<Lease> lock();\r\n\r\n default void withLock(Runnable body) {\r\n lock().thenAccept(lock -> {\r\n try {\r\n body.run();\r\n } finally {\r\n lock.unlock();\r\n }\r\n });\r\n }\r\n\r\n /**\r\n * Same as {@link #lock()} but if the lock can't be acquired within the given amount of time,\r\n * the {@link CompletableFuture} throws a {@link java.util.concurrent.ExecutionException} with a nested\r\n * {@link java.util.concurrent.TimeoutException} when it is accessed the next time.\r\n *\r\n * @param time the maximum waiting time for acquiring the lock.\r\n * @param unit the {@link TimeUnit} for the given time.\r\n * @return an instance of {@link CompletableFuture} which represents the actual lock.\r\n */\r\n CompletableFuture<Lease> lock(int time, TimeUnit unit);\r\n\r\n /**\r\n * Gets the lock only if possible. If lock is currently acquired, this returns immediately with an undefined option.\r\n */\r\n Optional<Lease> tryLock();\r\n\r\n /**\r\n * Gets the lock only if possible. If lock is currently acquired, this returns an undefined option after the\r\n * given timeout.\r\n */\r\n Lease tryLock(int time, TimeUnit unit);\r\n\r\n /**\r\n * For unlocking a successfully acquired lock.\r\n */\r\n interface Lease {\r\n\r\n /**\r\n * Releases the lock.\r\n */\r\n void unlock();\r\n }\r\n}",
        "Issue Links": []
    },
    "CURATOR-460": {
        "Key": "CURATOR-460",
        "Summary": "Timed tolerance for connection suspended leads to simultaneous leaders",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "3.3.0,                                            4.0.0,                                            4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": "Cam McKenzie",
        "Reporter": "Antonio Rafael Rodrigues",
        "Created": "24/Mar/18 19:17",
        "Updated": "11/Dec/18 19:25",
        "Resolved": "12/Apr/18 01:57",
        "Description": "Starting from Curator 3, after losing the connection to zookeeper, curator emits a SUSPENDED event and then starts an internal timer, if the time of the negotiatied session timeout get hit and curator didn't reconnect to zookeeper it emits a LOST event.\nFor example :\n Given negotiated session timeout = 40\n\n\n\nTime (seconds)\nEvent\n\n\n0\nSUSPENDED (curator has been disconnected from zookeeper)\n\n\n40\nLOST (curator is still disconnected, it must have been lost the sesion as it is disconnected for 40sec)\n\n\n\nGiven this scenario we could ,theoretically, ignore the SUSPENDED event and consider the leadership as lost just if the ConnectionStateListener receives the LOST event.\nBut this feature seems to have introduced a bug (from my point of view)\nCase of failure\n\n\n\nTime (seconds)\nEvent\n\n\n0\nSomething went wrong with the connected zookeeper (in my case, the network inteface of zookeeper's server has gone down). Curator stops hearing heart beats from zookeeper, but doesn't lose the connection (from some reason that I don't know, if the network interface of the server goes down, Curator doesn't lose the connection)\n\n\n~26.666\nSUSPENDED (after 26 seconds without hearing heartbeats, curator emits a SUSPENDED event) 26 is from \"readTimeout = sessionTimeout * 2 / 3\" from the class ClientCnxn.java from zookeeper client. At this point, curator starts counting 40 sec.\n\n\n26.666 to 40\nDuring this period, Curator is trying to connect to other zookeeper instances but the other instances, in my example case, are also uncheachable.\n\n\n40\nSession has expired and the other instance has taken leadership ( in my example, the other instance can connect to zookeeper )\n\n\n66.666\nLOST ( after 40sec from SUSPENDED, curator finally sends LOST)\n\n\n\nAs you can see, if we are ignoring the SUSPENDED event, the second application instance acquires the leadership ~26 seconds before the first instance notice that it lost the leadership.\n I understand it seems to be a very rare case, but either way I think it should be addressed.\nReproduce it\nI have came up with a way to reproduce this easily. I know this is not a recommended production setup, but it reproduces the problem anyway.\n1) On a virtual machine, with 2 network interfaces (eth0[192.168.0.101], eth1[192.168.0.102]) , I installed one zookeeper instance.\n 2) I setup application1 with the leadership receipe, with 40sec of negotiated timeout, pointing just to 192.168.0.101\u00a0 .Now it is the leader\n 3) I setup application2 with the leadership receipe, with 40sec of negotiated timeout, pointing just to 192.168.0.102\u00a0 .Now it it is NOT the leader\n 4) On the server I turn the eth0[192.168.0.101] interface down\u00a0 [ ifconfig eth0 down ]\n 5) After 26 seconds, application1 says : \n \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0 :ClientCnxn$SendThread@1111] - Client session timed out, have not heard from server in 26679ms for sessionid\n \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 ConnectionStateManager@237] - State change: SUSPENDED\n\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0NOTE: I'm ignoring the SUSPENDED event\n6) After 40 seconds, application2 takes leadership\n 7) After 66 seconds, application1 says :\n \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0ConnectionStateManager@237] - State change: LOST\n \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\n \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0NOTE: Just at this point, I consider that the application1 has lost leadership\nThen, for 26 seconds, we had 2 leaders.\nIf you confirm it as a bug, I think I could help.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/243",
            "https://github.com/apache/curator/pull/262",
            "https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=80446805"
        ]
    },
    "CURATOR-461": {
        "Key": "CURATOR-461",
        "Summary": "Update release artifact production to match new guidelines",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Apache",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "26/Mar/18 08:24",
        "Updated": "09/Dec/18 23:56",
        "Resolved": "09/Dec/18 23:56",
        "Description": "From Apache...\n\u00a0\nThe Release Distribution Policy[1] changed regarding checksum files.\n\u00a0\u00a0See under \"Cryptographic Signatures and Checksums Requirements\" [2].\n\u00a0\u00a0\u00a0\u00a0MD5-file == a .md5 file\n\u00a0\u00a0\u00a0\u00a0SHA-file == a .sha1, sha256 or .sha512 file\n\u00a0Old policy :\n\u00a0\u00a0\u00a0\u00a0-- MUST provide a MD5-file\n\u00a0\u00a0\u00a0\u00a0-- SHOULD provide a SHA-file [SHA-512 recommended]\n\u00a0New policy :\n\u00a0\u00a0\u00a0\u00a0-- MUST provide a SHA- or MD5-file\n\u00a0\u00a0\u00a0\u00a0-- SHOULD provide a SHA-file\n\u00a0\u00a0\u00a0\u00a0-- SHOULD NOT provide a MD5-file\n\u00a0\u00a0\u00a0\u00a0Providing MD5 checksum files is now discouraged for new releases,\n\u00a0\u00a0\u00a0\u00a0but still allowed for past releases.\n\u00a0Why this change :\n\u00a0\u00a0\u00a0\u00a0-- MD5 is broken for many purposes ; we should move away from it.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0https://en.wikipedia.org/wiki/MD5#Overview_of_security_issues\n\u00a0Impact for PMCs :\n\u00a0\u00a0\u00a0\u00a0-- for new releases :\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-- please do provide a SHA-file (one or more, if you like)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-- do NOT provide a MD5-file\n\u00a0\u00a0\u00a0\u00a0-- for past releases :\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-- you are not required to change anything\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-- for artifacts accompanied by a SHA-file /and/ a MD5-file,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0it would be nice if you removed the MD5-file\n\u00a0\u00a0\u00a0\u00a0-- if, at the moment, you provide MD5-files,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0please adjust your release tooling.\n\u00a0Please mail me (henkp@apache.org) if you have any questions etc.\n\u00a0FYI :\n\u00a0\u00a0Many projects are not (entirely, strictly) checksum file compliant.\n\u00a0\u00a0For an overview/inventory (by project) see :\n\u00a0\u00a0\u00a0https://checker.apache.org/dist/unsummed.html\n\u00a0At the moment :\n\u00a0\u00a0\u00a0\u00a0-- no checksum : 176 packages in 28 projects ; non-compliant\n\u00a0\u00a0\u00a0\u00a0-- only MD5 \u00a0\u00a0\u00a0: 495 packages in 44 projects ; update tooling\n\u00a0\u00a0\u00a0\u00a0-- only SHA \u00a0\u00a0\u00a0: 135 packages in 13 projects ; now comliant\n\u00a0\u00a0In many cases, only a few (among many) checksum file are missing ;\n\u00a0\u00a0you may want to fix that.\n\u00a0\u00a0[1] http://www.apache.org/dev/release-distribution\n\u00a0\u00a0[2] http://www.apache.org/dev/release-distribution#sigs-and-sums\n\u00a0Thanks, groeten,",
        "Issue Links": []
    },
    "CURATOR-462": {
        "Key": "CURATOR-462",
        "Summary": "InterProcessSemaphoreV2 leaves orphaned lease node if acquiring the semaphore is interrupted at the wrong time",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.11.0,                                            3.2.0,                                            3.2.1,                                            2.11.1,                                            3.3.0,                                            2.12.0,                                            4.0.0,                                            4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Gergely Krajcsovszki",
        "Created": "10/Apr/18 12:03",
        "Updated": "30/Apr/18 01:53",
        "Resolved": "30/Apr/18 01:53",
        "Description": "Since\u00a0CURATOR-315, lease nodes created\u00a0in\u00a0org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2.internalAcquire1Lease(Builder<Lease>, long, boolean, long) are not returned if the wait for them to become active is interrupted. This is because the lease node is now only added to the builder at the end of the method, so the caller doesn't know about this node and cannot close it when catching an exception.\nWe have already fixed this locally in our own dependent library so I'll make a PR with our fix and a test soon. We are still on the 2.x branch so we'd like if this fix could be in 2.13.0.",
        "Issue Links": [
            "/jira/browse/CURATOR-315",
            "https://github.com/apache/curator/pull/263"
        ]
    },
    "CURATOR-463": {
        "Key": "CURATOR-463",
        "Summary": "Update docs to point at Tech Notes where appropriate",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "TBD",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "13/Apr/18 03:11",
        "Updated": "11/Dec/18 19:19",
        "Resolved": null,
        "Description": "While the Curator website has some some links to the Tech Notes wiki, new users may still miss it. In\u00a0particular, the Errors page of the website (http://curator.apache.org/errors.html) should have a clear link to the Tech Notes.",
        "Issue Links": []
    },
    "CURATOR-464": {
        "Key": "CURATOR-464",
        "Summary": "Unable to instantiate client in OSGi",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Alexey Markevich",
        "Created": "25/Apr/18 10:31",
        "Updated": "22/Apr/20 03:39",
        "Resolved": "20/Apr/20 11:47",
        "Description": "112 | Active  |  80 | 2.12.0         | Curator Client\n113 | Active  |  80 | 2.12.0         | Curator Framework\n114 | Active  |  80 | 2.12.0         | Curator Recipes\n\n\r\nCuratorFramework client = CuratorFrameworkFactory.newClient(\"localhost:2181\", new RetryOneTime(1000));\r\n\n\n\nCaused by: java.lang.NoClassDefFoundError: org/apache/curator/shaded/com/google/common/cache/CacheBuilder\r\n\tat org.apache.curator.framework.imps.NamespaceWatcherMap.<init>(NamespaceWatcherMap.java:31) ~[?:?]\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:81) ~[?:?]\r\n\tat org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:145) ~[?:?]\r\n\tat org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:100) ~[?:?]\r\n\tat org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:81) ~[?:?]\r\n\n\ncurator-framework sources\n\n\r\nimport com.google.common.cache.CacheBuilder;\r\n\n\nbut jar contains\n\n\r\nimport org.apache.curator.shaded.com.google.common.cache.CacheBuilder;",
        "Issue Links": [
            "/jira/browse/CURATOR-440",
            "https://github.com/apache/curator/pull/355",
            "https://github.com/apache/curator/pull/360"
        ]
    },
    "CURATOR-465": {
        "Key": "CURATOR-465",
        "Summary": "Curator needs an abstraction that interrupts locking threads when there are connection problems",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "TBD",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "25/Apr/18 23:41",
        "Updated": "11/Dec/18 19:19",
        "Resolved": null,
        "Description": "For some reason, we don't have an abstraction that manages interrupting a thread that's using one of Curator's lock recipes. Our docs say that if you get SUSPENDED/LOST you should interrupt any locks/leaders. LeaderSelectorListenerAdapter handles this but we don't have something for locks or LeaderLatch.\u00a0\nI think something like the Locker class would work where when the lock is acquired, the current thread is recorded. The class adds a ConnectionStateListener and interrupts the thread when the connection is lost.",
        "Issue Links": []
    },
    "CURATOR-466": {
        "Key": "CURATOR-466",
        "Summary": "LeaderSelector gets in an inconsistent state when releasing resources.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Mikhail Pryakhin",
        "Created": "15/May/18 17:18",
        "Updated": "14/Nov/18 14:00",
        "Resolved": "13/Nov/18 15:53",
        "Description": "I'm using the leader election recipe that works well until I encountered application shutdown.\nhere is my example:\n\u00a0\n\n\r\nCuratorFramework framework = CuratorFrameworkFactory.builder()\r\n    .connectString(\"localhost:2181\")\r\n    .retryPolicy(new RetryOneTime(100))\r\n    .build();\r\n\r\nLeaderSelector leaderSelector = new LeaderSelector(\r\n    framework,\r\n    \"/path\",\r\n    new LeaderSelectorListener() {\r\n        volatile boolean stopped;\r\n        @Override\r\n        public void takeLeadership(CuratorFramework client) throws Exception {\r\n            System.out.println(\"I'm a new leader!\");\r\n            try {\r\n                while (!Thread.currentThread().isInterrupted() && !stopped) {\r\n                    TimeUnit.SECONDS.sleep(1);\r\n                }\r\n            } finally {\r\n                System.out.println(\"I'm not a leader anymore..\");\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public void stateChanged(CuratorFramework client, ConnectionState     newState) {\r\n            if (client.getConnectionStateErrorPolicy().isErrorState(newState)) {\r\n                stopped = true;\r\n            }\r\n         }\r\n  }\r\n);\r\n\r\nframework.start();\r\nleaderSelector.start();\r\n\r\nTimeUnit.SECONDS.sleep(5);\r\n\r\nleaderSelector.close();   //(1)\r\nframework.close();        //(2)\n\n\u00a0\nWhen I release resources by calling close method first on the LeaderSelector instance and then on the CurtorFramework instance (lines 1 and 2) I always get the following exception:\n\u00a0\n\n\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\nat org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[curator-client-4.0.1.jar:?]\r\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:424) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:347) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:124) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:449) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:466) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:65) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:246) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:240) [curator-recipes-4.0.1.jar:4.0.1]\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_141]\r\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_141]\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_141]\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_141]\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_141]\r\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0_141]\r\n\n\n\u00a0\nThe reason for the exception is that the non-blocking LeaderSelector.close method delegates call to the internal executor service, which abruptly cancels the running futures with the interptIfRunning flag set to true. Right after this, the CuratorFramework close method is called. By the meantime, the future being canceled executes the finally block where it calls methods on the already closed CuratorFramework instance which leads to throwing an exception.\nI thought I can wait a bit until the LeaderSelector instance is closed, so I tried to delay for some time before closing the CuratorFramework instance, but doing so leads to another exception:\n\n\r\nava.lang.InterruptedException: null\r\nat java.lang.Object.wait(Native Method) ~[?:1.8.0_141]\r\nat java.lang.Object.wait(Object.java:502) ~[?:1.8.0_141]\r\nat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1409) ~[zookeeper-3.4.12.jar:3.4.12--1]\r\nat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:874) ~[zookeeper-3.4.12.jar:3.4.12--1]\r\nat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:274) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:268) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[curator-client-4.0.1.jar:?]\r\nat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[curator-client-4.0.1.jar:?]\r\nat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:265) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:249) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:34) ~[curator-framework-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:347) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:124) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154) ~[curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:449) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:466) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:65) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:246) [curator-recipes-4.0.1.jar:4.0.1]\r\nat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:240) [curator-recipes-4.0.1.jar:4.0.1]\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_141]\r\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_141]\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_141]\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_141]\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_141]\r\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0_141]\r\n\n\nAt this time the exception is caused by the future being canceled with the interptIfRunning flag set to true in the LeaderSelector close method.\nAs the LeaderSelector implementation is based on the InterPorcessMutex that works with ephemeral nodes, do we really need to manually clean up on shutdown? As far as I know, the ephemeral nodes are deleted when the client disconnects.",
        "Issue Links": []
    },
    "CURATOR-467": {
        "Key": "CURATOR-467",
        "Summary": "Avoid logging errors after curator framework is closed.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Keith Turner",
        "Created": "01/Jun/18 21:41",
        "Updated": "05/Jun/18 20:06",
        "Resolved": null,
        "Description": "When the following operations are performed in rapid succession, it can lead annoying errors being logged.\n\nCreate curator framework\nCreate leader latch\nStart leader latch\nClose leader latch\nClose curator framework\n\nThe errors happen because background processing queued by the leader latch is still in flight when the curator framework is closed.  When these background ops try to access the closed curator framework the following exception happens.  It would be nice if the messages were logged at debug after close.\n\n2018-06-01 17:35:52,095 [imps.CuratorFrameworkImpl] ERROR: Background exception was not retry-able or retry gave up\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\n\tat org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:176)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:359)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderLatch.setNode(LeaderLatch.java:666)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderLatch.access$300(LeaderLatch.java:64)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderLatch$4.processResult(LeaderLatch.java:492)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:749)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:522)\r\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.sendBackgroundResponse(CreateBuilderImpl.java:584)\r\n\tat org.apache.curator.framework.imps.CreateBuilderImpl.access$900(CreateBuilderImpl.java:44)\r\n\tat org.apache.curator.framework.imps.CreateBuilderImpl$6.processResult(CreateBuilderImpl.java:534)\r\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)\r\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)",
        "Issue Links": [
            "/jira/browse/CURATOR-469"
        ]
    },
    "CURATOR-468": {
        "Key": "CURATOR-468",
        "Summary": "Closing an active leader selector always logs spurious error",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Keith Turner",
        "Created": "01/Jun/18 22:10",
        "Updated": "09/Dec/18 23:00",
        "Resolved": "09/Dec/18 23:00",
        "Description": "Closing a a leader selector seems to always log spurious error messages.  This only happens when it has assumed leadership because of CURATOR-337.  I think this spurious error logging highlights a conceptual problem with the code.   The primary reason that LeaderSelector.java line 449 is executed is because the thread was interrupted.   Since the interrupt is set for the thread the call to release the mutex will never work, so its a bit of a catch-22.\nBelow is an example of the exception I am seeing.\n\n2018-05-31 20:23:51,068 [leader.LeaderSelector] ERROR: The leader threw an exception\r\njava.lang.InterruptedException\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Object.wait(Object.java:502)\r\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1406)\r\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871)\r\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:250)\r\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:244)\r\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)\r\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:241)\r\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:225)\r\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35)\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:339)\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:123)\r\n\tat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:449)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:466)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:65)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:246)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:240)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n\nBelow is an example of an exception I see when closing a leader selector and then closing the curator framework.\n\n2018-05-31 20:23:49,905 [leader.LeaderSelector] ERROR: The leader threw an exception\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\n\tat org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:176)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:359)\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:339)\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:123)\r\n\tat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:449)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:466)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:65)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:246)\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:240)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)",
        "Issue Links": [
            "/jira/browse/CURATOR-337",
            "https://github.com/apache/curator/pull/279",
            "https://github.com/apache/curator/pull/285"
        ]
    },
    "CURATOR-469": {
        "Key": "CURATOR-469",
        "Summary": "Give background task a chance to execute in CuratorFramework.close()",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Keith Turner",
        "Created": "05/Jun/18 20:06",
        "Updated": "05/Jun/18 23:35",
        "Resolved": null,
        "Description": "The current impl of CuratorFramework.close() does not really give background task a chance to finish.  The following steps are currently taken in close().\n 1. Set the state to STOPPED\n 2. Call executorService.shutdownNow();\n 3. Call executorService.awaitTermination(maxCloseWaitMs, TimeUnit.MILLISECONDS);\nAfter step 1 is complete any background task accessing curator will get an IllegalStateException (See CURATOR-467).  Step 2 interrupts actively running task and dequeues any task waiting run.  In step 3 I wonder why bother to wait?\nMaking close do the following is one possible way to give background task a chance to run.\n 1. Call executorService.shutdown();\n 2. Call executorService.awaitTermination(maxCloseWaitMs, TimeUnit.MILLISECONDS);\n 3. Set the state to STOPPED\n 4. Call executorService.shutdownNow();\nStep 1 prevents new task from being added, but gives currently running and queued task a chance.  In step 2 we wait up to the user configured time for task to complete.  In step 3 and 4 we cause any background task that are still running to fail.",
        "Issue Links": [
            "/jira/browse/CURATOR-467"
        ]
    },
    "CURATOR-470": {
        "Key": "CURATOR-470",
        "Summary": "Newly added/Removed ServiceInstance should be passed to ServiceCacheListener#cacheChanged method",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "General,                                            Recipes",
        "Assignee": null,
        "Reporter": "Jerry Chin",
        "Created": "07/Jun/18 09:49",
        "Updated": "24/Jun/18 15:56",
        "Resolved": null,
        "Description": "Hey fellows,\nWe're\u00a0using service discovery extension to refactor our raw zookeeper based service monitor.\nEvery time a service instance exits or joins,\u00a0the monitor must be notified and push a notification to us, the only thing I found in\u00a0service discovery package meets the requirement is ServiceCacheListener interface, but when the interface is fired, there's no way we could know which service has changed.\nPlease help !\nThank in advance.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/265",
            "https://github.com/apache/curator/pull/268"
        ]
    },
    "CURATOR-471": {
        "Key": "CURATOR-471",
        "Summary": "ZooKeeper 3.5.4-beta is Java 8 minimum - Curator should follow",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client,                                            Framework,                                            Recipes,                                            Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "24/Jun/18 16:35",
        "Updated": "28/Jun/18 06:02",
        "Resolved": "28/Jun/18 06:02",
        "Description": "Apache ZooKeeper\u00a03.5.4-beta has made the minimum JDK 1.8. We should follow suit.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/269"
        ]
    },
    "CURATOR-472": {
        "Key": "CURATOR-472",
        "Summary": "Test-only code started to be called in prod",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "mskonovalov",
        "Created": "23/Jul/18 05:31",
        "Updated": "03/Dec/18 01:18",
        "Resolved": "03/Dec/18 01:18",
        "Description": "After commit\u00a0fe2c7c4cd606c0cf4bc4fab15deedc0f4c33ea0e the method\n\u00a0\n\n\r\nprotected boolean testableWaitForShutdown(int wait)\r\n    throws InterruptedException\r\n\n\nwith the following comment\n\u00a0\n\u00a0\n\n\r\n* Wait up to wait milliseconds for the underlying threads to shutdown.\r\n* THIS METHOD IS EXPECTED TO BE USED FOR TESTING ONLY!!!\r\n\n\nstarted to be called from real method ZooKeeper.close()\nIt prevents tests to finish.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/283"
        ]
    },
    "CURATOR-473": {
        "Key": "CURATOR-473",
        "Summary": "update the comment of AdvancedTracerDriver.java",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Do",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "nick chen",
        "Created": "05/Aug/18 14:08",
        "Updated": "25/Sep/22 14:58",
        "Resolved": "25/Sep/22 14:58",
        "Description": "previous comment was outdated.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/272"
        ]
    },
    "CURATOR-474": {
        "Key": "CURATOR-474",
        "Summary": "Remove usage of deprecated Guava methods",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Recipes",
        "Assignee": null,
        "Reporter": "William Thurston",
        "Created": "08/Aug/18 05:36",
        "Updated": "01/Sep/18 14:41",
        "Resolved": "29/Aug/18 18:28",
        "Description": "Usage of deprecated Guava methods can block guava version upgrades as methods are removed in future versions.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/273"
        ]
    },
    "CURATOR-475": {
        "Key": "CURATOR-475",
        "Summary": "Cannot effectively set connectionTimeout < 1000ms",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Martin Betak",
        "Created": "09/Aug/18 09:28",
        "Updated": "23/Sep/18 03:45",
        "Resolved": "23/Sep/18 03:45",
        "Description": "Regardless of the value set in\u00a0connectionTimeoutMs\u00a0the\nCuratorZookeeperClient#internalBlockUntilConnectedOrTimedOut method does\nthe initial wait with value of 1 second, effectively making settings lower than\nthis value ineffective. A simple change that would change the 1 second\u00a0to\u00a0min(remainingWaitTime, 1 second) would solve the problem.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/274"
        ]
    },
    "CURATOR-476": {
        "Key": "CURATOR-476",
        "Summary": "PathChildrenCache never gets initialized if a child dies even before a data watch is registered for it",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Rohan Suri",
        "Created": "10/Aug/18 12:15",
        "Updated": "14/Aug/18 00:16",
        "Resolved": "14/Aug/18 00:16",
        "Description": "What's happening:\nWhen PathChildrenCache is started with PathChildrenCache.StartMode.POST_INITIALIZED_EVENT there could be the following scenario:\n 1. PCC calls getChildren(\"../parent\") and gets the list of it's children\n 2. PCC calls processChildren(...) where it puts\u00a0them into it's {initialSet} \u2013 set of children whose data it will fetch and cache\n 3. Later in the same method it calls getDataAndStat(...\"/parent/somechild\") where it calls getData(...) on this child and registers a dataWatcher for this child\nMidst of step 2&3, child node could get deleted \u2013 but since step 3 hasn't completed yet, the dataWatcher isn't triggered (since it is not even registered) and we miss the NodeDeleted event.\nSince our {initialSet} still contains the child path, the initialized event is never fired.\nThe fix:\nAs of now there's only one codepath to removing a path from the {initialSet} (done from the data watcher)\nThe remove should also be called in the callback of our getData(...\"/parent/somechild\") being done in Step 3 if the resultCode=-101(KeeperException.NoNode)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/275"
        ]
    },
    "CURATOR-477": {
        "Key": "CURATOR-477",
        "Summary": "Ability to turn off Zk Watches in Curator Framework",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Rama Chavali",
        "Created": "20/Aug/18 03:06",
        "Updated": "10/Dec/18 03:47",
        "Resolved": "10/Dec/18 03:47",
        "Description": "In our use case, we use\u00a0TreeCache\u00a0to get Zk Data periodically. We start\u00a0TreeCache\u00a0read data and close it. In this use case, The\u00a0ZkWatchManager\u00a0of\u00a0ZooKeeper\u00a0class keeps growing for every TreeCache operation because new\u00a0TreeNode\u00a0objects are created and added there leading to a memory leak. Also since we do not want the Watcher to periodically watch, this creates unnecessary background operations.\nCan we introduce a builder flag in CuratorFramework's Builder some thing called \"createZkWatches\" that we can use to turn the watchers off? The default would be set to true to retain the current behaviour.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/277",
            "https://github.com/apache/curator/pull/278"
        ]
    },
    "CURATOR-478": {
        "Key": "CURATOR-478",
        "Summary": "LeaderLatch accumulates additional watcher handlers",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Tim Harper",
        "Created": "05/Sep/18 19:41",
        "Updated": "12/Sep/18 04:19",
        "Resolved": null,
        "Description": "In the event of a connection reconnect, LeaderLatch calls reset():\nhttps://github.com/apache/curator/blob/9a03ea93937af047e8ad13c2e3e3559520abfb0a/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L613\nUltimately, this results in another call to getChildren(), which calls checkLeadership(), which registers another getData watch for the ephemeral leader record preceding our new leader record. However, the watch in place from before reset() is in place, and will trigger yet another watch in the event that the record it is watching gets deleted.\nAs such, the number of pending watchers (at least client side) will continue to increase each time the connection fails over.\nMarked as trivial because I think it's unlikely these accumulate to the point that it's an issue, but it seems like it should at least be called out.",
        "Issue Links": []
    },
    "CURATOR-479": {
        "Key": "CURATOR-479",
        "Summary": "org.apache.curator.x.async.modeled.details.CachedModeledFrameworkImpl.children() does not work",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Hendrik Haddorp",
        "Created": "10/Sep/18 16:56",
        "Updated": "11/Dec/18 19:21",
        "Resolved": "11/Dec/18 19:21",
        "Description": "org.apache.curator.x.async.modeled.details.CachedModeledFrameworkImpl.children() and org.apache.curator.x.async.modeled.details.CachedModeledFrameworkImpl.childrenAsZNodes() do not seem to work. This filter condition looks wrong to me:\n.filter(path -> path.equals(cache.basePath()))\nGetting the children on an uncached model works just fine but on a cached model I always get an empty list. The list that cache.currentChildren(client.modelSpec().path()) returns within the methods looks correct but then there is this strange additional path filtering that throws away everything.\nThe test code for that class seems to be in https://github.com/apache/curator/blob/master/curator-x-async/src/test/java/org/apache/curator/x/async/modeled/TestCachedModeledFramework.java. Looks like the children calls are not tested.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/288",
            "https://github.com/apache/curator/pull/294",
            "https://github.com/apache/curator/pull/296"
        ]
    },
    "CURATOR-480": {
        "Key": "CURATOR-480",
        "Summary": "org.apache.curator.x.async.modeled.details.ModeledFrameworkImpl.delete(int) is not using version parameter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Hendrik Haddorp",
        "Created": "10/Sep/18 16:58",
        "Updated": "08/Dec/18 16:46",
        "Resolved": "08/Dec/18 16:46",
        "Description": "org.apache.curator.x.async.modeled.details.ModeledFrameworkImpl.delete(int) is defined as:\npublic AsyncStage<Void> delete(int version)\n{\r\n    return dslClient.delete().withVersion(-1).forPath(modelSpec.path().fullPath());\r\n}\n\nShouldn't the -1 be \"version\"?",
        "Issue Links": [
            "https://github.com/apache/curator/pull/287"
        ]
    },
    "CURATOR-481": {
        "Key": "CURATOR-481",
        "Summary": "Remove jackson-mapper-asl-version and update to latest version of jackson",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Maxim Pudov",
        "Created": "09/Oct/18 12:07",
        "Updated": "30/Jun/23 07:54",
        "Resolved": "03/Mar/19 19:15",
        "Description": "There is a vulnerability issue in\u00a0jackson-mapper-asl-version 1.9.13 and it is no longer supported. The same issue was present in jackson-databind till version 2.7.9.1.\nhttp://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-7525\nWe already have a dependency on\u00a0jackson 2.x. Let's replace\u00a0jackson-mapper-asl with jackson-databind and update jackson to the latest version.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/280",
            "https://github.com/apache/curator/pull/280"
        ]
    },
    "CURATOR-482": {
        "Key": "CURATOR-482",
        "Summary": "zookeeper exists waiting forever",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "zhaoziyan",
        "Created": "01/Nov/18 03:29",
        "Updated": "05/Dec/18 09:34",
        "Resolved": "05/Dec/18 09:34",
        "Description": "exists waiting forever\nserver version 3.4.6\nzookeeper cient version 3.4.8\u00a0\ncurator version curator-client-2.12\nbecause of the exists waiting forever, cause the cutator\u00a0TreeCache thread block\u00a0\u00a0\n\n\r\n// code placeholder\r\n\"ZZBOOK_UPDATE_JDID_JOB2_QuartzSchedulerThread\" #781 prio=5 os_prio=0 tid=0x00007f1a7611e800 nid=0x7661 in Object.wait() [0x00007f1936cee000]\r\n java.lang.Thread.State: WAITING (on object monitor)\r\n at java.lang.Object.wait(Native Method)\r\n at java.lang.Object.wait(Object.java:502)\r\n at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1406)\r\n - locked <0x00000000a6d2a5c8> (a org.apache.zookeeper.ClientCnxn$Packet)\r\n at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1097)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:237)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:226)\r\n at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForegroundStandard(ExistsBuilderImpl.java:223)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:216)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:175)\r\n at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:32)\r\n at com.dangdang.ddframe.job.reg.zookeeper.ZookeeperRegistryCenter.isExisted(ZookeeperRegistryCenter.java:210)\r\n at com.dangdang.ddframe.job.lite.internal.storage.JobNodeStorage.isJobNodeExisted(JobNodeStorage.java:66)\r\n at com.dangdang.ddframe.job.lite.internal.storage.JobNodeStorage.createJobNodeIfNeeded(JobNodeStorage.java:107)\r\n at com.dangdang.ddframe.job.lite.internal.execution.ExecutionService.setMisfire(ExecutionService.java:189)\r\n at com.dangdang.ddframe.job.lite.internal.schedule.JobTriggerListener.triggerMisfired(JobTriggerListener.java:47)\r\n at org.quartz.core.QuartzScheduler.notifyTriggerListenersMisfired(QuartzScheduler.java:1905)\r\n at org.quartz.core.SchedulerSignalerImpl.notifyTriggerListenersMisfired(SchedulerSignalerImpl.java:74)\r\n at org.quartz.simpl.RAMJobStore.applyMisfire(RAMJobStore.java:1354)\r\n at org.quartz.simpl.RAMJobStore.acquireNextTriggers(RAMJobStore.java:1412)\r\n - locked <0x00000000c4c9fb10> (a java.lang.Object)\r\n at org.quartz.core.QuartzSchedulerThread.run(QuartzSchedulerThread.java:272)\r\n\r\n\"ZZBOOK_UPDATE_JDID_JOB2_Worker-1\" #780 prio=5 os_prio=0 tid=0x00007f1a7611d000 nid=0x7660 in Object.wait() [0x00007f1936ef0000]\r\n java.lang.Thread.State: TIMED_WAITING (on object monitor)\r\n at java.lang.Object.wait(Native Method)\r\n at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:568)\r\n - locked <0x00000000c4c228c8> (a java.lang.Object)\r\n\r\n\"Curator-TreeCache-77\" #779 daemon prio=5 os_prio=0 tid=0x00007f19a4091000 nid=0x765f waiting for monitor entry [0x00007f1936def000]\r\n java.lang.Thread.State: BLOCKED (on object monitor)\r\n at org.quartz.simpl.RAMJobStore.storeTrigger(RAMJobStore.java:413)\r\n - waiting to lock <0x00000000c4c9fb10> (a java.lang.Object)\r\n at org.quartz.core.QuartzScheduler.triggerJob(QuartzScheduler.java:1187)\r\n at org.quartz.impl.StdScheduler.triggerJob(StdScheduler.java:341)\r\n at org.quartz.impl.StdScheduler.triggerJob(StdScheduler.java:331)\r\n at com.dangdang.ddframe.job.lite.internal.schedule.JobScheduleController.triggerJob(JobScheduleController.java:153)\r\n at com.dangdang.ddframe.job.lite.internal.server.JobOperationListenerManager$JobTriggerStatusJobListener.dataChanged(JobOperationListenerManager.java:103)\r\n at com.dangdang.ddframe.job.lite.internal.listener.AbstractJobListener.childEvent(AbstractJobListener.java:43)\r\n at org.apache.curator.framework.recipes.cache.TreeCache$2.apply(TreeCache.java:741)\r\n at org.apache.curator.framework.recipes.cache.TreeCache$2.apply(TreeCache.java:735)\r\n at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93)\r\n at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)\r\n at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85)\r\n at org.apache.curator.framework.recipes.cache.TreeCache.callListeners(TreeCache.java:734)\r\n at org.apache.curator.framework.recipes.cache.TreeCache.access$1700(TreeCache.java:71)\r\n at org.apache.curator.framework.recipes.cache.TreeCache$4.run(TreeCache.java:852)\r\n at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n at java.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\r\n\"main-EventThread\" #236 daemon prio=5 os_prio=0 tid=0x00007f1a7546a800 nid=0x74bf waiting on condition [0x00007f194e96e000]\r\n java.lang.Thread.State: WAITING (parking)\r\n at sun.misc.Unsafe.park(Native Method)\r\n - parking to wait for <0x00000000c24ca218> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\r\n at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:501)\r\n\r\n\"main-SendThread(tjtx-84-168.58os.org:2181)\" #235 daemon prio=5 os_prio=0 tid=0x00007f1a7596c000 nid=0x74be runnable [0x00007f194ea6f000]\r\n java.lang.Thread.State: RUNNABLE\r\n at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\r\n at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\r\n at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\r\n at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\r\n - locked <0x00000000c1da74d0> (a sun.nio.ch.Util$2)\r\n - locked <0x00000000c1da75a8> (a java.util.Collections$UnmodifiableSet)\r\n - locked <0x00000000c1ace0a0> (a sun.nio.ch.EPollSelectorImpl)\r\n at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\r\n at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:349)\r\n at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)",
        "Issue Links": []
    },
    "CURATOR-483": {
        "Key": "CURATOR-483",
        "Summary": "Sequential PersistentNodes with protection don't resync after reconnection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Nick Hill",
        "Created": "03/Nov/18 03:33",
        "Updated": "27/Nov/18 21:31",
        "Resolved": "27/Nov/18 21:31",
        "Description": "When using a PersistentNode with a sequential creation mode (in my case ephemeral but I think persistent would be the same) and \"protection\" turned on, unexpected duplicate znodes get created upon reconnection after a disconnect (without the sequence suffix), rather than it just taking ownership of the previously created znode.\nI tracked down the cause of this and have made a simple fix.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/281"
        ]
    },
    "CURATOR-484": {
        "Key": "CURATOR-484",
        "Summary": "CVE-2014-0085, CVE-2018-8012 known security vulnerabilities",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mujassim Sheikh",
        "Created": "12/Nov/18 12:20",
        "Updated": "13/Nov/18 21:09",
        "Resolved": null,
        "Description": "1. Due to the dependency on apache zookeeper 3.5.3-beta, curator is vulnerable to CVE-2018-8012\n We should change it to use 3.5.4-beta as soon as possible.",
        "Issue Links": []
    },
    "CURATOR-485": {
        "Key": "CURATOR-485",
        "Summary": "GroupMember API does not expose PersistentEphemeralNode or callbacks",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Andrey Belom",
        "Created": "20/Nov/18 00:52",
        "Updated": "20/Nov/18 00:52",
        "Resolved": null,
        "Description": "I create an instance of GroupMember using GroupMember(CuratorFramework client, String membershipPath, String thisId) constructor, invoke GroupMember.start() method and GroupMember.setThisData() method\ndepending on timing of things I sometimes get\n\n\r\nException in thread \"main\" java.lang.IllegalStateException: initial create has not been processed. Call waitForInitialCreate() to ensure.\r\n at org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444)\r\n at org.apache.curator.framework.recipes.nodes.PersistentNode.setData(PersistentNode.java:391)\r\n at org.apache.curator.framework.recipes.nodes.GroupMember.setThisData(GroupMember.java:95)\n\nI am sorry if I am missing something, but what API allows me to invoke pen.waitForInitialCreate or get some sort of a callback letting me know that I am allowed to change data?\nThere is a way to override newPersistentEphemeralNode() method in order to save pen reference and invoke waitForInitialCreate after GroupMember.start() but that does not seem right.\nI am really sorry if I am missing the point of this recipe.",
        "Issue Links": []
    },
    "CURATOR-486": {
        "Key": "CURATOR-486",
        "Summary": "InterProcessMutex.release fails to remove a lock on retries.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Purshotam Shah",
        "Created": "27/Nov/18 17:08",
        "Updated": "10/May/23 15:48",
        "Resolved": "10/May/23 15:48",
        "Description": "We were planning to build a release lock retry, looks like InterProcessMutex.release don't support it.\nIt removes currentThread entry from threadData, even if there is an issue releasing lock.\nInterProcessMutex.java\n\r\n   @Override\r\n    public void release() throws Exception\r\n    {\r\n        /*\r\n            Note on concurrency: a given lockData instance\r\n            can be only acted on by a single thread so locking isn't necessary\r\n         */\r\n\r\n        Thread      currentThread = Thread.currentThread();\r\n        LockData    lockData = threadData.get(currentThread);\r\n        if ( lockData == null )\r\n        {\r\n            throw new IllegalMonitorStateException(\"You do not own the lock: \" + basePath);\r\n        }\r\n\r\n        int newLockCount = lockData.lockCount.decrementAndGet();\r\n        if ( newLockCount > 0 )\r\n        {\r\n            return;\r\n        }\r\n        if ( newLockCount < 0 )\r\n        {\r\n            throw new IllegalMonitorStateException(\"Lock count has gone negative for lock: \" + basePath);\r\n        }\r\n        try\r\n        {\r\n            internals.releaseLock(lockData.lockPath);\r\n        }\r\n        finally\r\n        {\r\n            threadData.remove(currentThread);\r\n        }\r\n    }",
        "Issue Links": []
    },
    "CURATOR-487": {
        "Key": "CURATOR-487",
        "Summary": "GzipCompressionProvider produces a lot of finalizable objects",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "None",
        "Assignee": "Cam McKenzie",
        "Reporter": "Roman Leventov",
        "Created": "28/Nov/18 19:34",
        "Updated": "10/Dec/18 16:36",
        "Resolved": "10/Dec/18 03:44",
        "Description": "GzipCompressionProvider.compress() and decompress() methods are called a lot inside and outside of the framework and each such call produces a java.util.zip.Deflater or Inflater object that are finalizable, that is bad for GC.\n\u00a0\nCompressing or decompressing a\u00a0finite byte[] object (in contrast to\u00a0compressing/decompressing an InputStream or an OutputStream of unknown length) is actually a happy case because even PhantomReference object could be avoided, the native resources could be created and freed in a single try-finally\u00a0block.\n\u00a0\nCurator must avoid that. https://github.com/ymnk/jzlib\u00a0is a potential replacement, for example.\n\u00a0\nA similar issue could be also fixed in Apache Commons Compress: https://issues.apache.org/jira/browse/COMPRESS-473\u00a0and Curator is made to depend on Commons Compress.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/282"
        ]
    },
    "CURATOR-488": {
        "Key": "CURATOR-488",
        "Summary": "NoSuchMethodError calling client close() in 3.4 compatibility mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "TBD",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Boris Dainson",
        "Created": "04/Dec/18 16:47",
        "Updated": "04/Dec/18 22:10",
        "Resolved": "04/Dec/18 22:10",
        "Description": "Affected version is\u00a04.0.2-SNAPSHOT.\u00a0Version\u00a04.0.1 is not affected\nWhen closing client it is calling\u00a0zooKeeper.close(waitForShutdownTimeoutMs) This method is only supported starting from ZK 3.5. So when running in compatibility mode with ZK 3.4.x it throws\u00a0NoSuchMethodError.\u00a0 So for ZK 3.4.x need to use\u00a0 zooKeeper.close() with no arguments.\u00a0\n\u00a0\nTo reproduce:\nStart ZK 3.4 server locally.\u00a0\nIn pom.xml add dependency on\u00a0zookeeper 3.4.13 and exclude default dependency (on 3.5). Full pom.xml is attached.\u00a0\n\u00a0\nRun this code snippet:\u00a0\npublic class CloseTest {\n \u00a0 public static void main(String[] args) throws Exception\n\n{ \u00a0 \u00a0 CuratorFramework client = CuratorFrameworkFactory.newClient(\"localhost:2181\", new\u00a0 ExponentialBackoffRetry(1000, 3)); \u00a0 \u00a0 client.start(); \u00a0 \u00a0 client.close(); \u00a0 }\n\n}\n\u00a0\nIt will throw exception:\u00a0\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.zookeeper.ZooKeeper.close(I)Z\n at org.apache.curator.HandleHolder.internalClose(HandleHolder.java:158)\n at org.apache.curator.HandleHolder.closeAndClear(HandleHolder.java:78)\n at org.apache.curator.ConnectionState.close(ConnectionState.java:125)\n at org.apache.curator.CuratorZookeeperClient.close(CuratorZookeeperClient.java:266)\n at org.apache.curator.CuratorZookeeperClient.close(CuratorZookeeperClient.java:249)\n at org.apache.curator.framework.imps.CuratorFrameworkImpl.close(CuratorFrameworkImpl.java:388)\n at leader.CloseTest.main(CloseTest.java:11)",
        "Issue Links": []
    },
    "CURATOR-489": {
        "Key": "CURATOR-489",
        "Summary": "CreateOption.doProtected uses null protectedId when applied via AsyncCreateBuilderImpl",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "josh gruenberg",
        "Created": "04/Dec/18 18:02",
        "Updated": "05/Dec/18 17:02",
        "Resolved": "05/Dec/18 17:02",
        "Description": "When trying to apply \"protection\" to an async node-creation with\nCreateOption.doProtected, the protection is undermined because the node is created with protectionId = null.\nThe AsyncCreateBuilderImpl does pass doProtected = true to the CreateBuilderImpl constructor. However, this constructor (incorrectly?) assigns protectedId = null, resulting in a node-prefix of _c_null- (because getProtectedPrefix does not validate that the provided protectedId is non-null). This entirely undermines the effectiveness of the protection.\n\n\r\n    public CreateBuilderImpl(CuratorFrameworkImpl client, CreateMode createMode, Backgrounding backgrounding, boolean createParentsIfNeeded, boolean createParentsAsContainers, boolean doProtected, boolean compress, boolean setDataIfExists, List<ACL> aclList, Stat storingStat, long ttl)\r\n    {\r\n        this.client = client;\r\n        this.createMode = createMode;\r\n        this.backgrounding = backgrounding;\r\n        this.createParentsIfNeeded = createParentsIfNeeded;\r\n        this.createParentsAsContainers = createParentsAsContainers;\r\n        this.doProtected = doProtected;\r\n        this.compress = compress;\r\n        this.setDataIfExists = setDataIfExists;\r\n        protectedId = null;                     // *** incorrect if doProtected?! ***\r\n        this.acling = new ACLing(client.getAclProvider(), aclList);\r\n        this.storingStat = storingStat;\r\n        this.ttl = ttl;\r\n    }\r\n\n\nIt looks to me as if that constructor should be assigning protectedId if doProtected is true.\nI wonder if\u00a0the doProtected field should just be eliminated in favor a method that checks protectedId != null, to avoid this redundant encoding of intent?",
        "Issue Links": [
            "https://github.com/apache/curator/pull/284"
        ]
    },
    "CURATOR-490": {
        "Key": "CURATOR-490",
        "Summary": "Make the tests more resilient",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Dec/18 19:51",
        "Updated": "09/Dec/18 22:53",
        "Resolved": "09/Dec/18 22:50",
        "Description": "Some of the tests are flakey and fail intermittently. This task will remain open as a container for fixing tests.",
        "Issue Links": [
            "/jira/browse/CURATOR-26",
            "https://github.com/apache/curator/pull/286",
            "https://github.com/apache/curator/pull/293"
        ]
    },
    "CURATOR-491": {
        "Key": "CURATOR-491",
        "Summary": "PathChildrenCache can process background jobs after being closed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "08/Dec/18 21:26",
        "Updated": "09/Dec/18 07:29",
        "Resolved": "09/Dec/18 07:29",
        "Description": "CURATOR-332 fixed part of this, but there's another background handler that has the same problem.\nThe background handler in refresh() correctly checks if the instance has been closed before processing as well as clearing watchers that might have been set in the interim. This same treatment needs to be added to the background handler in getDataAndStat().",
        "Issue Links": [
            "https://github.com/apache/curator/pull/289"
        ]
    },
    "CURATOR-492": {
        "Key": "CURATOR-492",
        "Summary": "Time to upgrade the Apache parent pom again",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "09/Dec/18 00:26",
        "Updated": "09/Dec/18 07:28",
        "Resolved": "09/Dec/18 07:28",
        "Description": "Upgrade to the latest Apache parent pom",
        "Issue Links": [
            "https://github.com/apache/curator/pull/290"
        ]
    },
    "CURATOR-493": {
        "Key": "CURATOR-493",
        "Summary": "Fix test case with testDeleteChildrenConcurrently",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.1.0",
        "Component/s": "Framework,                                            Tests",
        "Assignee": null,
        "Reporter": "hebelala",
        "Created": "09/Dec/18 08:08",
        "Updated": "09/Dec/18 17:35",
        "Resolved": "09/Dec/18 17:35",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/curator/pull/291"
        ]
    },
    "CURATOR-494": {
        "Key": "CURATOR-494",
        "Summary": "Make ZKPaths.makePath() methods to allocate less garbage",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Roman Leventov",
        "Created": "10/Dec/18 18:40",
        "Updated": "11/Dec/18 15:25",
        "Resolved": "11/Dec/18 04:50",
        "Description": "Currently\u00a0ZKPaths.makePath() methods call substring() unnecessarily. I've seen those methods responsible for 15% of total\u00a0heap allocation in some application.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/295"
        ]
    },
    "CURATOR-495": {
        "Key": "CURATOR-495",
        "Summary": "Race and possible dead locks with RetryPolicies and several Curator Recipes",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.1.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "13/Dec/18 19:48",
        "Updated": "17/Dec/18 14:57",
        "Resolved": "17/Dec/18 03:26",
        "Description": "In\u00a0trying to figure out why TestInterProcessSemaphoreMutex is so flakey I've come across a fairly serious edge case in how several of our recipes work. You can see the issue in InterProcessSemaphoreV2 (which is what InterProcessSemaphoreMutex uses internally). Look here:\nInterProcessSemaphoreV2.java\nThe code synchronizes and then does client.getChildren().... This is where the problem is. If there are connection problems inside of getChildren() the retry policy will do configured sleeping, retries, etc. Importantly, this is all done while the thread doing the retries holds InterProcessSemaphoreV2's monitor. If the ZK connection is repaired past the session timeout, ZK will eventually call InterProcessSemaphoreV2's watcher with an Expired message. InterProcessSemaphoreV2's watcher calls this method:\n\n\r\nprivate synchronized void notifyFromWatcher()\r\n{\r\n    notifyAll();\r\n}\r\n\n\nYou can see that this is a race. The thread doing \"getChildren\" is holding the monitor and is in a retry loop waiting for the connection to be repaired. However, ZK's event loop is trying to obtain that same monitor as a result of trying to call the synchronized notifyFromWatcher(). This means that the retry policy will always fail because ZK's event loop is tied up until that thread exists. Worse still, if someone were to use a retry policy of \"RetryForever\" they'd have a deadlock.\nThis pattern is in about 10 files or so. I'm trying to think of a workaround. One possibility is to use a separate thread for this type of notification. i.e. notifyFromWatcher() would just signal another thread that the notifyAll() needs to be called. This would unblock ZK's event thread so that things can progress. I'll play around with this.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/297"
        ]
    },
    "CURATOR-496": {
        "Key": "CURATOR-496",
        "Summary": "Support ZooKeeper's HostProvider",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "19/Dec/18 03:51",
        "Updated": "22/Dec/18 18:19",
        "Resolved": "22/Dec/18 18:19",
        "Description": "ZOOKEEPER-1172 adds support for custom HostProviders. Curator should support this. Add support for custom HostProvider.",
        "Issue Links": []
    },
    "CURATOR-497": {
        "Key": "CURATOR-497",
        "Summary": "Add a downloads page to comply with Apache Standards",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "TBD",
        "Component/s": "Apache,                                            Documentation,                                            Website",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "19/Dec/18 19:49",
        "Updated": "19/Dec/18 19:49",
        "Resolved": null,
        "Description": "Apache announcements is rejecting our Curator release announcement due to a bad download page. Add one to the website that complies. Here's their automated message:\n\nAnnouncements of Apache project releases must contain a link to the\r\nrelevant download page. [1]\r\nThe download page must provide public download links where current official\r\nsource releases and accompanying cryptographic files may be obtained. [2]\r\n\r\nAnnouncements that contain a link to the dyn/closer page alone will be\r\nrejected by the moderators.\r\n\r\nAnnouncements that contain a link to a web page that does not include a\r\nlink to a mirror to the artifact plus links to the signature and at least\r\none sha checksum will be rejected.\r\n\r\nNote that https://www.apache.org/dist/curator/4.1.0/ is not a valid\r\ndownload page.\r\n\r\nWhilst it contains the sig and hash, these should be direct links to the\r\nfiles, and there is no link to the KEYS file.\r\nAlso the main zip file must use the mirrors, i.e. must use a dyn/closer URL.\r\n\r\nHave a look at how some other projects do this, e.g.\r\nhttps://httpd.apache.org/download.cgi\r\n\r\n[1] http://www.apache.org/legal/release-policy.html#release-announcements\r\n[2] https://www.apache.org/dev/release-distribution#download-links",
        "Issue Links": []
    },
    "CURATOR-498": {
        "Key": "CURATOR-498",
        "Summary": "Protected Mode creation can mistake closing session's node causing problems for many recipes such as LeaderLatch",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1,                                            4.1.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shay Shimony",
        "Created": "25/Dec/18 22:22",
        "Updated": "21/Feb/19 18:47",
        "Resolved": "11/Feb/19 12:48",
        "Description": "The Curator app I am working on uses the LeaderLatch to select a leader out of 6 clients.\nWhile testing my app, I noticed that when I make ZK lose its quorum for a while and then restore it, then after Curator in my app restores it's connection to ZK - sometimes not all the 6 clients are found in the latch path (using zkCli.sh). That is, I have 5 instead of 6.\nAfter investigating a little, I have a suspicion that LeaderLatch deleted the leader in method setNode.\nTo investigate it I copied the LeaderLatch code and added some log messages, and from them it seems like very old create() background callback was surprisingly scheduled and corrupted the current leader with its stale path name. Meaning, this old one called setNode with its stale name, and set itself instead of the leader and deleted the leader. This leaves client running, thinking it is the leader, while another leader is selected.\nIf my analysis is correct then it seems like we need to make this obsolete create callback cancelled (I think its session was suspended on 22:38:54 and then lost on 22:39:04 - so on SUSPENDED cancel ongoing callbacks).\nPlease see attached log file and modified LeaderLatch0.\n\u00a0\nIn the log, note that on 22:39:26 it shows that 0000000485 is replaced by 0000000480 and then probably deleted.\nNote also that at 22:38:52, 34 seconds before, we can see that it was in the reset() method (\"RESET OUR PATH\") and possibly triggered the creation of 0000000480 then.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-3269",
            "https://github.com/apache/curator/pull/298",
            "https://github.com/apache/curator/pull/299",
            "https://github.com/apache/curator/pull/303",
            "https://github.com/apache/curator/pull/303"
        ]
    },
    "CURATOR-499": {
        "Key": "CURATOR-499",
        "Summary": "ZKPaths strips trailing \"/\" incorrectly when creating sequential nodes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Marquis Wang",
        "Created": "01/Jan/19 00:23",
        "Updated": "21/Jun/23 13:09",
        "Resolved": null,
        "Description": "I think this was introduced in CURATOR-166, which changed ZKPaths to strip trailing slashes from child nodes.\nThis is fine in most cases: it made it so\u00a0\n\n\r\ncuratorFramework.create()\r\n\u00a0 \u00a0 .forPath(\"/path/to/node/\", data);\n\nwould create a node at /path/to/node.\nHowever, if you want to create a sequential node:\n\n\r\ncuratorFramework.create()\r\n\u00a0 \u00a0 .withMode(CreateMode.PERSISTENT_SEQUENTIAL)\r\n\u00a0 \u00a0 .forPath(\"/path/to/node/\", data);\n\nIn clients after CURATOR-166 (2.7.1 and higher), this will create a node /path/to/node00000001. Before, and if you call the zookeeper cli with the same options, it would create a node /path/to/node/00000001.\nThis effectively makes it so you cannot use curator to create sequential nodes where the entire node name is the sequence number.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/300",
            "https://github.com/apache/curator/pull/300"
        ]
    },
    "CURATOR-500": {
        "Key": "CURATOR-500",
        "Summary": "Apache is requiring us to move to Gitbox",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.2.0",
        "Component/s": "Apache,                                            General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Jan/19 18:41",
        "Updated": "30/Jan/19 19:25",
        "Resolved": "30/Jan/19 19:25",
        "Description": "Hello, curator folks.\nAs stated earlier in 2018, all git repositories must be migrated from\nthe git-wip-us.apache.org URL to gitbox.apache.org, as the old service\nis being decommissioned. Your project is receiving this email because\nyou still have repositories on git-wip-us that needs to be migrated.\nThe following repositories on git-wip-us belong to your project:\n\ncurator.git\n\nWe are now entering the mandated (coordinated) move stage of the roadmap,\nand you are asked to please coordinate migration with the Apache\nInfrastructure Team before February 7th. All repositories not migrated\non February 7th will be mass migrated without warning, and we'd appreciate\nit if we could work together to avoid a big mess that day .\nMoving to gitbox means you will get full write access on GitHub as well,\nand be able to close/merge pull requests and much more.\nTo have your repositories moved, please follow these steps:\n\nEnsure consensus on the move (a link to a lists.apache.org thread will\n suffice for us as evidence).\nCreate a JIRA ticket at https://issues.apache.org/jira/browse/INFRA\n\nYour migration should only take a few minutes. If you wish to migrate\nat a specific time of day or date, please do let us know in the ticket.\nAs always, we appreciate your understanding and patience as we move\nthings around and work to provide better services and features for\nthe Apache Family.\nShould you wish to contact us with feedback or questions, please do so\nat: users@infra.apache.org.\nWith regards,\nApache Infrastructure",
        "Issue Links": []
    },
    "CURATOR-501": {
        "Key": "CURATOR-501",
        "Summary": "TestRemoveWatches has expectedExceptions list with ZK 3.5 only class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "07/Jan/19 21:38",
        "Updated": "08/Jan/19 04:04",
        "Resolved": "08/Jan/19 04:04",
        "Description": "TestRemoveWatches.testRemoveUnregisteredWatcher is defined with the TestNG annotation of @Test(expectedExceptions=KeeperException.NoWatcherException.class). When this is run by \"curator-test-zk34\" using ZooKeeper 3.4.x it results in a deeply nested TypeNotPresentExceptionProxy as NoWatcherException doesn't exist in ZK 3.4. The test should be re-written to avoid this issue.",
        "Issue Links": []
    },
    "CURATOR-502": {
        "Key": "CURATOR-502",
        "Summary": "Update dependency com.google.guava:guava of org.apache.curator:curator-client",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "DW",
        "Created": "17/Jan/19 19:50",
        "Updated": "25/Apr/23 07:55",
        "Resolved": "25/Apr/23 07:55",
        "Description": "Please update the dependency com.google.guava:guava of org.apache.curator:curator-client due to open security vulnerability of the used com.google.guava:guava 20.0 [(including) 11.0 up to (excluding) 24.1.1]. Please upgrade to 24.1.1+. If you need the CVE number, let me know.",
        "Issue Links": [
            "/jira/browse/CURATOR-503",
            "https://github.com/apache/curator/pull/302"
        ]
    },
    "CURATOR-503": {
        "Key": "CURATOR-503",
        "Summary": "Update dependencies in January 2019",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Roman Leventov",
        "Created": "21/Jan/19 05:06",
        "Updated": "25/Apr/23 07:55",
        "Resolved": "03/Mar/19 13:37",
        "Description": "The main motivation for this update is updating Guava from 20.0 to 27.0.1-jre. Between these versions, the implementation of Maps.newConcurrentMap() was changed\u00a0and now it returns ConcurrentHashMap\u00a0instead of Guava's own implementation, that was less efficient.",
        "Issue Links": [
            "/jira/browse/CURATOR-502",
            "https://github.com/apache/curator/pull/301",
            "https://github.com/apache/curator/pull/301"
        ]
    },
    "CURATOR-504": {
        "Key": "CURATOR-504",
        "Summary": "Race conditions in LeaderLatch after reconnecting to ensemble",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Yuri Tceretian",
        "Created": "31/Jan/19 21:51",
        "Updated": "14/Jul/22 01:30",
        "Resolved": "05/Feb/19 22:01",
        "Description": "We use LeaderLatch in a lot of places in our system and when ZooKeeper ensemble is unstable and clients are reconnecting to logs are full of messages like the following:\n[2017-08-31 19:18:34,562][ERROR][org.apache.curator.framework.recipes.leader.LeaderLatch] Can't find our node. Resetting. Index: -1 {}\nAccording to the implementation, this can happen in two cases:\n\nWhen internal state `ourPath` is null\nWhen the list of latches does not have the expected one.\n\nI believe we hit the first condition because of races that occur after client reconnects to ZooKeeper.\n\nClient reconnects to ZooKeeper and LeaderLatch gets the event and calls reset method which set the internal state (`ourPath`) to null, removes old latch and creates a new one. This happens in thread \"Curator-ConnectionStateManager-0\".\nAlmost simultaneously, LeaderLatch gets another even NodeDeleted (here) and tries to re-read the list of latches and check leadership. This happens in the thread \"main-EventThread\".\n\nTherefore, sometimes there is a situation when method `checkLeadership` is called when `ourPath` is null.\nBelow is an approximate diagram of what happens:",
        "Issue Links": [
            "/jira/browse/CURATOR-644",
            "/jira/browse/CURATOR-505"
        ]
    },
    "CURATOR-505": {
        "Key": "CURATOR-505",
        "Summary": "A circuit breaking ConnectionStateListener would be very helpful",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Client,                                            Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "06/Feb/19 16:50",
        "Updated": "23/Jun/22 20:24",
        "Resolved": "13/Feb/19 15:53",
        "Description": "Create a circuit breaker style ConnectionStateListener. It would proxy any ConnectionStateListeners used by Curator recipe/classes such that when the connection is lost the circuit would open for a period of time and, while open, ignore any changes in state. After the time period expires the circuit would close and send whatever the current connection state is. This way, if the connection is going up/down/up/down/up/down, the application would only see the first down and then N ms later hopefully the connection is repaired and the application would only see the reconnection.",
        "Issue Links": [
            "/jira/browse/CURATOR-533",
            "/jira/browse/CURATOR-504",
            "/jira/browse/CURATOR-644",
            "https://github.com/apache/curator/pull/304"
        ]
    },
    "CURATOR-506": {
        "Key": "CURATOR-506",
        "Summary": "PersistentNode sometimes deletes more ZNodes that expected when closed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Chevaris",
        "Created": "07/Feb/19 16:43",
        "Updated": "07/Feb/19 16:57",
        "Resolved": "07/Feb/19 16:50",
        "Description": "Hi there,\nI have an application using PersistentNode with CreateMode.EPHEMERAL that is reporting in ZK cluster in ZNode '/level1/level2/level3' and I am relying also in the recipe to create '/level1' and 'level1/level2' ZNodes.\nWhile testing the application I observe that sporidally when the PersistentNode is closed '/level1/level2' ZNode is deleted. That ZNode is PERSISTENT so I think\u00a0only ''/level1/level2' ZNode should be deleted when closed. I was checking the code and when the PersistentNode is closed tries to delete what it is is nodePath variable, but the code does not check\u00a0what is the exact path that the variable holds, making the behaviour non 100% determistic under certain conditions.\n\u00a0\nIn my view, the delete operation under close() method should only affect the basePath declared in the constructor, and in my view the delete should only apply when the CreateMode was EPHEMERAL.\n\u00a0\nI think the code attach in the patch could solve the problem.\n\u00a0\nThx in advance,\n\u00a0\n/evaristo",
        "Issue Links": []
    },
    "CURATOR-507": {
        "Key": "CURATOR-507",
        "Summary": "PERSISTENT NODE ISSUE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Chevaris",
        "Created": "19/Feb/19 16:40",
        "Updated": "06/May/20 08:06",
        "Resolved": "06/May/20 08:06",
        "Description": "Hi there,\nI am using Apache Curator 4.1.0 with Zk 3.4.10 (Open JDK 1.8)\nopenjdk version \"1.8.0_191\"\nI am PersistentNode recipe and I am receiving this exception\n\u00a0\njava.lang.IllegalStateException: initial create has not been processed. Call waitForInitialCreate() to ensure.\n at org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444)\n at org.apache.curator.framework.recipes.nodes.PersistentNode.setData(PersistentNode.java:392)\n at com.cheva.udr.sm.RemoteReporter.reportToRemote(RemoteReporter.java:67)\n at com.cheva.udr.sm.RemoteReporter.lambda$2(RemoteReporter.java:84)\n at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n\u00a0\nMy class RemoteReporter.java (assumes that PersistentNode is thread safe)\n// The class is final, and the attribute is private final so allow safe publication\nprivate final Persistent pn;\nConstrcutor() \n{\r\n...\r\nPersistentNode pn = new PersistentNode(cf, CreateMode.EPHEMERAL, false, path, dataToReport());\r\n}\n\n// Class has also an start method\nstart() \n{\r\n...\r\n79 pn.start();\r\n...\r\n// Create new thread and execute the follwoing code\r\n84 pn.waitForInitialCreate(1000, TimeUnit.DAYS);\r\n85 pn.setData(\"XXx\");\r\n86 statusCache.getListenable().addListener(listener);\r\n}\n\n\nI was checking the PersistentNode code and the excepction happens because nodePath.get()==null variable. I am not 100% sure if the problem is in my code or in the recipe...\n\nMy code is using the waitForinitial as suggested\nThe recipe uses a CountDownLatch, but there are 2 different paths for the latch to be countDown (via initalisationComplete() method), and I am not sure that one of them assures that nodePath.get() != null\n\nIn this code snippet, you can see that the latch is countDown, but I am not sure that nodePath.set(whatever) is called, so I guess there could be a race condition\nprivate final BackgroundCallback setDataCallback = new BackgroundCallback()\n {\n@Override\n public void processResult(CuratorFramework dummy, CuratorEvent event)\n throws Exception\n {\n //If the result is ok then initialisation is complete (if we're still initialising)\n //Don't retry on other errors as the only recoverable cases will be connection loss\n //and the node not existing, both of which are already handled by other watches.\n if ( event.getResultCode() == KeeperException.Code.OK.intValue() )\n {\r\n //Update is ok, mark initialisation as complete if required.\r\n initialisationComplete();\r\n }\n else if ( event.getResultCode() == KeeperException.Code.NOAUTH.intValue() )\n {\n log.warn(\"Client does not have authorisation to write node at path {}\", event.getPath());\n authFailure.set(true);\n }\n }\n };\nThanks in adavance for the help,\nEvaristo",
        "Issue Links": []
    },
    "CURATOR-508": {
        "Key": "CURATOR-508",
        "Summary": "LeaderLatch#setNode might throws unexpected NoNodeException",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Zili Chen",
        "Created": "21/Feb/19 14:57",
        "Updated": "21/Feb/19 16:21",
        "Resolved": null,
        "Description": "private void setNode(String newValue) throws Exception\r\n{\r\n String oldPath = ourPath.getAndSet(newValue);\r\n if ( oldPath != null )\r\n {\r\n client.delete().guaranteed().inBackground().forPath(oldPath);\r\n }\r\n}\n\nWhat if the election node deleted for some reason, e.g., the session disconnected and reconnected? Should we use #quietly() here?",
        "Issue Links": []
    },
    "CURATOR-509": {
        "Key": "CURATOR-509",
        "Summary": "Incompatible with Java 11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.1.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Apache",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Brian Vella",
        "Created": "01/Mar/19 07:42",
        "Updated": "01/Mar/19 16:53",
        "Resolved": "01/Mar/19 16:44",
        "Description": "In java 11, this exception is thrown:\njava.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/apache/curator/framework/CuratorFramework. Method lambda$postSafeNotify$0(Ljava/lang/Object;)V at index 99 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef\nat org.apache.curator.framework.CuratorFramework.postSafeNotify(CuratorFramework.java:344)\nat org.apache.curator.framework.recipes.locks.InterProcessSemaphoreV2$1.process(InterProcessSemaphoreV2.java:90)\nat org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:77)\nat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:533)\nat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508)\nAs I understood it, this is because there is a lambda in a default interface method and earlier versions of java 8 incorrectly compiled the method reference as a Method instead of an Interface method. Compiling with the latest java 8 compiler (8.0.172) seems to correctly compile the classin a way that works in java 8 and 11 (did not verify 9 and 10 but I suspect they behave like 11)",
        "Issue Links": []
    },
    "CURATOR-510": {
        "Key": "CURATOR-510",
        "Summary": "A serious death cycle error. \u4f7f\u7528TreeCache\u65f6\uff0c\u5f53path\u7684\u4e0d\u5b58\u5728\u65f6\uff0c\u51fa\u73b0\u6b7b\u5faa\u73af\u8c03\u7528zookeeper server\u3002",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.8.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "jode",
        "Created": "13/Mar/19 08:18",
        "Updated": "13/Mar/19 14:49",
        "Resolved": null,
        "Description": "1. \u7528\u6cd5\nTreeCache treeCache = TreeCache.newBuilder(curatorFramework, path).build();\ntreeCache.start();\n2. \u5f53path\u4e0d\u5b58\u5728\u65f6\uff0cstart\u4e4b\u540e\u4f1a\u51fa\u73b0\u6b7b\u5faa\u73af\u8c03\u7528zookeeper server\u3002\n3. \u65e5\u5fd7\n[DEBUG][2019-03-13T16:12:22.190+0800][ClientCnxn.java:818] Reading reply sessionid:0x1689e19ba6a1545, packet:: clientPath:/zk/server-registry/development:/zk/server-registry/development finished:false header:: 6,4 replyHeader:: 6,18307,-101 request:: '/zk/server-registry/development,T response:: \n[DEBUG][2019-03-13T16:12:22.191+0800][ClientCnxn.java:818] Reading reply sessionid:0x1689e19ba6a1545, packet:: clientPath:/zk/server-registry/development serverPath:/zk/server-registry/development finished:false header:: 7,12 replyHeader:: 7,18307,-101 request:: '/zk/server-registry/development,T response:: v{}",
        "Issue Links": []
    },
    "CURATOR-511": {
        "Key": "CURATOR-511",
        "Summary": "Add toString to ZKPaths PathAndNode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "21/Mar/19 22:02",
        "Updated": "22/Mar/19 11:08",
        "Resolved": "22/Mar/19 11:08",
        "Description": "Was recently debugging a client application and this would have been helpful.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/305"
        ]
    },
    "CURATOR-512": {
        "Key": "CURATOR-512",
        "Summary": "Utilize NIO StandardCharsets",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Mollitor",
        "Created": "22/Mar/19 13:52",
        "Updated": "30/Apr/22 04:54",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/curator/pull/306"
        ]
    },
    "CURATOR-513": {
        "Key": "CURATOR-513",
        "Summary": "Add sorted() to GetChildrenBuilder",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "22/Mar/19 19:07",
        "Updated": "17/Jul/22 07:07",
        "Resolved": null,
        "Description": "ZKPaths has getSortedChildren().\nReturn the children of the given path sorted by sequence number\nhttps://curator.apache.org/apidocs/org/apache/curator/utils/ZKPaths.html#getSortedChildren-org.apache.zookeeper.ZooKeeper-java.lang.String-\nPlease add this as an option to GetChildrenBuilder",
        "Issue Links": []
    },
    "CURATOR-514": {
        "Key": "CURATOR-514",
        "Summary": "Utilize ThreadLocalRandom In QueueSharder",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.3.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "David Mollitor",
        "Created": "22/Mar/19 19:59",
        "Updated": "30/Jun/22 16:59",
        "Resolved": "30/Jun/22 16:59",
        "Description": "When applicable, use of ThreadLocalRandom rather than shared Random objects in concurrent programs will typically encounter much less overhead and contention.\nhttps://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html",
        "Issue Links": [
            "https://github.com/apache/curator/pull/307"
        ]
    },
    "CURATOR-515": {
        "Key": "CURATOR-515",
        "Summary": "Backgrounding CheckError",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Mollitor",
        "Created": "25/Mar/19 02:27",
        "Updated": "25/Sep/22 14:47",
        "Resolved": null,
        "Description": "This code is confusing to me:\n\u00a0\nBackgrounding.java\n\r\n    void checkError(Throwable e, Watching watching) throws Exception\r\n    {\r\n        if ( e != null )\r\n        {\r\n            if ( errorListener != null )\r\n            {\r\n                errorListener.unhandledError(\"n/a\", e);\r\n            }\r\n            else if ( e instanceof Exception )\r\n            {\r\n                throw (Exception)e;\r\n            }\r\n            else\r\n            {\r\n                Throwables.propagate(e);\r\n            }\r\n        }\r\n    }\r\n\u00a0\n\n\u00a0\nhttps://github.com/apache/curator/blob/master/curator-framework/src/main/java/org/apache/curator/framework/imps/Backgrounding.java#L123-L140\nI think the code here is meaning to take a run-time Exception and wrap it in a checked Exception. However, that is not actually happening here.\nIf the Throwable argument is an Exception, it is thrown as-is. Fair enough. However, if the Throwable is a RuntimeException it is also thrown here because RuntimeException is a sub-class of Exception. It is not turned into a checked exception. So, if the Throwable is not an Exception, the only other sub-class of Throwable is an Error. The call Throwables.propagate(e) will will see that it is an Error and simply throw it.\nhttps://docs.oracle.com/javase/8/docs/api/java/lang/RuntimeException.html\nhttps://github.com/google/guava/blob/master/guava/src/com/google/common/base/Throwables.java#L132-L134\nSo, really, whatever the Throwable argument is, it is simply re-thrown. This code could be simplified to:\nBackgrounding.java\n\r\n    void checkError(Throwable e, Watching watching) throws Exception\r\n    {\r\n        if ( e != null )\r\n        {\r\n            if ( errorListener != null )\r\n            {\r\n                errorListener.unhandledError(\"n/a\", e);\r\n            }\r\n            else if ( e instanceof Exception )\r\n            {\r\n                throw (Exception)e;\r\n            }\r\n            else\r\n            {\r\n                throw (Error)e;\r\n            }\r\n        }\r\n    }\r\n\u00a0\n\nIs this the intended behavior our should RuntimeException and Error be wrapped into a checked Error?",
        "Issue Links": [
            "https://github.com/apache/curator/pull/308"
        ]
    },
    "CURATOR-516": {
        "Key": "CURATOR-516",
        "Summary": "Remove Deprecated newSetFromMap",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "David Mollitor",
        "Created": "25/Mar/19 14:14",
        "Updated": "22/May/22 04:55",
        "Resolved": "22/May/22 04:55",
        "Description": "https://google.github.io/guava/releases/21.0/api/docs/com/google/common/collect/Sets.html#newSetFromMap-java.util.Map-\n\u00a0\nDeprecated. Use Collections.newSetFromMap(java.util.Map<E, java.lang.Boolean>) instead. This method will be removed in December 2017.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/309"
        ]
    },
    "CURATOR-517": {
        "Key": "CURATOR-517",
        "Summary": "Remove unused class AdvancedTracerDriver",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Zili Chen",
        "Created": "08/Apr/19 21:56",
        "Updated": "02/Sep/19 16:59",
        "Resolved": "02/Sep/19 16:59",
        "Description": "See no use point of AdvancedTracerDriver. Guessing it is safely to remove it for less confusion.",
        "Issue Links": []
    },
    "CURATOR-518": {
        "Key": "CURATOR-518",
        "Summary": "Curator. LeaderSelector. Two successive calls to interruptLeadership() will break autoRequeue.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1,                                            4.2.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Recipes",
        "Assignee": "Kezhu Wang",
        "Reporter": "Bulatov Oleg",
        "Created": "15/Apr/19 09:39",
        "Updated": "17/May/23 15:48",
        "Resolved": "19/Apr/23 20:47",
        "Description": "Curator. LeaderSelector. Two successive calls to interruptLeadership() will break autoRequeue\nIf we set autoRequeue to TRUE. But during execution interruptLeadership() will be called from another thread before internalRequeue() completed its work. Then it will break recursive call to internalRequeue(), so that client will not ask for leadership and get stuck.\nWe can solve this problem if we check hasLeadership() before calling interruptLeadership(). But it is strange that such check curator library does not do internally.",
        "Issue Links": [
            "/jira/browse/CURATOR-671"
        ]
    },
    "CURATOR-519": {
        "Key": "CURATOR-519",
        "Summary": "Curator 4.0.x/4.1.x release using Zookeeper 3.5.5",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1,                                            4.1.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Ra\u00fal Gracia Tinedo",
        "Created": "25/Apr/19 18:13",
        "Updated": "05/Sep/19 14:33",
        "Resolved": "05/Sep/19 14:33",
        "Description": "We are using Curator 4.0.1 in our project (Pravega,\u00a0https://github.com/pravega/pravega).\nJust for context, we are intensively deploying Pravega on Kubernetes. In this sense, Zookeeper (3.5.4-beta) is also being deployed as a service in Kubernetes. In this environment, Zookeeper instances (i.e., pods) can be frequently restarted or relocated and their IP may change. Properly handling this situation requires to re-resolve the IP for hostname of Zookeeper instances.\nIn fact, we are experiencing problems with this exact issue with Curator 4.0.1 (i.e., re-resolution of Zookeeper hostnames), which internally uses Zookeeper 3.5.3-beta as a dependency. Details can be found in this GitHub issue: https://github.com/pravega/pravega/issues/3651\n\u00a0\nFortunately,\u00a0the proper resolution of Zookeepr hostnames\u00a0has been actually reported and fixed in the Zookeeper project:\n\nIssue:\u00a0https://issues.apache.org/jira/browse/ZOOKEEPER-2184\nCommit on Zookeeper 3.5.5 branch:\u00a0https://github.com/apache/zookeeper/commit/1e65b9f4873fc995308972433ea8a664e98fe41f\n\n\u00a0\nTo verify that it helps us to solve our problem, I have built a custom Curator 4.0.1 library using Zookeeper 3.5.5 and then I used that library to build a Pravega image. A couple of comments to take into account:\n\nUsing Zookeeper 3.5.5 to build\u00a0Curator 4.0.1 required no code change, just changing the Zookeeper dependency version in the pom.xml (I had the same experience with Curator 4.1.0).\nI verified that\u00a0using Curator 4.0.1 and 4.1.0 with Zookeeper 3.5.5\u00a0solves the re-resolution of Zookeeper hostnames, which makes our system able to properly handle Zookeeper instance restarts.\n\n\u00a0\nTherefore, the main point of this issue is: is in the roadmap to release\u00a0Curator 4.0.x and/or 4.1.x using Zookeeper 3.5.5?\u00a0(Note that Zookeeper 3.5.5 is about to get released: https://github.com/apache/zookeeper/releases)\n\u00a0\nThanks in advance,\n Ra\u00fal.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/314"
        ]
    },
    "CURATOR-520": {
        "Key": "CURATOR-520",
        "Summary": "NoSuchMethod Exception on curator.close() in 3.4 compatibility mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Andrew Polack",
        "Created": "21/May/19 12:46",
        "Updated": "21/May/19 20:48",
        "Resolved": "21/May/19 20:35",
        "Description": "Running in 3.4 compatibility mode, Curator improperly appears to attempt to call the\u00a0close(I) method which does not exist on the 3.4 version of the Zookeeper API:\u00a0\n\n\r\nUncaught error from thread [timeline-manager-akka.actor.default-dispatcher-4]: org.apache.zookeeper.ZooKeeper.close(I)Z, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[timeline-manager]\r\njava.lang.NoSuchMethodError: org.apache.zookeeper.ZooKeeper.close(I)Z\r\nat org.apache.curator.HandleHolder.internalClose(HandleHolder.java:164)\r\nat org.apache.curator.HandleHolder.closeAndClear(HandleHolder.java:78)\r\nat org.apache.curator.ConnectionState.close(ConnectionState.java:125)\r\nat org.apache.curator.CuratorZookeeperClient.close(CuratorZookeeperClient.java:266)\r\nat org.apache.curator.CuratorZookeeperClient.close(CuratorZookeeperClient.java:249)\r\n\r\n\n\nThis results in a failure to reconnect to Zookeeper when the service is restarted (presumably because the old connection was never properly released).",
        "Issue Links": []
    },
    "CURATOR-521": {
        "Key": "CURATOR-521",
        "Summary": "LeaderLatch closed with CloseMode=null does not close",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Yuri Tceretian",
        "Created": "21/May/19 19:15",
        "Updated": "21/May/19 20:09",
        "Resolved": null,
        "Description": "The class LeaderLatch has two methods `close`, one of them accepts argument `CloseMode`. If the argument is null (unlikely but still possible) the instance changes the state to CLOSED but does not really close anything because\u00a0throws\u00a0a runtime exception `closeMode cannot be null` right after changing the status.\nhttps://github.com/apache/curator/blob/a3514d87b1036716e5dc9877c9980fdd81440458/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L201-L205\nThe quickest fix is to just swap preconditions checks.\u00a0The more correct solution is to\u00a0change state in the try block after the client removed all watchers (or in finally block).\u00a0It should not break anything because the method is synchronized.",
        "Issue Links": []
    },
    "CURATOR-522": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-523": {
        "Key": "CURATOR-523",
        "Summary": "Use --release flag when compiling on JDK9+",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Paco",
        "Created": "22/May/19 15:00",
        "Updated": "10/May/23 11:48",
        "Resolved": "17/Jul/22 06:39",
        "Description": "From the manifest 4.2.0 jars are compiled with jdk11:\nBuild-Jdk: 11.0.2\u00a0\nChanges to covariant return types for ByteBuffer\u00a0in JDK11 cause a java.lang.NoSuchMethodError\u00a0at runtime when running on JDK 8. These calls paths are impacted:\nhttps://github.com/apache/curator/blob/master/curator-framework/src/main/java/org/apache/curator/framework/imps/GzipCompressionProvider.java#L307-L319\nThis\u00a0can be mitigated by setting the\u00a0--release\u00a0to 8.\nhttp://openjdk.java.net/jeps/247",
        "Issue Links": [
            "/jira/browse/CURATOR-433",
            "https://github.com/apache/curator/pull/321"
        ]
    },
    "CURATOR-524": {
        "Key": "CURATOR-524",
        "Summary": "Apache is asking us to review our build files and make sure we're using https for all URLs.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "Apache,                                            General",
        "Assignee": "Zili Chen",
        "Reporter": "Jordan Zimmerman",
        "Created": "26/May/19 03:34",
        "Updated": "17/Jul/22 08:06",
        "Resolved": "17/Jul/22 08:06",
        "Description": "Update all URLs in our POMs to https",
        "Issue Links": []
    },
    "CURATOR-525": {
        "Key": "CURATOR-525",
        "Summary": "There is a race condition in Curator which might lead to fake SUSPENDED event and ruin CuratorFrameworkImpl inner state",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Mikhail Valiev",
        "Created": "28/May/19 08:28",
        "Updated": "26/May/23 10:44",
        "Resolved": "11/Apr/20 15:03",
        "Description": "This was originally found in the\u00a02.11.1\u00a0version of Curator, but I tested the latest release as well, and the issue is still there.\nThe issue is tied to guaranteed deletes and how it loops infinitely, if called when there is no connection:\nclient.delete().guaranteed().forPath(ourPath);\u00a0\nhttps://curator.apache.org/apidocs/org/apache/curator/framework/api/GuaranteeableDeletable.html\nThis schedules a background operation which\u00a0attempts to remove the node in infinite loop. Each time a background operation fails due to connection loss it performs\u00a0a check (validateConnection() function) to see if the main thread is already aware of connection loss, and if it's not - raises the connection loss event. The problem is that this peace of code is also executed by the\u00a0event watcher\u00a0thread when connection events are happening - this leads to race condition. So when connection is restored it's easily possible for the main thread to raise RECONNECTED event and after that for background\u00a0thread\u00a0to raise SUSPENDED event.\nWe might get unlucky and get a \"phantom\"\u00a0SUSPENDED event.\u00a0It breaks Curator inner Connection state and\u00a0leads to\u00a0curator behaving unpredictably\nAttached some illustrations and Unit test to reproduce the issue. (Put debug point in validateConnection() )\nPossible solution: in CuratorFrameworkImpl class adjust the processEvent() function and add the following:\nif(event.getType() == CuratorEventType.SYNC) \n{\r\n\r\nconnectionStateManager.addStateChange(ConnectionState.RECONNECTED);\r\n\r\n}\n\nIf this is a same state as before - it will be ignored, if background operation succeeded, but we are in SUSPENDED state - this would repair the Curator state and raise RECONNECTED event.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/357"
        ]
    },
    "CURATOR-526": {
        "Key": "CURATOR-526",
        "Summary": "Error logged for valid config - \"Invalid config event received: {properties}\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "5.2.0",
        "Component/s": "Framework",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Rajesh Singh",
        "Created": "03/Jun/19 23:23",
        "Updated": "30/Jul/21 16:37",
        "Resolved": "28/Mar/21 14:42",
        "Description": "Curator Framework v4.0.1 - EnsembleTracker.processConfigData(byte[] data) (Ln: 157)- seems to be logging error for valid configs when the config\u00a0string obtained from Zookeeper does not have the client info in it.\u00a0\nAs per the docs the config string should conform to below formats:\nserver_config or server_config;client_config where server_config is host:port:port or host:port:port:type and client_config is port or host:port\nIn our case it conforms to the first pattern i.e.\u00a0server_config\u00a0with\u00a0host:port:port:type",
        "Issue Links": []
    },
    "CURATOR-527": {
        "Key": "CURATOR-527",
        "Summary": "Concurrency issue in LockInternals",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jaechang Kim",
        "Created": "05/Jun/19 11:52",
        "Updated": "05/Jun/19 17:43",
        "Resolved": "05/Jun/19 17:43",
        "Description": "I'm using\u00a0InterProcessMutex and InterProcessMutex often failed to acquire lock.\nIn LockInternals.internalLockLoop(), watcher is registered to zookeeper and call wait() like below\n\n\r\nclient.getData().usingWatcher(watcher).forPath(previousSequencePath);\r\nif ( millisToWait != null )\r\n{\r\n    millisToWait -= (System.currentTimeMillis() - startMillis);\r\n    startMillis = System.currentTimeMillis();\r\n    if ( millisToWait <= 0 )\r\n    {\r\n        doDelete = true;    // timed out - delete our node\r\n        break;\r\n    }\r\n\r\n    wait(millisToWait);\r\n}\r\nelse\r\n{\r\n    wait();\r\n}\r\n\n\nIn my case, my program is waiting previousSequencePath=_c_f290140d-9856-42ad-b9bf-348ffc086062-lock-0000000759 to be deleted.\nBut\u00a0_c_f290140d-9856-42ad-b9bf-348ffc086062-lock-0000000759 is deleted between client.getData() and wait().\nif\u00a0_c_f290140d-9856-42ad-b9bf-348ffc086062-lock-0000000759 is deleted when\u00a0\nclient.getData().usingWatcher(watcher).forPath(previousSequencePath) is called, it will throw Exception but it was exist at that time.\nI'm using Curator 2.12.0 but latest version seems to have same issue.",
        "Issue Links": []
    },
    "CURATOR-528": {
        "Key": "CURATOR-528",
        "Summary": "mail-lists page in wiki, 404 not found",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": "Shay Shimony",
        "Reporter": "ningtao",
        "Created": "28/Jun/19 07:01",
        "Updated": "16/Feb/21 08:21",
        "Resolved": "16/Feb/21 08:21",
        "Description": "There is a 404 link(\u00a0http://curator.apache.org/mail-lists.html) in the wiki page:https://cwiki.apache.org/confluence/display/CURATOR/New+Committers\nIt\u2018s not\u00a0convenient for newcomers to\u00a0subscribe the maillist.\nThanks",
        "Issue Links": []
    },
    "CURATOR-529": {
        "Key": "CURATOR-529",
        "Summary": "Sync with inBackground doesnt complete if curator client is closed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework",
        "Assignee": null,
        "Reporter": "shivesh",
        "Created": "02/Jul/19 10:57",
        "Updated": "29/May/23 13:17",
        "Resolved": null,
        "Description": "Calling sync with a callback on a closed client doesnt fail synchronously. Nor is the callback invoked with failure details.\nFollowing is the code snippet to reproduce the issue:\nclient.close();\nclient.sync().inBackground(callback, executor).forPath(path);\n\u00a0\nThis works correctly for other api calls like `getData` etc where the call fails synchronously.\nExpected Behaviour:\nCallback should have been called with IllegalStateException or any other appropriate failure. Or attempt to call sync on a closed client should have failed synchronously. But neither does the api fail nor is the callback invoked unlike it is for all other types of calls.",
        "Issue Links": [
            "/jira/browse/CURATOR-673"
        ]
    },
    "CURATOR-530": {
        "Key": "CURATOR-530",
        "Summary": "Documentation on InterProcessSemaphoreMutex is misleading",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "4.3.0",
        "Component/s": "Documentation",
        "Assignee": "Shay Shimony",
        "Reporter": "Emre Tetik",
        "Created": "03/Jul/19 18:10",
        "Updated": "14/Jul/19 20:14",
        "Resolved": "12/Jul/19 17:43",
        "Description": "The documentation for the release() method in InterProcessSemaphoreMutex says that it will throw an exception if the method is called on a thread other than the one which acquired the lock:\u00a0https://curator.apache.org/apidocs/org/apache/curator/framework/recipes/locks/InterProcessSemaphoreMutex.html#release--\nThis is not correct, and should be changed.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/315"
        ]
    },
    "CURATOR-531": {
        "Key": "CURATOR-531",
        "Summary": "Documentation for Discovery Service Cache is Misleading",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "16/Jul/19 14:46",
        "Updated": "16/Jul/19 14:46",
        "Resolved": null,
        "Description": "The doc here - http://curator.apache.org/curator-x-discovery/index.html - discusses Service Cache and states \"If you need more than occasional querying of services you can use the ServiceCache\". The problem with this documentation is that it's misleading. ServiceProvider internally creates a cache. The documentation should explicitly state that if you use ServiceProvider you don't need to do manual caching.",
        "Issue Links": []
    },
    "CURATOR-532": {
        "Key": "CURATOR-532",
        "Summary": "Manually configurable ZooKeeper 3.4.x compatibility",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework",
        "Assignee": null,
        "Reporter": "Zili Chen",
        "Created": "17/Jul/19 04:48",
        "Updated": "18/Jul/19 01:28",
        "Resolved": null,
        "Description": "Currently, Curator auto detect whether it is in a zookeeper 3.4.x environment by\n\n\r\nstatic {\r\n        boolean localHasZooKeeperAdmin;\r\n        try\r\n        {\r\n            Class.forName(\"org.apache.zookeeper.admin.ZooKeeperAdmin\");\r\n            localHasZooKeeperAdmin = true;\r\n        }\r\n        catch ( ClassNotFoundException e )\r\n        {\r\n            localHasZooKeeperAdmin = false;\r\n            logger.info(\"Running in ZooKeeper 3.4.x compatibility mode\");\r\n        }\r\n        hasZooKeeperAdmin = localHasZooKeeperAdmin;\r\n}\r\n\n\nHowever, for some projects such as FLINK, both zookeeper and curator are relocated. Thus org.apache.zookeeper.admin.ZooKeeperAdmin is shaded as org.apache.flink.shaded.zookeeper.org.apache.zookeeper.admin.ZooKeeperAdmin. So the detection fails.\nA manually configurable compatibility option might solve this problem.",
        "Issue Links": []
    },
    "CURATOR-533": {
        "Key": "CURATOR-533",
        "Summary": "Improve CURATOR-505 by making the CircuitBreaker instance shared",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Framework,                                            Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "28/Jul/19 16:39",
        "Updated": "22/Apr/20 03:37",
        "Resolved": "13/Aug/19 14:59",
        "Description": "CURATOR-505 introduced circuit breaking behavior via CircuitBreakingConnectionStateListener and ConnectionStateListenerDecorator. Elastic has been using it to success but reports that the implementation can be improved. The existing implementation uses a new CircuitBreaker for each ConnectionStateListener set in a Curator client. It turns out that this is not ideal. Instead, a shared CircuitBreaker should be used per Curator client.\nUnfortunately, the best way to do this is to remove the ConnectionStateListenerDecorator semantics and use a different mechanism. This Issue proposes to do this and remove ConnectionStateListenerDecorator. This is a breaking change but given the short amount of time it's been in Curator it's unlikely that it's been widely adopted. \nIf the community considers a breaking change too harsh the older classes can be maintained for a while and marked as @Deprecated.",
        "Issue Links": [
            "/jira/browse/CURATOR-505",
            "/jira/browse/CURATOR-441",
            "https://github.com/apache/curator/pull/320"
        ]
    },
    "CURATOR-534": {
        "Key": "CURATOR-534",
        "Summary": "Fix Travis-CI config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "General",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Enrico Olivelli",
        "Created": "28/Jul/19 17:15",
        "Updated": "28/Jul/19 17:20",
        "Resolved": "28/Jul/19 17:19",
        "Description": "The Travis builds are failing currently. It's likely due to the config specifying Oracle JDK.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/319"
        ]
    },
    "CURATOR-535": {
        "Key": "CURATOR-535",
        "Summary": "TestServer random port selection has a race condition",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Laverne Schrock",
        "Created": "02/Aug/19 14:21",
        "Updated": "06/Apr/23 07:20",
        "Resolved": "21/Feb/23 13:29",
        "Description": "When using one of the constructors for org.apache.curator.test.TestingServer that doesn't take a port number, the org.apache.curator.test.InstanceSpec that is constructed will chose random available ports to use. However, InstanceSpec only binds those ports during construction and then unbinds them so that they can be used when TestingServer.start() is called.\nThis disconnect between port selection creates a race condition where some other process (or thread) could bind the port before TestingServer is started.\nI've seen this very rarely in our integration test suite that spins up and tears down TestingServer many times. I've attached a simple class for reproducing the issue. If you run it in an environment with log4j loaded and the attached log4j.properties, you should see output like the following (though it sometimes takes more iterations):\ncompleted iteration: 0\ncompleted iteration: 500\n2019-08-02 09:47:06 ERROR TestingZooKeeperServer:162 - From testing server (random state: false) for instance: InstanceSpec{dataDirectory=/tmp/1564753624792-1, port=34707, electionPort=33621, quorumPort=45995, deleteDataDirectoryOnClose=true, serverId=1286, tickTime=-1, maxClientCnxns=-1, customProperties={}, hostname=127.0.0.1} org.apache.curator.test.InstanceSpec@59c43d10\njava.net.BindException: Address already in use\n\u00a0\u00a0 \u00a0at sun.nio.ch.Net.bind0(Native Method)\n\u00a0\u00a0 \u00a0at sun.nio.ch.Net.bind(Net.java:433)\n\u00a0\u00a0 \u00a0at sun.nio.ch.Net.bind(Net.java:425)\n\u00a0\u00a0 \u00a0at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\u00a0\u00a0 \u00a0at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\u00a0\u00a0 \u00a0at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)\n\u00a0\u00a0 \u00a0at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:687)\n\u00a0\u00a0 \u00a0at org.apache.zookeeper.server.ServerCnxnFactory.configure(ServerCnxnFactory.java:76)\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingZooKeeperMain.internalRunFromConfig(TestingZooKeeperMain.java:239)\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingZooKeeperMain.runFromConfig(TestingZooKeeperMain.java:132)\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:158)\n\u00a0\u00a0 \u00a0at java.lang.Thread.run(Thread.java:748)\njava.lang.IllegalStateException: Timed out waiting for watch removal\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingZooKeeperMain.blockUntilStarted(TestingZooKeeperMain.java:146)\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingZooKeeperServer.start(TestingZooKeeperServer.java:167)\n\u00a0\u00a0 \u00a0at org.apache.curator.test.TestingServer.start(TestingServer.java:148)\n\u00a0\u00a0 \u00a0at BugReproducer.main(BugReproducer.java:15)",
        "Issue Links": [
            "/jira/browse/FLINK-20523",
            "https://github.com/apache/curator/pull/406",
            "https://github.com/apache/curator/pull/421"
        ]
    },
    "CURATOR-536": {
        "Key": "CURATOR-536",
        "Summary": "Infinite loop instead of timeout on DistributedDoubleBarrier::internalLeave",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Denis",
        "Created": "08/Aug/19 09:13",
        "Updated": "17/Jul/22 06:35",
        "Resolved": "10/Aug/19 19:03",
        "Description": "Hi,\nI am using DistributedDoubleBarrier (DDB) for synchronizing 3 instances in AP system, so I created it with quantity=2 and I got 3 children instead of 2.\nUpon leaving with timeout 20 minutes I get infinite loop sometime in DDB::internalLeave.\n\u00a0\nIn debugger I can see \"result = false\" on line 265 in DistributedDoubleBarrier.java but without break it is an infinite loop.\n\u00a0\nLooks like I am the only person who is trying to use DDB :--)",
        "Issue Links": [
            "/jira/browse/CURATOR-233",
            "https://github.com/apache/curator/pull/322"
        ]
    },
    "CURATOR-537": {
        "Key": "CURATOR-537",
        "Summary": "Expose ourPath in LeaderLatch readonly",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Zili Chen",
        "Created": "20/Aug/19 00:49",
        "Updated": "29/Apr/22 01:25",
        "Resolved": "05/Sep/19 14:31",
        "Description": "Follow the discussion in user list here we found it is worth a ticket to expose ourPath in LeaderLatch readonly. The path of the latch is the natural fencing token for an external leader and also the effective checker whether contender is still the leader.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/324"
        ]
    },
    "CURATOR-538": {
        "Key": "CURATOR-538",
        "Summary": "Background exception was not retry-able or retry gave up",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1,                                            4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "imic",
        "Created": "21/Aug/19 07:26",
        "Updated": "30/Aug/22 05:42",
        "Resolved": "12/Sep/19 01:51",
        "Description": "error log:\n\n\r\n//err code is here\r\n2019-08-21 14:24:30.654 0.0.0.0:30901\u00a0 INFO 45964\u00a0 \u00a0 \u00a0--- [e-1-EventThread] org.apache.zookeeper.ClientCnxn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : EventThread shut down for session: 0x10072ab8a8001b12019-08-21 14:24:30.654 0.0.0.0:30901\u00a0 INFO 45964\u00a0 \u00a0 \u00a0--- [e-1-EventThread] org.apache.zookeeper.ClientCnxn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : EventThread shut down for session: 0x10072ab8a8001b12019-08-21 14:24:30.655 0.0.0.0:30901 ERROR 45964\u00a0 \u00a0 \u00a0--- [e-1-EventThread] o.a.c.f.imps.CuratorFrameworkImpl\u00a0 \u00a0 \u00a0 \u00a0 : Background exception was not retry-able or retry gave up\r\njava.lang.NullPointerException: null at org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:179) at org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:200) at org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) at org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:144) at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) at org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:587) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)\r\n2019-08-21 14:24:30.655 0.0.0.0:30901\u00a0 INFO 45964\u00a0 \u00a0 \u00a0--- [e-1-EventThread] o.a.c.framework.imps.EnsembleTracker\u00a0 \u00a0 \u00a0: New config event received: {server.1=zookeeper-0.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=b00000000, server.3=zookeeper-2.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=zookeeper-1.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}2019-08-21 14:24:30.656 0.0.0.0:30901 ERROR 45964\u00a0 \u00a0 \u00a0--- [e-1-EventThread] o.a.c.f.imps.CuratorFrameworkImpl\u00a0 \u00a0 \u00a0 \u00a0 : Background exception was not retry-able or retry gave up\r\njava.lang.NullPointerException: null at org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:179) at org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:200) at org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) at org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:144) at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) at org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:587) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)\r\n\n\n\u00a0\n\u00a0 \u00a0 I use kubernates as registor,\u00a0 envirment is ubuntu18.04 + spring boot 2.x and curator is\u00a04.0.1(spring-cloud-starter-zookeeper-config),\u00a0 when i debug the project, curator framework throw NullPointerException.\n\u00a0 \u00a0 I debuged it , found EnsembleTracker.java(line 179) server.addr.getAddress() is null! debug info can be found at attachment!\n\u00a0\nQuorumVerifier is\u00a0\n\n\r\nserver.1=zookeeper-0.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181\r\nserver.2=zookeeper-1.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181\r\nserver.3=zookeeper-2.zk-headless-service.default.svc.cluster.local:2888:3888:participant;0.0.0.0:2181\r\nversion=b00000000",
        "Issue Links": [
            "/jira/browse/CURATOR-649"
        ]
    },
    "CURATOR-539": {
        "Key": "CURATOR-539",
        "Summary": "Remove link to obsolete Stack Overflow tag",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.3.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Stephan Hohe",
        "Created": "03/Sep/19 18:22",
        "Updated": "03/Sep/19 21:00",
        "Resolved": "03/Sep/19 21:00",
        "Description": "The\u00a0main Index page links to the Stack Overflow tags \"curator\" and \"apache-curator\".\nThe Stack Overflow \"curator\" tag got merged with \"apache-curator\" and both Stack Overflow links now redirect to the same page. The obsolete link doesn't add anything useful.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/323"
        ]
    },
    "CURATOR-540": {
        "Key": "CURATOR-540",
        "Summary": "BUILD_INITIAL_CACHE mode will post history child on start",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.12.0,                                            2.13.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "HonglunChen",
        "Created": "12/Sep/19 07:43",
        "Updated": "19/Sep/19 03:19",
        "Resolved": "16/Sep/19 14:48",
        "Description": "I debugged and found the root cause : In\u00a0PathChildrenCache, CONNECTED state is\u00a0triggered\u00a0before currentData initializes.",
        "Issue Links": []
    },
    "CURATOR-541": {
        "Key": "CURATOR-541",
        "Summary": "Infinite loop/repeat in BaseClassForTests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Sep/19 16:20",
        "Updated": "24/Sep/19 18:04",
        "Resolved": "24/Sep/19 18:04",
        "Description": "A Curator test that consistently fails will cause an infinite loop of retries when test class extends BaseClassForTests E.g.\n\n\r\n    @Test\r\n    public void infiniteLoop() {\r\n        Assert.fail();\r\n    }",
        "Issue Links": [
            "https://github.com/apache/curator/pull/326"
        ]
    },
    "CURATOR-542": {
        "Key": "CURATOR-542",
        "Summary": "Add recipe to help with leader-based transactions",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "21/Sep/19 19:35",
        "Updated": "29/Feb/20 08:49",
        "Resolved": null,
        "Description": "See discussion here: http://mail-archives.apache.org/mod_mbox/curator-user/201909.mbox/%3cCALL9TYLWPz-OtQuFZnLQCpXi2cBO3Fd_mRLGF+RKa5pUWAK6oA@mail.gmail.com%3e\nGiven the issues regarding GC pauses, etc. (https://cwiki.apache.org/confluence/display/CURATOR/TN10) there is no 100% guarantee that a instance using one of the leader election, lock, etc. recipes that they actually are they current leader (or lock owner). This has implications for any actions taken where leadership is assumed. For operations on ZooKeeper this can be improved by using a versioned coordination node. \nAdd a new recipe that complements leader selection, locking to manage a coordination node. When a client is elected leader (or owns a lock, etc.) and needs to perform a ZooKeeper operation it can ensure that it is the true leader by including the version number of the coordination node in a ZooKeeper transaction.",
        "Issue Links": []
    },
    "CURATOR-543": {
        "Key": "CURATOR-543",
        "Summary": "ZOOKEEPER-1392 broke TestLockACLs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "28/Sep/19 17:45",
        "Updated": "29/Sep/19 13:23",
        "Resolved": "29/Sep/19 13:23",
        "Description": "In order to read ACLS you now need READ perm. TestLockACLs.testLockACLs() needs to be updated to reflect this",
        "Issue Links": [
            "https://github.com/apache/curator/pull/327"
        ]
    },
    "CURATOR-544": {
        "Key": "CURATOR-544",
        "Summary": "Implement SessionFailedRetryPolicy",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Zili Chen",
        "Created": "29/Sep/19 09:24",
        "Updated": "08/May/20 07:21",
        "Resolved": "07/May/20 22:41",
        "Description": "https://lists.apache.org/x/thread.html/2549c310c1e3232d2f746bf1cf7110c094ea2ca9d1e54b34b75a508f@%3Cuser.curator.apache.org%3E\nCurrently Curator will always reset on session expire and instance a new ZK client as well as recover state.\nBecause on session expire ephemeral znodes get deleted, some of user cases possibly want to manually handle session expire before automatically reset o.a.c.ConnectionState.\nI propose implement a SessionConnectionHandlingPolicy whose callWithRetry use SessionFailRetryLoop so that o.a.c.ConnectionState fails on session expired.\nBTW, it would be a user-friendly way to make user of SessionFailRetryLoop.\nI volunteer to work on an implementation.\nWhat do you think?\nCC randgalt\nAs time goes by, we change a bit what API looks like. For implement the same functionality, we change code of RetryPolicy instead. To be brief, we (1) move RetryLook.isRetriableException to RetryPolicy#allowRetry(Exception) and existing impls have a default impl that keeps the manner (2) impl a SessionFailedRetryPolicy that throws when SessionExipredException tested, while the other examination delegated to another policy.",
        "Issue Links": []
    },
    "CURATOR-545": {
        "Key": "CURATOR-545",
        "Summary": "PersistentTtlNode can leave dangling containers behind",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Do",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Sylvain Wallez",
        "Created": "09/Oct/19 16:51",
        "Updated": "25/Nov/19 14:44",
        "Resolved": "25/Nov/19 14:44",
        "Description": "PersistentTtlNode creates a container as a PersistentNode and then adds a TTL node in it named touch that is refreshed periodically.\nWe encountered an issue were the application crashed (or was disconnected from ZooKeeper) between the creation of the container and the creation of the touch child.\nZooKeeper doesn't delete containers that never had any children. So this result in this container node to stay around forever and never expire.\nA fix for that is to create both container and touch child as part of a single transaction.\nSince PersistentTtlNode\u00a0delegates to\u00a0PersistentNode this may require quite some refactoring of the recipe. But I actually don't understand why the container has to be a\u00a0PersistentNode and is not a regular container.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-3546"
        ]
    },
    "CURATOR-546": {
        "Key": "CURATOR-546",
        "Summary": "currentData in ModeledCacheImpl removes ZPath from cache entries",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Client,                                            Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Yang",
        "Created": "09/Oct/19 17:32",
        "Updated": "12/Oct/19 06:04",
        "Resolved": "12/Oct/19 06:04",
        "Description": "I'm using\u00a0AsyncCuratorFramework with cache, during testing I noticed that\u00a0\nclient.cache().currentChildren(<zkPath>) not also return the full list of children, by digging into ModeledCacheImpl, apparently some children been deleted when I call client.cache().currentData(<zkPath>).\u00a0\nWhat's the alternative for me if I wants to get cached node while don't remove it from the cache?\u00a0\n\u00a0\npublic Optional<ZNode<T>> currentData(ZPath path)\n {\n Entry<T> entry = entries.remove(path);\n if ( entry != null )\n\n{ return Optional.of(new ZNodeImpl<>(path, entry.stat, entry.model)); }\n\nreturn Optional.empty();\n }",
        "Issue Links": [
            "https://github.com/apache/curator/pull/329"
        ]
    },
    "CURATOR-547": {
        "Key": "CURATOR-547",
        "Summary": "Make JAX-RS MessageBodyReader/-Writer impl (JsonServiceInstanceMarshaller) reuse Jackson ObjectMapper",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Tatu Saloranta",
        "Created": "11/Oct/19 05:22",
        "Updated": "09/Nov/19 14:38",
        "Resolved": "09/Nov/19 14:29",
        "Description": "I noticed that the JAX-RS reader/writer implementation, `JsonServiceInstanceMarshaller` creates `ObjectMapper`s on-the-fly for every read and write. This is pretty inefficient, and it is easy to create and use a static mapper instance instead.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/328"
        ]
    },
    "CURATOR-548": {
        "Key": "CURATOR-548",
        "Summary": "Bump zookeeper version to 3.5.7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.3.0",
        "Component/s": "Apache,                                            Client",
        "Assignee": null,
        "Reporter": "Oleksandr Porunov",
        "Created": "16/Oct/19 08:23",
        "Updated": "16/Feb/20 22:06",
        "Resolved": "16/Feb/20 22:06",
        "Description": "Zookeeper 3.5.7\n is released. I think it would be good to update this dependency.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/330"
        ]
    },
    "CURATOR-549": {
        "Key": "CURATOR-549",
        "Summary": "ZooKeeper 3.6 will add support for Persistent Recursive Watchers - Add Curator support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Client,                                            Documentation,                                            Framework,                                            Recipes,                                            Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "03/Nov/19 23:04",
        "Updated": "09/Apr/20 12:18",
        "Resolved": "09/Apr/20 12:18",
        "Description": "ZOOKEEPER-1416 will add support for Persistent Recursive Watchers. This feature allows Curator to re-think/re-write the various cache recipes so that they're simpler, use less resources and more stable.\n\nChange Curator to depend on ZooKeeper 3.6.x\nAdd a new module so that backward compatibility with 3.5 is ensured\nAdd new methods corresponding to ZooKeeper.addWatch()\nAdd a new recipe that enables a fully managed Persistent watch\nCreate new Cache recipes that use the new Persistent watch recipe\nReplace internal usages of TreeCache/PathChildrenCache/NodeCache with new cache recipe\nDeprecate TreeCache/PathChildrenCache/NodeCache\nUpdated docs/tests/etc",
        "Issue Links": [
            "https://github.com/apache/curator/pull/333",
            "https://github.com/apache/curator/pull/334",
            "https://github.com/apache/curator/pull/335",
            "https://github.com/apache/curator/pull/336",
            "https://github.com/apache/curator/pull/337",
            "https://github.com/apache/curator/pull/338",
            "https://github.com/apache/curator/pull/356"
        ]
    },
    "CURATOR-550": {
        "Key": "CURATOR-550",
        "Summary": "Add Automatic-Module-Name to MANIFEST.MF",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Michael Miller",
        "Created": "20/Nov/19 20:17",
        "Updated": "21/Nov/19 19:23",
        "Resolved": null,
        "Description": "Add Automatic-Module-Name to the project jars in support of the Java 9 module system.\nI am opening this ticket because Accumulo requires its dependencies to have stable module names before it can release modules of its own.\u00a0 Here is some discussion and more information about this topic: https://github.com/apache/accumulo/issues/1434",
        "Issue Links": []
    },
    "CURATOR-551": {
        "Key": "CURATOR-551",
        "Summary": "Curator client always call updateServerList with original serverList value, not the ones updated by EnsembleTracker",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Client",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "kelgon wu",
        "Created": "09/Dec/19 02:55",
        "Updated": "10/May/23 12:46",
        "Resolved": "12/Feb/20 22:08",
        "Description": "Whenever ConnectionState.handleNewConnectionString is triggered, the value of\u00a0 newConnectionString is always the value used to construct the FixedEnsembleProvider\n\u00a0\nin ConnectionState:\n\n\r\nString newConnectionString = zooKeeper.getNewConnectionString();\r\nif ( newConnectionString != null ) {\r\n    handleNewConnectionString(newConnectionString);\r\n}\r\n\r\n...\r\n\r\nif ( ensembleProvider.updateServerListEnabled() ) {\r\n    zooKeeper.updateServerList(newConnectionString);\r\n} else {\r\n    reset();\r\n}\n\n\u00a0\nin HandleHolder:\n\n\r\nString getNewConnectionString() {\r\n    String helperConnectionString = (helper != null) ? helper.getConnectionString() : null;\r\n    return ((helperConnectionString != null) && !ensembleProvider.getConnectionString().equals(helperConnectionString)) ? helperConnectionString : null;\r\n}\n\ncode above shows that the\u00a0newConnectionString is provided by helper.getConnectionString(), which only instantiated on first call of getZookeeper:\n\n\r\n@Override\r\npublic ZooKeeper getZooKeeper() throws Exception\r\n{\r\n    synchronized(this)\r\n    {\r\n        if ( zooKeeperHandle == null )\r\n        {\r\n            connectionString = ensembleProvider.getConnectionString();\r\n            zooKeeperHandle = zookeeperFactory.newZooKeeper(connectionString, sessionTimeout, watcher, canBeReadOnly);\r\n        }\r\n...\n\n\u00a0\nAs a result, when I reconfig zookeeper serverList, curator client always call updateServerList with the initial value, not the ones I just reconfigured",
        "Issue Links": [
            "/jira/browse/CURATOR-7",
            "/jira/browse/CURATOR-570",
            "https://github.com/apache/curator/pull/345"
        ]
    },
    "CURATOR-552": {
        "Key": "CURATOR-552",
        "Summary": "Update jackson-databind dependencies",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "TBD",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "Vladimir Pligin",
        "Created": "19/Dec/19 15:10",
        "Updated": "19/Dec/19 15:14",
        "Resolved": "19/Dec/19 15:14",
        "Description": "There are some CVE reports related to 2.9.8 and 2.9.9 versions, for example:\n\nCVE-2019-14379\nCVE-2019-17267\nCVE-2019-16335\nCVE-2019-14540\nCVE-2019-16943\nCVE-2019-16942\nCVE-2019-17531\nCVE-2019-14439\nCVE-2019-12086\nCVE-2019-12814\nCVE-2019-12384\n\nMost of them suggest to update to the 2.9.10 version.",
        "Issue Links": []
    },
    "CURATOR-553": {
        "Key": "CURATOR-553",
        "Summary": "Use slf4j's place-holder instead String.format and concat.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "Tianzhou Wang",
        "Created": "20/Dec/19 02:41",
        "Updated": "25/Sep/22 14:51",
        "Resolved": null,
        "Description": "String.format and concat make additional worst on log, comparing to place-holder provided by slf4j-api.\nAlso, place-holder may help some log aggregation component work better.",
        "Issue Links": []
    },
    "CURATOR-554": {
        "Key": "CURATOR-554",
        "Summary": "restart zk , task in thread \"Curator-ConnectionStateManager-0\" stop ,leaderLatch failed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.0.1,                                            4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Husky Zeng",
        "Created": "09/Jan/20 08:20",
        "Updated": "10/Jan/20 13:20",
        "Resolved": "10/Jan/20 13:20",
        "Description": "stop zk \uff0cafter two minutes restart it.\nthe thread in\u00a0ConnectionStateManager named \"Curator-ConnectionStateManager-0\" break out the dead Loop.\ncode : org.apache.curator.framework.state.ConnectionStateManager#start\nthe task stopped and would not restasrt ,so that when a new connectionState (such as reconnect) coming , there would no any response.\nit will lead to leaderLatch failed.\n\u00a0\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\u00a0\nHi,all\n\u00a0 \u00a0I have already\u00a0resolved this problem , it caused by\u00a0version compatibility .I uesd zookeeper-client- 3.5.1.jar\u00a0 and\u00a0 curaot-framwork-4.1.0.jar .\n\u00a0 \u00a0while the JVM initializing static\u00a0block in class\u00a0org.apache.curator.utils.Compatibility\u00a0 ,it throw a error,but not Exception!\n\u00a0\n-----------------------------------------------------------\n\u00a0\u00a02020-01-10T04:33:27,568|ERROR|Curator-ConnectionStateManager-0|org.apache.curator.framework.state.ConnectionStateManager|Could not inject session expiration\njava.lang.ExceptionInInitializerError: null\n at org.apache.curator.utils.Compatibility.injectSessionExpiration(Compatibility.java:66) ~[curator-client-4.1.0.jar:?]\n at org.apache.curator.framework.state.ConnectionStateManager.checkSessionExpiration(ConnectionStateManager.java:315) [curator-framework-4.1.0.jar:4.1.0]\n at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:287) [curator-framework-4.1.0.jar:4.1.0]\n at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:45) [curator-framework-4.1.0.jar:4.1.0]\n at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:121) [curator-framework-4.1.0.jar:4.1.0]\n at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_201]\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201]\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201]\n at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]\nCaused by: java.lang.RuntimeException: Could not access internal ZooKeeper fields\n at org.apache.curator.utils.InjectSessionExpiration.<clinit>(InjectSessionExpiration.java:75) ~[curator-client-4.1.0.jar:?]\n ... 9 more\nCaused by: java.lang.NoSuchMethodException: org.apache.zookeeper.ClientCnxnSocket.wakeupCnxn()\n at java.lang.Class.getDeclaredMethod(Class.java:2130) ~[?:1.8.0_201]\n at org.apache.curator.utils.InjectSessionExpiration.<clinit>(InjectSessionExpiration.java:70) ~[curator-client-4.1.0.jar:?]\n ... 9 more\n------------------------------------------------------------------------------\n\u00a0\nI change the code \u201ccatch Exception \u201d\u00a0 to \"catch throwable\"\uff0cand the error finnaly appeared .\n\u00a0\nthe error made my\u00a0Curator-ConnectionStateManager-0 down , so that my appliacation can not known th reconnect of zk!",
        "Issue Links": []
    },
    "CURATOR-555": {
        "Key": "CURATOR-555",
        "Summary": "Curator Client leaks the com.google.guava:guava regardless of it being shaded.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "Bernardo Gomez Palacio",
        "Created": "16/Jan/20 23:17",
        "Updated": "25/Apr/23 13:53",
        "Resolved": null,
        "Description": "The curator-client leaks the com.google.guava:guava regardless of it being shaded within the artifact.\nhttps://search.maven.org/remotecontent?filepath=org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.pom\n\r\n<dependency>\r\n    <groupId>com.google.guava</groupId>\r\n    <artifactId>guava</artifactId>\r\n</dependency> \r\n\n\nNote that since curator-client 4.2.0 was moved to Guava 27.0.1 as pat of CURATOR-502 this can cause problems for projects which depend on older versions of Guava but get upgraded by the transitive dependency to 27.0.1.",
        "Issue Links": [
            "/jira/browse/CURATOR-670"
        ]
    },
    "CURATOR-556": {
        "Key": "CURATOR-556",
        "Summary": "Fix typo",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "4.3.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "KangZhiDong",
        "Created": "20/Jan/20 01:37",
        "Updated": "20/Jan/20 20:52",
        "Resolved": "20/Jan/20 01:40",
        "Description": "Fix typo in\u00a0NodeCache.java",
        "Issue Links": [
            "https://github.com/apache/curator/pull/342"
        ]
    },
    "CURATOR-557": {
        "Key": "CURATOR-557",
        "Summary": "ServiceCacheImpl does not close ExecutorService",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "TBD",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Roelof Naude",
        "Created": "22/Jan/20 11:18",
        "Updated": "28/May/20 23:06",
        "Resolved": null,
        "Description": "ServiceCacheImpl does not close the ExecutorService instance created from the ThreadFactory:\nhttps://github.com/apache/curator/blob/master/curator-x-discovery/src/main/java/org/apache/curator/x/discovery/details/ServiceCacheImpl.java#L64\n\u00a0\nCloseableExecutorService::CloseableExecutorService(ExecutorService) call this(executorService, false) which sets shutdownOnClose to false. \n\u00a0\nthis has an impact from callers, eg ServiceProviderImpl, which only allow only a ThreadFactory to be specified.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/343"
        ]
    },
    "CURATOR-558": {
        "Key": "CURATOR-558",
        "Summary": "ZooKeeper 3.6.0 has many API changes - bring Curator up to date",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Client,                                            Documentation,                                            Framework,                                            Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "01/Feb/20 16:00",
        "Updated": "16/Mar/20 17:14",
        "Resolved": "16/Mar/20 17:14",
        "Description": "ZooKeeper 3.6.0 (soon to be released) has a number of internal changes that require significant changes to Curator. We should consider:\n\nMajor version bump (5.0? 4.4?)\nRemove ZK 3.4 compatibility code - Curator 4.2.x can remain the supported version for 3.4 compatibility\nShould we remove the few Guava classes that have leaked into our public APIs? ListenerContainer has been deprecated for a version or two now.\nWe can also remove the \"Reaper\" classes which are no longer necessary given Container nodes\nRemove Exhibitor support",
        "Issue Links": [
            "/jira/browse/KNOX-2283",
            "https://github.com/apache/curator/pull/344",
            "https://github.com/apache/curator/pull/350"
        ]
    },
    "CURATOR-559": {
        "Key": "CURATOR-559",
        "Summary": "Inconsistent ZK timeouts",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0,                                            4.3.0",
        "Fix Version/s": "4.3.0,                                            5.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Grant Digby",
        "Created": "17/Feb/20 11:10",
        "Updated": "22/Apr/20 03:33",
        "Resolved": "22/Apr/20 03:33",
        "Description": "I've configured a reasonable timeout using BoundedExponentialBackoffRetry, and generally it works as I'd expect if ZK is down when I make a call like \"create.forPath\". But if ZK is unavailable when I call acquire on an InterProcessReadWriteLock, it takes far longer before it finally times out.\nI call acquire which is wrapped in \"RetryLoop.callWithRetry\" and it goes onto call findProtectedNodeInForeground which is also wrapped in \"RetryLoop.callWithRetry\". If I've configured the BoundedExponentialBackoffRetry to retry 20 times, the inner retry tries 20 times for every one of the 20 outer retry loops, so it retries 400 times.\n\u00a0\nThis class recreates it, if you put break points at the commented sections and bring ZK down you can see the different times until it disconnects and the stack traces which I've included below.\n\u00a0\n\n\r\npublic class GoCurator {\r\npublic static void main(String[] args) throws Exception {\r\n\r\n    CuratorFramework cf = CuratorFrameworkFactory.newClient(\r\n            \"localhost:2181\",\r\n            new BoundedExponentialBackoffRetry(200, 10000, 20)\r\n    );\r\n    cf.start();\r\n\r\n    String root = \"/myRoot\";\r\n    if(cf.checkExists().forPath(root) == null) {\r\n        // Stacktrace A showing what happens if ZK is down for this call\r\n        cf.create().forPath(root);\r\n    }\r\n\r\n    InterProcessReadWriteLock lcok = new InterProcessReadWriteLock(cf, \"/grant/myLock\");\r\n\r\n    // See stacktrace B showing the nested re-try if ZK is down for this call\r\n    lcok.readLock().acquire();\r\n\r\n    lcok.readLock().release();\r\n\r\n    System.out.println(\"done\");\r\n} \n\n\u00a0\nStacktrace A (if ZK is down when I'm calling create().forPath). This shows the single retry loop so it exist after the correct number of attempts:\n\u00a0\n\n\r\n java.lang.Thread.State: WAITING\r\n  at java.lang.Object.wait(Object.java:-1)\r\n  at java.lang.Object.wait(Object.java:502)\r\n  at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1499)\r\n  at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1487)\r\n  at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:2617)\r\n  at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:242)\r\n  at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:231)\r\n  at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64)\r\n  at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)\r\n  at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:228)\r\n  at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:219)\r\n  at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:41)\r\n  at com.gebatech.curator.GoCurator.main(GoCurator.java:25) \n\nStacktrace B (if ZK is down when I call InterProcessReadWriteLock#readLock#acquire). This shows the nested re-try loop so it doesn't exit until 20*20 attempts.\n\u00a0\n\n\r\n java.lang.Thread.State: WAITING\r\n  at sun.misc.Unsafe.park(Unsafe.java:-1)\r\n  at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)\r\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)\r\n  at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)\r\n  at org.apache.curator.CuratorZookeeperClient.internalBlockUntilConnectedOrTimedOut(CuratorZookeeperClient.java:434)\r\n  at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:56)\r\n  at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.findProtectedNodeInForeground(CreateBuilderImpl.java:1239)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.access$1700(CreateBuilderImpl.java:51)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1167)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1156)\r\n  at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64)\r\n  at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1153)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:607)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:597)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:575)\r\n  at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:51)\r\n  at org.apache.curator.framework.recipes.locks.StandardLockInternalsDriver.createsTheLock(StandardLockInternalsDriver.java:54)\r\n  at org.apache.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:225)\r\n  at org.apache.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:237)\r\n  at org.apache.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:89)\r\n  at com.gebatech.curator.GoCurator.main(GoCurator.java:29)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/346",
            "https://github.com/apache/curator/pull/359",
            "https://github.com/apache/curator/pull/361"
        ]
    },
    "CURATOR-560": {
        "Key": "CURATOR-560",
        "Summary": "Make sure tickTime and minSessionTimeout are set",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "4.3.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "18/Feb/20 18:56",
        "Updated": "18/Feb/20 23:25",
        "Resolved": "18/Feb/20 23:25",
        "Description": "The default settings for tickTime and minSessionTimeout make our test take longer than they need to. Explicitly set them to smaller values.\nAlso, while I'm at it, I'll make sure the random server port code calls setReuseAddress(true).",
        "Issue Links": [
            "https://github.com/apache/curator/pull/347"
        ]
    },
    "CURATOR-561": {
        "Key": "CURATOR-561",
        "Summary": "Race condition preventing reconnection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.2.1",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Grant Digby",
        "Created": "20/Feb/20 20:53",
        "Updated": "02/Apr/23 06:50",
        "Resolved": "08/Nov/21 20:45",
        "Description": "Heavy GC caused several rounds of suspend, lost, reconnected - as you'd expect. But eventually it failed to reconnect, even though ZK was up the entire time. My thread waited on acquireLock until the retry period was up. I believe I've tracked this down to a race condition between the Curator thread which calls ZookKeeperTestable#injectSessionExpiration to post the expiry and death events and the Zookeeper SendThread which posts the same.\n\u00a0\nThe EventThread is meant to receive the expiry event first so it can set up its replacement, then it receives the death event making it shut down.\nThe race condition allows these two events to be received in the wrong order so the EventThread shuts down before a replacement can be set up.\n\u00a0\nThe \"Curator-ConnectionStateManager\" thread calls\u00a0injectSessionExpiration which calls EventThread#queueEvent. It goes as far as my comment in the next code snippit, sets the status and let's say it pauses here, for whatever reason.\n\u00a0\n\n\r\nprivate void queueEvent(WatchedEvent event,\r\n        Set<Watcher> materializedWatchers) {\r\n    if (event.getType() == EventType.None\r\n            && sessionState == event.getState()) {\r\n        return;\r\n    }\r\n    sessionState = event.getState();   // The curator thread sets the status here\r\n  // It then goes onto send the expiry event, but for now it's here..the event has not yet been sent\n\n\u00a0\nThe SendThread then goes into the same method with its Expiry message, but the state has already been set to expired by the curator thread so it returns without sending the expiry message. It then sends the event of death:\n\n\r\n// from ClientCnxn#onConnected\r\neventThread.queueEvent(new WatchedEvent(\r\n        Watcher.Event.EventType.None,\r\n        Watcher.Event.KeeperState.Expired, null));\r\neventThread.queueEventOfDeath(); \n\n\u00a0\nThe EventThread receives this event of death and shuts down. The curator thread then continues and sends the original expiry message but it's now too late. With no EventThread created it remains in suspended state with any operations just retrying until they give up.\n\u00a0\nI find it difficult to explan race conditions, but hopefully that made sense. I can recreate the issue in Intellij using Suspend-All break points to cause the session expiry and Suspend-Thread break points to order the threads in the correct way:\n\u00a0\nRun the following and put a suspend-all break point at the System.out\n\n\r\npackage com.gebatech.curator;\r\n\r\nimport org.apache.curator.framework.CuratorFramework;\r\nimport org.apache.curator.framework.CuratorFrameworkFactory;\r\nimport org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock;\r\nimport org.apache.curator.retry.BoundedExponentialBackoffRetry;\r\n\r\n/**\r\n *\r\n */\r\npublic class GoCurator {\r\n    public static void main(String[] args) throws Exception {\r\n\r\n        CuratorFramework cf = CuratorFrameworkFactory.builder()\r\n                .connectString(\"localhost:2181\")\r\n                .retryPolicy(new BoundedExponentialBackoffRetry(200, 10000, 20))\r\n                .sessionTimeoutMs(30 * 1000)\r\n                .build();\r\n\r\n        cf.start();\r\n\r\n        InterProcessReadWriteLock lcok = new InterProcessReadWriteLock(cf, \"/grant/myLock\");\r\n\r\n        System.out.println(\"Set a Suspend-ALL break point here and leave it until the session expires, 30 seconds ish\");\r\n\r\n        lcok.readLock().acquire();\r\n\r\n        System.out.println();\r\n    }\r\n} \n\nWhlst suspended here, put a Suspend-Thread break poing in EventThread#queueEvent, on the if statement from the first code snippet.\n\u00a0\nWhen you see the following log line, you can let the program resume;\n\n\r\n2020-02-20 20:15:05,912 [myid:] - INFO [SessionTracker:ZooKeeperServer@398] - Expiring session 0x1006300f2390000, timeout of 30000ms exceeded \n\nThe SendThread will hit your break point first with a Disconnect event, you can let this run.\nNext you'll get the SendThread with the Expiry event, wait here until you get the popup telling you that the Curator thread has hit the same break point. Switch to this and step through until it has set the expiry status, then leave it held there and switch back to the SendThread. Let the sendThread run, then remove break points and let everything else run.\n\u00a0\nThe program will then be stuck and you'll see this stacktrace for the acquire lock method\n\n\r\n\"main@1\" prio=5 tid=0x1 nid=NA waiting\"main@1\" prio=5 tid=0x1 nid=NA waiting\u00a0 java.lang.Thread.State: WAITING \u00a0 at java.lang.Object.wait(Object.java:-1) \u00a0 at java.lang.Object.wait(Object.java:502) \u00a0 at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1499) \u00a0 at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1487) \u00a0 at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1547) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1180) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1156) \u00a0 at org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) \u00a0 at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1153) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:607) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:597) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:575) \u00a0 at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:51) \u00a0 at org.apache.curator.framework.recipes.locks.StandardLockInternalsDriver.createsTheLock(StandardLockInternalsDriver.java:54) \u00a0 at org.apache.curator.framework.recipes.locks.LockInternals.attemptLock(LockInternals.java:225) \u00a0 at org.apache.curator.framework.recipes.locks.InterProcessMutex.internalLock(InterProcessMutex.java:237) \u00a0 at org.apache.curator.framework.recipes.locks.InterProcessMutex.acquire(InterProcessMutex.java:89) \u00a0 at com.gebatech.curator.GoCurator.main(GoCurator.java:26) \n\n\u00a0\nI know this issue falls between Zookeeper and Curator but thought I'd report it in case you can think of a solution. I think ideally they'd send a single expiry event with a death flag set or, performance impact allowing, synchronize that method.",
        "Issue Links": [
            "/jira/browse/CURATOR-633",
            "/jira/browse/CURATOR-627",
            "https://github.com/apache/curator/pull/400"
        ]
    },
    "CURATOR-562": {
        "Key": "CURATOR-562",
        "Summary": "Remove ConnectionHandlingPolicy",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Zili Chen",
        "Created": "07/Mar/20 04:20",
        "Updated": "12/Jul/22 10:18",
        "Resolved": "14/Mar/20 18:09",
        "Description": "Back to the day we bump Curator from 2.x to 3.0 we introduce ConnectionHandlingPolicy as a bridge that provides different strategy on handling different session timeout logic.\nThings changed and now only the StandardConnectionHandlingPolicy survive, who has two methods\n1. checkTimeouts totally static utility. Besides, some values in CheckTimeoutsResult seems out of day that we might have a follow-up to handle it. Anyway, I don't thing anyone outside Curator should change the behavior here since it is tight couple with how o.a.c.ConnectionState works.\n2. callWithRetry it is actually a consignee of RetryLoop#callWithRetry. According to its implementation it seems hardly an outside user can properly define his callWithRetry method.\nThus, I propose that we flatten this legacy class.\nDISCLAIMER:\nThe place from where I come here is CURATOR-544 that I'd like to implement a user-friendly option to enable Curator blocks its regenerate ZooKeeper client logic on SESSIONEXPIRED.\nHowever, neither ConnectionHandlingPolicy nor RetryLoop nor RetryPolicy is a good place since implementations coupled quite a bit. And the real regeneration logic hided inside ConnectionState.\nThus I'm willing to do a bit code cleanups to see where we can properly inject such logics.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/348"
        ]
    },
    "CURATOR-563": {
        "Key": "CURATOR-563",
        "Summary": "Fix tests that take a long time to run",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "15/Mar/20 14:40",
        "Updated": "15/Mar/20 14:50",
        "Resolved": null,
        "Description": "We have some tests that take several minutes. This isn't serving a good purpose and makes the entire test suite take a long time to run. E.g. TestInterProcessSemaphore.testMaxPerSession() takes over 2 minutes on my new MBP. \nLook at the output from a recent test run and try to update/eliminate/etc. tests that take a long time.\nSome candidates from a recent Travis run:\n\n[INFO] Running org.apache.curator.framework.imps.TestEnabledSessionExpiredState\r\n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 77.809 s - in org.apache.curator.framework.imps.TestEnabledSessionExpiredState\r\n\r\n[INFO] Running org.apache.curator.framework.imps.TestWatchesBuilder\r\n[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 125.593 s - in org.apache.curator.framework.imps.TestWatchesBuilder\r\n\r\n[INFO] Running org.apache.curator.framework.imps.TestFramework\r\n[INFO] Tests run: 35, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 62.262 s - in org.apache.curator.framework.imps.TestFramework\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.queue.TestDistributedQueue\r\n[INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 72.033 s - in org.apache.curator.framework.recipes.queue.TestDistributedQueue\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.shared.TestSharedCount\r\n[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 72.177 s - in org.apache.curator.framework.recipes.shared.TestSharedCount\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.leader.TestLeaderLatch\r\n[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 193.627 s - in org.apache.curator.framework.recipes.leader.TestLeaderLatch\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.locks.TestInterProcessSemaphore\r\n[INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 232.355 s - in org.apache.curator.framework.recipes.locks.TestInterProcessSemaphore\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.locks.TestInterProcessSemaphoreCluster\r\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 87.917 s - in org.apache.curator.framework.recipes.locks.TestInterProcessSemaphoreCluster\r\n\r\n[INFO] Running org.apache.curator.framework.recipes.nodes.TestPersistentEphemeralNode\r\n[INFO] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 143.867 s - in org.apache.curator.framework.recipes.nodes.TestPersistentEphemeralNode\r\n\r\n[INFO] Running org.apache.curator.x.async.modeled.TestCachedModeledFramework\r\n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.803 s - in org.apache.curator.x.async.modeled.TestCachedModeledFramework",
        "Issue Links": []
    },
    "CURATOR-564": {
        "Key": "CURATOR-564",
        "Summary": "Changes to retry failed TestingServer starts should be applied to TestingCluster",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "23/Mar/20 17:18",
        "Updated": "23/Mar/20 20:12",
        "Resolved": "23/Mar/20 20:11",
        "Description": "We recently made changes to catch server start issues (usually bind problems) which close the server and re-create/re-start it (see BaseClassForTests.setup). We should apply these to TestingCluster as well as we see Travis tests occasionally fail for TestingCluster startup issues.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/351"
        ]
    },
    "CURATOR-565": {
        "Key": "CURATOR-565",
        "Summary": "checkExists didn't check useContainerParentsIfAvailable flag",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Tianzhou Wang",
        "Created": "26/Mar/20 06:26",
        "Updated": "06/Sep/22 13:23",
        "Resolved": null,
        "Description": "When use zookeeper server 3.4.x with 3.5.x client.\nwhen use checkExists().creatingParentContainersIfNeeded() would lead Unimplemented Exception even dontUseContainerParents() was called for CuratorFrameworkFactory.builder().",
        "Issue Links": [
            "https://github.com/apache/curator/pull/354"
        ]
    },
    "CURATOR-566": {
        "Key": "CURATOR-566",
        "Summary": "Re-assess tech debt in Connection Handling",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "None",
        "Component/s": "Client,                                            Framework",
        "Assignee": null,
        "Reporter": "Jordan Zimmerman",
        "Created": "11/Apr/20 15:03",
        "Updated": "11/Apr/20 15:03",
        "Resolved": null,
        "Description": "As seen in CURATOR-525, there is a lot of tech debt and, frankly, incomprehensible code in parts of Curator's connection handling. CURATOR-525 had to add an ugly hack due to races between ZooKeeper's watcher thread and Curator's connection state setting thread. \nConsider, reworking or rewriting the connection handling in Curator.",
        "Issue Links": []
    },
    "CURATOR-567": {
        "Key": "CURATOR-567",
        "Summary": "Remove some suspect calls to TestCleanState.closeAndTestClean",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "TBD",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "19/Apr/20 14:09",
        "Updated": "28/May/20 23:06",
        "Resolved": null,
        "Description": "Calls to TestCleanState.closeAndTestClean() in the code have realized their purpose and can likely be removed. They add flakiness to our tests. I'll leave this ticket open and slowly remove calls from the code as Travis complains about them.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/358"
        ]
    },
    "CURATOR-568": {
        "Key": "CURATOR-568",
        "Summary": "New option allowing CuratorFramework skip ZK ensemble tracking",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Chevaris",
        "Created": "02/May/20 07:40",
        "Updated": "12/Apr/23 13:55",
        "Resolved": "07/May/20 22:43",
        "Description": "CuratorFrameworkImpl lacks an option to skip Ensemble tracking.\nProposal is to include a new option in CuratorFrameworkFactory.Builder that allows to skip ensemble tracking. This can be useful in certain scenarios in which CuratorFramework is accessing to ZK clusters via load balancer or Virtual IPs. In this case ensemble tracking is avoiding CuratorFramework to support this kind of connectivity.\nProposal suggest including in CuratorFrameworkFactory.Builder 2 new methods:\n\nensembleTracker(boolean) that allows to enable / disable ensembleTracker.\nwithEnsembleTracker() that will return the value set via\u00a0ensembleTracker method. If ensembleTracker was not used, then default value will return true for backwards compatibility.",
        "Issue Links": [
            "/jira/browse/FLINK-31780"
        ]
    },
    "CURATOR-569": {
        "Key": "CURATOR-569",
        "Summary": "API TO HANDLE PROTECTED MODE ZNODE NAMES",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.0.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Chevaris",
        "Created": "10/May/20 16:13",
        "Updated": "01/Jun/20 15:26",
        "Resolved": "15/May/20 20:01",
        "Description": "Curator provides protectedMode to handle with some corner cases with ephemeral nodes. When protectedMode is used, Curator appends a prefix to provided name composed by a String concatenating:\n\n\"c\"\nCanonical text representation of a random generated UUID\n\"-\"\n\nPurpose of this ticket is:\n\nImprove Javadoc to fully specify the prefix used with protected Mode\nProvide method that allows to identify if a ZNode name has the protected prefix\nProvide method that allows to separate the prefix name from the provided name on Curator Framework when creating the node",
        "Issue Links": []
    },
    "CURATOR-570": {
        "Key": "CURATOR-570",
        "Summary": "Excessive calls to ZooKeeper.updateServerList (which can result in session death)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0,                                            4.3.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Rhys Yarranton",
        "Created": "12/May/20 00:37",
        "Updated": "24/Jun/20 22:46",
        "Resolved": null,
        "Description": "On suspend and reconnect, Curator calls ZooKeeper.updateServerList via ConnectionState.checkState --> ConnectionState.handleNewConnectionString.\u00a0 In addition, recipes may be triggered by this as well, and they too make calls ZooKeeper.updateServerList via ConnectState.checkTimeouts --> ConnectionState.handleNewConnectionString.\nThis happens even though the connection string has not actually changed.\nDue to\u00a0ZOOKEEPER-3825, this can cause the connection to be closed immediately.\u00a0 On its own this would be perceived as a glitch.\u00a0 But due to the Curator-induced calls, what we see is a cycle of SUSPENDED/RECONNECTED, until eventually the session dies and a new session is recreated.\nBased on the source code (at time of writing), ZooKeeper.updateServerList is not intended to be called frequently like this.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-3825",
            "/jira/browse/CURATOR-551"
        ]
    },
    "CURATOR-571": {
        "Key": "CURATOR-571",
        "Summary": "PathUtils.validatePath() allows \\u001f (unit separator)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "4.2.0,                                            4.3.0",
        "Fix Version/s": "None",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "V\u00e1clav Haisman",
        "Created": "20/May/20 15:19",
        "Updated": "21/May/20 09:58",
        "Resolved": null,
        "Description": "This piece of code in PathUtils.java seems to use odd ranges for disallowed characters:\n\n\r\n } else if (c > '\\u0000' && c < '\\u001f'\r\n        || c > '\\u007f' && c < '\\u009F'\r\n        || c > '\\ud800' && c < '\\uf8ff'\r\n        || c > '\\ufff0' && c < '\\uffff') \n\nI can understand that 0 is disallowed by earlier condition in the code. But why is 0x1f (unit separator) allowed? Maybe the author wanted to use c<='\\u001f'? Similarly, the term of the next condition allows 0x7f. And the same goes for the other two ranges.\nEither I am missing something here and the ranges are somehow OK, or I am right and the ranges are wrong and should include the boundaries.\nI would expect the following test to pass but they don't, except the first one:\n\n\r\n@Test\r\npublic void testPathUtilsInCurator() {\r\n    PathUtils.validatePath(\"/test\");\r\n}\r\n\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testPathUtils0x1f() {\r\n    PathUtils.validatePath(\"/test\\u001f\");\r\n}\r\n\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testPathUtils0x7f() {\r\n    PathUtils.validatePath(\"/test\\u007f\");\r\n}\r\n\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testPathUtils0xffff() {\r\n    PathUtils.validatePath(\"/test\\uFFFF\");\r\n}",
        "Issue Links": []
    },
    "CURATOR-572": {
        "Key": "CURATOR-572",
        "Summary": "Download page must use HTTPS for KEYS, sigs and hashes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Sebb",
        "Created": "29/May/20 11:53",
        "Updated": "29/May/20 13:41",
        "Resolved": "29/May/20 13:10",
        "Description": "The download page must use HTTPS for KEYS, sigs and hashes.\nIt is not sufficient to rely on an http=>https redirect",
        "Issue Links": []
    },
    "CURATOR-573": {
        "Key": "CURATOR-573",
        "Summary": "No leader is getting selected intermittently",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "None",
        "Component/s": "Apache,                                            Framework,                                            Recipes",
        "Assignee": null,
        "Reporter": "Viniti",
        "Created": "04/Jun/20 15:43",
        "Updated": "06/Jun/20 17:57",
        "Resolved": "06/Jun/20 16:32",
        "Description": "I am using Apache Curator Leader Election Recipe : https://curator.apache.org/curator-recipes/leader-election.html in my application.\nZookeeper version : 3.5.7\nCurator : 4.0.1\nBelow are the sequence of steps:\n1. Whenever my tomcat server instance is getting up, I create a single CuratorFramework instance(single instance per tomcat server) and start it : \nStartUp Code\n\r\nCuratorFramework client = CuratorFrameworkFactory.newClient(connectionString, retryPolicy);\r\nclient.start();\r\nif(!client.blockUntilConnected(10, TimeUnit.MINUTES)){\r\n LOGGER.error(\"Zookeeper connection could not establish!\");\r\n throw new RuntimeException(\"Zookeeper connection could not establish\");\r\n}\r\n\n\n2. Create an instance of LSAdapter and start it:\nLSAdapter initializing\n\r\nLSAdapter adapter = new LSAdapter(client, <some_metadata>);\r\nadapter.start();\r\n\n\nBelow is my LSAdapter class :\nLSAdapter.java\n\r\npublic class LSAdapter extends LeaderSelectorListenerAdapter implements Closeable {\r\n\r\n//<Class instance variables defined>\r\n public LSAdapter(CuratorFramework client, <some_metadata>) {\r\n leaderSelector = new LeaderSelector(client, <path_to_be_used_for_leader_election>, this);\r\n leaderSelector.autoRequeue();\r\n }\r\n\r\npublic void start() throws IOException {\r\n leaderSelector.start();\r\n }\r\n\r\n@Override\r\n public void close() throws IOException {\r\n leaderSelector.close();\r\n }\r\n\r\n@Override\r\n public void takeLeadership(CuratorFramework client) throws Exception {\r\n final int waitSeconds = (int) (5 * Math.random()) + 1;\r\n\r\nLOGGER.info(name + \" is now the leader. Waiting \" + waitSeconds + \" seconds...\");\r\n LOGGER.debug(name + \" has been leader \" + leaderCount.getAndIncrement() + \" time(s) before.\");\r\n while (true) {\r\n try {\r\n Thread.sleep(TimeUnit.SECONDS.toMillis(waitSeconds));\r\n //do leader tasks\r\n } catch (InterruptedException e) {\r\n LOGGER.error(name + \" was interrupted.\");\r\n //cleanup\r\n\r\n/*Here, code is creating a znode. If client 's current state is CLOSED, this line will throw exception resulting in takeLeadership() exit. Else if, client state is STARTED, znode should be created. In case when LSAdaptor.close() is called, the client state will always be CLOSED at this line, and an exception is expected to be thrown.*/\r\n\r\n//This line will always throw exception when client state is \"CLOSED\" and because of which takeLeadership will exit\r\nZookeeperUtil.createEphemeral(client, <some_path>);\r\n\r\nThread.currentThread().interrupt();\r\n } finally {\r\n\r\n}\r\n }\r\n }\r\n}\r\n\n\n4. When server instance is getting down, close LSAdapter instance(which application is using) and close CuratorFramework client created\nPreDestroy code\n\r\nCloseableUtils.closeQuietly(lsAdapter);\r\ncuratorFrameworkClient.close();\r\n\n\nThe issue I am facing is that at times, when server is restarted, no leader gets elected. I checked that by tracing the log inside takeLeadership(). I have two tomcat server instances with above code, connecting to same zookeeper quorum and most of the times one of the instance becomes leader but when this issue happens, both of them becomes follower. Please suggest what am I doing wrong.",
        "Issue Links": []
    },
    "CURATOR-574": {
        "Key": "CURATOR-574",
        "Summary": "DiscoveryService fatal error on deserializing an empty byte[] as JSON",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "5.0.0",
        "Fix Version/s": "5.1.0",
        "Component/s": "Recipes",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "15/Jun/20 14:29",
        "Updated": "23/Jun/20 13:59",
        "Resolved": "23/Jun/20 11:30",
        "Description": "Below you can see the error in 5.0.0 tests, but I have seen this error on a client application. The error seems a blocker for the upgrade to 5.0.0\n\n\r\nERROR org.apache.curator.framework.listen.MappingListenerManager  Listener (org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2@329dbdbf) threw an exception [Curator-SafeNotifyService-0]\r\njava.lang.RuntimeException: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.MismatchedInputException: No content to map due to end-of-input\r\n at [Source: (byte[])\"\"; line: 1, column: 0]\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCacheListenerWrapper.sendEvent(PathChildrenCacheListenerWrapper.java:75)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCacheListenerWrapper.event(PathChildrenCacheListenerWrapper.java:42)\r\n\tat org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2.lambda$event$0(CuratorCacheListenerBuilderImpl.java:149)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1510)\r\n\tat org.apache.curator.framework.recipes.cache.CuratorCacheListenerBuilderImpl$2.event(CuratorCacheListenerBuilderImpl.java:149)\r\n\tat org.apache.curator.framework.recipes.cache.CuratorCacheImpl.lambda$putStorage$7(CuratorCacheImpl.java:279)\r\n\tat org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)\r\n\tat org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)\r\n\tat org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)\r\n\tat org.apache.curator.framework.recipes.cache.CuratorCacheImpl.lambda$callListeners$10(CuratorCacheImpl.java:293)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1800)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.MismatchedInputException: No content to map due to end-of-input\r\n at [Source: (byte[])\"\"; line: 1, column: 0]\r\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:209)\r\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.childEvent(ServiceCacheImpl.java:175)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCacheListenerWrapper.sendEvent(PathChildrenCacheListenerWrapper.java:71)\r\n\t... 13 more\r\nCaused by: com.fasterxml.jackson.databind.exc.MismatchedInputException: No content to map due to end-of-input\r\n at [Source: (byte[])\"\"; line: 1, column: 0]\r\n\tat com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:59)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:4344)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4189)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3300)\r\n\tat org.apache.curator.x.discovery.details.JsonInstanceSerializer.deserialize(JsonInstanceSerializer.java:86)\r\n\tat org.apache.curator.x.discovery.details.ServiceCacheImpl.addInstance(ServiceCacheImpl.java:204)\r\n\t... 15 more",
        "Issue Links": []
    },
    "CURATOR-575": {
        "Key": "CURATOR-575",
        "Summary": "TestingServer shut down can cause NullPointerException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.0.0",
        "Fix Version/s": "5.1.0",
        "Component/s": "Tests",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Jordan Zimmerman",
        "Created": "25/Jun/20 01:11",
        "Updated": "25/Jun/20 06:47",
        "Resolved": "25/Jun/20 06:47",
        "Description": "There is a race in TestingServer that is identified in ZOOKEEPER-3803. The FileTxnSnapLog is closed in a different thread than the ZooKeeperServer is shutdown in. It can cause a NullPointerException. This doesn't hurt any behavior but's an irritant and could potentially cause problems later. It should be fixed.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-3803"
        ]
    },
    "CURATOR-576": {
        "Key": "CURATOR-576",
        "Summary": "Import merge script from ZooKeeper project",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.0.0",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "25/Jun/20 06:33",
        "Updated": "14/Jul/21 13:33",
        "Resolved": "14/Jul/21 13:33",
        "Description": "We can import zk-merge-pr.py script from ZooKeeper project.\n\u00a0\nThe script:\n\nimports a PR from github\nsquashes and merges all of the commits\ncompute Reviewers from github API\nupdates Jira and mark the issue as resolved\n\n\u00a0\nThis is the guide on ZooKeeper project\nhttps://cwiki.apache.org/confluence/display/ZOOKEEPER/Merging+Github+Pull+Requests\nWe will have our own page once the script gets merged\n\u00a0\nSetup:\n\u00a0\n\u00a0\n\n\r\ngit remote add apache\u00a0https://github.com/apache/curator.git\r\ngit remote add apache-github\u00a0https://github.com/apache/curator.git\r\nexport JIRA_USERNAME=my jira username \u00a0\n\n\u00a0\nrun the script:\u00a0\n\n\r\npython3 merge-pr.py\n\n\u00a0\nJust answer to the questions....\n\u00a0\nYou will have to install the \"jira\" module in python3 if you want the script to do the JIRA paperwork for you\n\n\r\npip3 install jira",
        "Issue Links": []
    },
    "CURATOR-577": {
        "Key": "CURATOR-577",
        "Summary": "WebSite should not like to Confluence for downloads",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.0",
        "Component/s": "Website",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Enrico Olivelli",
        "Created": "09/Jul/20 13:54",
        "Updated": "19/Jul/21 22:00",
        "Resolved": "10/Jul/20 06:57",
        "Description": "We are currently linking to this page as \"download link\"\nhttps://cwiki.apache.org/confluence/display/CURATOR/Releases\nThis is no more accepted by \"announce@apache.org\" bot\nThis is the error message:\n\"The announcement has been rejected because the download links do not use\nthe ASF mirror system.\"\n\u00a0\nWe should create a dedicated page on the website",
        "Issue Links": []
    },
    "CURATOR-578": {
        "Key": "CURATOR-578",
        "Summary": "EnsembleTracker replace hostname connectString with wrong ip from zk config",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.0.1",
        "Fix Version/s": "5.4.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "ying.li",
        "Created": "16/Jul/20 12:12",
        "Updated": "17/Jul/22 08:04",
        "Resolved": "17/Jul/22 08:04",
        "Description": "I have a zookeeper cluster\u00a0 which run on a k8s cluster. and I use host name to\u00a0 connect the zookeeper(like : zookeeper-0.zookeeper-headless.default.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.default.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.default.svc.cluster.local:2181).\n\u00a0\nWhen the zookeeper restart. the zk pod's ip will change.\u00a0 then\u00a0 I find my client will use the IP to recreate a client without using the hostname . but the IP is not the latest IP from hostname.so, it will make client never connect to zk , unless restart the client\n\u00a0\nAfter some debug ,I find the\u00a0EnsembleTracker will change the connectString from hostname to ip when receive the congfig change\u00a0 event. But in many case, the IP get from hostname will not change after\u00a0 zk\u00a0 restart in k8s. so, it will make client never connect to zk , unless restart the client",
        "Issue Links": [
            "/jira/browse/CURATOR-638"
        ]
    },
    "CURATOR-579": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-580": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-581": {
        "Key": "CURATOR-581",
        "Summary": "Possible malformed OSGi export pattern",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "08/Aug/20 00:49",
        "Updated": "24/Aug/20 01:40",
        "Resolved": "24/Aug/20 01:40",
        "Description": "Every time when I open Curator in IDEA it complains with\nThe package 'org.apache.curator.framework.api' is not exported by the bundle dependencies\nand so on if the code refers a class/method outside its module.\nI can \"fix\" the warnings if the patch attached applied. But I'm not an expert of OSGi so I don't know if it is a valid \"fix\".",
        "Issue Links": []
    },
    "CURATOR-582": {
        "Key": "CURATOR-582",
        "Summary": "Migrate to jUnit 5.6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "Tests",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Tamas Penzes",
        "Created": "12/Aug/20 10:35",
        "Updated": "20/Oct/20 13:19",
        "Resolved": "20/Oct/20 13:19",
        "Description": "Most related components use jUnit and slowly migrate to jUnit 5.\nCurator uses TestNG, which was superior to jUnit4, but looks like jUnit 5 got over it.\nIt's time to go forward and migrate to jUnit 5.6 just as ZooKeeper did.\nI'll create subtasks and work on them.",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-3732",
            "https://github.com/apache/curator/pull/372"
        ]
    },
    "CURATOR-583": {
        "Key": "CURATOR-583",
        "Summary": "Fix ArrayIndexOutOfBoundsException when passing empty list parameter to reconfigure API",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "hyeong jun yun",
        "Created": "19/Sep/20 08:16",
        "Updated": "20/Sep/20 09:35",
        "Resolved": "20/Sep/20 09:35",
        "Description": "Whenever I add zookeeper servers by using reconfig API of the curator client, it always throw ArrayIndexOutOfBoundsException.\nIf there are no servers to add or remove when using reconfigure API of ZooKeeperAdmin, I think it must pass not empty list parameter but null to reconfigure API. Because ZooKeeperAdmin tries to join strings by accessing first index of the server list.\nPlease refer to the link below.\nhttps://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/admin/ZooKeeperAdmin.java#L267-L269\nhttps://github.com/apache/zookeeper/blob/release-3.6.2/zookeeper-server/src/main/java/org/apache/zookeeper/common/StringUtils.java#L57",
        "Issue Links": [
            "https://github.com/apache/curator/pull/374"
        ]
    },
    "CURATOR-584": {
        "Key": "CURATOR-584",
        "Summary": "Curator Client Fault Tolerance Extensions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.0",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Josh Slocum",
        "Created": "06/Dec/20 18:00",
        "Updated": "11/Feb/22 13:43",
        "Resolved": "18/Feb/21 15:26",
        "Description": "Tl;dr My team at Indeed has developed ZooKeeper functionality to handle stateful retrying of connectionloss for write operations, and we wanted to reach out to discuss if this is something the Curator team may be interested in incorporating.\nWe initially reached out to the Zookeeper team (https://issues.apache.org/jira/browse/ZOOKEEPER-3927) but were redirected to Curator as the better place to contribute them. The changes could be relatively easily added as additional parameters and/or extensions of the existing retry behavior in Curator's write operations.\n\u00a0\nHi Curator Devs,\nMy team uses zookeeper extensively as part of a distributed key-value store we've built at Indeed (think HBase replacement). Due to our deployment setup co-locating our database daemons with our large hadoop cluster, and the network-intensive nature of a lot of our compute jobs, we were experiencing a large amount of transient ConnectionLoss issues. This was especially problematic on important write operations, such as the creation\u00a0deletion of distributed locks/leases or updating distributed state in the cluster. \nWe saw that some existing zookeeper client wrappers handled retrying in the presence of ConnectionLoss, but all of the ones we looked at (including Curator) didn't allow for retrying writes wiith all of the proper state. Consider the case of retrying a create. If the initial create had succeeded on the server, but the client got connectionloss, the client would get a NodeExists exception on the retried request, even though the znode was created. This resulted in many issues. For the distributed lock/lease example, to other nodes, it looked like the calling node had been successful acquiring the \"lock\", and to the calling node, it appeared that it was not able to acquire the \"lock\", which results in a deadlock.\nCurator has parameters that can modify the behavior upon retry, but those were not sufficient. For example, create() has orSetData(), and delete() has guaranteed().\nTo solve this, we implemented a set of \"connection-loss tolerant primitives\" for the main types of write operations. They handle a connection loss by retrying the operation in a loop, but upon error cases in the retry, inspect the current state to see if it matches the case where a previous round that got connectionloss actually succeeded.\n\ncreateRetriable(String path, byte[] data)\nsetDataRetriable(String path, byte[] newData, int currentVersion)\ndeleteRetriable(String path, int currentVersion)\ncompareAndDeleteRetriable(String path, byte[] currentData, int currentVersion)\n\nFor example, in createRetriable, it will retry the create again on connection loss. If the retried call gets a NodeExists exception, it will check to see if (getData(path) == data and dataVersion == 0). If it does, it assumes the first create succeeded and returns success, otherwise it propagates the NodeExists exception.\nThese primitives have allowed us to program our ZooKeeper layer as if ConnectionLoss isn't a transient state we have to worry about, since they have essentially the same guarantees as the non-retriable functions in the zookeeper api do (with a slight difference in semantics).\nBecause these behaviors could be relatively easily added to Curator as additional parameters to the existing mechanisms, and (to my knowledge) aren't implemented anywhere else, we think it could be a useful contribution to the Curator project. If this isn't something that Curator is interested in incorporating, Indeed may also consider open sourcing it as a standalone library.",
        "Issue Links": [
            "/jira/browse/CURATOR-589",
            "/jira/browse/ZOOKEEPER-4024",
            "/jira/browse/FLINK-26068",
            "/jira/browse/FLINK-24543"
        ]
    },
    "CURATOR-585": {
        "Key": "CURATOR-585",
        "Summary": "Discovery sample did not check for exceptions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "\u738b\u6770",
        "Created": "12/Dec/20 07:24",
        "Updated": "19/Jul/21 22:01",
        "Resolved": "18/Jan/21 11:03",
        "Description": "I tried to run `DiscoveryExample` to simulate service discovery, when I first entered list to see the list of registrations, I found that the process exited directly, I checked the code and found that the code threw `NoNodeException` but it was ignored.\nI think if there is no service registered, using `list` should tell me that no service is registered instead of just ignoring the exception and closing the connection, so I am asking this question, what do you think?",
        "Issue Links": []
    },
    "CURATOR-586": {
        "Key": "CURATOR-586",
        "Summary": "NamespaceFacade return null if namespace is empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Nikita Ryanov",
        "Created": "17/Jan/21 15:13",
        "Updated": "23/May/22 14:40",
        "Resolved": "23/May/22 14:40",
        "Description": "According to java doc framework should return empty string if namespace is empty instead of null:\n\u00a0\n\n\r\n/**\r\n * Return the current namespace or \"\" if none\r\n *\r\n * @return namespace\r\n */\r\npublic String getNamespace();\r\n\n\n\u00a0\nIf namespace is claimed from CuratorFramework client directly all is ok, but if NamespaceFacade is used then null is returned.\u00a0\nHow to reproduce:\n\n\r\nRetryPolicy policy = ...;\r\nCuratorFramework client = CuratorFrameworkFactory.newClient(\"connectionString\", policy);\r\nCuratorFramework clientWithNamespaceFacade = client.usingNamespace(null);\r\nclientWithNamespaceFacade.getNamespace(); // <- expected \"\", but got null",
        "Issue Links": []
    },
    "CURATOR-587": {
        "Key": "CURATOR-587",
        "Summary": "Use ZooKeeper 3.7+ ZooKeeperServerEmbedded in order to start TestingServer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "Tests",
        "Assignee": "Zili Chen",
        "Reporter": "Enrico Olivelli",
        "Created": "20/Jan/21 09:57",
        "Updated": "31/Aug/22 06:33",
        "Resolved": "31/Aug/22 06:33",
        "Description": "with 3.7.0 ZooKeeper provides an official API to run a ZooKeeper node.\nWe should use that API in order to launch ZooKeeper, this way we won't have any compatibility issues.\nIn fact now (5.1) we are starting the ZooKeeper node using internal ZK classes, that are subject to change in the future.\nThe ZooKeeperServerEmbedded API is designed to be forward compatible.\nIt is a very skinny API, and it basically reproduces the usage for the users, that is to provide a configuration file and a base directory.\nIt also provides a way to prevent ZooKeeper to exit the JVM and to shutdown gracefully the server.\n\n\r\n \r\n    @TempDir\r\n    Path tmpDir;\r\n   \r\n    @Test\r\n     public void hello() throws Exception {\r\n         Properties configuration = new Properties();\r\n         configuration.setProperty(\"clientPort\", \"2181\");\r\n         try (ZooKeeperServerEmbedded embedded = ZooKeeperServerEmbedded\r\n                 .builder()\r\n                 .exitHandler(ExitHandler.LOG_ONLY)\r\n                 .baseDir(tmpDir)\r\n                 .configuration(configuration)\r\n                 .build();) {\r\n             embedded.start();\r\n             CountDownLatch l = new CountDownLatch(1);\r\n             try (ZooKeeper zk = new ZooKeeper(\"localhost:2181\", 40000, new Watcher() {\r\n                 @Override\r\n                 public void process(WatchedEvent event) {\r\n                     System.out.println(\"event \"+event)                            ;\r\n                     l.countDown();\r\n                 }                 \r\n             }\r\n             )) {\r\n                 l.await();\r\n                 System.out.println(\"WHOAMI\"+ zk.whoAmI());\r\n                 zk.create(\"/foo\", \"foo\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\r\n             }\r\n         }\r\n     \r\n     }",
        "Issue Links": []
    },
    "CURATOR-588": {
        "Key": "CURATOR-588",
        "Summary": "Upgrade ZooKeeper to 3.6.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "Framework",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "29/Jan/21 07:42",
        "Updated": "30/Apr/22 04:58",
        "Resolved": "06/Jul/21 09:08",
        "Description": null,
        "Issue Links": [
            "/jira/browse/HADOOP-17612"
        ]
    },
    "CURATOR-589": {
        "Key": "CURATOR-589",
        "Summary": "Add Idempotent operations to Curator Async framework",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Josh Slocum",
        "Created": "29/Jan/21 18:10",
        "Updated": "29/Jan/21 18:12",
        "Resolved": null,
        "Description": "In CURATOR-584, I added idempotent write operations to the main curator framework.\nI would like to also add support for these to the async framework.",
        "Issue Links": [
            "/jira/browse/CURATOR-584"
        ]
    },
    "CURATOR-590": {
        "Key": "CURATOR-590",
        "Summary": "Adds possibility to disable parent creation for PersistentNode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.3.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Boutes Paul",
        "Created": "06/Mar/21 15:30",
        "Updated": "09/Mar/21 07:12",
        "Resolved": "09/Mar/21 07:12",
        "Description": "We should be able to add an option to the PersistentNode in order to turn parent creation off.\nFor example:\ngiven this znode path containing some data:\n\n/one/two\n\n\u00a0and a PersistentNode under\n\n\r\n/one/two/three\n\nIf the former path is deleted (with deleting children if needed),\nthe PersistentNode will recreate the full hierarchy but with empty data for\n\n\r\n/one/two\n\nThat can be misleading for external process looking at these paths, thinking that something is up (wrongfully).\nThe idea is to add a boolean flag to the PersistentNode recipe to disable the parent creation when creating / recreating znode in order to avoid such situation.",
        "Issue Links": []
    },
    "CURATOR-591": {
        "Key": "CURATOR-591",
        "Summary": "Update PersistentNode documentation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.0",
        "Component/s": "Website",
        "Assignee": null,
        "Reporter": "Boutes Paul",
        "Created": "09/Mar/21 16:08",
        "Updated": "19/Jul/21 22:01",
        "Resolved": "28/Mar/21 14:44",
        "Description": "Update the PersistentNode documentation.",
        "Issue Links": []
    },
    "CURATOR-592": {
        "Key": "CURATOR-592",
        "Summary": "Provide a callback that is fired after the InterProcessMutex creates its ephemeral node",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Mohamed Mohsen",
        "Created": "14/Mar/21 11:40",
        "Updated": "14/Jun/21 19:36",
        "Resolved": null,
        "Description": "InterProcessMutex performs two significant steps to acquire its lock:\n\nCreates an ephemeral sequential node. This practically means that it took a position in the lock queue (i.e. based on its sequence value).\nBlocks until the lock is acquired.\n\nThis improvement is about providing a callback that notifies the caller that the lock order is reserved.\nThe use for this, at least for us, is that we needed to differentiate between the two phases for higher concurrency because we can do other actions after knowing that the\u00a0InterProcessMutex order is known. Especially since the lock acquisition can take too long, depending on the situation of course.",
        "Issue Links": []
    },
    "CURATOR-593": {
        "Key": "CURATOR-593",
        "Summary": "EnsembleTracker  \"configToConnectionString\" method doesn't include zookeeper chroot",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Yang",
        "Created": "18/Mar/21 00:24",
        "Updated": "29/May/23 13:13",
        "Resolved": "29/May/23 13:13",
        "Description": "The bug was found on curator 4.2 version and it's still in the master branch.\nIt can be reproduced by running a zookeeper\u00a0quorum\u00a0\nIf you have a zookeeper connection string localhost:2181/foobar\u00a0and using curator library.\u00a0\nEnsembleTracker.configToConnectionString\u00a0will try to re-build the zookeeper connection string by using all the hosts in the quorum into the connection string(127.0.0.1:2181,127.0.0.1:2182/), but\u00a0doesn't include zookeeper zk_chroot when reconstructing connection string.\u00a0\nIt's normally fine because it's not in-used, but when there is a session timeout trigger zookeeper watcher expire\u00a0\u00a0ConnectionState class will try to reset the connection by using\u00a0HandleHolder.getZooKeeper , the new connection string generated by above\u00a0EnsembleTracker.configToConnectionString\u00a0 method will be in-use during the reconnect.\nso it will try to connect to\u00a0127.0.0.1:2181,127.0.0.1:2182 (without /foobar zh_chroot)",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-4565",
            "/jira/browse/CURATOR-611",
            "https://github.com/apache/curator/pull/460"
        ]
    },
    "CURATOR-594": {
        "Key": "CURATOR-594",
        "Summary": "TestingZooKeeperMain isn't setting tickTime, if configured",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Francesco Nigro",
        "Created": "20/Apr/21 14:31",
        "Updated": "02/Apr/23 06:43",
        "Resolved": "02/Apr/23 05:20",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/curator/pull/383"
        ]
    },
    "CURATOR-595": {
        "Key": "CURATOR-595",
        "Summary": "InterProcessSemaphoreV2 LOST isn't releasing permits for other clients",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Information Provided",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Francesco Nigro",
        "Created": "22/Apr/21 16:07",
        "Updated": "16/Jun/23 07:07",
        "Resolved": "16/Jun/23 07:07",
        "Description": "I'm not sure this is the right place to raise this, but I've added this test on TestInterProcessSemaphore:\n\n\r\n    @Test\r\n    public void testAcquireAfterLostServerOnRestart() throws Exception {\r\n        final int sessionTimout = 4000;\r\n        final int connectionTimout = 2000;\r\n        try (CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString(), sessionTimout, connectionTimout, new RetryNTimes(0, 1))) {\r\n            client.start();\r\n            client.blockUntilConnected();\r\n            final InterProcessSemaphoreV2 semaphore = new InterProcessSemaphoreV2(client, \"/1\", 1);\r\n            assertNotNull(semaphore.acquire());\r\n            CountDownLatch lost = new CountDownLatch(1);\r\n            client.getConnectionStateListenable().addListener((client1, newState) -> {\r\n                if (newState == ConnectionState.LOST) {\r\n                    lost.countDown();\r\n                }\r\n            });\r\n            server.stop();\r\n            lost.await();\r\n        }\r\n        server.restart();\r\n        try (CuratorFramework client = CuratorFrameworkFactory.newClient(server.getConnectString(), sessionTimout, connectionTimout, new RetryNTimes(0, 1))) {\r\n            client.start();\r\n            client.blockUntilConnected();\r\n            final InterProcessSemaphoreV2 semaphore = new InterProcessSemaphoreV2(client, \"/1\", 1);\r\n            final int serverTick = ZooKeeperServer.DEFAULT_TICK_TIME;\r\n            Thread.sleep(sessionTimout + serverTick);\r\n            assertNotNull(semaphore.acquire(0, TimeUnit.SECONDS));\r\n        }\r\n    }\r\n\n\nAnd this is not passing: the doc of InterProcessSemaphoreV2 state that \n\"However, if the client session drops (crash, etc.), any leases held by the client are automatically closed and made available to other clients.\" \nmaybe I'm missing something obvious on the ZK server config instead.\nJust checked out that by running on separated processes the same test:\n\nstart server on process A\nstart lease acquire on process B, listening for LOST events before suicide\nrestart server on Process A cause process B to suicide (as expected)\nstart lease acquire on process C, now succeed\n\nIt seems that there is something going on in the intra-process case that's not working as expected (to me, at least).\nNOTE: as written in newer comments, raising the timeout doesn't seems to work too and different boxes are getting different outcomes (making this an intermittent failure).",
        "Issue Links": []
    },
    "CURATOR-596": {
        "Key": "CURATOR-596",
        "Summary": "Upgrade ZooKeeper to 3.7.1",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.4.0",
        "Component/s": "General",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "17/May/21 13:16",
        "Updated": "14/Jul/22 05:39",
        "Resolved": "14/Jul/22 05:39",
        "Description": "This is a prerequisite of\u00a0CURATOR-587",
        "Issue Links": []
    },
    "CURATOR-597": {
        "Key": "CURATOR-597",
        "Summary": "Background exception was not retry-able or retry gave up",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.4.0",
        "Component/s": "Framework",
        "Assignee": "Zili Chen",
        "Reporter": "Stan Henderson",
        "Created": "14/Jun/21 21:01",
        "Updated": "30/Aug/22 05:42",
        "Resolved": "30/Aug/22 05:42",
        "Description": "I see a similar issue as https://issues.apache.org/jira/browse/CURATOR-538 with curator-framework 5.1.0 where Zoo Servers are running in a docker container with CentOS Linux release 7.9.2009 (Core)\nIn the tests I'm running, it does not appear to occur all of the time and does not appear to cause anything to fail.\nserver.1=9.48.164.32:2888:3888:participant;0.0.0.0:2181\nserver.2=9.48.164.34:2888:3888:participant;0.0.0.0:2181\nserver.3=9.48.164.35:2888:3888:participant;0.0.0.0:2181\nserver.4=9.48.164.39:2888:3888:observer;0.0.0.0:2181\nserver.5=9.48.164.40:2888:3888:observer;0.0.0.0:2181\nserver.6=9.48.164.42:2888:3888:observer;0.0.0.0:218\nCompatibility.java:116: return (address != null) ? address.getAddress().getHostAddress() : \"unknown\";\n14:56:26.943 [main-EventThread] ERROR org.apache.curator.framework.imps.CuratorFrameworkImpl - Background exception was not retry-able or retry gave up\njava.lang.NullPointerException: null\nat org.apache.curator.utils.Compatibility.getHostAddress(Compatibility.java:116) ~[curator-client-5.1.0.jar:?]\nat org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:185) ~[curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:206) ~[curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) ~[curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:150) ~[curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:892) [curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:649) [curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [curator-framework-5.1.0.jar:5.1.0]\nat org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) [curator-framework-5.1.0.jar:5.1.0]\nat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:644) [zookeeper-3.6.3.jar:3.6.3]\nat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:563) [zookeeper-3.6.3.jar:3.6.3]\nI'm using ExponentialBackoffRetry(5000, 29, 60 * 1000 * 2) for the CuratorFrameworkFactory retry policy.\n/**\n@param baseSleepTimeMs initial amount of time to wait between retries\n@param maxRetries max number of times to retry\n@param maxSleepMs max time in ms to sleep on each retry\n*/\npublic ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs)",
        "Issue Links": [
            "/jira/browse/CURATOR-649"
        ]
    },
    "CURATOR-598": {
        "Key": "CURATOR-598",
        "Summary": "InterProcessSemaphoreV2::getData to check against data changes on the semaphore",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Francesco Nigro",
        "Created": "20/Jun/21 16:01",
        "Updated": "20/Jun/21 16:02",
        "Resolved": null,
        "Description": "InterProcessSemaphoreV2 allows holders to set data on the acquired lease: exposing an Optional<byte[]> getData would allow an observer to check against mutex state change despite not being the lock owner (assuming owner has used setData prior a mutex acquisition).\nThis can be used to weakly enforce order of mutex acquisition or just check owner's identity and/or presence (assuming owner setData prior a mutex acquisition).",
        "Issue Links": []
    },
    "CURATOR-599": {
        "Key": "CURATOR-599",
        "Summary": "Hanging indefinitely on some scenarios since zookeeper.request.timeout cannot be configured (add support for ZKClientConfig)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.0",
        "Component/s": "Client",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Liran Mendelovich",
        "Created": "29/Jun/21 12:56",
        "Updated": "19/Jul/21 22:01",
        "Resolved": "15/Jul/21 10:30",
        "Description": "On some executions where ZooKeeper server is not available, Curator client got waiting and hanging indefinitely, with thread dump stack trace which can be seen\nbelow.\nAs this is not reproduced consistently, it seems like a race condition from Curator/ZooKeeper client, since\u00a0zookeeper.request.timeout cannot be configured in Curator client.\nAs a work-around solution, initialization is executed in a separate thread in order to interrupt it if it hangs. This has been identified and handled here:\njoin_while_zookeeper_down_issue\nThe wanted solution is expose configuration to be able to configure\u00a0zookeeper.request.timeout, then it should wait until the request timeout, which is treated at\u00a0org.apache.zookeeper.ClientCnxn.submitRequest().\n\u00a0\nstacktrace size: 31\njava.lang.Object.wait(Native Method)\njava.lang.Object.wait(Object.java:502)\norg.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1561)\norg.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1533)\norg.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1834)\norg.apache.curator.framework.imps.CreateBuilderImpl$16.call(CreateBuilderImpl.java:1131)\norg.apache.curator.framework.imps.CreateBuilderImpl$16.call(CreateBuilderImpl.java:1113)\norg.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93)\norg.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1110)\norg.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:593)\norg.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:583)\norg.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:48)\norg.apache.curator.x.discovery.details.ServiceDiscoveryImpl.internalRegisterService(ServiceDiscoveryImpl.java:237)\norg.apache.curator.x.discovery.details.ServiceDiscoveryImpl.reRegisterServices(ServiceDiscoveryImpl.java:456)\norg.apache.curator.x.discovery.details.ServiceDiscoveryImpl.start(ServiceDiscoveryImpl.java:135)\n...",
        "Issue Links": []
    },
    "CURATOR-600": {
        "Key": "CURATOR-600",
        "Summary": "ModeledFramework.delete() doesn't use DeleteOptions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Daniel Nilsson",
        "Created": "30/Jun/21 07:47",
        "Updated": "10/Mar/23 12:16",
        "Resolved": "10/Mar/23 12:16",
        "Description": "ModeledFrameworkImpl.delete() does not pass modelSpec.deleteOptions() to the delete builder. So any delete options set in the ModelSpec, such as deletingChildrenIfNeeded, are not respected.",
        "Issue Links": []
    },
    "CURATOR-601": {
        "Key": "CURATOR-601",
        "Summary": "TestingCluster Threads leak with Zk 3.6.X",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.0.0,                                            5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Mario Uso Hernandez",
        "Created": "30/Jun/21 15:04",
        "Updated": "12/Aug/21 14:14",
        "Resolved": null,
        "Description": "We are testing Curator 5.1.0 with ZK 3.6.0 to 3.6.3 and found out our test cases are failing due to VM Crashing (Which is not happening with ZK 3.5.X). After further investigation, it seems some threads are being leaked after each test executed.\nWe are using TestingClusters of 1 and 3 members in this tests\u00a0\nAfter a single test of using 3 TestingCluster we can see this threads still alive:\n\n\r\nThreads after test is finished\r\nActive Threads: 14\r\n0: Thread[main,5,main]\r\n1: Thread[Monitor Ctrl-Break,5,main]\r\n2: Thread[Log4j2-TF-2-Scheduled-1,5,main]\r\n3: Session Sets (0)/(0):4: Thread[WorkerSender[myid=2],5,main]\r\n5: Thread[WorkerSender[myid=3],5,main]\r\n6: Thread[WorkerSender[myid=1],5,main]\r\n7: Thread[WorkerReceiver[myid=2],5,main]\r\n8: Thread[WorkerReceiver[myid=1],5,main]\r\n9: Thread[WorkerReceiver[myid=3],5,main]\r\n10: Session Sets (0)/(0):11: Session Sets (2)/(0):\r\n0 expire at Wed Jun 30 16:53:55 CEST 2021:\r\n0 expire at Wed Jun 30 16:53:58 CEST 2021:12: Thread[QuorumConnectionThread-[myid=2]-3,5,main]\r\n13: Thread[QuorumConnectionThread-[myid=1]-3,5,main]\r\n\n\nAfter some tests it scalates quickly to 200-300 theards that never ends until VM Crash.",
        "Issue Links": []
    },
    "CURATOR-602": {
        "Key": "CURATOR-602",
        "Summary": "A typo in a variable of TestGroupMember",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.0",
        "Component/s": "Tests",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Colin Kuo",
        "Created": "03/Jul/21 01:28",
        "Updated": "19/Jul/21 22:02",
        "Resolved": "05/Jul/21 09:36",
        "Description": "A typo in the variable name of TestGroupMember.java\n\ncurator-recipes/src/test/java/org/apache/curator/framework/recipes/nodes/TestGroupMember.java\n\n\n\r\nMap<String, byte[]> currentMembers1 = groupMember1.getCurrentMembers();\r\nMap<String, byte[]> currentMembers2 = groupMember2.getCurrentMembers();\r\nMap<String, String> convertMembers1 = Maps.transformValues(currentMembers1, new Function<byte[], String>()\r\n{\r\n    @Override\r\n    public String apply(byte[] input)\r\n    {\r\n        return new String(input);\r\n    }\r\n});\r\n//should be currentMembers2\r\nMap<String, String> convertMembers2 = Maps.transformValues(currentMembers1, new Function<byte[], String>()\r\n{\r\n    @Override\r\n    public String apply(byte[] input)\r\n    {\r\n        return new String(input);\r\n    }\r\n});\r\nassertEquals(convertMembers1.size(), 2);\r\nassertEquals(convertMembers2.size(), 2);\r\nassertEquals(convertMembers1, convertMembers2);",
        "Issue Links": []
    },
    "CURATOR-603": {
        "Key": "CURATOR-603",
        "Summary": "Use Awaitility to instead of Thread sleep method.",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Qiang Zhao",
        "Created": "13/Jul/21 06:26",
        "Updated": "08/Nov/22 08:41",
        "Resolved": "08/Nov/22 08:41",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-604": {
        "Key": "CURATOR-604",
        "Summary": "Double locking issue while using InterProcessMutex lock code",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.5.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": "Kezhu Wang",
        "Reporter": "Viswanathan Rajagopal",
        "Created": "29/Jul/21 07:59",
        "Updated": "23/Jun/23 04:38",
        "Resolved": null,
        "Description": "Recently came across \u201cDouble Locking Issue (i.e. two clients acquiring lock)\u201d using Curator code ( InterProcessMutex lock APIs ) in our application\nOur use case:\n\nTwo clients attempts to acquire the zookeeper lock using Curator InterProcessMutex and whoever owns it would release it once sees the connection disconnect ( on receiving Connection.SUSPENDED / Connection.LOST Curator Connection Events from Connection listener)\n\nIssue we noticed:\n\nAfter session expired & reconnected with new session, both client seems to have acquired the lock. Interesting thing that we found is that one of the clients still holds the lock while its lock node (ephemeral) was gone\n\nThings we found:\n\nBased on our initial analysis and few test runs, we saw that Curator acquire() method acquires the lock based on \u201cabout to be deleted lock node of previous session\u201d. Explanation : Ephemeral node created by previous session was \u00a0still seen by client that reconnected with new session id until server cleans that up. If this happens, Curator acquire() would hold the lock.\n\n\u00a0\n\nClearly we could see the race condition (in zookeeper code) between 1). Client reconnecting to server with new session id and 2). server deleting the ephemeral nodes of client\u2019s previous session.\n\n\u00a0\n\nOn the above mentioned race condition, if client manage to reconnect to server with new session id before server cleans up the ephemeral nodes of client\u2019s previous session, \u00a0Curator lock acquire() who is trying to acquire the lock will hold the lock as it still sees the lock node in zookeeper directory. Eventually server would be cleaning up the ephemeral nodes leaving the Curator local lock thread data stale giving the illusion that it still hold the lock while its ephemeral node is gone\n\n\u00a0\n\nTrying to categorize the timeline events below to see if that would help us for better understanding,\n\n[ CONNECTED ] :\n\n\n\nClient A & Client B calls acquire creating Node N1 & N2 respectively\nClient A acquire() -> holds the lock as its Node N1 is first node in sequence\nClient B acquire() -> created Node N2, watches for previous sequence node (blocked on wait())\n\u00a0\n\n\n\n[ SUSPENDED CONNECTION OCCURS ] :\n\n\n\nNetwork partition happens\nCurator fires SUSPENDED\nClient A, on receiving SUSPENDED event, attempts to release the lock\nClient B, on receiving SUSPENDED event, does nothing as it was not holding any lock (as blocked on acquire() -> wait() ) \u00a0\n\u00a0\n\n\n\n[ SESSION EXPIRED ] :\n\n\n\nClient A attempts to release the lock (with retries)\nClient B does nothing as it was not holding any lock (and is still blocked on acquire() -> wait() )\nServer prepared to cleanup previous client session and deleting lock nodes created as part of previous client session\n\u00a0\n\n\n\n[ RECONNECTED ] :\n\n\n\nClient A and Client B reconnected to another server with new session id (Note : This happens before server managed to cleanup / delete ephemeral nodes of previous client session)\nClient A released the lock successfully (means it would delete its lock node N1) and attempts to acquire lock by creating lock node N3 and watches for previous sequence node (N2)\nClient B who was blocked on acquire() -> wait() would be notified with previous sequence node (N1) deletion -> getChildren, sorting them, and seeing if the lock's node is the first node in the sequence. So, Client B sees its lock node N2 still (which I call it as about to be deleted node by server) and thereby acquires the lock\n\u00a0\n\n\n\n[ AFTER FEW SECONDS ] :\n\n\n\nServer managed to delete the ephemeral node N2 as part of previous client session cleanup\nClient A who was blocked on acquire() -> wait() would be notified with previous sequence node (N2) deletion -> getChildren, sorting them, and seeing if the lock's node is the first node in the sequence and thereby acquires the lock\nClient B \u2013 its local lock thread data went stale (as its lock path N2 not has been deleted by Server)**\n\n\n\n\u00a0\n\nCreated a test case simulator https://github.com/ViswaNXplore/curator/blob/curator-2.5.0/patch/double_locking_test_simulator/curator-recipes/src/test/java/org/apache/curator/framework/recipes/CuratorRecipeTest.java\u00a0\n\nApproach #1 (Artificial way of reproducing the issue)\n\n\n\nRun the test case with (SIMULATE_ISSUE_BY_EXPLICIT_DELETE set to true). This would delete the lock node after a pause just to simulate the state where the zkClient had reconnected and it still happens to see the ephermal node just before the server deletes it since its session has expired, but the node is deleted afterwards by the server.\nApproach #2 (A little manual interruption needed to reproduce the issue)\n\n\n\n\n\n\nRun the test case in Debug mode (SIMULATE_ISSUE_BY_EXPLICIT_DELETE set to false)\nArtificially delaying / pausing the ephemeral lock nodes deletion as part of session cleanup process in server code (ZookeeperServer.close() method)\nAfter a pause (say 5s) to make one of the instance to acquire the lock,\u00a0Artificially break the socket connection between client and server for 30s (by keeping breakpoint in ClientCnxnSocketNIO.doIO() method). After 30s, we would see session closing logs logged in server code\nAfter 1min, remove breakpoint in ClientCnxnSocketNIO.doIO() and resume both Thread 2 and Thread 3\nAfter that, resume server thread (thread name would be \u201cSessionTracker\u201d\n\n\n\nNote: \n\nWe are not certain that this race condition that we noticed in zookeeper code is intentional design.",
        "Issue Links": []
    },
    "CURATOR-605": {
        "Key": "CURATOR-605",
        "Summary": "DistributedQueue#processWithLockSafety can give a message to two consumers if first consumer session dies",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0421\u043e\u043a\u043e\u043b\u043e\u0432",
        "Created": "01/Aug/21 07:21",
        "Updated": "01/Aug/21 07:25",
        "Resolved": null,
        "Description": "The is similar to one described here:\nhttps://issues.apache.org/jira/browse/CURATOR-537\nhttps://issues.apache.org/jira/browse/CURATOR-542\nThere is no way for consumer to check if the lock still holds. It could be solved by sharing a transactional CheckOp for lockNodePath with a consumer, so he could consume the message transactionally.",
        "Issue Links": []
    },
    "CURATOR-606": {
        "Key": "CURATOR-606",
        "Summary": "ModeledFrameworkImpl.update() ignores version for uncompressed data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0,                                            5.2.0",
        "Fix Version/s": "5.2.1",
        "Component/s": "Client",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Matthew E. Dawson",
        "Created": "03/Aug/21 21:43",
        "Updated": "04/Oct/21 20:33",
        "Resolved": "04/Oct/21 20:33",
        "Description": "To reproduce:\n\nCreate a ZNode with uncompressed data through the mechanism of your choice, producing data version 0, then update that ZNode to produce data version 1.\nCreate a ModeledFrameworkImpl with default create/delete settings, then use the update(T, int) method with version 0.\nRead the ZNode with the mechanism of your choice, and notice that version 2 has been created.",
        "Issue Links": []
    },
    "CURATOR-607": {
        "Key": "CURATOR-607",
        "Summary": "Method getLockPath of InterProcessReadWriteLock locks is not exposed for use in transactions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.1",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0421\u043e\u043a\u043e\u043b\u043e\u0432",
        "Created": "04/Aug/21 11:39",
        "Updated": "03/Oct/21 20:07",
        "Resolved": "03/Oct/21 19:12",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-608": {
        "Key": "CURATOR-608",
        "Summary": "ZPath resolved state documentation improvements",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.2.1",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Ryan Ruel",
        "Created": "04/Aug/21 17:13",
        "Updated": "18/Aug/21 16:51",
        "Resolved": "18/Aug/21 15:30",
        "Description": "ZPaths support parameters, which are special path elements in parenthesis (i.e., {one}) that may be substituted with values at runtime.\nZPaths also have a resolved state, which checks if a ZPath parsed with `parseWithIds()` still contains any parameters left to be substituted.\nAdditionally, ZPaths may be created with `parse()` which contain string elements which appear to be parameters (elements in parenthesis) but are not treated as such, always resulting in a path which is considered fully-resolved.\nThe documentation for these methods should be updated to better indicate this behavior difference, clearly stating what \"resolved\" state indicates based on the method that generated the ZPath.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/395"
        ]
    },
    "CURATOR-609": {
        "Key": "CURATOR-609",
        "Summary": "ModeledCache attempts to deserialize empty ZNodes on deletion, resulting in exceptions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Framework",
        "Assignee": "Zili Chen",
        "Reporter": "Ryan Ruel",
        "Created": "05/Aug/21 15:35",
        "Updated": "12/Apr/23 11:38",
        "Resolved": "12/Apr/23 11:38",
        "Description": "When using the ModeledCache implementation, when a ZPath is deleted the implementation assumes that all ZNodes in the path contain a valid instance of the Model.\nIf intermediary paths have been created that do not contain a Model, the implementation is calling the Jackson deserialize method with null data, resulting in an exception.\nInstead, the code should ensure there is valid data present prior to attempting to deserialize and call the \"accept()\" (callback) method on action NODE_REMOVED.\nExample:\nTestModel child1 = new TestModel(\"d\", \"e\", \"f\", 1, BigInteger.ONE);\n ZPath path1 = path.child(\"foo\").child(\"bar\").child(\"child1\");\ntry (CachedModeledFramework<TestModel> client = ModeledFramework.wrap(async, modelSpec).cached())\n\n{ \u00a0 \u00a0CountDownLatch latch = new CountDownLatch(1); \u00a0 \u00a0client.listenable().addListener((t, p, s, m) -> latch.countDown()); \u00a0 \u00a0client.start(); \u00a0 \u00a0complete(client.withPath(path1).set(child1)); \u00a0 \u00a0assertTrue(timing.awaitLatch(latch)); \u00a0 \u00a0assertDoesNotThrow(() -> rawClient.delete().deletingChildrenIfNeeded().forPath(path.toString())); }\n\nAfter calling \"delete()\", an exception is thrown:\nERROR org.apache.curator.x.async.modeled.TestCachedModeledFramework$$Lambda$409/0x0000000800e0c850 Could not process cache message [Curator-SafeNotifyService-0]\n java.lang.IllegalArgumentException: argument \"src\" is null\n at com.fasterxml.jackson.databind.ObjectReader._assertNotNull(ObjectReader.java:2120)",
        "Issue Links": [
            "https://github.com/apache/curator/pull/396"
        ]
    },
    "CURATOR-610": {
        "Key": "CURATOR-610",
        "Summary": "Refactor CountCuratorWatcher in TestWatcherIdentity.java to improve test logic",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.1",
        "Component/s": "Framework",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Xiao Wang",
        "Created": "08/Aug/21 03:22",
        "Updated": "09/Aug/21 12:09",
        "Resolved": "09/Aug/21 12:08",
        "Description": "Description\nI noticed that there is a test class CountCuratorWatcher implements production interface curatorWatcher to assist testing production class CuratorFrameworkImpl. This might not be the best priactice in unit testing and can be improved by leveraging mocking frameworks.\nCurrent Implementation\n\n\u00a0CountCuratorWatcher implements CuratorWatcher and creates a new variable to keep tracking of the method invocation status for process(WatchedEvent).\nIn test cases, after executing test target, the new variable will be used in assertion statement to check the execution status of process(WatchedEvent).\n\nProposed Implementation\n\nReplace CountCuratorWatcher with a mocking object created by Mockito.\nExtract the AtomicLong attribute and use the extracted attribute to check method invocation status.\nUse method stub to control the behavior of the mocking object.\n\nMotivation\n\nDecouple test class `CountCuratorWatcher` from production interface `CuratorWatcher`.\nMake test logic more clear by using method stub instead of method overriding.",
        "Issue Links": []
    },
    "CURATOR-611": {
        "Key": "CURATOR-611",
        "Summary": "EnsembleTracker not appending the chroot node when setting the new connection string",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Framework",
        "Assignee": "Kezhu Wang",
        "Reporter": "Leonissimo",
        "Created": "18/Aug/21 11:48",
        "Updated": "05/May/23 07:56",
        "Resolved": "05/May/23 07:56",
        "Description": "The EnsembleTracker does not add the chroot node when setting the connection string upon receiving a new configuration event.\nThe new connection string is created by the method\u00a0 EnsembleTracker.configToConnectionString(QuorumVerifier data) as a comma separated list of <ip:port> for each zookeeper server in the QuorumVerfier, but\n the chroot node is not appended to the newly created connection string.\u00a0\nSo for example if the initial connection string is\u00a0\nzookeeper01:2080,zookeeper02:2080/config/prop\nwhen zookeeper03 joins the cluster, the client connection string is set to\n171.19.10.23.2080,171.19.10.24.2080,171.19.10.25.2080\nand \"/config/prop\" gets stripped out.\nThe problem manifests itself upon a re-connection to zookeeper, because the client will point to the wrong paths.",
        "Issue Links": [
            "/jira/browse/CURATOR-593",
            "/jira/browse/ZOOKEEPER-4565"
        ]
    },
    "CURATOR-612": {
        "Key": "CURATOR-612",
        "Summary": "Why CuratorFrameworkImpl#start do not throw exception when it's not retry-able",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Aitozi",
        "Created": "27/Aug/21 05:33",
        "Updated": "27/Aug/21 06:33",
        "Resolved": null,
        "Description": "Hi community, I'am new for curator. I have a question about CuratorFrameWorkImpl#start()\nI notice that when we start a client, But it may be failed by some not retry-able exception. But no exception is thrown, this may lead to some unexpected behavior. I'am looking forward to hearing some suggestion or some reason for this design. Thanks.",
        "Issue Links": []
    },
    "CURATOR-613": {
        "Key": "CURATOR-613",
        "Summary": "please implement setAcl Command -R option !!!",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "startjava",
        "Created": "03/Sep/21 04:55",
        "Updated": "17/May/23 11:13",
        "Resolved": "17/May/23 11:13",
        "Description": "please implement setAcl Command -R option !!!",
        "Issue Links": []
    },
    "CURATOR-614": {
        "Key": "CURATOR-614",
        "Summary": "curator5.2 support zookeeper watch recursion ??",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "startjava",
        "Created": "03/Sep/21 07:30",
        "Updated": "17/May/23 12:10",
        "Resolved": "17/May/23 12:10",
        "Description": "curator5.2 support zookeeper watch recursion ??",
        "Issue Links": []
    },
    "CURATOR-615": {
        "Key": "CURATOR-615",
        "Summary": "please implement LS Command -R option !!!",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "startjava",
        "Created": "04/Sep/21 06:58",
        "Updated": "17/May/23 12:04",
        "Resolved": "17/May/23 12:04",
        "Description": "please implement LS Command -R option !!!",
        "Issue Links": []
    },
    "CURATOR-616": {
        "Key": "CURATOR-616",
        "Summary": "Refactor DummyLayout in TestPropertiesConfiguration to improve test design",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "Xiao Wang",
        "Created": "08/Sep/21 01:53",
        "Updated": "08/Sep/21 06:48",
        "Resolved": "08/Sep/21 06:48",
        "Description": "Description\nI noticed that there is a test class DummyLayout implements production class PropertiesConfigurationLayout to assist testing method PropertiesConfiguration.propertyLoaded(String, String, Deque<URL>). This might not be the best priactice in unit testing and can be improved by leveraging mocking frameworks.\nCurrent Implementation\n\nDummyLayout extends PropertiesConfigurationLayout and creates a new variable to keep tracking of the method invocation status for load(PropertiesConfiguration, Reader).\nIn test cases, after executing PropertiesConfiguration.propertyLoaded(String, String, Deque<URL>), the new variable will be used in assertion statement to check the execution status of load(PropertiesConfiguration, Reader).\n\nProposed Implementation\n\nReplace DummyLayout with a mocking object created by Mockito.\nRemove the int variable that used to keep tracking the invocation times of load(PropertiesConfiguration, Reader).\nUse Mockito.verify() to check execution status of load(PropertiesConfiguration, Reader).\n\nMotivation\n\nDecouple test class DummyLayout from production class PropertiesConfigurationLayout.\nMake test logic more clear by removing the overridden method in DummyLayout.\nMake test condition more explict by directly using Mockito.verify() to verify method execution status.",
        "Issue Links": []
    },
    "CURATOR-617": {
        "Key": "CURATOR-617",
        "Summary": "UnregisterService Method in ServiceDiscoveryImpl is non-Idempotent in case of connection-failure",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Angad Singh",
        "Created": "20/Sep/21 08:01",
        "Updated": "19/Oct/21 04:41",
        "Resolved": null,
        "Description": "The unregisterService( ) method in ServiceDiscoveryImpl throws an unhandled KeeperException.ConnectionLossException the first time it is invoked while the server connection is down. The next time, however, the service has already been removed from the internal\u00a0services cache. Hence the internalUnregisterService function does not proceed since the entry is null the second time around.\nWe are not using the blockUntilConnected option, and instead are trying to implement a local retry mechanism. We are dependent upon the ConnectionLossException to be thrown, in order to catch and evaluate it before retrying with an exponential back-off.",
        "Issue Links": []
    },
    "CURATOR-618": {
        "Key": "CURATOR-618",
        "Summary": "Allow compression level to be configurable in the GzipCompressionProvider.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Benjamin Mahler",
        "Created": "05/Oct/21 18:39",
        "Updated": "05/Oct/21 18:39",
        "Resolved": null,
        "Description": "Currently the compression level is burned in as the default (-1):\nhttps://github.com/apache/curator/blob/ed3cc848f51b309db69f4e453d0509a48e9212d6/curator-framework/src/main/java/org/apache/curator/framework/imps/GzipCompressionProvider.java#L96\nIt would help in some cases to allow this to be configurable (e.g. when compression ratio matters most or when cpu consumption / latency matters most). Ideally this needs to be configurable at each compress call (which then needs to bubble up into the relevant higher level functions), rather than at construction time, since one gzip compression provider instance is serving all compression FWICT.",
        "Issue Links": []
    },
    "CURATOR-619": {
        "Key": "CURATOR-619",
        "Summary": "Replace OutstandingOps with JDK bundled Phaser",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.1",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "16/Oct/21 01:37",
        "Updated": "17/Oct/21 00:50",
        "Resolved": "16/Oct/21 11:35",
        "Description": "Back to the day we implemented CuratorCache, there is an outstanding discussion about OutstandingOps can be replaced with JDK bundled Phaser class so that we don't have to implement and maintain our own.\nSee also https://github.com/apache/curator/pull/335#discussion_r399591042\nThis is the very issue to follow this conversation.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/365"
        ]
    },
    "CURATOR-620": {
        "Key": "CURATOR-620",
        "Summary": "Double Leadership Issue while using Leader Latch Recipe",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.5.0,                                            5.2.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Viswanathan Rajagopal",
        "Created": "21/Oct/21 01:55",
        "Updated": "21/Oct/21 01:55",
        "Resolved": null,
        "Description": "While using Curator Leader Latch Recipe in our application, \u00a0we observed a potential issue where two clients have become a leader (Double Leadership Issue).\nQuick summary of below description\n\nOur use case explained\nIssue details\nTimeline of events mentioned\nAttached test code to reproduce the reported issue\nPossible solution given, where we need your suggestions\u00a0\n\nOur use case:\n\nTwo clients trying to get the leadership using Curator Leader Latch Recipe. On LeaderLatchListener.isLeader() Client would become a leader and on LeaderLatchListener.notLeader() Client would lose its leadership\n\nIssue details:\n\nOne of the clients on receiving two CuratorConnectionListener RECONNECTED events in quick succession, we observed that LeaderLatch EventThreads interleave with each other, resulting in \"latch node deletion\" happen after \"client becoming a leader\", thereby the client will still be a leader though its corresponding latch node has been deleted\nAnd the other client who tried to get leadership creates its latch node and sees itself in first index and thus become a leader\nSo at this point, two clients have become a leader\n\nTimeline of events:\n\nTimeline events of Client A whose corresponding latch node is deleted but still be a leader\n\t\nAt t1, 1st RECONNECTED event fired\nAt t2, [ EventThread of 1st RECONNECTED event ] Resets leadership (true -> false)\nAt t3, [ EventThread of 1st RECONNECTED event ] Fire \u201clistener.notLeader()\u201d\nAt t4, [ EventThread of 1st RECONNECTED event ] Deletes latch node\nAt t5, [ EventThread of 1st RECONNECTED event ] Creates new latch node\nAt t6, 2nd RECONNECTED event fired\nAt t7, [ EventThread of 2nd RECONNECTED event ] Resets leadership (false -> false), Basically NOP\nAt t8, [ EventThread of 2nd RECONNECTED event ] Fire nothing. Basically NOP\nAt t9, [ EventThread of 1st RECONNECTED event ] Get children -> sort them -> check leadership -> Set leadership to true -> Fire \u201cHas become a leader\u201d leader listener event\nAt t10, [ EventThread of 2nd RECONNECTED event ] Delete latch node (which actually deletes the latch node with which the Client A has become a leader through previous step)\n\n\n\n\nTimeline events of Client B who also become a leader\n\t\nAt t11, Client B creates its latch node -> Get children -> sort them -> check leadership -> Set leadership to true -> Fire \u201cHas become a leader\u201d leader listener event\n\n\n\nThis ends up in a situation where both Client A and Client B have become a leader\nAs we observe, over the period (t8 -> t10), Client A\u2019s LeaderLatch EventThreads interleave with each other causing leadership latch node deleted but local state still assumes that it\u2019s a leader\nReproducing the issue:\n\nWrote a Junit test case firing an artificial curator connection reconnected events and simulated LeaderLatch EventThreads interleave through CountDownLatches\nTest simulator for 2.5.0:\n\nhttps://github.com/ViswaNXplore/curator/commit/6a78a3a0de032212175d80caa64f140c743219ae\nhttps://github.com/ViswaNXplore/curator/commit/d2b1b33a6885c05619c058aa2bee63962fd6fa08\n\n\nTest Simulator for latest Curator version:\n\nhttps://github.com/ViswaNXplore/curator/commit/0949137f7323a1d5f34afc85a7042e8d9e85a8bc\nhttps://github.com/ViswaNXplore/curator/commit/1aadd4b5dbc8811a2e7a49b92f29170333e8ba4a\n\n\n\nPossible Solution (where we would like to hear your thoughts/suggestions):\n\nThe current curator code during reset() does\n\t\nsetLeadership(false) first followed by\nsetNode(null) (i.e. deleting its latch node)\n\n\n\n\nSwapping these two should resolve the issue, as we setting leadership to false once after its latch node gets deleted\n\t\nsetNode(null) (i.e. deleting its latch node) first followed by\nsetLeadership(false)",
        "Issue Links": []
    },
    "CURATOR-621": {
        "Key": "CURATOR-621",
        "Summary": "InterProcessReadWriteLock allows the write lock to be taken when the read one is not released",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "alex",
        "Created": "08/Nov/21 15:01",
        "Updated": "28/Apr/23 13:15",
        "Resolved": "01/Apr/23 15:00",
        "Description": "Hi,\nI have the following piece of code\n\u00a0\n\n\r\n01. rwLock.readLock().acquire();\r\n02.\r\n03. if (data needs to be refreshed) {\r\n04.  rwLock.readLock().release(); // releaseing read lock becore acquire write one\r\n05. \u00a0rwLock.writeLock().acquire();\r\n06.  if (data needs to be refreshed) { // check again\r\n07.     try {\r\n08.       // refresh data\r\n09. \u00a0 \u00a0 \u00a0 rwLock.readLock().acquire();\r\n10.     } finally {\r\n11.       rwLock.writeLock().release();\r\n12.     }\r\n13.  }\r\n14.  try {\r\n15.   // read data\r\n16.  } finally {\r\n17. \u00a0 \u00a0 \u00a0rwLock.readLock().release();\r\n18.  }\r\n19. }\r\n\n\nthis is the standard read/write lock syntax described here https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/locks/ReentrantReadWriteLock.html\nI noticed that when 2 processes run that code, if they both try to execute\n\n\r\n05. rwLock.writeLock().acquire();\r\n\n\nas expected only one goes through, but as soon the process that went through releases the write lock\n\n\r\n11. rwLock.writeLock().release();\r\n\n\nthen the other process is able to acquire it and goes to line 06! Even if the first process acquired a read lock just before:\n\n\r\n09. rwLock.readLock().acquire();\r\n\n\nI tested the same code using 2 threads on the same jvm rather than 2 processes and behaves the same.\nThen I tested 2 threads using the non distributed ReentrantReadWriteLock provided with java, and that one behaves correctly not allowing the second thread to lock on the write lock until the first thread releases its read lock.",
        "Issue Links": [
            "/jira/browse/CURATOR-276",
            "https://github.com/apache/curator/pull/445"
        ]
    },
    "CURATOR-622": {
        "Key": "CURATOR-622",
        "Summary": "Add Randomness to LeaderLatch Elections",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Tim Black",
        "Created": "13/Nov/21 18:29",
        "Updated": "14/Nov/21 22:03",
        "Resolved": null,
        "Description": "Currently, LeaderLatch uses EPHEMERAL_SEQUENTIAL nodes, with the next leader chosen by the lowest numbered node. In a multi-server environment where each server is a participant in multiple elections, the result is that the leader will always be the server that has been up the longest.(Or first to be restarted during a rolling restart)\nInstead of using sequentially numbered nodes, I propose instead that the node number for a new participant be created by adding a random number(From a constrained range) to the current leader number.(Defaults to zero) If a node with that number exists, repeat until an available node is found. After initial node creation, all other aspects of the leader election will remain unchanged.\nI have an implementation for this that I am testing locally and will submit a PR once the tests are complete.",
        "Issue Links": []
    },
    "CURATOR-623": {
        "Key": "CURATOR-623",
        "Summary": "DistributedQueue stops filling after long disconnect from cluster",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.3.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0421\u043e\u043a\u043e\u043b\u043e\u0432",
        "Created": "13/Nov/21 20:10",
        "Updated": "30/Jun/22 16:58",
        "Resolved": "22/May/22 01:59",
        "Description": "One of our VMs had network down for 12 minutes and after the network was up, the queues have stopped being filled by external processes as curator gave up on all watchers. Here is a test reproducing the issue:\n\n\r\nimport junit.framework.TestCase;\r\nimport org.apache.curator.ensemble.fixed.FixedEnsembleProvider;\r\nimport org.apache.curator.framework.CuratorFramework;\r\nimport org.apache.curator.framework.CuratorFrameworkFactory;\r\nimport org.apache.curator.framework.recipes.queue.DistributedQueue;\r\nimport org.apache.curator.framework.recipes.queue.QueueBuilder;\r\nimport org.apache.curator.framework.recipes.queue.QueueConsumer;\r\nimport org.apache.curator.framework.recipes.queue.QueueSerializer;\r\nimport org.apache.curator.framework.state.ConnectionState;\r\nimport org.apache.curator.retry.ExponentialBackoffRetry;\r\nimport org.apache.curator.test.TestingCluster;\r\n\r\nimport java.util.concurrent.CompletableFuture;\r\nimport java.util.concurrent.TimeUnit;\r\nimport java.util.function.Consumer;\r\n\r\npublic class DistributedQueueTest extends TestCase {\r\n    public void test() throws Exception {\r\n        final var done = new CompletableFuture<>();\r\n        try (\r\n            final var testingCluster = started(new TestingCluster(1));\r\n            final var dyingCuratorFramework = getCuratorFramework(testingCluster.getConnectString());\r\n            final var dyingQueue = newQueue(dyingCuratorFramework, item -> {\r\n                if (item.equals(\"0\")) {\r\n                    done.complete(null);\r\n                }\r\n            })\r\n        ) {\r\n            dyingQueue.start();\r\n            testingCluster.killServer(testingCluster.getInstances().iterator().next());\r\n            Thread.sleep(2 * 60_000);\r\n            testingCluster.restartServer(testingCluster.getInstances().iterator().next());\r\n            try (\r\n                final var aliveCuratorFramework = getCuratorFramework(testingCluster.getConnectString());\r\n                final var aliveQueue = newQueue(aliveCuratorFramework, __ -> {})\r\n            ) {\r\n                aliveQueue.start();\r\n                aliveQueue.put(\"0\");\r\n                done.get(1, TimeUnit.MINUTES);\r\n            }\r\n        }\r\n    }\r\n\r\n    private static DistributedQueue<String> newQueue(CuratorFramework curatorFramework, Consumer<String> consumer) {\r\n        curatorFramework.start();\r\n        return QueueBuilder.builder(\r\n            curatorFramework,\r\n            new QueueConsumer<String>() {\r\n                @Override\r\n                public void consumeMessage(String o) {\r\n                    consumer.accept(o);\r\n                }\r\n\r\n                @Override\r\n                public void stateChanged(CuratorFramework curatorFramework, ConnectionState connectionState) {\r\n                }\r\n            },\r\n            new QueueSerializer<>() {\r\n                @Override\r\n                public byte[] serialize(String item) {\r\n                    return item.getBytes();\r\n                }\r\n\r\n                @Override\r\n                public String deserialize(byte[] bytes) {\r\n                    return new String(bytes);\r\n                }\r\n            },\r\n            \"/MyChildrenCacheTest/queue\"\r\n        ).buildQueue();\r\n    }\r\n\r\n    private static TestingCluster started(TestingCluster testingCluster) throws Exception {\r\n        try {\r\n            testingCluster.start();\r\n            return testingCluster;\r\n        } catch (Throwable throwable) {\r\n            try (testingCluster) {\r\n                throw throwable;\r\n            }\r\n        }\r\n    }\r\n\r\n    private static CuratorFramework getCuratorFramework(String connectString) {\r\n        return CuratorFrameworkFactory.builder()\r\n            .ensembleProvider(new FixedEnsembleProvider(connectString, true))\r\n            .retryPolicy(new ExponentialBackoffRetry(1000, 3))\r\n            .build();\r\n    }\r\n}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/401"
        ]
    },
    "CURATOR-624": {
        "Key": "CURATOR-624",
        "Summary": "Migrate CI from Travis to GitHub actions",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Enrico Olivelli",
        "Created": "15/Dec/21 12:18",
        "Updated": "28/Apr/23 13:25",
        "Resolved": "30/Mar/22 12:07",
        "Description": "Travis is broken, we need to migrate CI somewhere else",
        "Issue Links": [
            "/jira/browse/CURATOR-413",
            "/jira/browse/CURATOR-634"
        ]
    },
    "CURATOR-625": {
        "Key": "CURATOR-625",
        "Summary": "Fix \"Cannot find a single highest directory for this project set. First two candidates directories don't share a common root\" build error",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.2.1",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Nicol\u00f3 Boschi",
        "Created": "15/Dec/21 14:45",
        "Updated": "28/Feb/22 13:42",
        "Resolved": "28/Feb/22 13:42",
        "Description": "When running mvn clean install -DskipTests on my Mac i get:\n\n\r\nFailed to execute goal org.commonjava.maven.plugins:directory-maven-plugin:0.1:highest-basedir (directories) on project apache-curator: Cannot find a single highest directory for this project set. First two candidates directories don't share a common root. -> [Help 1]\r\n\n\nUpgrading the plugin I'm able to compile",
        "Issue Links": []
    },
    "CURATOR-626": {
        "Key": "CURATOR-626",
        "Summary": "NullPointerException in watcher of nullNamespace",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Viktor Feklin",
        "Created": "30/Dec/21 05:34",
        "Updated": "2 days ago 03:50",
        "Resolved": null,
        "Description": "NPE processing lock released event:\n\n\r\njava.lang.NullPointerException: null\r\n\u00a0 \u00a0 at java.util.concurrent.CompletableFuture.screenExecutor(CompletableFuture.java:415)\r\n\u00a0 \u00a0 at java.util.concurrent.CompletableFuture.runAsync(CompletableFuture.java:1871)\r\n\u00a0 \u00a0 at org.apache.curator.framework.imps.CuratorFrameworkImpl.runSafe(CuratorFrameworkImpl.java:191)\r\n\u00a0 \u00a0 at org.apache.curator.framework.CuratorFramework.postSafeNotify(CuratorFramework.java:344)\r\n\u00a0 \u00a0 at org.apache.curator.framework.recipes.locks.LockInternals$2.process(LockInternals.java:69)\r\n\u00a0 \u00a0 at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:77)\r\n\u00a0 \u00a0 at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:535)\r\n\u00a0 \u00a0 at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510) \n\nSteps to reproduce:\n\n\r\nInterProcessMutex lock = new InterProcessMutex(curatorFramework.usingNamespace(null), path);\r\nlock.acquire();\n\nIf lock is holded by another process (at the moment of call) - we will never acquire it (after lock was released by holder) because notification event lost while processing on watcher.\nThe root cause of bug - is wrong init code of \u0421uratorFrameworkImpl:\n\n\r\npublic CuratorFrameworkImpl(CuratorFrameworkFactory.Builder builder)\r\n{\r\n    .... \r\n    failedDeleteManager = new FailedDeleteManager(this);\r\n    failedRemoveWatcherManager = new FailedRemoveWatchManager(this);\r\n    \r\n    // here you pass not fully initialized instance (this)\r\n    namespaceFacadeCache = new NamespaceFacadeCache(this);\r\n\r\n    ensembleTracker = zk34CompatibilityMode ? null : new EnsembleTracker(this, builder.getEnsembleProvider());\r\n\r\n    runSafeService = makeRunSafeService(builder);\r\n} \n\nIn NamespaceFacadeCache:\n\n\r\nNamespaceFacadeCache(CuratorFrameworkImpl client)\r\n{\r\n    this.client = client;\r\n    // here you create facade for null namespace based on not fully initialized client \r\n    nullNamespace = new NamespaceFacade(client, null);\r\n} \n\nNamespaceFacade - clones client fields, but not all fields initialized at this moment (ensembleTracker\u00a0 and runSafeService - are both nulls).\nSo then we use null namespace - we use this broken client and get NPE on access this null fields (se stacktrace).\nFix is very easy:\n\n\r\npublic CuratorFrameworkImpl(CuratorFrameworkFactory.Builder builder)\r\n{\r\n    ...\r\n    failedDeleteManager = new FailedDeleteManager(this);\r\n    failedRemoveWatcherManager = new FailedRemoveWatchManager(this);\r\n\r\n    ensembleTracker = zk34CompatibilityMode ? null : new EnsembleTracker(this, builder.getEnsembleProvider());\r\n\r\n    runSafeService = makeRunSafeService(builder);\r\n    \r\n    // initialization of cache should be the last operation in init method (all fields are initialized)\r\n    namespaceFacadeCache = new NamespaceFacadeCache(this);\r\n}",
        "Issue Links": []
    },
    "CURATOR-627": {
        "Key": "CURATOR-627",
        "Summary": "Curator remains in SUSPENDED state ignoring session timeout or network recovery",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.2.1,                                            5.3.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Bai Yu",
        "Created": "11/Jan/22 11:54",
        "Updated": "02/Apr/23 06:48",
        "Resolved": "02/Apr/23 06:48",
        "Description": "Description\nOur program encountered a problem where curator got stuck in the SUSPENDED state. It kept injecting \"session expire\" event, generating logs like this every 15s (same as session timeout):\nSession timeout has elapsed while SUSPENDED. Injecting a session expiration\nHowever, curator never transited to the LOST state after session timeout, nor transied to the RECONNECTED state upon network recovery. \n\u00a0\nAccording to logs containing \"zookeeper\" or \"curator\" (see attachment \"log4j.log\"), related events are as follows:\n(All events happened in date 2021.12.05)\n\n23:34:07,122: ZooKeeper not heard from server, thus curator transited to the SUSPENDED state.\n\n\n23:34:09,128: ZooKeeper opening socket to zk server.\n\n\n23:34:22,223: curator injected a \"session expiration\" event, but had not transited to the LOST state at that time; never transited in the future.\n\n\n23:34:22,885: ZooKeeper opened socket to zk server, and received reponse indicating session expiration; thus, the current ZooKeeper object was closed. However, another ZooKeeper object was not created as expected.\n\n\n23:34:37,224 and later: curator kept injecting \"session expiration\" events every session timeout (15s). The log was filled with lines like this: \"Session timeout has elapsed while SUSPENDED. Injecting a session expiration.\"\n\nIn summary, curator stayed in the SUSPENED state and never transited to the LOST or RECONNECTED state. Besides, the underlying ZooKeeper object was never recreated according to logs and the \"jstack\" command. For a more detailed analysis, please refer to attachment \"log analysis.md\".\n\u00a0\nThe problem above was encountered only once in our testing environment within months, and has never occurred in our production enviromnent. We failed to find out how to reproduce, but suspect there is a racing condition when these events happen simultaneously: while curator is injecting a \"session expire\" event, the underlying \"ClientCnxnSocketNIO\" has just reconnected to a zk server and got a \"session expire\" response.\nEnvironment\nOS: Linux version 4.17.3-1.el7.elrepo.x86_64\n\u00a0\nProject type: maven\nCurator version: 4.2.0\n\u00a0\nSession timeout: 15s, which is equal to that negotiated with the server.\nRetry policy: no retry, where RetryPolicy#allowRetry() always returns false. That's because we handle retry at application level. Write operations should NOT be retried immediately after reconnected, until some extra validations pass.\n\u00a0\nOur program creates exactly one \"CuratorFramework\" instance connecting to the only zk server in the testing environment. The server's connection string is \"6x.xx.xx.27:2181\", with some digits consealed for security. Ip addresses in logs are also such processed. \n\u00a0\nZkClient is imported for forward compatiblility with some legacy codes (still dependent on ZkClient), but it is bridged to curator instead of connecting to zk server itself. For more details about dependencies, please refer to file \"pom.xml\" in attachments.\nAttachments\nlog4j.log: log of our program, with ip addresses and zk paths consealed.\nlog analysis.md: a more detailed analysis about the logs.\npom.xml: part of file \"pom.xml\", showing zookeeper-related dependencies.",
        "Issue Links": [
            "/jira/browse/CURATOR-561"
        ]
    },
    "CURATOR-628": {
        "Key": "CURATOR-628",
        "Summary": "Background operation throw unknown host exception when ConnectionState instance invoke reset method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.12.0",
        "Fix Version/s": "4.2.0",
        "Component/s": "Client",
        "Assignee": null,
        "Reporter": "ruilonghe",
        "Created": "13/Jan/22 16:32",
        "Updated": "15/Jan/22 02:11",
        "Resolved": "15/Jan/22 02:11",
        "Description": "CuratorFrameworkImpl class's performBackgroundOperation method will invoke client.getZookeeper() method.\nwhen elapsed time exceeds sessionTimeout, ConnectionState will invoke reset().\nduiring reset, if new Zookeeper() method throw\u00a0 UnknownHostException in new StaticHostProvider method.\nthen background operation will gave up and client will not retry\u00a0 connecting any more.\n\n\u00a0\nPlease check the log.",
        "Issue Links": []
    },
    "CURATOR-629": {
        "Key": "CURATOR-629",
        "Summary": "curator 5.2.0 recipes : client event notify service is single thread",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "L-Job",
        "Created": "24/Jan/22 04:37",
        "Updated": "10/May/23 16:06",
        "Resolved": "10/May/23 16:06",
        "Description": "when I try to create a new node in zookeeper in a event, to trigger another event. The original event takes\u00a0 a long time to end , and then find the new event what I expected is never hadppend.\nI found the thread[NotifyService-0] is only thread when client try to notify the event happended, I think this is not the elegant way to notify",
        "Issue Links": []
    },
    "CURATOR-630": {
        "Key": "CURATOR-630",
        "Summary": "Upgrade jetty to 9.4.latest",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "None",
        "Component/s": "General",
        "Assignee": null,
        "Reporter": "Tamas Penzes",
        "Created": "15/Feb/22 17:35",
        "Updated": "30/Mar/22 13:16",
        "Resolved": "30/Mar/22 13:16",
        "Description": "Curator is currently pulling in 6.1.26. This is a very old version and is EOL.\nPlease upgrade to 9.4.latest.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/413"
        ]
    },
    "CURATOR-631": {
        "Key": "CURATOR-631",
        "Summary": "Upgrade Jersey to 2.35 or later and upgrade resteasy-jaxrs to a newer and compatible version",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "D\u00f3ra Horv\u00e1th",
        "Created": "16/Feb/22 14:26",
        "Updated": "30/Jun/22 12:56",
        "Resolved": null,
        "Description": "Curator is pulling in resteasy-jaxrs 2.3.5 which is affected by multiple CVEs inlcuding CVE-2016-9606 and CVE-2014-3490.\u00a0\n2.3.5 is also deprecated and needs to be upgraded.\u00a0\nCurator is also pulling jersey 1.19.4 which is an old version and needs to be upgraded to 2.35 or later (3.0.4).\nresteasy-jaxrs dependency cannot be higher than 2.x for compatibility with Jersey 1.x, this is why they need to be upgraded together.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/424"
        ]
    },
    "CURATOR-632": {
        "Key": "CURATOR-632",
        "Summary": "Update Maven plugins",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "5.2.1,                                            5.3.0",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "25/Feb/22 12:55",
        "Updated": "02/Apr/23 06:18",
        "Resolved": "02/Apr/23 06:18",
        "Description": "Several Maven plugins are outdated and could be updated.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/408"
        ]
    },
    "CURATOR-633": {
        "Key": "CURATOR-633",
        "Summary": "Run org.apache.curator.framework.state.TestConnectionStateManager#testConnectionStateRecoversFromUnexpectedExpiredConnection only for Zookeeper 3.6.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "25/Feb/22 13:16",
        "Updated": "27/Feb/22 08:43",
        "Resolved": "27/Feb/22 08:43",
        "Description": "The build is broken by CURATOR-561:\n\n\r\n [INFO] Running org.apache.curator.framework.state.TestConnectionStateManager\r\n[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.387 s <<< FAILURE! - in org.apache.curator.framework.state.TestConnectionStateManager\r\n[ERROR] org.apache.curator.framework.state.TestConnectionStateManager.testConnectionStateRecoversFromUnexpectedExpiredConnection \u00a0Time elapsed: 0.489 s \u00a0<<< ERROR!\r\njava.lang.NoSuchMethodError: org.apache.zookeeper.Testable.queueEvent(Lorg/apache/zookeeper/WatchedEvent;)V\r\n\u00a0 \u00a0 at org.apache.curator.framework.state.TestConnectionStateManager.testConnectionStateRecoversFromUnexpectedExpiredConnection(TestConnectionStateManager.java:110)",
        "Issue Links": [
            "/jira/browse/CURATOR-561"
        ]
    },
    "CURATOR-634": {
        "Key": "CURATOR-634",
        "Summary": "Re-introduce TravisCI for testing on Linux ARM64",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Martin Tzvetanov Grigorov",
        "Created": "28/Feb/22 09:13",
        "Updated": "28/Apr/23 13:30",
        "Resolved": null,
        "Description": "With CURATOR-624 the project moves from TravisCI to Github Actions.\nI'd like to ask whether TravisCI can still be used for testing on (Linux) ARM64 CPU architecture.\nCurator is a Java project so it should work on any CPU architecture supported by the JVM, but it has dependencies on libraries which use native code, e.g. snappy-java.\nAnother problem which I found recently was with org.commonjava.maven.plugins:directory-maven-plugin. Because of it one cannot build on Linux and Mac ARM64. See https://github.com/apache/curator/pull/402\nAt the moment Apache Kyuubi (incubating) cannot be used on Linux ARM64 due to Curator. They use quite an old version though - 2.12.0",
        "Issue Links": [
            "/jira/browse/CURATOR-624",
            "https://github.com/apache/curator/pull/412"
        ]
    },
    "CURATOR-635": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-636": {
        "Key": "CURATOR-636",
        "Summary": "Make PersistentTtlNode initialDelay more configurable",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Lim Yang Wei",
        "Created": "31/Mar/22 03:17",
        "Updated": "31/Mar/22 03:17",
        "Resolved": null,
        "Description": "Currently when PersistentTtlNode starts, it will\n\ncreate a CONTAINER node\ncreate a CHILD node `_touch`\n\nBut there is a initialDelay between 1 and 2. If the initialDelay is long enough and the client is disconnected, 1 will not be deleted forever (because CONTAINER node only delete its parent when all its child is gone, but in this case, it never has any of them).\nThe initialDelay could be configured, but not so flexible. The initialDelay is calculated by the formula `ttl/touchScheduleFactor`, but this formula is also used to how frequent we want to ping Zookeeper to keep our znodes alive, aka period.\nAs you can see here (2nd parameter is initialDelay and 3rd parameter is for period)\n\n\n\n// PersistentTtlNode.java\nFuture<?> future = executorService.scheduleAtFixedRate(touchTask, ttlMs / touchScheduleFactor, ttlMs / touchScheduleFactor, TimeUnit.MILLISECONDS);\n\u00a0\n// Interface ScheduledExecutorService\nScheduledFuture<?>\u00a0scheduleAtFixedRate(Runnable\u00a0command, long\u00a0initialDelay, long\u00a0period, TimeUnit unit)\n\n\n\n\n\nLet's say we have ttl of 5 minutes. We want to have 0 initialDelay so we will set the touchScheduleFactor to 5 minutes too. This will give us 1ms (5 minutes/5 minutes) of initialDelay (which solves our problem) but spamming Zookeeper every second.\nSo it would be nice if we can set `initialDelay`",
        "Issue Links": []
    },
    "CURATOR-637": {
        "Key": "CURATOR-637",
        "Summary": "Get Leader zNode actual path from LeaderSelector",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.2.1",
        "Fix Version/s": "TBD",
        "Component/s": "Recipes",
        "Assignee": "Kezhu Wang",
        "Reporter": "Robin Singh",
        "Created": "02/May/22 03:15",
        "Updated": "28/Apr/23 13:32",
        "Resolved": null,
        "Description": "We are exploring LeaderSelector to implement leader election in our spring application. To implement fencing, we have to fetch all the participating zNodes in the leader election and then match the leaderSelector Id with each child's data.\n\u00a0\nThis code is being executed in a Timer.\n\u00a0\n\n\r\nif (isLeader.get()) {\r\n     try {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 curator.createContainers(\"/spring-integration/fence\");\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Stat stat = curator.checkExists().forPath(\"/spring-integration/leader\");\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (null != stat) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0List<String> allChilds = curator.getChildren().forPath(\"/spring-integration/leader\");\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  if (allChilds != null && !allChilds.isEmpty()) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String myChildPath = null;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (String child : allChilds) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  String childPath = \"/spring-integration/leader/\" + child;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  try {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String childId = new String(curator.getData().forPath(childPath), \"UTF-8\");\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (myId.equals(childId)){\r\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 myChildPath = childPath; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n                                   break; \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n                              }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0} catch (Exception e){\r\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 //ignore \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n                         }\r\n  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0if (null != myChildPath){\r\n                   curator.transaction()\r\n                   .forOperations(curator.transactionOp().setData()\r\n                   .forPath(\"/spring-integration/fence\",new String(\"\" + counter.incrementAndGet()).getBytes()), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0curator.transactionOp().check().forPath(myChildPath)); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n               }\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0  String stringData = new String(curator.getData().forPath(\"/spring-integration/fence\"));\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0  System.out.println(\"SAVED DATA: \" + stringData);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\r\n\u00a0 \u00a0 \u00a0 \u00a0} catch (Exception e){ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n            e.printStackTrace(); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n       }\r\n} else{ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n   System.out.println(\"I am Not the leader, I cannot write anything\");\r\n   leaderSelector.requeue(); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n}\r\n\n\n\u00a0\nAutoRequeue is disabled, we are requeing leader selection if isLeader is false.\n\u00a0\nThis is the leaderSelectorListener.\n\n\r\n \r\n@Override\r\npublic void takeLeadership(CuratorFramework client) throws Exception {\r\n\u00a0  try{ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  \r\n       System.out.println(\"I AM THE LEADER\"); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n       isLeader.set(true); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n       Thread.sleep(Long.MAX_VALUE); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n   } catch (@SuppressWarnings(\"unused\") InterruptedException e){ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n   } finally{ \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n        System.out.println(\"I AM NOT THE LEADER ANYMORE\"); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n        myId = generateId(); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n        leaderSelector.setId(myId); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n        isLeader.set(false); \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \r\n    }\r\n}\r\n\n\n\u00a0\nGenerating a new ID so that when requeing for leaderElection, a new node will be generated with a different ID.\nQuestion 1. Do I have to re generate ID so that when requeing for leaderElection a new zNode will be created using that new ID? Can I assume that in all the scenarios the zNode will always be deleted and then only a new leader will be elected from the participants?\n\u00a0\nImprovement 1. If only we could get the zNode path for our application using leaderSelection.getCurrentPath() as InterProcessMutex stores that path in threadData map the code for fencing would be cleaned up.\n\u00a0\nI was checking some issues related to fencing and leader based dataStore. And from there I found this document https://docs.google.com/document/d/1cBY1t0k5g1xNqzyfZby3LcPu4t-wpx57G1xf-nmWrCo/edit#",
        "Issue Links": []
    },
    "CURATOR-638": {
        "Key": "CURATOR-638",
        "Summary": "Curator disconnect from zookeeper when IPs change",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.1",
        "Fix Version/s": "5.4.0",
        "Component/s": "Client,                                            Recipes",
        "Assignee": "Zili Chen",
        "Reporter": "Francis Simon",
        "Created": "02/May/22 19:18",
        "Updated": "02/Apr/23 05:16",
        "Resolved": "17/Jul/22 08:04",
        "Description": "Blocking usage of Zookeeper in production. \u00a0 Tried testing a few versions all had the issue. \u00a0Effects any recipes that use ephemeral nodes. \u00a0Example attached.\nWe use multiple Apache Curator recipes in our system which is running in Docker and Kubernetes. \u00a0 The behavior I am seeing is that curator appears to resolve to the IP address of the containers rather than being tied to DNS names. \u00a0 I have seen old tickets on this, but the behavior is reproducible on the latest code release. \u00a0 \u00a0\nWe are running zookeeper in containers on kubernetes. \u00a0In kubernetes many things could cause a container to move hosts, the pod disruption budget ensures that a quorum is always present. \u00a0 But with this bug if all nodes move for any reason and get new IP addresses clients will disconnect when they shouldn't. \u00a0Disconnecting has the bad side effect that all ephemeral nodes are lost. \u00a0 This effects for us coordination, distributed locking and service discovery. \u00a0 Causes production downtime so marked as a Blocker.\nI have a simple sample which just uses the service discovery recipe to register a bunch of services in zookeeper. \u00a0I run the example in docker compose. \u00a0 It is 100% reproducible.\n\u00a0\n\n\r\n# Standup zookeeper and wait for it to be healthy\r\ndocker-compose up -d zookeeper1 zookeeper2 zookeeper3 \r\n\r\n# Stand up a server and make sure it is connected and working as expected\r\ndocker-compose up -d server1\r\n\r\n# Take down a single zookeeper node and stand up another agent.\r\n# The agent will grab the old zookeepers IP address\r\ndocker-compose rm -s zookeeper1\r\ndocker-compose up -d server2\r\n\r\n# Bring the zookeeper node back up.  \r\n# Wait for it be healthy\r\ndocker-compose up -d zookeeper1\r\n\r\n# Then take down the next zookeeper node and stand up another agent.\r\n# The agent will grab the old zookeepers IP address\r\ndocker-compose rm -s zookeeper2\r\ndocker-compose up -d server3\r\n\r\n# Bring the zookeeper node back up. \r\n# Wait for it be healthy\r\ndocker-compose up -d zookeeper2\r\n\r\n# Then take down the next zookeeper node and stand up another agent.\r\n# The agent will grab the old zookeepers IP address\r\ndocker-compose rm -s zookeeper3 \r\ndocker-compose up -d server4 \r\n\r\n# Bring the zookeeper node back up. \r\n# Wait for it be healthy \r\ndocker-compose up -d zookeeper3\n\n\u00a0\nAt the time of taking down the 3rd zookeeper node, the first server1 that was stood up will now receive a disconnected status because the IP of all three nodes has no changes form the original IP addresses. \u00a0\n\u00a0\n\n\r\nserver1_1 \u00a0 \u00a0 | Query instances for servicetest\r\nserver1_1 \u00a0 \u00a0 | Exception in thread \"main\" java.lang.RuntimeException: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /myservices/test/62e23a0b-dfdb-46f5-966f-8dc7a4978c70\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.shaded.com.google.common.base.Throwables.propagate(Throwables.java:241)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.utils.ExceptionAccumulator.propagate(ExceptionAccumulator.java:38)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.x.discovery.details.ServiceDiscoveryImpl.close(ServiceDiscoveryImpl.java:171)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.shaded.com.google.common.io.Closeables.close(Closeables.java:78)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.utils.CloseableUtils.closeQuietly(CloseableUtils.java:59)\r\nserver1_1 \u00a0 \u00a0 | \tat zkissue.App.main(App.java:72)\r\nserver1_1 \u00a0 \u00a0 | Caused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /myservices/test/62e23a0b-dfdb-46f5-966f-8dc7a4978c70\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.zookeeper.KeeperException.create(KeeperException.java:102)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.zookeeper.KeeperException.create(KeeperException.java:54)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:2001)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:274)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:268)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:265)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:249)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:34)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.x.discovery.details.ServiceDiscoveryImpl.internalUnregisterService(ServiceDiscoveryImpl.java:520)\r\nserver1_1 \u00a0 \u00a0 | \tat org.apache.curator.x.discovery.details.ServiceDiscoveryImpl.close(ServiceDiscoveryImpl.java:157)\r\nserver1_1 \u00a0 \u00a0 | \t... 3 more\r\n\n\n\u00a0\n\u00a0This causes it to disconnect and lose its discovery state which can be seen from the other services.\n\n\r\nserver2_1 \u00a0 \u00a0 | Query instances for servicetest\r\nserver2_1 \u00a0 \u00a0 | test\r\nserver2_1 \u00a0 \u00a0 | \tservice description: http://server-4:57456\r\nserver2_1 \u00a0 \u00a0 | \tservice description: http://server-3:37740\r\nserver2_1 \u00a0 \u00a0 | \tservice description: http://server-2:40219\n\n\u00a0\nShould mention that the Zookeeper cluster is always happy and healthy. \u00a0 This is a client side issue.",
        "Issue Links": [
            "/jira/browse/CURATOR-578",
            "/jira/browse/CURATOR-664",
            "https://github.com/apache/curator/pull/425",
            "https://github.com/apache/curator/pull/452"
        ]
    },
    "CURATOR-639": {
        "Key": "CURATOR-639",
        "Summary": "Session Expired Race Condition",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "Shawn Weeks",
        "Created": "16/May/22 14:02",
        "Updated": "16/May/22 14:02",
        "Resolved": null,
        "Description": "It appears that the fix in CURATOR-561 doesn't fully resolve the issue the NiFi project was seeing. Instead of the error message seen in that issue you get a session expired message instead but the behavior is the same. Curator never reconnects to Zookeeper.",
        "Issue Links": []
    },
    "CURATOR-640": {
        "Key": "CURATOR-640",
        "Summary": "IllegalThreadStateException thrown when access PathChildrenCache in different thread groups",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.1",
        "Fix Version/s": "5.3.0",
        "Component/s": "Recipes",
        "Assignee": "Zili Chen",
        "Reporter": "Xiaogang Shi",
        "Created": "27/May/22 15:17",
        "Updated": "18/Jul/22 15:46",
        "Resolved": "17/Jun/22 08:01",
        "Description": "When we create PathChildrenCache instances without specifying thead factory, the defaultThreadFactory will be used to create threads. The defaultThreadFactory is typically backed by java.util.concurrent.DefaultThreadFactory which will add newly created threads into the thread group where it is instantiated. That is, the defaultThreadFactory will always add newly created threads into the first thread group that loads the PathChildrenCache class.\nIn cases where PathChildrenCache is accessed in different thread groups, if the first thread group that load the PathChildrenCache is destroy, then all attempts to start the PathChildrenCache will fail.\nThe problem can be reproduced using the following code: \n\n\r\nThreadGroup threadGroup1 = new ThreadGroup(\"testGroup1\");\r\nThread thread1 = new Thread(threadGroup1, () -> {\r\n        PathChildrenCache cache1 =\r\n                new PathChildrenCache(client, \"/test1\", true);\r\n        try {\r\n            cache1.start();\r\n            Thread.sleep(1000);\r\n        } catch (Throwable t) {\r\n            t.printStackTrace();\r\n        } finally {\r\n            try {\r\n                cache1.close();\r\n            } catch (IOException e) {\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n});\r\n\r\nthread1.start();\r\nthread1.join();\r\n\r\n// After the thread group is destroyed, all PathChildrenCache instances \r\n// will fail to start due to inability to add new threads into the first thread group\r\nthreadGroup1.destroy();\r\n\r\nThreadGroup threadGroup2 = new ThreadGroup(\"testGroup2\");\r\nThread thread2 = new Thread(threadGroup2, () -> {\r\n        PathChildrenCache cache2 =\r\n            new PathChildrenCache(client, \"/test1\", true);\r\n        try {\r\n            cache2.start();\r\n            Thread.sleep(1000);\r\n        } catch (Throwable t) {\r\n            t.printStackTrace();\r\n        } finally {\r\n            try {\r\n                cache2.close();\r\n            } catch (IOException e) {\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n});\r\n\r\nthread2.start();\r\nthread2.join();\r\n\n\nWhen starting cache2, the following exception is thrown:\n\n\r\njava.lang.IllegalThreadStateException\r\n\tat java.base/java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:892)\r\n\tat java.base/java.lang.Thread.<init>(Thread.java:434)\r\n\tat java.base/java.lang.Thread.<init>(Thread.java:708)\r\n\tat java.base/java.util.concurrent.Executors$DefaultThreadFactory.newThread(Executors.java:660)\r\n\tat org.apache.curator.shaded.com.google.common.util.concurrent.ThreadFactoryBuilder$1.newThread(ThreadFactoryBuilder.java:163)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:920)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1353)\r\n\tat java.base/java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:721)\r\n\tat org.apache.curator.utils.CloseableExecutorService.submit(CloseableExecutorService.java:191)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.submitToExecutor(PathChildrenCache.java:841)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.offerOperation(PathChildrenCache.java:792)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.start(PathChildrenCache.java:300)\r\n\tat org.apache.curator.framework.recipes.cache.PathChildrenCache.start(PathChildrenCache.java:239)",
        "Issue Links": []
    },
    "CURATOR-641": {
        "Key": "CURATOR-641",
        "Summary": "\u6b7b\u9501  deadlock",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "5.2.1",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "ysz",
        "Created": "30/May/22 05:25",
        "Updated": "26/Jun/22 13:53",
        "Resolved": "26/Jun/22 13:53",
        "Description": "describe:\nmaybe deadlock\n\u00a0\nquestion:\nthe question in org.apache.curator.framework.recipes.locks.LockInternals#attemptLock\nand\u00a0\nthe question in org.apache.curator.framework.recipes.locks.LockInternals#watcher\n\u00a0\ndetail\nWhen the method named watcher is triggered\u00a0\nBut the wait method in the method named attemptLock has not yet been executed\nthen Will deadlock\n\u00a0\n\u00a0The solution is attached",
        "Issue Links": []
    },
    "CURATOR-642": {
        "Key": "CURATOR-642",
        "Summary": "Upgrade Guava to 31.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "David Handermann",
        "Created": "03/Jun/22 16:47",
        "Updated": "13/Jul/22 01:38",
        "Resolved": "13/Jul/22 01:38",
        "Description": "Google Guava 31.1 includes a number of improvements and fixes over version 27.0.1, including a resolution for CVE-2020-8908 related to temporary directory creation.",
        "Issue Links": []
    },
    "CURATOR-643": {
        "Key": "CURATOR-643",
        "Summary": "Adds possibility to disable automatic parent creation for PersistentTtlNode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.1",
        "Fix Version/s": "5.3.0",
        "Component/s": "Recipes",
        "Assignee": "Zili Chen",
        "Reporter": "Boutes Paul",
        "Created": "15/Jun/22 09:11",
        "Updated": "26/Jun/22 12:36",
        "Resolved": "26/Jun/22 12:36",
        "Description": "Since CURATOR-590 has been resolved, we should be able now to configure PersistentTtlNode in order to turn parent creation off for the underlying PersistentNode.",
        "Issue Links": []
    },
    "CURATOR-644": {
        "Key": "CURATOR-644",
        "Summary": "CLONE - Race conditions in LeaderLatch after reconnecting to ensemble",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "4.2.0",
        "Fix Version/s": "5.4.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Ken Huang",
        "Created": "23/Jun/22 20:24",
        "Updated": "15/Mar/23 02:37",
        "Resolved": "27/Sep/22 03:13",
        "Description": "Clone from CURATOR-504.\nWe use LeaderLatch in a lot of places in our system and when ZooKeeper ensemble is unstable and clients are reconnecting to logs are full of messages like the following:\n[2017-08-31 19:18:34,562][ERROR][org.apache.curator.framework.recipes.leader.LeaderLatch] Can't find our node. Resetting. Index: -1 {}\nAccording to the implementation, this can happen in two cases:\n\nWhen internal state `ourPath` is null\nWhen the list of latches does not have the expected one.\n\nI believe we hit the first condition because of races that occur after client reconnects to ZooKeeper.\n\nClient reconnects to ZooKeeper and LeaderLatch gets the event and calls reset method which set the internal state (`ourPath`) to null, removes old latch and creates a new one. This happens in thread \"Curator-ConnectionStateManager-0\".\nAlmost simultaneously, LeaderLatch gets another even NodeDeleted (here) and tries to re-read the list of latches and check leadership. This happens in the thread \"main-EventThread\".\n\nTherefore, sometimes there is a situation when method `checkLeadership` is called when `ourPath` is null.",
        "Issue Links": [
            "/jira/browse/CURATOR-504",
            "/jira/browse/CURATOR-505",
            "/jira/browse/FLINK-29173"
        ]
    },
    "CURATOR-645": {
        "Key": "CURATOR-645",
        "Summary": "LeaderLatch generates infinite loop with two LeaderLatch instances competing for the leadership",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "5.4.0",
        "Component/s": "Recipes",
        "Assignee": "Zili Chen",
        "Reporter": "Matthias Pohl",
        "Created": "30/Jun/22 13:48",
        "Updated": "27/Sep/22 05:40",
        "Resolved": "27/Sep/22 03:14",
        "Description": "We experienced a strange behavior of the LeaderLatch in a test case in Apache Flink (see FLINK-28078) where two LeaderLatch instances are competing for the leadership resulting in an infinite loop.\nThe test includes three instances of a wrapper class that has a LeaderLatch as a member. This is about ZooKeeperMultipleComponentLeaderElectionDriverTest::testLeaderElectionWithMultipleDrivers. In the test, the first LeaderLatch acquires the leadership, which results in the LeaderLatch to be closed and, as a consequence, losing the leadership. The odd thing now is that the two left-over LeaderLatch instances end up in a infinite loop as shown in the ZooKeeper server logs:\n\n\r\n16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch\r\n16:17:07,864 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:getChildren2 cxid:0x21 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch\r\n16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a\r\n16:17:07,866 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:delete cxid:0x22 zxid:0xc txntype:2 reqpath:n/a\r\n16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a\r\n16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x100cf6d9cf60000 type:create2 cxid:0x23 zxid:0xd txntype:15 reqpath:n/a\r\n16:17:07,869 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - Processing request:: sessionid:0x100cf6d9cf60000 type:getData cxid:0x24 zxid:0xfffffffffffffffe txntype:unknown reqpath:/flink/default/latch/_c_6eb174e9-bb77-4a73-9604-531242c11c0e-latch-0000000001\r\n\n\nIt looks like the close call of the LeaderLatch with the initial leadership is in some kind of race condition deleting the corresponding ZNode and the watcher triggering reset() for the left-over LeaderLatch instances instead of retrieving the left-over children:\n\nThe\u00a0reset()\u00a0triggers\u00a0getChildren\u00a0through the\u00a0LeaderLatch#getChildren\u00a0after a new child is created (I would assume\u00a0create2\u00a0entry in the logs before\u00a0getChildren\u00a0entry which is not the case; so, I might be wrong in my observation)\nThe callback of\u00a0getChildren\u00a0triggers\u00a0checkLeadership.\nIn the meantime, the predecessor gets deleted (I'd assume because of the deterministic ordering of the events in ZK). This causes the\u00a0callback in checkLeadership\u00a0to fail with a\u00a0NONODE\u00a0event and triggering the reset of the current\u00a0LeaderLatch\u00a0instance which again triggers the deletion of the current's\u00a0LeaderLatch's child zNode and which is executed on the server later on.",
        "Issue Links": [
            "/jira/browse/FLINK-28078",
            "/jira/browse/FLINK-29173"
        ]
    },
    "CURATOR-646": {
        "Key": "CURATOR-646",
        "Summary": "Fix RAT failure reports and add a CI task to prevent regression",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "03/Jul/22 10:26",
        "Updated": "05/Jul/22 11:57",
        "Resolved": "05/Jul/22 11:57",
        "Description": "See also https://lists.apache.org/thread/9vgl1v8ncn7n8mbrvh2g6n0t3l47hmws",
        "Issue Links": []
    },
    "CURATOR-647": {
        "Key": "CURATOR-647",
        "Summary": "Cannot build 5.3.0: IllegalThreadState with TestTreeCache Test",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": null,
        "Reporter": "David White",
        "Created": "18/Jul/22 15:30",
        "Updated": "10/May/23 14:11",
        "Resolved": "10/May/23 14:11",
        "Description": "Please advice:\n[ERROR] Errors:\u00a0\n[ERROR] org.apache.curator.framework.recipes.cache.TestTreeCache.testIsolatedThreadGroup\n[ERROR] \u00a0 Run 1: TestTreeCache.testIsolatedThreadGroup:671 \u00bb IllegalThreadState Has threads\n[ERROR] \u00a0 Run 2: TestTreeCache.testIsolatedThreadGroup:671 \u00bb IllegalThreadState Has threads\n[ERROR] \u00a0 Run 3: TestTreeCache.testIsolatedThreadGroup:671 \u00bb IllegalThreadState Has threads\n[INFO]\u00a0\n[INFO]\u00a0\n[ERROR] Tests run: 125, Failures: 0, Errors: 1, Skipped: 1\n[INFO]\u00a0\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\u00a0\n[INFO] Apache Curator ..................................... SUCCESS [ \u00a02.011 s]\n[INFO] Curator Testing .................................... SUCCESS [ \u00a08.989 s]\n[INFO] Curator Client ..................................... SUCCESS [ 31.152 s]\n[INFO] Curator Framework .................................. SUCCESS [10:45 min]\n[INFO] Curator Recipes .................................... SUCCESS [29:38 min]\n[INFO] Curator Service Discovery .......................... SUCCESS [01:01 min]\n[INFO] Curator Async ...................................... SUCCESS [03:25 min]\n[INFO] Curator Examples ................................... SUCCESS [ \u00a09.240 s]\n[INFO] Curator Service Discovery Server ................... SUCCESS [ 37.418 s]\n[INFO] curator-test-zk35 .................................. FAILURE [10:48 min]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 57:08 min\n[INFO] Finished at: 2022-07-18T11:17:06-04:00\n[INFO] Final Memory: 95M/1024M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project curator-test-zk35: There are test failures.",
        "Issue Links": []
    },
    "CURATOR-648": {
        "Key": "CURATOR-648",
        "Summary": "CuratorFramework#blockUntilConnected does now wait forever if waitTime <= 0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "Documentation",
        "Assignee": "Zili Chen",
        "Reporter": "Jo\u00e3o Alves",
        "Created": "19/Jul/22 14:51",
        "Updated": "25/Jul/22 15:51",
        "Resolved": "25/Jul/22 15:51",
        "Description": "CuratorFramework#blockUntilConnected documentation remarks the following:\u00a0\n\nmaxWaitTime - The maximum wait time. Specify a value <= 0 to wait indefinitely\n\nThis does not seem to be correct, if maxWaitTime <= 0 then blockUntilConnected returns immediately.\nI am able to reproduce this behaviour locally and from the code it is apparent to me that the cause is the logic in ConnectionStateManager#blockUntilConnected:\n\n\r\nboolean hasMaxWait = (units != null);\n\nNot sure what if the current behaviour is intended but I think misalignment probably should be fixed to prevent unexpected behaviour.\nWhat is the suggested fix here for this mismatch?\nIf the implementation is changed to wait indefinitely, would it make sense to introduce a new method in CuratorFramework \u00a0to check the current curator connection state without having to wait?",
        "Issue Links": []
    },
    "CURATOR-649": {
        "Key": "CURATOR-649",
        "Summary": "Background exception was not retry-able or retry gave up",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.2.0",
        "Fix Version/s": "5.4.0",
        "Component/s": "Client",
        "Assignee": "Zili Chen",
        "Reporter": "Sergey Nuyanzin",
        "Created": "16/Aug/22 10:00",
        "Updated": "30/Aug/22 05:43",
        "Resolved": "30/Aug/22 05:43",
        "Description": "This is is probably a kind of reopen of CURATOR-538\nthe case is we have 3 flink-nodes which under the hood using curator.\nThen just add one node and remove another one. \nAfter that some nodes start having NPE.\n\njava.lang.NullPointerException: null\r\nat org.apache.curator.utils.Compatibility.getHostAddress(Compatibility.java:116) \r\nat org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:185)\r\nat org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:206) \r\nat org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50)\r\nat org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:150)\r\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926) \r\nat org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)\r\nat org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)\r\nat org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222)\r\nat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:598)\r\nat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510) \r\n\n\nWe've got same issue and trace is very close it fails with NPE trying get Address\nthe only difference is that it is trying to get Address at org.apache.curator.utils.Compatibility#getHostAddress\nat line \n\nreturn (address != null) ? address.getAddress().getHostAddress() : \"unknown\";\r\n\n\nWe do not use telepresence mentioned in comments",
        "Issue Links": [
            "/jira/browse/FLINK-28947",
            "/jira/browse/CURATOR-538",
            "/jira/browse/CURATOR-597"
        ]
    },
    "CURATOR-650": {
        "Key": "CURATOR-650",
        "Summary": "Make  InterProcessSemaphoreV2 revocable",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Boris Dainson",
        "Created": "23/Aug/22 18:59",
        "Updated": "23/Aug/22 18:59",
        "Resolved": null,
        "Description": "Hi. I use InterProcessSemaphoreV2 in my application and I need to revoke an acquired lease under certain conditions. Locks like InterProcessMutex support cooperative revocation by implementing Revocable interface. Can same be done for InterProcessSemaphoreV2?\u00a0\nThank you",
        "Issue Links": []
    },
    "CURATOR-651": {
        "Key": "CURATOR-651",
        "Summary": "Why use Collection instead of List as return value",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "lilu",
        "Created": "30/Aug/22 13:09",
        "Updated": "30/Aug/22 13:11",
        "Resolved": null,
        "Description": "org.apache.curator.x.discovery.ServiceDiscovery#queryForInstances\nThe implementation class internally calls org.apache.curator.x.discovery.details.ServiceDiscoveryImpl#queryForInstances(java.lang.String, org.apache.zookeeper.Watcher)\nThe specific code is\uff1a\n\u00a0\n\n\r\n@Override\r\npublic Collection<ServiceInstance<T>> queryForInstances(String name) throws Exception\r\n{\r\n    return queryForInstances(name, null);\r\n}\r\n\r\nList<ServiceInstance<T>> queryForInstances(String name, Watcher watcher) throws Exception\r\n{\r\n// ...\r\n}\n\nWhy is List implicitly converted to Collection?\nThis way I won't be able to use List.get(0) directly. but requires a conversion.\nMay I ask what is the reason for doing this?\nThank you very much.",
        "Issue Links": []
    },
    "CURATOR-652": {
        "Key": "CURATOR-652",
        "Summary": "double-leader",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "5.1.0,                                            5.2.0,                                            5.3.0",
        "Fix Version/s": "awaiting-response",
        "Component/s": "None",
        "Assignee": "Jordan Zimmerman",
        "Reporter": "shixiaoxiao",
        "Created": "26/Sep/22 02:45",
        "Updated": "26/Sep/22 02:47",
        "Resolved": "26/Sep/22 02:47",
        "Description": "When I use the LeaderLatch to select leader, there is a double-leader phenomenon.\nThe timeline is as follows\uff1a\n1. The zk cluster switch leader node bescause of zxid overflow. The cluster is unavailable to the outside world\n2. A client(not leader befor zxid overflow) and B client(is leader before zxid overflow) enter the suspend state, B client set its leader status to false\n3. The zk cluster complete the leader node election and the cluster back to normal\n4. A client enter the reconnect state and call the reset function, set its leader status to false.\n5. B client enter the reconnect state, call the reset function. set its leader status to false. Delete its old path.\n6. A client receive preNodeDeleteEvent. Then getChildren from zkServer. Find itself is the smallest number and set itself as a leader.\n7. B client create a new temporary node and then getChildren from zkServer. Find itself not the node with the smallest serial number and listen to the previous node delete event.\n8. A client delete its old path.\n9. B client receive the preNodeDeleteEvent. then getchildren from zkServer. Find itself is the smallest sequence number and then set itself as a leader\n10. A client create a new temporary node and then getChildren from zkServer. Find itself not the node with the smallest serial number and listen to the previous node delete event. but it doesn't set itself as a non-leader state. because of the sixth step operation, A still is leader state now.\n11. now A client and B client are the leader at the same time",
        "Issue Links": []
    },
    "CURATOR-653": {
        "Key": "CURATOR-653",
        "Summary": "Double leader for LeaderLatch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.4.0",
        "Component/s": "Recipes",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "27/Sep/22 03:16",
        "Updated": "18/Oct/22 10:09",
        "Resolved": "18/Oct/22 10:09",
        "Description": "Reported by @woaishixiaoxiao:\nWhen I use the LeaderLatch to select leader,  there is a double-leader phenomenon.\nThe timeline is as follows\uff1a\n1. The zk cluster switch leader node bescause of zxid overflow. The cluster is unavailable to the outside world\n2. A client(not leader befor zxid overflow) and B client(is leader before zxid overflow) enter the suspend state, B client set  its leader status to false\n3. The zk cluster complete the leader node election and the cluster back to normal\n4. A client enter the reconnect state  and  call the reset function, set its leader status to false. \n5. B client enter the reconnect state, call the reset function. set its leader status to false.  Delete its old path.\n6. A client receive preNodeDeleteEvent.  Then getChildren from zkServer.  Find itself is the smallest number and set itself as a leader.\n7. B client create a new temporary node  and then getChildren from zkServer.  Find itself not the node with the smallest serial number and listen to the previous node delete event.\n8. A client delete its old path.\n9. B client receive the preNodeDeleteEvent. then getchildren from zkServer. Find itself is the smallest sequence number and then set itself as a leader\n10. A client create  a new temporary node  and then getChildren from zkServer.  Find itself not the node with the smallest serial number and listen to the previous node delete event. but it doesn't  set itself as a non-leader state. because of the sixth step operation, A still is leader state now.\n11. now  A client and B client are  the leader at the same time",
        "Issue Links": [
            "/jira/browse/CURATOR-657"
        ]
    },
    "CURATOR-654": {
        "Key": "CURATOR-654",
        "Summary": "DistributedBarrier watcher leak",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Stuart Scott",
        "Created": "05/Oct/22 05:18",
        "Updated": "28/Apr/23 12:57",
        "Resolved": null,
        "Description": "The DistributedBarrier adds a zookeeper exists watcher in the\u00a0waitOnBarrier() method. It appears as though the watcher is never removed. In a system where DistributedBarriers are used heavily this eventually results in the system running out of memory.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/435"
        ]
    },
    "CURATOR-655": {
        "Key": "CURATOR-655",
        "Summary": "Working Instagram Hack no human verification ios",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Working Instagram Hack no human verification ios",
        "Created": "11/Oct/22 02:56",
        "Updated": "10/May/23 14:01",
        "Resolved": "10/May/23 14:01",
        "Description": "LINK HERE TO GO TO HACK >>>> \n\u00a0\nInstagram Profile Story Viewer hack no human verification no survey no offers it is made by you better programmers will give the opportunity to get unlimited Account generator without human verification and is built for mobile Version MOD APK IOS to guarantee you that actually work for FREE Instagram Profile Story Viewer cheat codes for android mobile and iphone for all passionate players access 99,999 K resources How do you get free and real Account generator no verification to redeem codes 2022 or 2023 tested by many people and result is 100% Working Here!\nIt is a long and tedious process to gain followers naturally on Instagram, but buying followers from some unfamiliar websites is so expensive and risky that doesn\u2019t make the deal cost-efficient. So, you may need to consider using hack Instagram followers 10k free apps. Here are 9 safety-guaranteed hack Instagram followers apps that allow users on Instagram hack unlimited followers free and fast. With their help, you can even hack Instagram followers 10k/50k free within just one day!\u00a0\nTags:\ninstagram followers hack free link instagram hack unlimited followers instagram followers hack free 2021 instagram free followers instagram followers hack free 1k hack instagram followers 10k how to hack instagram followers in 2 minutes instagram followers hack apk",
        "Issue Links": []
    },
    "CURATOR-656": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-657": {
        "Key": "CURATOR-657",
        "Summary": "TestPathChildrenCache timed out in Java11 test run",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes,                                            Tests",
        "Assignee": null,
        "Reporter": "Matthias Pohl",
        "Created": "12/Oct/22 08:10",
        "Updated": "12/Oct/22 08:10",
        "Resolved": null,
        "Description": "We observed a timeout in a test run for CURATOR-653 that was caused by TestPathChildrenCache in the Java 11 job.",
        "Issue Links": [
            "/jira/browse/CURATOR-653"
        ]
    },
    "CURATOR-658": {
        "Key": "CURATOR-658",
        "Summary": "Add Support for TLS-enabled TestingZooKeeperMain",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Do",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "None",
        "Component/s": "Tests",
        "Assignee": "Zili Chen",
        "Reporter": "Aayush Atharva",
        "Created": "17/Oct/22 12:06",
        "Updated": "14/Mar/23 10:54",
        "Resolved": "14/Mar/23 10:54",
        "Description": "When `ServerConfig#getSecureClientPortAddress` is not `null` then the user has indicated the use of TLS. In this case, we will configure the Server factory to use TLS.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/437"
        ]
    },
    "CURATOR-659": {
        "Key": "CURATOR-659",
        "Summary": "Error log message when closing LeaderSelector",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.1.0",
        "Fix Version/s": "None",
        "Component/s": "Recipes",
        "Assignee": null,
        "Reporter": "Christophe Bornet",
        "Created": "16/Dec/22 15:45",
        "Updated": "16/Dec/22 15:45",
        "Resolved": null,
        "Description": "When closing the client quickly after closing a LeaderSelector, we get an annoying error log message because the client is in state STOPPED when mutex.release() is called in LeaderSelector:\n\n\r\n16:29:50.451 [Curator-LeaderSelector-0:org.apache.curator.framework.recipes.leader.LeaderSelector@464] ERROR org.apache.curator.framework.recipes.leader.LeaderSelector - The leader threw an exception\r\njava.lang.IllegalStateException: Expected state [STARTED] was [STOPPED]\r\n\tat org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:823) ~[curator-client-5.1.0.jar:?]\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.checkState(CuratorFrameworkImpl.java:423) ~[curator-framework-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:443) ~[curator-framework-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:347) ~[curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:124) ~[curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154) ~[curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:454) [curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:483) [curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:66) [curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:247) [curator-recipes-5.1.0.jar:5.1.0]\r\n\tat org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:241) [curator-recipes-5.1.0.jar:5.1.0]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\r\n\tat java.lang.Thread.run(Thread.java:834) [?:?]\r\n\n\nHere is a short code sample to reproduce the issue (change the connectString to your ZK cluster):\n\n\r\n  @Test\r\n  void curatorTest() throws Exception {\r\n    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\r\n    CuratorFramework client =\r\n        CuratorFrameworkFactory.builder()\r\n            .connectString(config.getConfigurationMetadataStoreUrl())\r\n            .sessionTimeoutMs(5000)\r\n            .connectionTimeoutMs(5000)\r\n            .retryPolicy(retryPolicy)\r\n            .build();\r\n    client.start();\r\n\r\n    TestLeaderSelectorListener selectorListenerAdapter = new TestLeaderSelectorListener(client);\r\n\r\n    Thread.sleep(1000);\r\n    selectorListenerAdapter.close();\r\n    client.close();\r\n  }\r\n\r\n  public static class TestLeaderSelectorListener extends LeaderSelectorListenerAdapter implements Closeable {\r\n\r\n    private final LeaderSelector leaderSelector;\r\n\r\n    public TestLeaderSelectorListener(CuratorFramework client) {\r\n      leaderSelector = new LeaderSelector(client, \"/test\", this);\r\n      leaderSelector.autoRequeue();\r\n      leaderSelector.start();\r\n    }\r\n\r\n    @Override\r\n    public void takeLeadership(CuratorFramework client) {\r\n      try {\r\n        Thread.sleep(5000);\r\n      } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n      }\r\n    }\r\n\r\n    @Override\r\n    public void close() throws IOException {\r\n      leaderSelector.close();\r\n    }\r\n  }\r\n\n\nCan this be detected and the log not produce ? It makes the shutting down unclean.",
        "Issue Links": []
    },
    "CURATOR-660": {
        "Key": "CURATOR-660",
        "Summary": "Enable code coverage reporting to SonarQube",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "D\u00f3ra Horv\u00e1th",
        "Created": "24/Jan/23 18:24",
        "Updated": "13/Apr/23 03:06",
        "Resolved": "13/Apr/23 01:07",
        "Description": "Enable coverage report generation and publishing to SonarQube.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/442"
        ]
    },
    "CURATOR-661": {
        "Key": "CURATOR-661",
        "Summary": "Cleanup and refactor build and CI settings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.5.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "25/Feb/23 05:21",
        "Updated": "19/Apr/23 14:49",
        "Resolved": "19/Apr/23 14:49",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-662": {
        "Key": "CURATOR-662",
        "Summary": "getLockPath is not public from InterProcessReadWriteLock's ReadLock and WriteLock",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Recipes",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Kezhu Wang",
        "Created": "03/Mar/23 01:43",
        "Updated": "09/Apr/23 08:24",
        "Resolved": "03/Mar/23 07:37",
        "Description": null,
        "Issue Links": []
    },
    "CURATOR-663": {
        "Key": "CURATOR-663",
        "Summary": "ZooKeeperServerEmbeddedAdapter.configure might fail with ClassCastException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Framework",
        "Assignee": null,
        "Reporter": "Matthias Pohl",
        "Created": "28/Mar/23 20:15",
        "Updated": "28/Apr/23 12:56",
        "Resolved": null,
        "Description": "There might be a ClassCastException happening when starting the TestingZooKeeper instance. The following test reproduces the issue:\n\n\r\n@Test\r\npublic void testTestingZooKeeperServer() throws Exception\r\n{\r\n  final Map<String, Object> customProperties = new HashMap<>();\r\n  customProperties.put(\"maxSessionTimeout\", 60_000);\r\n\r\n  final Path tmpDirPath = Files.createTempDirectory(\"tmp-\");\r\n  final InstanceSpec instanceSpec = new InstanceSpec(tmpDirPath.toFile(), 0, 0, 0, true, 0, 10, 1, customProperties, \"host-name\");\r\n  final QuorumConfigBuilder quorumConfigBuilder = new QuorumConfigBuilder(instanceSpec);\r\n  final TestingZooKeeperServer zkServer = new TestingZooKeeperServer(quorumConfigBuilder);\r\n\r\n  zkServer.start();\r\n}\r\n\n\nThe test above will fail with a ClassCastException because InstantSpec allows a Map<String, Object> for custom properties, i.e. a non-String value like Integer can be used.\nThis Map is then passed over into a Properties instance in QuorumConfigBuilder:163. These Properties are then written to a temporary file in ZooKeeper's ZooKeeperServerEmbeddedImpl:58 which expects only to have String values. The previously set Integer value isn't converted to a String anywhere, though.",
        "Issue Links": []
    },
    "CURATOR-664": {
        "Key": "CURATOR-664",
        "Summary": "EnsembleTracker use resolved server address in connection string if client address is wildcard",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Framework",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "29/Mar/23 01:29",
        "Updated": "02/Apr/23 06:58",
        "Resolved": "31/Mar/23 11:59",
        "Description": "In construction of connection string, EnsembleTracker resorts to server address if client address is wildcard. In case where server address is DNS name, it should use that but not a resolved ip address. Otherwise, if server is migrated to another machine, it becomes unreachable through ip address in connection string.\nCURATOR-638 probably cover this, but https://github.com/apache/curator/pull/425 did not resolve this case. Create a new issue to keep its \"Fix Version\" unchanged.",
        "Issue Links": [
            "/jira/browse/CURATOR-638",
            "/jira/browse/CURATOR-384",
            "https://github.com/apache/curator/pull/452"
        ]
    },
    "CURATOR-665": {
        "Key": "CURATOR-665",
        "Summary": "ModeledFramework does not throw expected exception and instead hangs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Framework",
        "Assignee": "Zili Chen",
        "Reporter": "Ryan Ruel",
        "Created": "29/Mar/23 18:32",
        "Updated": "03/Apr/23 08:23",
        "Resolved": "03/Apr/23 08:23",
        "Description": "When writing data to ZooKeeper via Curator, I found that when I was receiving a KeeperException NoAuth back from ZooKeeper, my call would hang indefinitely.\nThe NoAuth was expected as I was testing writing to a path where the ACL was set to prevent my client from writing (X509 authentication scheme).\nThe call which hangs:\n\n\r\nmyFramework.set(myModel).toCompletableFuture().get();\n\nThe logs from the call:\n\n\r\n2023-03-29 14:20:29,511 [Curator-Framework-0] ERROR imps.CuratorFrameworkImpl - Background exception was not retry-able or retry gave up\r\norg.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /test/foo \n\nI'd expect this exception to bubble up wrapped in a CompletionException.\nInstead, CuratorFrameworkImpl just logs the exception and then the call to get() hangs indefinitely.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/453"
        ]
    },
    "CURATOR-666": {
        "Key": "CURATOR-666",
        "Summary": "Background create strip namespace twice from path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Framework",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "03/Apr/23 13:45",
        "Updated": "19/Apr/23 14:44",
        "Resolved": "19/Apr/23 14:44",
        "Description": "CreateBuilderImpl::sendBackgroundResponse strips namespace twice:\n\nOne in body of sendBackgroundResponse or newly introduced createResponseEvent.\nOne in construction of CuratorEventImpl.\n\nThis could cause incorrect resulting path if we are creating \"/zoo\" under \"/zoo\" namespace.\nThis recall me ZOOKEEPER-4601, I verified that CuratorFramework::getConfig get \"keeper/config\" in \"/zoo\" and \"/config\" in \"/zookeeper\" namespace. We might need extra coordination with ZooKeeper community to maintain consistent between curator and zookeeper but I think we could fix `CreateBuilderImpl` independently. I remembered that I sent email in zookeeper dev list before, I will bump that up for discussion.",
        "Issue Links": [
            "/jira/browse/CURATOR-667",
            "https://github.com/apache/curator/pull/454"
        ]
    },
    "CURATOR-667": {
        "Key": "CURATOR-667",
        "Summary": "Background `getConfig` and `reconfig` could deliver abnormal path",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Framework",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "09/Apr/23 13:26",
        "Updated": "05/May/23 15:48",
        "Resolved": null,
        "Description": "In reporting CURATOR-666, I verified that CuratorFramework::getConfig get \"keeper/config\" in \"/zoo\" and \"/config\" in \"/zookeeper\" namespace. ZOOKEEPER-4601 and ZOOKEEPER-4565 reported similar issues, we might need extra coordination with ZooKeeper community to maintain consistent between curator and zookeeper. I have bumped up an earlier discussion in both dev@curator and dev@zookeeper",
        "Issue Links": [
            "/jira/browse/ZOOKEEPER-4601",
            "/jira/browse/CURATOR-666"
        ]
    },
    "CURATOR-668": {
        "Key": "CURATOR-668",
        "Summary": "blockUntilConnected(int, java.util.concurrent.TimeUnit)  method annotation  is ambiguity",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.4.0",
        "Fix Version/s": "5.5.0",
        "Component/s": "Client",
        "Assignee": "Zili Chen",
        "Reporter": "yiku123",
        "Created": "11/Apr/23 22:02",
        "Updated": "12/Apr/23 11:37",
        "Resolved": "12/Apr/23 11:37",
        "Description": "However, what is correct is that\nSpecify a value <= 0 and units!=null to return immediately\n\nSpecify a value <= 0 and units=null to wait indefinitely,is same as {@link #blockUntilConnected()}",
        "Issue Links": [
            "https://github.com/apache/curator/pull/457"
        ]
    },
    "CURATOR-669": {
        "Key": "CURATOR-669",
        "Summary": "Integrate with ge.apache.org Gradle Enterprise server",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "Tests",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "12/Apr/23 10:53",
        "Updated": "06/May/23 03:10",
        "Resolved": "06/May/23 03:10",
        "Description": "See -\n\nhttps://github.com/apache/pulsar/pull/19133\nhttps://github.com/apache/pulsar/pull/19194\nhttps://the-asf.slack.com/archives/CBX4TSBQ8/p1672826514725099\n\nI'd prefer to use this method for analyzing our flaky tests and time-consuming tests for further improving.",
        "Issue Links": [
            "https://github.com/apache/curator/pull/459"
        ]
    },
    "CURATOR-670": {
        "Key": "CURATOR-670",
        "Summary": "CuratorTransactionResult.ofTypeAndPath leaks guava Predicte to public",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Kezhu Wang",
        "Created": "25/Apr/23 13:47",
        "Updated": "28/Apr/23 12:56",
        "Resolved": null,
        "Description": "I found this in verifying tests against staging artifacts.\nFrom description, CURATOR-558 tried to drop usages of guava in public API. This seems a missing.\nPS: https://cwiki.apache.org/confluence/display/CURATOR/TN13 seems dated in upgrading to 5.x.",
        "Issue Links": [
            "/jira/browse/CURATOR-555"
        ]
    },
    "CURATOR-671": {
        "Key": "CURATOR-671",
        "Summary": "TestLeaderSelectorCluster.testLostRestart failed three runs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Tests",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "06/May/23 01:38",
        "Updated": "08/Jun/23 11:42",
        "Resolved": "08/Jun/23 11:42",
        "Description": "Our tests are run at most three times to avoid ci failure in case of flaky tests. TestLeaderSelectorCluster.testLostRestart failed three runs in following ci actions:\n\nhttps://github.com/apache/curator/actions/runs/4891616112/jobs/8732352455#step:6:738\nhttps://github.com/apache/curator/actions/runs/4895418678/jobs/8740966351#step:6:711",
        "Issue Links": [
            "/jira/browse/CURATOR-518",
            "https://github.com/apache/curator/pull/462"
        ]
    },
    "CURATOR-672": {
        "Key": "CURATOR-672",
        "Summary": "Ensure JDK 17 support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "06/May/23 08:27",
        "Updated": "10/May/23 11:48",
        "Resolved": "08/May/23 00:52",
        "Description": null,
        "Issue Links": [
            "/jira/browse/CURATOR-433",
            "https://github.com/apache/curator/pull/461"
        ]
    },
    "CURATOR-673": {
        "Key": "CURATOR-673",
        "Summary": "Complete BackgroundCallback if curator got closed or exceptions from no-zookeeper world",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "17/May/23 11:41",
        "Updated": "08/Jun/23 16:33",
        "Resolved": "08/Jun/23 16:33",
        "Description": "Currently, BackgroundCallback has only one method to complete it in success call to zookeeper server. But if curator client got closed during background work, it is possible that there is no call to BackgroundCallback. I think it is better to notify callback in all cases. It is also a keypoint to turn BackgroundCallback to CompletableFuture.",
        "Issue Links": [
            "/jira/browse/CURATOR-677",
            "/jira/browse/CURATOR-675",
            "/jira/browse/CURATOR-529",
            "/jira/browse/CURATOR-52",
            "https://github.com/apache/curator/pull/464"
        ]
    },
    "CURATOR-674": {
        "Key": "CURATOR-674",
        "Summary": "Apply a consistent code style to our codebase",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Zili Chen",
        "Reporter": "Zili Chen",
        "Created": "01/Jun/23 14:55",
        "Updated": "02/Jun/23 08:11",
        "Resolved": "02/Jun/23 08:11",
        "Description": "Use spotless as the automatic style formatter and check.\nBackground -\n\nhttps://lists.apache.org/thread/12rwy3y1zcs3pjh9q30851mldgdh4sgg\nhttps://lists.apache.org/thread/ymsc5ypxjpdd2b839090nskof5nbbl7x",
        "Issue Links": [
            "https://github.com/apache/curator/pull/463"
        ]
    },
    "CURATOR-675": {
        "Key": "CURATOR-675",
        "Summary": "AsyncStage could stuck from completion due to framework level error listener",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "Framework",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "04/Jun/23 13:37",
        "Updated": "03/Jul/23 13:33",
        "Resolved": "03/Jul/23 13:33",
        "Description": "AsyncCuratorFrameworkDsl AsyncCuratorFramework.with(UnhandledErrorListener listener) can be used to construct a new framework level object with a global-like error listener. I may be ok for a operation level error listener(e.g. ErrorListenerPathable) to swallow exception from BackgroundCallback but it is too crude to swallow exceptions in global level.\nThe call chain is similar to async.with(listener).create().forPath(\"/foo\"). From this, I don't think we are binding an error listener for create() only.\nBesides this, I think it is a good to complete CompletableFuture in all cases.",
        "Issue Links": [
            "/jira/browse/CURATOR-673",
            "https://github.com/apache/curator/pull/465"
        ]
    },
    "CURATOR-676": {
        "Key": "CURATOR-676",
        "Summary": "CuratorListener is not called for retriable errors",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "04/Jun/23 14:53",
        "Updated": "04/Jun/23 14:53",
        "Resolved": null,
        "Description": "For no retriable errors, CuratorListener will receive events if no BackgroundCallback specified. For retriable errors, CuratorListener is ignored. CuratorListener is catch all fallback for BackgroundCallback in case of of not specified in operation level. It should be called in all places where BackgroundCallback is expected but not specified.\nSee Backgroundable::inBackground() and Backgroundable::inBackground(Object context).",
        "Issue Links": []
    },
    "CURATOR-677": {
        "Key": "CURATOR-677",
        "Summary": "Auxiliary background operations are created with no callbacks from main operations",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.5.0",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "05/Jun/23 12:13",
        "Updated": "12/Jun/23 07:01",
        "Resolved": null,
        "Description": "Some of our background operations use auxiliary background operations to complete task in case of primary conditions are not satisfied. But these auxiliary background operations are created with no callbacks from main operations. So in case of closing, background exceptions and retriable errors, these are either abandoned or treated as no callback background operations. Currently, there are three operations which have auxiliary background operations. They are CreateBuilderImpl, DeleteBuilderImpl and SetDataBuilderImpl.\nThis should be part of CURATOR-673, but I think all these three could be fixed using same pattern. And I want to keep #464 simple.",
        "Issue Links": [
            "/jira/browse/CURATOR-673",
            "https://github.com/apache/curator/pull/467"
        ]
    },
    "CURATOR-678": {
        "Key": "CURATOR-678",
        "Summary": "InterProcessMutex#release caused inconsistency between zk node and local cache if encountering zk connection lost",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "5.3.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Enrico Olivelli",
        "Reporter": "Ken Huang",
        "Created": "06/Jun/23 01:08",
        "Updated": "06/Jun/23 05:36",
        "Resolved": null,
        "Description": "We experienced a problem that\nan InterProcessMutex participant acquired the lock -> when release() was running, it encountered zk connection lost, then there was inconsistency as in codes https://github.com/apache/curator/blob/master/curator-recipes/src/main/java/org/apache/curator/framework/recipes/locks/InterProcessMutex.java#L139\nto line 143, that the zk node deletion threw exception for connection lost, but the local cached `threadData` still removed it.\nAs a result, even when the zk connection recovered later, ALL following acquire() failed due to the inconsistency (not present in local `threadData` but the OLD zk node were still present).\n\u00a0\nPlease help confirm this behavior. I think it is bug and curator should fix the inconsistency, a suggestion is to remove the local data ONLY after znode deletion is a success. Also, the same problematic code seems appearing in many other similar recipes such as\u00a0`InterProcessSemaphore`.\n\u00a0\nStacktrace:\n```\nFailed to release mutex for xxxxxxxxxxxxx org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /xxxx/_c_65fb02ef-9b1d-4c8c-b715-5c97f82ae0d3-lock-0000000000 at org.apache.zookeeper.KeeperException.create(KeeperException.java:102) ~[zookeeper-3.6.3.jar:3.6.3] at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[zookeeper-3.6.3.jar:3.6.3] at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:2001) ~[zookeeper-3.6.3.jar:3.6.3] at org.apache.curator.framework.imps.DeleteBuilderImpl$6.call(DeleteBuilderImpl.java:313) ~[curator-framework-5.3.0.jar:5.3.0] at org.apache.curator.framework.imps.DeleteBuilderImpl$6.call(DeleteBuilderImpl.java:301) ~[curator-framework-5.3.0.jar:5.3.0] at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[curator-client-5.3.0.jar:?] at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:298) ~[curator-framework-5.3.0.jar:5.3.0] at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:282) ~[curator-framework-5.3.0.jar:5.3.0] at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35) ~[curator-framework-5.3.0.jar:5.3.0] at org.apache.curator.framework.recipes.locks.LockInternals.deleteOurPath(LockInternals.java:347) ~[curator-recipes-5.3.0.jar:5.3.0] at org.apache.curator.framework.recipes.locks.LockInternals.releaseLock(LockInternals.java:124) ~[curator-recipes-5.3.0.jar:5.3.0] at org.apache.curator.framework.recipes.locks.InterProcessMutex.release(InterProcessMutex.java:154) ~[curator-recipes-5.3.0.jar:5.3.0] at\u00a0\n... ...\n```",
        "Issue Links": []
    },
    "CURATOR-679": {
        "Key": "CURATOR-679",
        "Summary": "Bump snappy-java from 1.1.7 to 1.1.10.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kezhu Wang",
        "Created": "20/Jun/23 05:31",
        "Updated": "20/Jun/23 05:39",
        "Resolved": "20/Jun/23 05:39",
        "Description": "dependabot security update: https://github.com/apache/curator/pull/469#issue-1759172778",
        "Issue Links": [
            "https://github.com/apache/curator/pull/469"
        ]
    },
    "CURATOR-680": {
        "Key": "CURATOR-680",
        "Summary": "Bump guava from 31.1-jre to 32.0.0-jre",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kezhu Wang",
        "Created": "20/Jun/23 05:41",
        "Updated": "20/Jun/23 05:54",
        "Resolved": "20/Jun/23 05:45",
        "Description": "Security update from dependabot: https://github.com/apache/curator/pull/468#issue-1757770176",
        "Issue Links": [
            "https://github.com/apache/curator/pull/468"
        ]
    },
    "CURATOR-681": {
        "Key": "CURATOR-681",
        "Summary": "TestInterProcessMutex.testReentrantSingleLock considered flaky",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "5.6.0",
        "Component/s": "Tests",
        "Assignee": "Kezhu Wang",
        "Reporter": "Kezhu Wang",
        "Created": "21/Jun/23 03:00",
        "Updated": "21/Jun/23 03:12",
        "Resolved": null,
        "Description": "It fails three runs at https://github.com/apache/curator/actions/runs/5318910191/jobs/9630834195#step:6:619\n\njava.util.concurrent.ExecutionException: org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\nCaused by: org.opentest4j.AssertionFailedError: expected: <true> but was: <false>\r\n\n\n\norg.apache.curator.framework.recipes.locks.TestInterProcessMutex.testReentrantSingleLock\r\nError:    Run 1: TestInterProcessMutex>TestInterProcessMutexBase.testReentrantSingleLock:296 \u00bb Execution\r\nError:    Run 2: TestInterProcessMutex>TestInterProcessMutexBase.testReentrantSingleLock:296 \u00bb Execution\r\nError:    Run 3: TestInterProcessMutex>TestInterProcessMutexBase.testReentrantSingleLock:296 \u00bb Execution",
        "Issue Links": [
            "https://github.com/apache/curator/pull/470"
        ]
    }
}